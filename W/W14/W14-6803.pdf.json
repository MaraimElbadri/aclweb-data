{"sections":[{"title":"","paragraphs":["Proceedings of the Third CIPS-SIGHAN Joint Conference on Chinese Language Processing, pages 3–10, Wuhan, China, 20-21 October 2014"]},{"title":"COV Model and its Application in Chinese Part-of-Speech Tagging Xing Fukun Song Rou Luoyang Foreign Languages University 471003 Henan Beijing Language and Cultural University 100086 Beijing xingfukun@126.com songrou@126.com  Abstract ","paragraphs":["This article presents a new sequence labeling model named Context OVerlapping (COV) model, which expands observation from single word to n-gram unit and there is an overlapping part between the neighboring units. Due to the co-occurrence constraint and transition constraint, COV model reduces the search space and improves tagging accuracy. The 2-gram COV is applied to Chinese PoS tagging and the precision rate of the open test is as high as 96.83%, which is higher than the second order HMM, which is 95.73%. The result is also comparable to the discriminative models but COV takes much less training time than them. With symbol decoding COV prunes many nodes before statistics decoding and the search space of COV is about10-20% less than that of HMM."]},{"title":"1 Introduction","paragraphs":["Part of Speech (PoS) can provide much useful information for most natural language processing tasks such as word sense disambiguation, chunk detection, sentence parsing, speech synthesis, machine translation and so on. Therefore lots of efforts have been made to build effective and robust models for automatic PoS tagging. According to Doug Cutting (1992), a practical PoS tagger should be “robust, efficient, accurate, tunable and reusable”. With regard to efficiency the basic requirement for a PoS tagger is that training and test time should not be too long. And for a robust tagger the tagging accuracy should be as high as possible and can well deal with the sparseness data. Most of the approaches to PoS tagging can be divided into two main classes, rule-based and statistics-based approach. In rule-based approaches, words are assigned tags based on a set of rules and a lexicon. These rules can either be manually crafted, or learned, as in the transformation-based error-driven approach of Brill (1995). In the statistics-based approaches HMM is the representative of generative models and is widely used in PoS tagging (Church, 1988; Cutting et al. 1992; Thede & Harper 1999, Huang et al. 2007, etc.) . Maximum Entropy model and Conditional Random Fields (CRFs) model are the representatives of discriminative models and are also applied in PoS tagging. Thanks to the flexibility of features selection these discriminative models achieve higher precision rates than the generative models in PoS tagging (Adwait, 1996; Lafferty, 2001 etc.). But the training of discriminative models is 3 time-consuming and requires high-quality computer processing power, which affects their applications in the real tasks. Concerning all the characteristics of generative and discriminative models, we proposed a new model on the basis of HMM. The new model expand the observation from one single word to n-gram unit and between the neighboring units there is an n-1 gram part, which is shared by the neighboring units. So the new model is called Context OVerlapping (COV) model. COV is a general sequence labeling model and has been applied to Chinese and English PoS tagging tasks. In these tasks COV achieves better performance than HMM and its performance is comparable to the discriminative models. Meanwhile its training time is much less than the discriminative models, which makes the model more efficient and robust in the real tasks. The structure of the article is that: the first part will briefly introduce PoS tagging, in the second part we will introduce COV model. The third part will compare COV with HMM. The fourth part will address how to estimate parameters and handle sparseness data. The fifth part is about the algorithm of symbol decoding. The sixth part is about evaluation criteria and the seventh part presents the experiments and results. The final part is some discussions and future work to do."]},{"title":"2 COV Model","paragraphs":["COV model is based on HMM. HMM is a form of generative model, that defines a joint probability distribution p(X,Y) where X and Y are random variables respectively ranging over observation sequences and their corresponding state sequences. In order to define a joint distribution, generative models must enumerate all possible observation sequences. For most domains, it is intractable unless observation elements are represented as isolated units, independent from the other elements in an observation sequence. More precisely, the observation element at any given time may only directly depend on the state at that time. This is an appropriate assumption for a few simple data sets, however most real-world observation sequences such as sentences are best represented in terms of multiple interacting features and even long-range dependencies between observation elements. Due to the observation independence assumption the performance of HMM is limited in PoS tagging. For example, here are 2 Chinese sentences: (1) 市长/n 强调/v 深入/v a 细致/a 的/u 工作/vn 作风/n (The mayor put emphasis on the careful working style.) (2) 市长/n 要/v 深入/v a 困难/a 的/u 群众/n 中间/f (The mayor should care about those people in troubles.) For the convenience of analysis we assume that in each sentence only “深入”(careful or care) has two parts of speech, adjective (a) or verb (v), and other words only have one PoS. If we use the first-order HMM model to predict the PoS of “深入” the prediction will be like: n)|p( vn)|a)p(|a)p(|X)p(|p( v)|n)p(|vn)p(|u)p(n|a)p(vn|p(u","X)|v)p(a|n)p(X|p(n)p(vmaxarg },{ 1 作风 工作的细致深入 强调市长 vaX Q  1"]},{"title":"Q","paragraphs":["denotes the state sequence of sentence (1) and X denotes the possible state of “深入”. For only “深入” is ambiguous and other words all have only one PoS, the formula can be simplified as: 1"]},{"title":"Q","paragraphs":["X)|X)p(|v)p(a|p(Xmaxarg },{ 深入 vaX 4 And as same as sentence (1) we can get the prediction formula of sentence (2) as: 2"]},{"title":"Q","paragraphs":["X)|X)p(|v)p(a|p(Xmaxarg },{ 深入 vaX Comparing the two formulae, we find that 1"]},{"title":"Q ","paragraphs":["and 2"]},{"title":"Q","paragraphs":["are the same, which means that HMM tagger will not distinguish between the different PoSs of “深入” in the two sentences. In fact “深入” in sentence (1) is an adjective and in sentence (2) is a verb. So HMM must make one mistake either in sentence (1) or sentence (2). The mistake shows the limitation of HMM in PoS tagging. In order to overcome the shortcomings of observation independence assumption of HMM and combine more context information into the model, COV model is proposed in this paper. The formalism of 2-gram COV is as follows and the formalisms of other n-gram COV (n>2) models can be gotten according to the 2-gram model. In the 2-gram COV there is a basic state set"]},{"title":"},...,{","paragraphs":["21 s"]},{"title":"qqqQ ","paragraphs":[". The observation sequence is S= h"]},{"title":"ww ...","paragraphs":["1 . The corresponding state of a 2-gram observation unit ii"]},{"title":"ww","paragraphs":["1 (2≤i≤h)is a state set"]},{"title":"}{","paragraphs":["1 j i j ii"]},{"title":"qqe","paragraphs":[""]},{"title":"","paragraphs":[", in which j i"]},{"title":"q","paragraphs":["1 is one of the basic states of 1i"]},{"title":"w","paragraphs":["and j i"]},{"title":"q","paragraphs":["is one of the basic states of i"]},{"title":"w","paragraphs":[". The state sequence j i j i"]},{"title":"qq","paragraphs":["1","is called one state unit of the observation unit","ii"]},{"title":"ww","paragraphs":["1 . It is notable that i"]},{"title":"e","paragraphs":["is the state set when the word 1iw and i"]},{"title":"w","paragraphs":["co-occur, which","is called Co-occurrence Constraint(CC). When","1iw and i"]},{"title":"w","paragraphs":["co-occur the amount of possible states of ii"]},{"title":"ww","paragraphs":["1 will not be more than the amount of the combination of states of 1iw and i"]},{"title":"w","paragraphs":[". The search for the state sequence with the highest joint probability can be computed like:"]},{"title":"Q","paragraphs":["="]},{"title":"S)|P(Qmaxarg","paragraphs":["="]},{"title":"Q)|P(Q)P(Smaxarg","paragraphs":["≈ ))|()|( )|()|()((maxarg 2 1111 3 121121 ,1"]},{"title":" ","paragraphs":["     h i iiii h i iiii qq qqoopqop qqqqpqqpqp","ii"," Q denotes the state sequence and S denotes the observation sequence."]},{"title":"Q","paragraphs":["denotes the final state sequence, whose joint probability is the highest. For the convenience of computation, we insert 2 “*B*”, whose state is “B” at the beginning of the sequence and insert 2 “*E* ”, whose state is “E” at the end of the sequence. And then the above formula will be:"]},{"title":"))|( )|((maxargˆ","paragraphs":["2 1 11 2 1 121 ,1"]},{"title":" ","paragraphs":["      "]},{"title":"","paragraphs":["h i iiii h i iiii qq"]},{"title":"qqoop qqqqpQ","paragraphs":["ii In this model there is an overlapping part between the neighboring observation units","12  ii"]},{"title":"ww","paragraphs":["and ii"]},{"title":"ww","paragraphs":["1 . For 1i"]},{"title":"w","paragraphs":["is shared by the neighboring units, the corresponding states units of 12  ii"]},{"title":"ww","paragraphs":["and ii"]},{"title":"ww","paragraphs":["1 should also share the same overlapping state. If k i k i"]},{"title":"qq","paragraphs":["12  is one state of 12  ii"]},{"title":"ww","paragraphs":["and j i j i"]},{"title":"qq","paragraphs":["1 is one state of ii"]},{"title":"ww","paragraphs":["1 , then only if k i"]},{"title":"q","paragraphs":["1 is the same as j i"]},{"title":"q","paragraphs":["1 then it is possible to transmit from state k i k i"]},{"title":"qq","paragraphs":["12  to j i j i"]},{"title":"qq","paragraphs":["1 , otherwise there is no transition path from k i k i"]},{"title":"qq","paragraphs":["12  to j i j i"]},{"title":"qq","paragraphs":["1 . The constraint k i"]},{"title":"q","paragraphs":["1 = j i"]},{"title":"q","paragraphs":["1 is called Transition Constraint (TC)."]},{"title":"Q ","paragraphs":["is a sequence consisting of h+1 2-gram","state units like: EqqqqqqqqB hhh ,̂ˆ̂,...,ˆ̂,ˆ̂,̂132211  ("]},{"title":"Qq","paragraphs":["i"]},{"title":"ˆ","paragraphs":[") It is obvious that the final state sequence can be gotten from the above sequence. 5"]},{"title":"3 Comparisons between COV and HMM","paragraphs":["There are 3 different points between COV and HMM. First, in the nth HMM if each observation has k states and then the amount of the history states will be kn",". But in the n-gram COV the amount of the history states will usually be smaller than kn","because of the Constraint of Co-occurrence. And then the search space of COV will also be smaller than HMM. Second, in the nth order HMM the emission probability of t"]},{"title":"q","paragraphs":["to t"]},{"title":"o","paragraphs":["is only P( t"]},{"title":"o","paragraphs":["| t"]},{"title":"q","paragraphs":["). But in the n-gram COV, there are n emission probabilities relevant to t"]},{"title":"q","paragraphs":["and t"]},{"title":"o","paragraphs":[", which are P( tnt"]},{"title":"oo ...","paragraphs":["1 | tnt"]},{"title":"qq ...","paragraphs":["1 ) , ... , P( 1"]},{"title":"...","paragraphs":["ntt"]},{"title":"oo","paragraphs":["| 1"]},{"title":"...","paragraphs":["ntt"]},{"title":"qq","paragraphs":[") . For all of these emission probabilities are related to t"]},{"title":"q","paragraphs":["and t"]},{"title":"o","paragraphs":[", these observation units will make constraints on the possible state units. Third, in the nth order HMM the transition probability from the history state to the current state is P( 1"]},{"title":",...|","paragraphs":[" inii"]},{"title":"qqq","paragraphs":["). But in the n-gram COV the transition path must obey TC, which requires the overlapping part of the neighboring state units must be the same. If the neighboring state units obey TC the transition probability is the same as that in nth order HMM. If the neighboring state units don’t obey TC there will be no transition path between them. With TC a great amount of paths are pruned, which makes the search space reduced. Here is an example to illustrate the lattice building and tagging process by 2-gram COV. In particular, this example needn’t any probability computation and can get the final state sequence just with symbol comparing.  1 2 3 4 5 *B*-*B*","*B*-领 导 领导- 强调 强调- 深入","深入-细 致","B-B B-n n-v v-a a-a","B-vn v-ad ad-ad","B-v  6 7 8 9 10 细致-的 的-工作 工作-作风 作风- *E* *E*- *E*","a-u u-v vn-n n-E E-E","u-n","u-vn Table 1: An example to illustrate COV tagging process (For the space limitation the table is split to two)  In the above table each column is a 2-gram observation unit and the neighboring units share an overlapping part. For example, unit 2 is “*B*-领导”(*B*-leader) and unit 3 is “领 导 - 强调” ( leader-emphasizes ) , “ 领导” (leader) is the overlapping part between unit 2 and unit 3. Unit 2 has 3 possible state units, which are “B-n, B-vn, B-v”, and unit 3 has only one possible state unit, which is “n-v”. With Transition Constraint only if the overlapping part of state unit 2 and state unit 3 is the same there can be a transition path. So in the state units of unit 2 only “B-n” is remained and the state units “B-vn” and “B-v” are all eliminated for their overlapping parts (vn and v) are not the same as the overlapping part of state unit 3 (n). The shadowed grids in the table are all the impossible states and are eliminated. In this example after the symbol comparing and elimination there remains only one path for the sentence and the path is the final tagging result. So this sentence is tagged without any probability computation but only with the symbol comparing. The process of symbol comparing and elimination is called symbol decoding. Most times there may be more than one possible paths remained after symbol decoding and then the Viterbi algorithm will be applied to get the best tagging sequence. Although HMM also applies Viterbi for decoding, the search space of HMM is bigger than that of COV because COV has eliminated many impossible states in the step of symbol decoding."]},{"title":"4 Parameters estimation and strategy of handling sparseness data","paragraphs":["There are 2 main parameters to be estimated in COV: (1) t"]},{"title":"P","paragraphs":[":State transition probability; (2) e"]},{"title":"P","paragraphs":[":State emission probability. 6 We apply the maximum likelihood to estimate these parameters from the tagged corpus. The details of the estimation will not be introduced here. For the expansion of the observation the sparseness problem in n-gram COV is more serious than that in HMM. COV applies back-off strategy to deal with the sparseness data. The main idea is that if n-gram (n>2)","ini"]},{"title":"ww ...","paragraphs":["1 is not in the n-gram vocabulary, which is gotten from the training corpus, it will be replaced by n-1 gram ini"]},{"title":"ww ...","paragraphs":["2 . And if ii"]},{"title":"ww","paragraphs":["1 is not in the 2-gram vocabulary then the state units of ii"]},{"title":"ww","paragraphs":["1 will be replaced by the combination of states of 1i"]},{"title":"w","paragraphs":["and i"]},{"title":"w","paragraphs":[". If i"]},{"title":"w","paragraphs":["is not in the unigram vocabulary it will be handled as same as in HMM."]},{"title":"5 Tagging Procedures and Decoding Algorithm","paragraphs":["The main procedures of COV tagging is described in the following flow diagram.  Figure 1: Flow diagram of PoS tagging by COV There are two steps of decoding in PoS tagging by COV: (1) Symbol decoding (2) Statistics decoding Statistics decoding applies Viterbi algorithm, which is explained in detail by Rabiner (1989) and will not be repeated here. Here we will describe the symbol decoding algorithm in detail. First we define the suffix and prefix of a state sequence: Suffix of ini"]},{"title":"qq ...","paragraphs":["1 is defined as ini"]},{"title":"qq ...","paragraphs":["2 Prefix of ini"]},{"title":"qq ...","paragraphs":["1 is defined as 11"]},{"title":"...","paragraphs":[" ini"]},{"title":"qq","paragraphs":["The symbol decoding algorithm is as follows: Input: word sequence S= h"]},{"title":"ww ...","paragraphs":["0 and all the possible state units of each n-gram unit (1) Comparing the neighboring n-gram state units from left to right. For any given neighboring observation units","1i"]},{"title":"s","paragraphs":["= 1"]},{"title":"...","paragraphs":[" ini"]},{"title":"ww","paragraphs":["and i"]},{"title":"s","paragraphs":["= ini"]},{"title":"ww ...","paragraphs":["1 , they have the corresponding state unit sets 1i"]},{"title":"e","paragraphs":["and i"]},{"title":"e","paragraphs":[". And each state unit in the set is called state node. For each node Ei-1 in the state set of 1i"]},{"title":"e","paragraphs":[", a comparison is made between the suffix of Ei-1 and the prefix of the node Ei in i"]},{"title":"e","paragraphs":[". If they are the same then a parent-child relation is built between the neighboring nodes Ei-1 and Ei . If node Ei in i"]},{"title":"e","paragraphs":["has no parent node in 1i"]},{"title":"e","paragraphs":["then Ei will be eliminated and if node Ei-1 in 1i"]},{"title":"e","paragraphs":["has no child node in i"]},{"title":"e","paragraphs":[", Ei-1 will also be eliminated. (2) Backward from right to left","A. If a node Ei-1 is eliminated in step (1) for it doesn’t have any child node in i"]},{"title":"e","paragraphs":[", then the relation between Ei-1 and its parent node Ei-2 will also be eliminated.","B. If the parent-child relation between Ei-2 and Ei-1 is eliminated in step A and Ei-2 doesn’t have any child node then Ei-2 will also be eliminated. Backward to the left end of the sequence and the process of symbol decoding finishes. Table 2 Symbol Decoding Algorithm  After symbol decoding the remaining nodes construct a node lattice. If there is only one path from left to right in the lattice then  No Yes Tagging Result Text One Path Symbol Decoding Statistics Decoding Preprocessing  7 decoding finishes and the state sequence is output. Otherwise Viterbi algorithm is used to calculate and select the most probable path."]},{"title":"6 Evaluation Criteria","paragraphs":["We use the following criteria to evaluate the performances of COV. (1) PA: Overall precision rate (2) PM: Precision rate of the multi-class words, (3)PO: Precision rate of OOV (Out Of Vocabulary), not including the personal names, location names and organization names, etc. (4) PE: Error reduction rate, comparing with the baseline model. All the above criteria have been introduced in Kupiec (1992) and Cutting (1992) etc and will not be repeated here. (5) PS : State certainty rate In order to measure the statistics decoding complexity, we define State certainty rate PS.",")( )__( nsObservatiocount NodesStateTotalcount","PS  Count(Total_State_Nodes) denotes the total number of possible states for all the observations in statistics decoding. Due to the symbol decoding many states have been pruned in COV and the search space for statistics decoding is reduced accordingly. The level of search space reduction can be indicated by the criteria of Ps."]},{"title":"7 Experiments 7.1 Corpus and Preprocessing","paragraphs":["The training and test data are all taken from the People’s Daily of 2000 year, which has been segmented and manually assigned PoS tags by the Peking university. The division of corpus is as follows: Group Usage of corpus Months","Amount of tokens 1 Training","Feb. 1050934 2 Feb.-June. 6142402 3 Open Test Jan. 1235628 4 Close Test Feb. 1050934","Table 3 Division of corpus The baseline model is the 2nd order HMM, whose results will be compared with that of 2-gram COV. Before training and tagging the corpus is preprocessed. All the named entities such as personal names, location names, organization names and all the digits are replaced by some particular symbols. For example, personal names are all replaced by “*PerN*”. 7.2 Results ","PA PM 2nd order HMM 96.54% 92.76%","2-gram COV 98.29% 96.44%","PE 50.58% 50.83% Table 4: Results of the close test. Corpus of group 2 in table 3 is used as the training corpus. Group 1 Group 2 2nd order HMM 94.63% 95.73%","2-gram COV 95.53% 96.79%","3-gram COV 95.63% 96.83% Table 5: PA of HMM, 2-gram and 3-gram COV in open test. The corpus of Group 1 and 2 are used as training corpus. The above results show that 2-gram and 3-gram COV all outperform second order HMM. And 3-gram COV outperforms 2-gram COV, which indicates that with the expansion of observation the precision rate of COV will not decline but increase. Group 1 Group 2 2nd order HMM 90.75% 92.02%","2-gram COV 92.66% 94.24%","PE 20.64% 27.85% Table 6: PM of HMM and COV in open test. The result shows that COV has a better performance in tagging multi-class words than 8 HMM. Group 1 Group 2 HMM 53.21% 55.07% COV(2-gram) 92.24% 93.99% COV (unigram) 53.27% 55.35% Table 7: PO of HMM and COV With regard to the 2-gram OOV, the OOV precision rate of COV is higher than 90%, which indicates that COV can well deal with the OOV problem when the observation unit is expanded. We have done some experiments to compare the time cost and precision rate among HMM, COV and discriminative models such as MaxEnt and CRFs. For the limitation of computer processing power, we choose the People’s Daily of January, 2000 as the training data and the first 5000 paragraphs of the People’s Daily of February , 2000 as test data. The taggers are the MaxEnt tagger developed by Standford University and CRF++. HMM COV","MaxEn t CRF 1 CRF 2","Trainin g","time 1mins 2mins 4.6hrs 63hrs 60 hrs Test time 4mins 8mins 11mins 17mins 11mins PA","94.23 %","95.43 % 95.69%","95.67 %","95.80","% Table 8: Training, test time and PA of different models The template of MaxEnt is: w-1, w0, w+1, prefix of w0, suffix of w0, length of w0 The template of CRF1 is: w-1, w0, w+1, prefix of w0 The template of CRF2 is: w-1, w0, w+1, prefix of w0, suffix of w0, length of w0 The above data show that the precision rate of COV is higher than HMM, and comparable to the discriminative models. Moreover, training time of COV is much less than the discriminative models and almost at the same level as HMM. High precision rate and low time cost makes COV more competitive and practical than other models. Training Group HMM COV Reduction of Ps Reduction rate of Ps","1 1.79 1.66 0.14 7.82%","2 2.03 1.57 0.46 22.66% Table 9: Ps of 2nd order HMM and 2-gram COV The above result shows that the search space in statistics decoding of COV is smaller than HMM. We also count the tokens which can be tagged with symbol decoding. Training Group Tokens of Symbol Decoding Percentage of Symbol Decoding PA","1 86187 6.98% 99.24%","2 92174 7.46% 99.42% Table 10: Results of symbol decoding The total tokens of test corpus is 1235631. The above data shows that there are about 7% tokens which can be tagged with symbol decoding and without any probability computation. Moreover, the precision rate of symbol decoding is above 99%, which is much higher than the average precision rate. The smaller search space and higher precision rate proves the efficiency and robustness of COV in PoS tagging. We also conducted some experiments of English PoS tagging. The training and test data are from the Wall Street Journal (WSJ) in Penn Tree Bank. We use the texts of group 00 to 19 in WSJ as training data and group 00 to 04 as close test data and group 23 to 24 as open test data. The baseline model is also the 2nd order HMM. Results are as follows.","PA of PA of PM of PM of 9 HMM COV HMM COV Close Test 97.85% 98.29% 94.85% 96.44% Open Test 96.48% 96.79% 93.92% 95.18% Table 11 Results of English PoS tagging Experiments The above results show that COV also outperforms HMM in English PoS tagging."]},{"title":"8 Discussion","paragraphs":["COV is not only suitable to PoS tagging task. We have applied it to the Chinese word segmentation, sentence boundary detection and chunk detection, in which COV also achieves satisfactory results. COV is not limited to the certain language but can be applied in the tagging tasks of different languages. Comparing with HMM, COV has the advantages of smaller search space and higher tagging precision rate. Comparing with the discriminative models, COV has the advantages of less training time and comparable precision rate. All of these prove that COV is a general, efficient and robust model for sequence labeling. Meanwhile we also find that it is difficult for COV to combine more context and lexical features as discriminative models can do. For example, COV has not taken the suffix or prefix of a word into the model. In fact such information is important for guessing the PoS of unknown words. In the future we will make efforts to take more context and lexical information into the model and improve its performance."]},{"title":"References","paragraphs":["L. Rabiner. 1989. A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition, Proc. of the IEEE, 77(2). Church, K. 1988. A stochastic parts program and noun phrase parser for unrestricted text. Proceedings of Second ACL Applied NLP, 136-143. Scott M. Thede, Mary P. Harper. 1999. Second-order hidden Markov model for part-of-speech tagging. In ACL 37, 175–182. Eric Brill. 1995. Transformation-based error-driven learning and natural language processing: A case study in part of speech tagging. Computational Linguistics, 21(4):543-565. Julian Kupiec. 1992. Robust part-of-speech tagging using a hidden Markov model. Computer Speech and Language, 6(3):225-242. Adwait Ratnaparkhi. 1996. A maximum entropy model for part-of-speech tagging. Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 133-142. Doug Cutting, Julian Kupiec, Jan Pedersen, and Penelope Sibun. 1992. A practical part-of-speech tagger. In Proceedings of the 3rd Conference on Applied Natural Language Processing (ACL), pages 133-140. Scott M. Thede and Mary P. Harper. 1999. A second-order hidden Markov model for part-of-speech tagging. In ACL, pages 175–182. J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of ICML-01, pages 282-289. Zhongqiang Huang , Mary P. Harper , Wen Wang. 2007. Mandarin Part-of-Speech Tagging and Discriminative Reranking. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pp. 1093–1102. 10"]}]}
