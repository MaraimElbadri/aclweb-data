{"sections":[{"title":"Clustering Co-occurrence Graph based on Transitivity","paragraphs":["K,,mlko TANAKA-Ishii Electrotechnical Laboratory","1-1-4 Umezono, Tsukuba~ Ibaragi 305 JAPAN.","klmiko~etl, go. jp","Hideya IWASAKI","Tokyo Univ. of Agriculture and Technology","2-24-16 Naka-cho, Koganei, Tol~'o 184 JAPA.N.","iwasaki@ipl, el. runt. ac. jp Abstract Word co-occurrences form a graph, regarding words as nodes and co-occurrence relations as branches. Thus, a co-occurrence graph can be constructed by co-occurrence relations in a corpus. This paper discusses a clustering method of the co-occurrence graph, the decomposition of the graph, from a graph-theoretical viewpoint. Since one of the applications for the clustering results is the ambiguity resolution, each output cluster is expected to have no ambiguity and be specialized in a single topic. We observed that a graph has no ambiguity if its branches representing co-occurrence relations are transitive. An algorithm to extract such graphs are proposed and its uniqueness of the output is discussed. The effectiveness of our method is examined by an experiment using co-occurrence graph obtained from a 30M bytes corpus. 1 Introduction Clustering is the operation to group words by some criterion. Thesauri and synonym dictionaries are some of its manual examples. Automatic outputs can be used not only to revise them, but also to aid ambiguity resolution, an essential problem in natural language processing. For instance, the me~ing of an ambiguous word can be decided by"]},{"title":"e.xamln'i~g","paragraphs":["the duster it belongs to. Furthermore, clusters grouped according to topics have many application areas such as automatic document classification. The input in this paper is the word co-occurrence graph obta~ued from corpus. The output is its subgraphs with the condition that each subgraph is specialized in a topic.","Many automatic clustering methods have been already proposed. Most of them are based on the statistical similarity between two words. Our approach is different; it is graph theoretical. We tried to find out the special structure in linguistic graph.","Having a huge co-occurrence graph obtained from a corpus, we first tried to decompose it to analyze its graph structure using graph theoretical tools, such as maximum strongly connected components, or biconnected components. Although both tools decompose a graph into tightly connected subgraphs, these trials resulted in vain. The question arose; what must be taken into account to decompose the co-occurrence graph. 7 The answer is the ambiguity. Furthermore, we reached to the conclusion that the ambiguity can be explained in terms of intransitivity. This feature is developed into an algorithm for clustering.","This paper is organized as follows. The following chapter describes the relationship between the transitivity in the graph and the ambiguity resolution. Chapter 3 shows the relationships between clustering and transitivity. Chapter 4 proposes and discusses an algorithm for clustering. Related work is resumed in Chapter 5. Our method is examined in Chapter 6 by some experiments. 2 Word Ambiguity and Transitivity Two words are said to co-occur when they frequently appear close to each other within texts. Regarding words as nodes and co-occurring re-91 latious as branches, a graph can be constructed from a given corpus. We define such a graph as co-occurrence graph.","When a portion of a corpus specializes in a topic, we can sti]l extract a co-occurrence graph from the portion. A general corpus, such as newspaper corpus, contains many corpus portious, each specializing in one topic. Therefore, the whole co-occurrence graph obtained from a general corpus cont~.in.q subgraphs, each specializing in one topic. Our question is to extract such subgraphs of topics from a co-occurrence graph.","We denote V as the set of nodes (words), E as the set of branches (co-occurrence relations), G=< V, E ~ as an input graph and I1/\"1 as the number of nodes. English words referred as examples will be w~itten in this font.","2.1 TrAnsitivity in Co-occurrence Relation The most basic mathematical laws discussed about relations between elements in a set are reflective, symmetric and transitive laws. Having a, b, c E V and R as a relation, they can be described as follows: Reflective aRa. Symmetric aRb ~ bRa. Transitive aRb, bRc ~ arc. Let V be word set and R be co-occurrence relation. When each property holds for .EL, words a, b and c can be explained as follows from the linguistic viewpoint:","Reflective Word a co-occurs with itself.","Symmetric Co-occurrence relation does not depend on the occurrence order.","Transitive Word b does not have two-sided meanings (ambiguity). For instance, doctor, which has both medical and academic meanings, co-occurs with nurse within a medical topic, and co-occurs with professor within an academic topic. However, nurse and professor do not co-occur, so the transitivity between nurse: doctor and professor does not hold."]},{"title":"! I G @","paragraphs":["a C b a c a c '"]},{"title":"db % - i | (5) d (6) e d","paragraphs":["c c c b bb (7) a d (8) d","c @ c O. = ~..=mO ~clmr branch duplicate brm-tch Figure 1: Graph Decomposition Our request is to extract subgraphs each of which focuses on one topic with no ambiguity. Therefore, we perform clustering by extracting subgraphs whose branches form transitive co-occurrence relations."]},{"title":"3 TrAnsitivity and Clustering 3.1","paragraphs":["Decomposition and Duplication The simplest case is a graph of three nodes. Figure 1-(1) is a graph in which the transitivity does not hold. For example, when b is doctor, a is nurse and c is professor, nurse and professor do not co-occur due to the node b's two-sided meanings. Therefore, as in Figure 1-(2), we \"duplicate\" b so that each duplicated node corresponds only to a single me~in,~. Then the ambiguity within b is resolved and the entire graph is divided into two subgraphs, the academic one and the medical one. To sum up, when the transitivity does not hold, a graph can be decomposed by duplicating the ambiguous node.","On the other hand, when the transitivity holds among three nodes (Figure 1-(3)), the 92"]},{"title":"I graph cannot be decomposed by duplication of I b (Figure I-(4)). This can be explained that 2\"~::'\" T","paragraphs":["the graph does not have the ambiguity. (1) .=.~..q.g~ (2) m~.umm m * m~ttt:~m ~","We extend the above into the case of four ''\" ~ - \"","i nodes(Figure 1-(5)). Here, transitivity does not • hold in a-b-d because there is no branch be- b b tween a-d. When b--c is duplicated, the graph ~"]},{"title":"I can be decomposed into two subgraphs (Figure (3) a d (4)a d i-(6)) in which the transitivity holds On the contrary, Figure 1-(7) c~not be decomposed c","paragraphs":["c by duplicating b-c due to the branch a-d (Figure 1-(8)); this shows that b-c is not ambiguous. Note that Figure 1-(7) is a complete graph of 4 nodes. We deYme duplicate branch as a branch to be duplicated for graph decomposition (such as b-c) and anchor branch as a branch which i~hlbit graph decomposition by duplication (such as a-d).","In general, when a graph could not be separated by duplicating its subgraph, then the subgraph is regarded not to have ambiguity. Therefore, ideal clustering is to decompose graphs into subgraphs which cannot be decomposed further by duplication. Unfortunately, this constraint is too strict because such a graph is restricted to a complete graph. In addition, extracting complete graphs withln a given graph is NP-complete. Therefore we discuss in the following how to loosen the constraint. 3.2 Transitive Graph","There are two methods to loosen the constraint.","The first is to decrease the nllmber of anchor branches. In the complete graph of more than 5 nodes, several anchor branches exist for each duplicate branch (Figure 2-(1)). However, only one anchor branch is sufficient to inhibit the decomposition. The less the number of anchor branch is, the looser the constraint is.","This intuitively corresponds to loosen the sharpness of the focus of the topic in the resulting cluster. For instance, two words pneumonia and cancer do not always co-occur, but they do co-occur with words as doctor, nurse and hospital forming the core of medical topics. anchor distance 2 anchor distance n Figure 2: Loose Constraint for Graph Decomposition Pneumonia will be included into a cluster if it is connected with these three words even if it is not connected with cancer. If cancer is also connected with these three words, both cancer and pneumonia, the different subtopical words within a medical topic are included in a cluster.","The second is to loosen the transitivity itself. It was defined in Section 2.1 within three nodes. We may prepare a loose transitivity as follows:","VlJ~:~21 -'. , ~n_l~:~Vn ---> ~l_~n We define anchor distance as the maximum distance of the minimum distances of a--b-d and a-c--d. For example, when minimum distance of a-b-d is 4 and that of a-c-d is 6 then the anchor distance is 6. The tightest constraint is when anchor distance is 2 as in Figure 2- (3). This also blurs the topic focus of a cluster. In the example of pneumonia, the word will be included if it is connected directly with at least one of the words among"]},{"title":"doctor, hospital,","paragraphs":["nurse,","and cancer, and connected indirectly with the","others.","For m,n <"]},{"title":"IVI-","paragraphs":["1, G is called (m,n)-","transitive graph when","for all e E E, there ave m anchor","branches e t E E of anchor distance","_<n.","(m, n)-transitive graphs can be extracted as","the subgraphs of the input graph. Figure 3","shows a map of (m,n)-transitive graphs. The","axis of ordinates describes the number of an-","chor branches (m). The axis of abscissas de-93 .° tlght constraint = > Ioese loose 2-fight 3-4- • IVI.I - 2 4 I -1","! o -~GS1 ....... G52 $ i ---~SO.....¢om pier e graph number of anchor branches m Figure 3: (m, n)-Transitive Graph scribes the anchor distance (n). The constraint is loose when m is small and n is large. For the same input, (ml,n)-transitive graphs are included in (m2, n)-transitive graphs when ml < m2, and (m, nl)-transitive graphs are included in (m, n2)-transitive graphs when n2 < hi."]},{"title":"GS2","paragraphs":["in Figure 3 is the clusters obtained under the loosest constraint: m is the maximum and n is the ~ni~um. In GS2, all ambiguity of a branch and nodes at its ends are resolved. G$O are the transitive graphs of the tightest constraint. All transitive graphs on the horizontal line including G$O are complete. 4 Extraction of Transitive Graphs So far, we did not explain how to detect the duplicate and anchor branches, given a graph. An algorithm for clustering can be top-down or bottom-up. The former gives clusters by decomposing the input graph by detecting duplicate branches.","Although we have explained our clustering method top-down up to now, we propose our clustering method as bottom-up. We obtain clusters by accumulating adjacent nodes so that every branch has anchor branches and the resulting clusters include no duplicate branch. Thus, in the bottom-up method, we need no~ detect duplicate branches. This is convenient, because the condition for anchor and duplicate branches is denoted by local relationships among nodes.","The branches in the input graph are assumed to be all symmetric. In this section, we use terms clusters as our output and subgraphs as their candidates. 4.1 Definition of Clusters We extract"]},{"title":"GS1,","paragraphs":["the (1, 2)-transitive graph.","A subgraph A including a branch e in the","input graph can be extracted as follows:","Step 1. Put a triangle graph including e into A.","Step 2. Take a branch e ~ in A and a node v which makes a triangle with e t (Figure 4). If the following condition is satisfied, put v into A.","There exists a node v t E G"]},{"title":"(input","paragraphs":["graph) whose distance from","e ~ is 1, and it is connected to v","with a branch. Here, the branch","J-~v is the anchor branch so that","e t is hindered to be the duplicate","branch in the resulting cluster.","Additionally, put every branch between","v\" E G and v into A. Step 3. Repeat Step 2 until A c~nnot be ex-","tended. Performing the above procedure starting from every branch in the input graph, we obtain many subgraphs. Considering the inclusion relation between subgraphs, they constitute a partial order (Figure 5). We define clusters as maximal subgraphs in this partial order chain. They are subgraphs not included in any other subgraphs. The uniqueness of the clusters for an input is self-evident. 4.2 Algorithm for Clustering In the previous section, the procedure to obtain subgraphs should begin from every branch in the input. However, it is su~cient to calculate as follows. Step O. i -- 0"]},{"title":"I I il i I I l I I ! I I I a I I 94","paragraphs":["Y Figure 4: Extraction of (1,2)-Transitive Graph"]},{"title":"~ D cl/~~ste~","paragraphs":["Figure 5: Subgrapl~ and their Partial Order","Step 1. Choose a branch e E G not included in Go,\"',"]},{"title":"Gi-1. If","paragraphs":["no e is found, go to Step 5."]},{"title":"Gi","paragraphs":["--- < 0,0 >. Put a triangle graph including e into"]},{"title":"Gi.","paragraphs":["Step 2 and 3. Extend"]},{"title":"Gi using","paragraphs":["Step 2 and 3","of the previous section","Step 4. Set"]},{"title":"i -","paragraphs":["i q- 1 and goto Step 1.","Step 5. Examine every pair of subgraphs (A, B), andifA includes B, then drop B. The remaining subgraphs are defined as clusters. A maximal subgraph c~ot be missed. Its starting branch is encountered without fail in the above algorithm. If it is encountered as the starting branch in Step 1, the maximal subgraph is obtained. If it is captured into a subgraph and becomes e ~ in Step 2, the subgraph extends to the size of maximal subgraph; if it gets larger, the subgraph contradicts being maximal as the result of the last section.","The algorithm halts since the input graph is finite, and the output is unique for an input."]},{"title":"5 Related Work","paragraphs":["[Li and Abe, 1996] compared clustering methods proposed so far [Hindle, 1990] [Brown"]},{"title":"et ¢1.,","paragraphs":["1992] [Pereira"]},{"title":"et al.,","paragraphs":["1993] [Tokunaga"]},{"title":"et el.,","paragraphs":["1995][Li and Abe, 199@ Most of them are so-called"]},{"title":"hard ch~tering:","paragraphs":["each word is included only in one cluster. We do not follow the trend, from the sense that our objective is the extraction of clusters of topics. It is natural that an ambiguous word should be included in different clusters.","[Pereira"]},{"title":"et ¢l.,","paragraphs":["1993] adopts sof~"]},{"title":"clustering.","paragraphs":["They measured co-occurrence between nouns and verbs, and clustered nouns of the same distribution of verbs.","[Fukumoto and Tsujii, 1994]'s work has common motivation with us: the ambiguity should be resolved for clustering. They clustered verbs using the gravity of multivariate analysis.","[Sugihara, 1995]'s approach has a common point in that it focuses on graph structure for clustering and tries to structurize the input graph, a bipartite graph of words and concepts (such as food, fruit etc.). His clustering method is so called Dlllrnage-Mendelsohn decomposition in graph theory. The output naturally gives a partial order of clusters which can be compared with conventional thesauri.","Our input is not bipartite. In the beginning, we tried to decompose input graph into maximum strongly connected components to obtain graphs of topics from the observation that nodes in a cycles are strongly related 1. However, subgraphs about different topics is merged into the same cluster by two ambiguous words which bridge these two subgraphs(Figure 6). Next, we observed that articulation nodes are ambiguous, so we performed decomposition into biconnected components. In this case when several biconnected components are connected in a ring, articulation nodes could not be detected (Figure 7). The observation that there are no co-occurrence relationship between","X[Tokunagaaud Tanaka, 1990] discusses on extraction of cycles formed by trauslation relations fzom bilingual dictionary."]},{"title":"95","paragraphs":["Figure 6: Problematic ~Structure in MSCC Clustering  mblguous word clusters of dffferem toph:s Figure 7: Problematic Structure in Biconnected Component Clustering two biconnected graph across the articulation node was the start point of this paper. 6 Experiments 6.1 Procedure of Clustering First, we make the input graph from a 30M bytes of Wall Street Journal. Co-occurrences of no~s and verbs are extracted by a morphological analyzer. We defined that a word co-occurs with 5 words ahead of the word within a sentence. Co-occurrence degree is measured by mutual information[Church and Hanks, 1990]. We set a certain threshold to the values to extract the input graph.","The number of resulting clusters depends on the input graph as follows:","1 One huge connected subgraph.","2 Several medium sized subgraphs.","3 Many small sized subgraphs. When the threshold value is too high, the output is 3. On the contrary, when it is too low, then the output becomes 1. Both 1 and 3 are"]},{"title":"I I","paragraphs":["Number of dusters of size more than 7"]},{"title":"40 i 30 I 20 I 10 1 r t-,I, .Thresh°Id ~1 1 2s4ss~ Figure 8: Threshold and the Output","paragraphs":["not interesting, because 1 is a graph including all topics, and 3 generates graphs of too small topics to check the global trend of topics in the input. Therefore, we varied threshold from 1.0 up to 7.0 by 0.5 steps to make the input graph, applied our algorithm to each input in order to detect the best threshold.","The result is shown in Figure 8. The number of clusters whose sizes are more than 7 is plotted against the threshold value. When the threshold is 3.5, such dusters were most numerous. In 39 dusters, there were 727 different words out of 15347 in the input graph."]},{"title":"6.2 Evaluation of Clusters","paragraphs":["In Appendix, 39 clusters are shown their con-","tents sorted by size. Words judged inappro-","priate in each cluster are attached ~t'- Words","tmdecidable being suitable in their clusters are","put \"~\". All 39 clusters are attached four items as fol-","lows: • Subjectively judged topic • Cluster size (CS): number of words in a","cluster • Error rate (ER): rate of inappropriate","words (attached \"t') • Uncertainty rate (UR): rate of uncertain","words (attached \"~\" and \"~\") The average of the above items were CS=20,"]},{"title":"96","paragraphs":["ER=10.3%, UP,.= 14.7%; hence, the ambiguity was removed from clusters up to 85% on average. The number of the cluster whose topic was inestimable is only 1. The estimation of topic becomes clltTicult with two factors, CS and UR. When CS is too small, even when UR is 0.0, the cluster itself lacks in information. When UR is high, it is natural that the topic becomes inestimable.","6.3 Evaluation of Words Contained in More thun Two Clusters The number of words belonging to more than two clusters amounts to 57. They are classified as follows (numbers in parenthesis are cluster number in Appendix):","i. A word with different","words). men-lugs (10"]},{"title":"l cell (1, 15)I ice (3,8) I panel (1, 7) treat (1, 3)","paragraphs":["2. A word with the same me~nlng but used in different contexts (32 words)."]},{"title":"star (9,12,14) brand (3,22)","paragraphs":["3. A word with the same meaning in the same","context (7 words).","4. Others (One of the words is uncertain, or","its cluster's context is not estimated. 6"]},{"title":"words)","paragraphs":["Words of class 1 is the ambiguous words. Cell in Cluster I means cells of tissue, whereas that in Cluster 15 means battery. Ice in Cluster 3 means ice for cooling beverage, whereas that in Cluster 8 means ice to skate on.","According to our objectives to obtain subgraphs of topics, words in class 2 is quite important to be duplicated. For instance, star in Cluster 9 is a sport player star, that in Cluster 12 is a singer star and that in Cluster 14 is a movie star. If star were not duplicated, the three different topics would be merged into a single subgraph. The same situation is observed for children: it would merge topics of childbirth and education into a graph if it was not duplicated. We are apt to pay attention only to the words of class 1, but that of class 2 plays an important role in clustering.","Words in class 3 is not ambiguous: they should connect two subgraphs into one (see Section7). 6.4 Cluster Hierarchy An output subgraph of higher threshold is included as that of lower threshold. With this inclusion relation, the clusters form a hierarchy(Figure 9). A part of the hierarchy is shown below: Threshold 3.75"]},{"title":"A admission college scholarship ]3 admission applicant college C campus children classroom college edu- cation enroll faculty grade math parent scholarship school","paragraphs":["student sugar"]},{"title":"teach teacher tuition","paragraphs":["tutor university voucher D birth child children couple marriage","marry parent wedlock woman E birth infant weight F"]},{"title":"birth boy","paragraphs":["marry Threshold 3.5 Cluster 6,24 in Appendix. Threshold"]},{"title":"3.25","paragraphs":["G"]},{"title":"admission applicant baby birth boy","paragraphs":["campus century child children"]},{"title":"classroom college couple daughter education endowment enroll","paragraphs":["enrollment establishment faculty father gift"]},{"title":"girl god grade homework husband","paragraphs":["infant ivy"]},{"title":"kid live love man marriage marry math","paragraphs":["mother oxford parent professor"]},{"title":"psychologist scholar scholarship school son student","paragraphs":["study sugar taught teach teacher teaching"]},{"title":"toy tuition","paragraphs":["tutor university","voucher wed wedlock woman","At threshold 3.75, the origins of education (Cluster 6) and childbirth(Cluster 24) clusters are already formed. Among education, there are subtopics on scholarship school and school entrance. They are merged into Cluster 6 when the threshold is lowered to 3.5. Cluster 24 is also formed from Clusters D,E,F of threshold 3.75. Then Cluster 6 and 24 are merged into Cluster G when the threshold is 97 Threshold 3.25 3.5 3.75"]},{"title":"cA","paragraphs":["A B C Figure 9: Cluster Hierarchy lowered again to 3.25. The clusters' hierarchical relationships are shown in Figure 9.","We may see that the topic is more specialized when the threshold is high. Clusters which are merged between threshold 3.75 and 3.5 were those within a topic (A,B,C or D,E,F), but topics of different clusters are merged at 3.25 (Clusters 6 and 24). Thus, the lower the threshold is, the more the cluster contains ambiguity. The reason is that the two words in different topics do not co-occur. 7 Discussion The best threshold differs in topics. Some examples are:","Economic topic: Although Wall Street Journal articles have economic tendency, clusters of economic topic cannot be found in the dusters with more than ? words of threshold 3.5. They appear in clusters at threshold 3.0 as follows:","Â• accountant audit bracket deduction","filer income offset tax taxpayer","Â• convert conversion debenture debt","holder out.stand prefer redeem redemp-","tion repay tidewater The threshold should be lower for this topic.","Medical topic: Cluster 1 have too many words. Despite of a medical cluster, potato appears in the cluster. At the threshold 3.0, Cluster 3 is completely merged with Cluster 1. The appearance of potato shows the sign that the merge of two clifferent topic has already begun. Therefore, a higher threshold is preferred. 98 Trial topic: Several clusters exist on trial in","Appendix. They should form a cluster","with relatively lower threshold. Consequently, one of the most important future work is to integrate two stages, the first stage of malc;ng input graph with the static threshold, and the second stage of clustering, into a single stage with dynamic threshold. 8 Conclusion We have discussed a method to duster a co-occurrence graph obtained from a corpus, from a graph-theoretical viewpoint. A graph has no ambiguity if its branches, co-occurrence relations are transitive. This graph theoretical approach using graph is characteristic and it differs from the conventional clustering method. We proposed an algorithm to extract subgraphs whose branches are transitive co-occurrence relations and discussed its features. The effectiveness of our method was examined using the 30M corpus. References","[Li and Abe, 1996] H. Li and N. Abe. Clustering Words with the MDL Principle. In Proceedings of the 15th International Confervnce on Computational Linguistics, roll. pp.4-10, 1996.","[Sugihara, 1995] K. Sugihara. A Graph-Theoretical Method For Monitoring Concept Formation. Pattern Recognition, Vol.28, No.ll, pp. 1635--1643, 1995.","[Tokunaga et al., 1995] T. Tokunaga, M. Iwayama and H. Tanaka. Automatic Thesaurus Construction based on Grammatical Relations. In Proceedings of the International Joint Conference on Artificial Intelligence '95, 1995.","[Fukmnoto and Tsujii, 1994] F. Fu.kumoto and J. Tsujii. Automatic Recognition of Verbal Polysemy. In Proceedings of the igth International Conference on Computational Linguistics , vol.2, pp.762-768, 1994.","[Pereira et al., 1993] F. Pereira, N. Tishby and L. Lee. Distributional Clustering of English Words. In"]},{"title":"Proceedings of t~e Mst ACt,","paragraphs":["pp. 183-190, 1993."]},{"title":"[Brown et aL, 1992] P. Brown, V. Pietra, et al. Class-based n-gram Models of Natural Language. Computational Lingui.~ics, 18 (4), pp. 283-298, 1992. [TokLmaga and Tanaka~ 1990] Tokunaga, T. and Tauaka, H. The Automatic Extraction of Conceptual Items from Bilingual Dictionaries. PRICAI, 1990. ~Rindle, 1990] D. Hindle. Noun Classification from Predicate -- Argument Structures. In Procceed~gs of fAe ~8~h ACL, pp.168-175, 1990. [Church and Hanks,","paragraphs":["1990]"]},{"title":"K. W. Church and P. Hanks. Word Association Norms, Mutual Information, and Lexicography. Computatior~al L~guistics, vol. 16 (1), pp. 22-29, 1990. Appendix The triples","paragraphs":["are"]},{"title":"(CS, Ei~ UR) (See Section","paragraphs":["6.2)."]},{"title":"Cluster 1: medicine (105, 4.8%, 5.7~)","paragraphs":["afflict cancer disease gene researcher cell hepatitis lung patient bacterium protein repair scientist therapy mice tissue virus fibrositis implant mouse vaccine antigen nicotine treat infect receptor switch~ blood enzyme in-ject insulin molecule pill aid heart infection transplant prostate stroke symptom transmit treatment cure depression diagnose kidney trial t bone chemotherapy dose marrow ulcer cause stomach doctor breast disorder prescribe blockers clot eradicate medication sample brain nerve potato~ donate plasma patch arthritis drug lithium placebo version t test laboratory protease prevention syndrome panel schizophrenia substance tumor psychotheraw sclerosis suffer die pain physician lab urine rat radiation animal cholesterol pharmacist pharmacy collagen internist hospital prescription medicine care referral~nonprescription"]},{"title":"Cluster 2: transportation (101,","paragraphs":["12.6%,14.6%) air surveillance mlssilet plane fighte~ radar aircraft patriot~ransport airline jet tank boat pilot serb~ trainer airliner crash engine flight fly aviation passenger bus carrier bump airport hub saudi airplane delivery surfaces machinist attendant wing airway shuttle nonstop route delta fare hanoi~ walkout mechanic"]},{"title":"transportation","paragraphs":["haul denver railroad courier rail freight mile truck shipment diesel pickup minivan gasoline fuel sportt buiitt midsize~ assembly heat inventory detroit vehicle model oil petroleum car auto styling'~ emission:[: brake cylinder"]},{"title":"sedan","paragraphs":["equip antilock omega airbag luxury bag driver jaguar"]},{"title":"dodger","paragraphs":["accident pump motor rover"]},{"title":"wheel","paragraphs":["neont ford cherokee~ rear front dealershiptexplorer plant build shreveport"]},{"title":"Cluster 3: meal (60, 8.3%, iL7% )","paragraphs":["alcohol beer brew label liquor miller beverage taste bottle brewer brewery ale ice brand vodka coca cola wine can drink milk coke diet fruit juice dinner draft dairy tea nestle supermarket grocer toronto'~ bottler crownf flavor [ithuania~ atlantat pepper cranberry infection~, soup cheese cow hormone meat refrigerate calorie category spray t eat ocean:[: meal chicken variety treat food sauce beef restaurant"]},{"title":"Cluster 4: agriculture (32, 3.1%,","paragraphs":["15.6%) acre flood corn farmer grain harvest agriculture hog ,soybean wheat bushel crop depress'~ exporter"]},{"title":"feed","paragraphs":["livestock cotton rain africa~ cattle farm meat brazil~ july rice shipment forecast~, bale frost season flake~ sugar Cluster 5: Near East Asia (32, 0.0%, 0.0%) arab qatar saud} egypt kuwait israel king peace arabia bahrain gaza emirate oil jew jordan lebanon occupy fighter jerusalem syria palestine barrel settler massacre territory kill strip cabinet liberation jericho"]},{"title":"Cluster 6: education (31, 3.2%,","paragraphs":["12.9%) admission scholarship school college applicant student university education professor taught teach tuition classroom faculty girl t teacher enroll enrollment grade homework math parent campus psychologist.~ scholar ivy voucher children tutor sugar t endowment"]},{"title":"C]uster 7: politics (25, 24.0, 40.0%)","paragraphs":["bidden t senate senator democrat nomination bipartisan vermont.~ chafe t dole ]nman~ maine committee subcommittee confirmation rhode~ bob t columnists judiciary.~ ranking chairmen panel chairman hearing oversight t oregon~ Cluster 8: winter sports (24, 4.2%, 12.5%) alpine race ski cup norway winter medal skate skier boa~ cross kilometer sport silver snow tommy# men t athlete skater boot ice meter speed track"]},{"title":"Cluster 9: ball","paragraphs":["games"]},{"title":"(24,","paragraphs":["16.7%, 16.7%o) baseball sports basketball fan football league liberty\"l\" team pro soccer televise coach stadium star foxy cowboyj\" bowl conferenceS\" franchise game hockey player tournament college"]},{"title":"Cluster 10: computer network (18, 5.6%,","paragraphs":["5.6%) bulletin user prodigy~ computer message internet mail pager laptop facsimile machine desk'top modem note-book dell subscription communicate subscriber"]},{"title":"Cluster 11: Asia (17, 23.5%, 23.5%)","paragraphs":["asia singapore thailand indonesia malaysia philippines vietnam taiwan korea burma china interbanktneighbor visito~ pakistan status t human~ belling"]},{"title":"Cluster 12: song","paragraphs":["(17, 0.0%, 0.0%) album pop sing song music radio singer jackson artist recording disk band channel jazz star copy concert 99 Cluster 13:"]},{"title":"Soviet Union","paragraphs":["(17, 0.0%, 5.9%) arsenal russia soviet ukraine weapon dismantle moscow baltic ruble bloc warhead kiev parliament reformer cabinet uranium quit:~"]},{"title":"Cluster","paragraphs":["ld:"]},{"title":"movie (17, 5.9%, 5.9%) academy hollywood film picture studio movie poly-","paragraphs":["gram actor library video turner lansin~screen script actress star theater C]uster 15: satellite broadcast (17, 5.9%,"]},{"title":"zz.s%)","paragraphs":["affiliate station broadcast fox~ network radio television clutteQ antenna satellite channel beam signal transmit format dish cell 'Cluster 16:"]},{"title":"software","paragraphs":["(16,0.0%, 0.0%) apple version window mac macintosh user application software unix lotus spreadsheet developer copyright excel feature word"]},{"title":"Cluster 17: Eoxope (15, 0.0%, 0.0%)","paragraphs":["austria belgium france italy netherlands sweden finland spain britain denmark holland switzerland norway kingdom membership Cluster 18: petrolum (14, 14.3%, 28.6%) alberta,: energy calgary~ gas oil pipeline feet barrel basin~ refine chevron 1\" exploration petroleum condensate"]},{"title":"Cluster 19: art (13, 15.4%, 15.4%)","paragraphs":["art works exhibit exhibition paint collection gallery museum photograph landscape artist avenue1\" flood C]oster 20: Korea (13, 15.4%, 23.1%) artillery tank weapon missile korea pyongyang regime seoul iraqt patriot inspector scuds stance t"]},{"title":"Cluster","paragraphs":["21:"]},{"title":"Balks-","paragraphs":["Peninsula"]},{"title":"(12, 0.0%,","paragraphs":["0.0%)","artillery serb strike weapon muslim bomb troop peace","peacekeeper croat negotiator withdraw","C.luster 22: ciga.tette (11, 0.0%, 0.0%)","addict smoker cigarette nicotine smoke camel morris","tobacco ban antismoke brand"]},{"title":"Cluster 23: multimedia. (11, 0.0%, 0.0%)","paragraphs":["audio text video disk cassette library multimedia tape music videocassette entertainment"]},{"title":"Cluster 24: childbirth (11, 0.0%, 0.0%)","paragraphs":["birth wedlock child marriage marry mother parent woman couple children infant Cluster 25: calamity (11, 9.1%, 9.1%) crop relief disaster flood earthquake riot~ hurricane quake rebuilding victim francisco"]},{"title":"Cluster 26: goods (10,","paragraphs":["10.0%,"]},{"title":"10.0%)","paragraphs":["apparel retailer store furnishing mart merchandise warehouse outlet mass'~ chain"]},{"title":"Cluster 27: trial (10, 20.0%, 20.0%)","paragraphs":["appeal reinstate ruling court: upheld circuit~ arbitration judge overturn t arbitrator"]},{"title":"Cluster 28: trash","paragraphs":["(10,"]},{"title":"50.0%, 50.0%)","paragraphs":["cleanup sitet superfundt dump pollute insurert waste trash ferrist gluQ Cluster 29: ? (10, 100.0%, 100.0%) arkansasf rock~ thriftt guaranty'~ madison'~ supervise'~ failure'~ limitationt surroundin~ investigation'~"]},{"title":"Cluster 30: earthquake (9, 22.2%, 22.2%)","paragraphs":["aftershock quake damage earthquake repair epicenter freewayt homeownert inspect Chzster 31: real estate (9, 0.0%, 11.1%) apartment bedroom rent avenue manhattan tower tenant subsidizer landlord"]},{"title":"Cluster 32: trial (9,","paragraphs":["0.0%, 0.0%) convict prison sentence jail parole conviction 'fine of-fender plead"]},{"title":"Cluster","paragraphs":["33:"]},{"title":"trial (9, 0.0%, 0.0%)","paragraphs":["award damage jury plaintiff verdict defendant case juror convict"]},{"title":"Cluster 34: winter resort (9, 0.0%, 0.0%)","paragraphs":["crest resort ski skier mountain snow valley slope sky"]},{"title":"Cluster 35: trial (8, 0.0%, 0.0%)","paragraphs":["bureau probe subpoena investigation prosecutor counsel inquiry suicide"]},{"title":"Cluster","paragraphs":["36:"]},{"title":"telephone network (8, 0.0%, 0.0%)","paragraphs":["cable wire fiber phone transmission voice fax microwave"]},{"title":"Cluster 37: bond (8, 0.0%, 75.0%)","paragraphs":["bondholder reorganizations chapters creditor reorganizer pet'rl:ion~ proceedings 'filingt"]},{"title":"Cluster 38: book (8, 12.5%, 12.5%)","paragraphs":["audio'~ publisher title book bestseller bookstore reference copy"]},{"title":"Cluster 39 guercilla (8, 12.5%, 12.5%) guerrilla","paragraphs":["syria negotiate peace rebel uprising negotiator"]},{"title":"mexicot 100","paragraphs":[]}]}
