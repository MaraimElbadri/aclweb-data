{"sections":[{"title":"Meta-discourse markers and problem-structuring in scientific articles Simone Teufel","paragraphs":["Centre for Cognitive Science","University of Edinburgh S. Teal elQed, ac. uk Abstract Knowledge about the argumentative structure of scientific articles can, amongst other things, be used to improve automatic abstracts. We argue that the argumentative structure of scientific discourse can be automatically detected because reasordng about problems, research tasks and solutions follows predictable patterns. Certain phrases explicitly mark the rhetorical status (communicative function) of sentences with respect to the global argumentative goal. Examples for such meta-diacaurse markers are \"in this paper, we have presented...\" or \"however, their method fails to\". We report on work in progress about recognizing such meta-comments automatically in research articles from two disciplines: computational linguistics and medicine (cardiology). 1 Motivation We are interested in a formal description of the document structure of scientific articles from different disciplines. Such a description could be of practical use for many applications in document management; our specific motivation for detecting document structure is quality improvement in automatic abstracting.","Researchem in the field of automatic abstracting largely agree that it is currently not technically feasible to create automatic abstracts based on full text understanding (Sparck Jones 1994). As a result, many researchers have turned to sentence extraction (Kupiec, Pedersen, & Chen 1995; Brandow, Mitze, & Rau 1995; Hovy & Lin 1997). Sentence extraction, which does not involve any deep analysis, has the huge advantage of being robust with respect to individual writing style, discipline and text type (genre). Instead of produc-ing abstract, this results produces only extracts: document surrogates consisting of a number of sentences selected verbatim from the original text.","We consider a concrete document retrieval (DR) scenario in which a researcher wants to select one or more scientific articles from a large scientific database (or even from the Internet) for further inspection. The main task for the searcher is relevance decision for each paper: she needs to decide whether or not to spend more time on a paper (read or skim-read it), depending on how useful it presumably is to her current information needs. Traditional sentence extracts can be used as rough-and-ready relevance indicators for this task, but they are not doing a great job at representing the contents of the original document: searchers often get the wrong idea about what the text is about. Much of this has to do with the fact that extracts are typically incoherent texts, consisting of potentially unrelated sentences which have been taken out of their con-text. Crucially, extracts have no handle at revealing the text's logical and semantic organisation.","More sophisticated, user-tailored abstracts could help the searcher make a fast, informed relevance decision by taking factors like the searcher's expertise and current information need into account. If the searcher is deal-ing with research she knows well, her information needs might be quite concrete: during the process of writing her own paper she might want to find research which supports her own claims, find out if there are contradictory results to hers in the literature, or compare her results to those of researchers using a similar methodology. A different information need arises when she wants to gain an overview of a new research area - as an only \"partially informed user\" in this field (Kircz 1991) she will need to find out about specific research goals, the names of the researchers who have contributed the main research ideas in a given time period, along with information of methodology and results in this research field.","There are new functions these abstracts could fulfil. In order to make an informed relevance decision, the searcher needs to judge differences and similarities between papers, e.g. how a given paper relates to similar papers with respect to research goals or methodology, so that she can place the research described in a given paper in the larger picture of the field, a function we call navigation between research articles. A similar operation is navigation within a paper, which supports searchers in non-linear reading and allows them to find relevant information faster, e.g. numerical results.","We believe that a document surrogate that aims at supporting such functions should characterize research articles in terms of the problems, research tasks and 43 solutions/methodology presented in the specific paper, but it should also represent other researchers\" problems, research tasks and solutions mentioned in the paper. Our long-term goal is to automatically reconstruct this problem-solution structure from unrestricted text for the searcher in the form of a"]},{"title":"problem-structured ab-","paragraphs":["stract.","But how can we find problems, research questions, research tasks and solutions in text without fully understanding the text? We take sentence extraction as a starting point due to its inherent robustness. Using additional information about each sentence, viz. their rhetorical status with respect to the entire paper, we are in a better position to perform shallow, but guided information extraction, in order to find the information units we are interested in.","In the next section we introduce the level of document structure we are talking about, and the kind of meta-comments we employ in discovering it. In the rest of the paper, we report on ongoing work on automatically filtering meta-comments from annotated and unannotated text. Finding recta-comments in text is an attractive task because it would allow for the automatic adaptation of sytem s using such phrases to new domains.","2 Discourse structure and argumentation in scientific articles Discourse linguistic theory suggests that texts serv-ing a common purpose among a community of users eventually take on a predictable structure of presentation (Kintsch & van Dijk 1978) - and scientific articles certainly serve a well-defined communicative purpose: they \"present, retell and refer to the results of specific research\" (Salager-Meyer 1992). Particularly in the life and experimental sciences, a rigid building plan for research articles has evolved over the years, where rhetorical divisions tend to be very clearly marked in section headers. Prototypical rhetorical divisions include"]},{"title":"Introduction, Purpose, Experimental Design, Resuits, Discussion, Conclusions.","paragraphs":["One of the reasons for this rigidly-defined structure seems to be that the scientific community in these fields has more or less agreed on how to do research: methodologies and evaluation methods are long-lived research entities that do not change often.","One of the corpora we are using is a good example of such texts. It consists of 129 articles in cardiology, taken from the"]},{"title":"American Heart","paragraphs":["Journa/, which have a fixed structure with respect to rhetorical divisions and section headers. The other corpus, in contrast, consisting of 123 (mostly conference) articles in computational linguistics (CL), displays an heterogeneous mixture of methodologies and traditions of presentation one would expect in an interdisciplinary field. Most of the articles cover more than one single discipline, but as a rough estimate one can say that about 45% of the articles in the collection are predominantly technical in style, describing implementations (i.e. engineering solutions); about 25% report on research in theoretical linguistics, with an argumentative tenet; the remaining 30% are empirical (psycholinguistic or psychological experiments or corpus studies). Even though most of the articles have an introduction and conclusions (sometimes occurring under headers with different names), and almost all of them cite previous work, the presentation of the problem and the methodology/solution are idiosyncratic and depend on individual writing style. Very few of the headers in the computational linguistics articles correspond to prototypical rhetorical divisions; the rest contain content specific terminology (cf. Figure 1 which compares relative frequencies of headers for the two corpora).","Computational","Linguistics I Header Freq:'l Introduction 85% Introduction Conclusion 46% Results Conclusions 22% Discussion Acknowledgments 1.7% Methods Discussion 12% Tables Results 11% Statistics Experimental Patients","Results 9% Limitations Related Work 77% Conclusions Implementation 7~ Statistical Evaluation 7~ Analysis Example 6Â¢~ . Conclusion Background 6% Patient Summary 4cX Characteristics","Cardiology","Header Freq. too% 94% 94% 92% 79% 40% 29% 2s% 25% 2Â°.% 17% 9% Figure 1: Highest-frequency headers (with relative occurrence frequencies)","Because the type of research reported in the computational linguistics corpus differs so much, the description of document structure we were looking for had to be flexible enough to generalize over differences in presentation, yet formal enough for the extraction of the information units which are useful for automatic abstracts. We base our model of argumentation in scientific articles on Swales' (1990) CAlkS model (\"Create a Research Space\"). Swales' claim is that the main communicative goal of an author of a research article is to convince readers (potential reviewers) that the research described in the paper constitutes an actual contribu-tion to science, in order to have the paper reviewed positively and thus published; this is the case whether or not the paper tries to give the impression that it reports research in an objective, disinterested way. In order to successfully present their case, authors argue in a goal-directed and prototypical way about problem-solving activities -- their own and other researchers'. Swales identified prototypical rhetorical building plans of introduction sections, along with linguistics surface cues that signal rhetorical moves. Examples for rhetor-44 ical moves include the claim that the paper addresses a new problem or, if it is a"]},{"title":"well-known","paragraphs":["problem, then the presented solution has to be better than that of other researchers.","A first analysis of the corpora confirmed many of the rhetorical building blocks suggested by Swales. We adapted Swales' scheme to the one shown in Figure 8 (at the end of this paper). In the medical corpus, almost all of the moves we found were of type I"]},{"title":"(Explicit mention);","paragraphs":["the rigid document structure seems to have replaced much of the \"argument about problem-solving activities\" (types II-V) for which we found ample evidence in the computational linguistics corpus.","We are interested in identifying these moves automatically and shallowly in text, and we believe that this is technically feasible, because the stereotypical, predictable overall structure of the argument can be exploited in doing so. 3 Meta-discourse markers In this paper we focus on the linguistic realisations of rhetorical moves, i.e. the surfacy signals of the argumentative status of a given sentence. Consider the strings in boldface on the right hand side of Figure 8. They cover the activities of reporting about research (reporting and presenting verbs), the problem-solving process (problems, solutions, tasks); they also include other semantic links like necessity, causality and contrast. Due to explicit or implicit argumentation, many of these strings are evaluative"]},{"title":"(\"efficient, elegant, innovative, insight.","paragraphs":["ftd\" vs."]},{"title":"\"impossible, inadequate, inconclusive, insufficient\").","paragraphs":["We call them"]},{"title":"meta-comments","paragraphs":["because they"]},{"title":"talk about","paragraphs":["information units, as opposed to being sub-","ject matter (scientific content). Such meta-comments","are very frequent in our collection.","Our meta-comments are similar Paice's (1981)"]},{"title":"indicator phrases","paragraphs":["(he was the first to use such phrases for abstracting); they are less similar to"]},{"title":"cue phrases,","paragraphs":["the discourse markers usually studied in discourse analysis, because they are"]},{"title":"not","paragraphs":["sentence connectives (with some exceptions), and because they are typically considerably longer and far more varied."]},{"title":"The","paragraphs":["fact that the computational linguistics texts stem from an unmoderated medium (i.e. they are neither chosen for publication nor edited by a central authority), means that there were no external restrictions on how exactly to say things. Authors use idiosyncratic style, which can vary from formal to informal. There are meta-comments that tend to get used in a fixed, formulaic way, but interestingly, we observed a wide range of linguistic variability with respect to the realization of some of the meta-comments (whereas their semantics is usually perfectly unambiguous). This effect makes the meta-commen~"]},{"title":"in this","paragraphs":["text type interesting linguistic","objects to study.","We observed that there are certain meta-comments","which are restricted to certain moves, mostly the evaluative and contrastive phrases and the phrases occurring in moves of type I"]},{"title":"(Explicit mention).","paragraphs":["Others occur frequently across moves, particularly general argumentative phrases and relevance markers such as"]},{"title":"\"important\", \"in this paper, we\".","paragraphs":["Argumentative phrases like"]},{"title":"\"we ar. 9ue that\"","paragraphs":["appeared with solution and problem-related moves almost as often as with claims and conclusions. These phrases seemed to be the ones that were most formulaic/fixed across texts.","Our goal is to automatically find meta-markers and associate them with rhetorical moves (where this makes sense). In the next section, we report on a first experiment in that direction. 4 Our experiment If it is true that most meta-comments are formulaic, recurring expressions, then frequency information should help us separate meta-comments from domain-specific parts of the sentence. Those strings which occur rarely in the corpus will most likely be domain-specific and will appear low on frequency listings of strings, whereas meta-commeats should appear high on the lists.","We also used a lexicon of 433"]},{"title":"lexical","paragraphs":["seeds. Lexical seeds are words which are semantically related to the activities of reporting, problem-solving, argument-ing or evaluating, or expressions of deixis ("]},{"title":"\"we... \")","paragraphs":["or other textual cues (e.g. literature references in text were marked up using the symbol [REF], which is a signal for mentions of other researchers' solutions, tasks or problems).","The computational linguistics corpus was drawn from the computation and language archive (h~p://xxx.lanl.gov/cmp-lg) and contains 123 articles; the 129 articles of the cardiology corpus appeared in the"]},{"title":"American","paragraphs":["Heart"]},{"title":"Journal.","paragraphs":["The medical corpus is smaller in overall size (436,909 words vs. 654,477; 14,770 sentences vs. 23,072).","For the computational linguistics corpus, we additionally had a collection of 948 sentences that had been identified as relevant by a human annotator in a prior experiment (Teufel & Moens 1997). A human judge annotated these with respect to the 23 rhetorical moves introduced in Figure 8. 4.1"]},{"title":"Filtering","paragraphs":["First, we compiled the two corpora into those bigrams, trigrams, 4-grams, 5-grams and 6-grams which did not cross sentence boundaries. We worked with a short stop-list compiled from the corpus (60 highest-frequent words) from which we had excluded those which we expected to be important in an argumentative domain, e.g. personal and demonstrative pronouns. We low-ercased all words and counted punctuation (including brackets) as a full word.","We then filtered the n-grams through our seed lexicon, i.e. we retained those expressions which contain at 45 least one of the words of the seed-lexicon (or a morphological variant of it). We also compiled and counted n-grams for the 948 computational linguistics target sentences, to see how similar these phrases from the annotated parts were to the filtered or unfiltered bigrams from the entire corpus.","Before filtering 354 [ref], [ref] ,"]},{"title":"3or [ref] ),","paragraphs":["297 , [ref], [ref]"]},{"title":"tTs Ccrefl ).","paragraphs":["144 [ref], [ref] . 139 on the other hand 134 for example , the 118 in this paper, 116 the other hand , 111 Â• in ( [cref] ) 110 can be used to","After filtering","118 in this paper,","111 in ([cref])","110 can be used to","106 in figure [cref] .","100 ( [cref] ) ,","99 on the basis of","83 shown in figure [cref]","83 in section [cref] .","75 this paper, we","75 in section [cref] ,","71 it is possible to Figure 2: 4-grams in entire CL corpus","The list of 4-grams for the computational linguistics corpus shows a typical picture of the outcome of this process (Figure 2). Before filtering, the frequent corpus n-grams contain general comments and expressions like"]},{"title":"\"for example\"","paragraphs":["but content specific expressions are already filtered out. (Very rarely, there are some content-specific phrases like"]},{"title":"\"natural language ~'","paragraphs":["in the lists -- this is due to the fact that the corpus, even though interdisciplinary in nature, is composed of papers focusing on language.) After filtering, the meta-comments on the lists have two properties: (a) they are frequent (b) they contain lexical items that"]},{"title":"could","paragraphs":["be related to argumentation about problems, research tasks, solutions -- in the computational linguistics corpus, these conditions seem to be enough to produce expressions that are good candidates for meta-comments. Unfortunately, condition (a) means that a large number of recta-comments were lost, because they were of low frequency.","Target sentences 49 in this paper , [0 can be used to 37 this paper , we 9 in this paper is 25 Cref], [ref], 7 [ref], [ref]. 22 , Ire.f], Cref] 7 described in this paper 18 in this paper we 7 this paper ~ i Figure 3: 4-grams in CL target sentences","How similar are the lists for annotated text and entire corpus in the computational linguistics domain? Table 3 shows that they look very similar apart from minor differences, e.g. the fact that the list gained from annotated data contains more [CREF] items (internal cross reference like"]},{"title":"~section [CREF]\")","paragraphs":["which tend to appear frequently in the sentences where authors state the organisation of the paper. The expressions used in such organisation statements are typically formulaic and re-","Before filtering","120 p < 0.05 )","102 p<0.0t)","93 left ventricular ejec-tion fraction","86 p < 0.001 )","86 p < 0.0001 ) 85 on the basis of 72 at the time of 66 of this study was 64 in this study , 59 95 % confidence in-","terval 58 this study was to","55 coronary artery dis-","ease .","49 acute myocardial in-","farction .","~fthr filterin~ 85 on the basis of 66 of this study was 64 in this study , 58 this study was to 57 patients with heart","failure 45 the purpose of this 44 there were no signifi-","cant 40 new york heart asso-","ciation 39 purpose of this study 35 there was no signifi-","cant 32 were no significant","differences 31 p = not significant 30 has been shown to Figure 4: 4-grams m medical corpus current, but not"]},{"title":"many","paragraphs":["of these sentences were considered relevant when the 948 target sentences were determined.","The medical corpus shows significant differences (cf. Figure 4). Firstly, unfiltered bigrams do not separate content matter from meta-discourse. In these lists, there are phrases pertaining to statistical analyses (\"p <"]},{"title":"0.0l\")","paragraphs":["and several domain-specific phrases. Filtering (right hand side of Figure 4) forces the few meta-comments that are being used to the top of the list; they are linguistically invariant. For instance,"]},{"title":"\"studf","paragraphs":["seems the only acceptable expression used for the current research, whereas the range is much wider in the other corpus ("]},{"title":"\"paper, article, study, work, research... \",","paragraphs":["and all the meta-comment candidates in the top part of the list belonged to one single rhetorical move, viz."]},{"title":"Explicit Mention","paragraphs":["of the Research"]},{"title":"Task","paragraphs":["(Ex-T in Figure 8). A certain amount of noise has been introduced through the seed-lexicon because word senses were not","disambiguated:"]},{"title":"'failure\"","paragraphs":["was included in the seed lexicon to indicate mentions of failure of other researchers' solution. Because this term obviously has the different meaning of"]},{"title":"\"heart failure\"","paragraphs":["in the cardiology context, the desired distinction between subject matter strings and recta-comments got lost; similarly"]},{"title":"\"New York\"","paragraphs":["was included because the word"]},{"title":"\"new\"","paragraphs":["could potentially point to novel approaches. This might mean that it is necessary to use different stop-lists and/or seed lexicons for different domains.","As we have seen before, associating meta-comments with rhetorical moves is a more difficult task for some meta-comments than for others. We tried to anchor the probable rhetorical move of a phrase in the lexical seed it contains, a simplification we are forced to make due"]},{"title":"46","paragraphs":["to the small amount of annotated text we have available (which is reflected in the low numbers). We are thus in the process of working on a larger scale annotation.","We used the human judgements to count how often each word contained in the target sentences appears with a certain rhetorical move. If the difference in frequency between the best-scoring moves for that word was large enough, we assumed it was a good indicator for the highest-scoring move, and we then manually associated the given rhetorical move with the word if it was contained in the seed lexicon, or to semantically similar seeds. For example, seeds that are the most likely associated with the OWN SOLUTION BETTER (53 examples of this move in the target sentences) were"]},{"title":"\"than\"","paragraphs":["(39),"]},{"title":"\"better\"","paragraphs":["(36),"]},{"title":"\"results\"","paragraphs":["(21),"]},{"title":"\"method\"","paragraphs":["(19),"]},{"title":"\"using\"","paragraphs":["(15),"]},{"title":"\"significantly\"","paragraphs":["(14),"]},{"title":"\" outperforms\"","paragraphs":["(12) and"]},{"title":"\"more\"","paragraphs":["(12). Filtered meta-comments are then assigned the rhetorical move predicted by the first seed they contain. Figure 5 shows the meta-comments filtered for the seed"]},{"title":"\"better\"","paragraphs":["from both corpora. In the medical corpus, there is less argument about methodology/solutions, and as a result the phrases found are unfortunately not meta-comments but contain medical terminology.","Computational","Linguistics 64 better than 50 a better 23 better than the 20 much better 19 the better 19 is better 16 significantly better 16 be better 13 better performance 11 are better 10 better the 9 significantly better","than 6 better suited","6 better than that of ,= Cardiology It a better 7 better than 6 to better 6 significantly better 6 better in 5 better left 5 better left ventricu.lar 5 and better 4 better preserved 4 better in smokers 3 to be somewhat bet-","ter tolerated 3 failure symptoms in","spite of better 3 better preserved left","ventricular systolic","function 3 better in smokers","than in nonsmokers Figure 5: Potential meta-comments with"]},{"title":"\"better\"","paragraphs":["from both corpora","Also, we observed that it is not easy to predict the optimal length of a certain meta-comment which is indicative of a certain rhetorical move. For moves containing"]},{"title":"ocher problems/solutions~tasks","paragraphs":["the very short string"]},{"title":"\"[REF]\" is","paragraphs":["contained in all successful meta-comments, whereas for explicit mention of research goals, the maximal length 6 of meta-comments which we chose for these experiments might even be too short. As another example, for the STEP move (\"goa/is"]},{"title":"achieved by doing solution\"),","paragraphs":["the best indicator we found was"]},{"title":"\"in order to\".","paragraphs":["4.2 Evaluation We evaluate the quality of these automatically generated meta-comment lists by comparing them to a manually created meta-comment list used by a summarisation system, cf. (Teufel & Moens To Appear). The performance of the system - with the two different meta-comment lists - is measured by precision and recall values of co-selection with the target extracts defined by human annotators mentioned earlier. The summarisation process consists of two consecutive steps, sentence extraction and rhetorical classification, and uses other heuristics like location and term frequency.","The summarisation system requires a list of meta-comments of arbitrary length, containing a"]},{"title":"quality","paragraphs":["score for each phrase which estimates how predictive these phrases are in pointing to extract-worthy sentences, and the most likely rhetorical label that sentences with this meta-comment will receive. Quality Class 2 3 2 3 2 I i 3 2 2 I I I i t 2 i I I I I I I i Rhetorical Move STEP Ex-T Co-S Ex-T Ex-T Ex-P Ex-T Ex-T Ex-T Ex-C Ex-T Ex-T Ex-E NEc-S-T Co-C Ex-T Meta-comment paper , this paper presents a in order to in this paper , we will in this paper we have unlike [ref] this paper is to in this paper , we describe paper is paper we this paper has presented , we propose a method in p~sage ( [cref] , we argue that argue that method for we show that the show how property and the number the advantages of the wall street journal the importance of however , we be used to Figure 6: Extracts from automatic list of meta-comments","We automatically built the meta-comment list in Figure 6 (containing 318 entries). We started from all n-grams compiled from the target sentences and took the following heuristics into account: Firstly, choose phrases with a high ratio of target frequency to corpus frequency, because these are"]},{"title":"indicative","paragraphs":["phrases. Set the quality value accordingly. Secondly, exclude phrases with a low overall frequency, or decrease their quality score, because including/overestimating them might construct a model that is over-fitted to the data. Thirdly, associate each phrase with its most likely 47 rhetorical move, by taking the ratio between frequency in each rhetorical class and the frequency of the rhetorical label itself into account. If below a certain threshold, don't associate any move at all (e.g. \"paper ,\" in Figure 6).","The manual meta-comment list, in contrast, was compiled in an extremely labour intensive manner and refined over the months. It consists of 1791 meta-comments (some of which are much longer than the maximum of 6 words that the automatic phrases consisted of), along with their most plausible rhetorical moves and quality scores. Extraction Classification Manual Automatic 66.4% 52.5% 66.3% 54.3% Figure 7: Evaluation results: precmon and recall of co-selection","As Figure 7 shows, using the automatic meta-comment list instead of the manually created one decreased the summarizer's performance from 66.4% to 52.5% precision and recall for extraction, and from 66.3% to 54.3% precision and recall for classification."]},{"title":"5 Discussion and further work","paragraphs":["The evaluation indicates that the quality of the automatic meta-comment list is not yet high enough to replace the manual list in our summarization system. However, a look at the automatic list itself shows that, even though it is far from perfect, most of the high-frequent strings found are plausible candidates for recta-comments (or parts of recta-comments). In most cases, subject matter can be successfully filtered out.","We regard the simple method for automatic meta-comment identification discussed in this paper as a baseline for further work. We have simplified the problem of finding meta-comments enormously by only considering verbatim substrings. By doing so, we have ignored discontinuous strings, morphological variation and statistical interaction between the words in the string. In addition, the phrases considered so far have been short, and we have not collected many of them, because we wanted to rely only on the ones with reasonably high frequencies.","The biggest problem for now is that highly indicative, but infrequent recta-comments cannot be found with a simple method like ours. Therefore, it is essential to perform some generalization over similar phrases. One way would be the automatic clustering of similar concepts, e.g. to find out that \"argue\" and \"show\" are presentational verbs with similar semantics. Another idea would be to allow for more flexible patterns consisting of short n-grams and other words, in order to skip over intervening words like adjectives and adverbs. This might avoid the data sparseness problems encountered with the longer n-grams."]},{"title":"6 Summary","paragraphs":["We have presented some baseline results from our ongoing work concerning automatic filtering of meta-comments (indicator phrases) from scientific papers. Meta-comments can vary considerably from one domain to another, as the comparison of the two corpora we considered shows. In the computational linguistics articles, authors argue explicitly about problems, solutions and research tasks, whereas this is less the case in the medical domain, where meta-comments are less frequent and more formulaic.","We have shown that lists of recta-comments acquired in a simple automatic process can be used to automatically identify a shallow document structure in scientific text, albeit with a certain quality loss when compared to manually constructed resources."]},{"title":"7 Acknowledgements","paragraphs":["Data collection of the computational linguistics corpus took place collaboratively with Byron Georgantopolous. We thank Kathy McKeown. Vasileios Hatzivassiloglou and Olga Merport from Columbia University, NY, for kindly allowing us to use their cardiography corpus."]},{"title":"References","paragraphs":["Brandow, R.; Mitze, K.; and Rau, L. F. 1995. Automatic condensation of electronic publications by sentence selection. Information Processing and Management 31(5):675-685.","Hovy, E. H., and Lin, C. Y. 1997. Automated text summarization in SUMMARIST. ACL/EACL-97 Workshop 'Intelligent Scalable Text Summarization'.","Kintsch, W., and van Dijk, T. A. 1978. Toward a model of text comprehension and production. Psychological Review 85{5):363-.-394.","Kircz, J. G. 1991. The rhetorical structure of scientific articles: the case for argumentational analysis in information retrieval. Journal of Documentation 47(4):354-372.","Kupiec, J.; Pedersen, J. O.; and Chen, F. 1995. A trainable document summarizer. In Proceedings of the 18th ACM-SIG IR Conference, Association for Computing Machinery, Special Interest Group Information Retrieval, 68-73.","Salager-Meyer, F. 1992. A text-type and move analysis study of verb tense and modality distributions in medical English abstracts. English for Specific Purposes 11:93-113.","Sparck Jones, K. 1994. Discourse modelling for automatic summarising. Technical report TR-290, Computer Laboratory, University of Cambridge.","Swales, J. 1990. Genre analysis: English in academic and research settings. Cambridge University Press.","Teufel, S., and Moens, M. 1997. Sentence extraction as a classification task. In ACL/EACL.97 Workshop 'Intelligent Scalable Text Summarization', 58---65.","Teufel, S., and Moens, M. To Appear. Argumentative classification of extracted sentences as a first step towards flexible abstracting. In Mani, I., and Maybury, M., eds., Advances in automatic text summarization. MIT Press."]},{"title":"48 I. Introduction of problems, tasks,","paragraphs":["solutions by explicit mention","Ex-T Own research task The aim of this paper is to examine the role that training plays in the","tagging process.","Ex-S Own solution The basic idea for the analysis can be seen as a logical counterpart at the","glue level of the standard type assignment for generalized quant/fiers [REF].","Ex-O-S Other solution The traditlonal approach has been to plot isoglosses, delineating regions","where the same word is used for the same concept. Ex-C Ex-O-C Ex-E Ex-R Own conclusions/ claims Other conclusions/ claims Own evaluation methodology Own (numerical) results In our corpus study, we found that three types of utterances (prompts, repetitions and summaries) were consistently used to signa/control shifts. It has often been stated that discourse is an inherently collaborative process. In this section we evaluate the performance of the methodology implemented by predicting succeeding words by using preced/ng words. The evaluation of the accuracy of the rhetorical structure analysis carried out previously ([REF]) showed 74 %. II. Contrastive introduction of problems, tasks, solutions Co-S Co-C Contrast between, own and other solutions Contrast between own and other tasks/ problems Contrast between own and other claims In this paper, we argue that instead of applying the arbitration process to the discourse level, it should be applied to ~he beliefs proposed by the discourse actions. Unlike most research in pragmatics that focuses on certain types of prey suppositions or implicatures, we provide a global framework in which one can express all these types of pragmatic inferences. Despite the hypothesis that the free word order of German leads to poor performance of low order HMM taggers when compared with a language like Engfish, we have shown that the overall results for German are very much Mong the lines of comparable implementations for English, if not better. III. Attribution of properties to IMP-T HAR.D-T NEw-T GOoD-S BAD-O-S Own research task is important Own research task is hard Own problem/ research task is new Own solution is advantageous Other solution is flawed problems, tasks, solutions The last decade has seen a growing interest in the application of machine learning to d/fferent kinds of linguistic domains. One of the difficult problems in machine translation from Japanese to English or other European languages is the treatment of articles and numbers. No formal framework has been proposed, to our knowledge, to regulate the interaction between regular and exceptional grammatical resources. First, it is in certain respects simpler, in that it requires no postulation of otherwise unmotivated ambiguities in the source clause. However, we argue that such formalisms offer little help to computational linguists in practice. IV. Functional relations between SOLVES Own solution solves own Avoms STEP NEc-S-T NoTSO NEw-P research task Own solution problems avoids Own solution is a step towards research task Own solution necessary to achieve research task Other solution does not solve problem/ task Other solution introduces new problems problems, tasks, solutions This account also explains s/m/Mr differences m felicity for other coordinat-ing conjunctions as discussed in [REF]. We have introduced a simple, natural definition of synchronous tree-adjoining derivation that avoids the expressivity and implementability problems of the original rewriting definition. We have thus developed an evaluation heuristic that combines several different measures, in order to select the parse that is deemed overall \"best\". We have argued that obligations play an important role in accounting for the interactions in diMog. Dependency grammar runs into substantial difficulty trying to account for the proform one. Specifically, ifa treatment such as [REF]'s is used to explain the forward progression of time in example [CREF], then it must be explained why sentence [CREF] is as felicitous as sentence [CREF]. V. Direct BETTEa-S HARDEa-T comparison of problems, tasks, solutions Own solution is better We found that the MDL-based method performs better than the MLE-than other solution based method. Own research task is ... disambiguating word senses to the level of l~ne-grainedness found in Word-harder than other task Net is quite a bit more difficult than disambiguation to the level of ho-","mographs [REF], [REF].","Figure 8: Rhetorical moves in scientific papers; examples from our corpus of computational linguistics 49"]}]}
