{"sections":[{"title":"Word-Sense Distinguishability and Inter-Coder Agreement * Rebecca Brucet and Janyce Wiebet t Department of Computer Science University of North Carolina at Asheville Asheville, NC 28804-8511 tDepartment of Computer Science New Mexico State University, Las Cruces, NM 88003 bruce@cs.unca.edu, wiebe@cs.nmsu.edu Abstract","paragraphs":["It. is common in NLP that the categories into which text is classified do not have fully objective def initions. Examples of such categories are lexical distinctions such as part-of-speech tags and word sense distinctions, sentence level distinctions such as phrase attachment, and discourse level distinc t.icms such as topic or speech-act categorization. This p>1per presents an approach to analy?-ing the agrcen1ent arnong lnnnan judges for the purpose of formulating a refined and more reliable set of category designations. We use these techniques to analyze the sense tags assigned by five judgps to the noun intcr·est. The initial tag set is takmi from Longman's Dictionary of Contemporary }i:nglish. Through this process of analysis, we automatically identify and assign a revised set of sense tags for the data. The revised tags exhibit high reliabil ity as measured by Cohen's r;.. Such techniques are important for formulating and evaluating both human and automated classification systems."]},{"title":"Introduction","paragraphs":["It is common in Natural Language Processing (NLP) that the categories into which text is classi fied do not have fully objective definitions. Exam ples of such categories are lexical distinctions such as part-of-speech tags and word-sense distinctions, sentence level distinctions such as phrase attach ment, and discourse level distinctions such as topic or speech-ac.t categorization. This paper presents an approach to analyzing the agreement among hu man judges for the purpose of formulating a refined","1~his research was supported by the Of-fice of Naval Re search under grant number N00014-95-l-0776. 53 and more reliable set of category designations.","We performed a case study of the classification process, involving multiple judges performing a word-sense disambiguation task. Table 1 presents the data for two judges assigning one of six senses to each instance of inter-est used as a noun in the corpus. The data is represented as a contingency table, often referred to as a confusion matrix; it depicts the \"confusion\" among the judges' classi fications. Evidence of eonfusion among the classi fications in Table 1 can be found in the marginal totals, ni+ and n+.i> where i and j range h·om 1 to 6. We see that, on average, judge A has a higher preference for senses 1 and 3 than judge E does, while judge E has a higher preference for sense 2 than judge A does. These biases are one aspect of agreement (or the lack of it) among judges.","A seeond aspect of agreement is the extent to which judges agree on the tags of individual words (mtegory distinguishability). We see from the diag onal frequencies in Table 1 that these judges agree on 2097 out of 2369 of them, which is 88.5% of the individual tags.","Cohen (1960) proposed the coefficient of agree ment, r;, for measuring the agreement between two judges. r; compares the actual agreement to that which would be expected if the decisions made by each judge were statistically independent (i.e., \"chance agreement\"). A number of previous stud ies have used r; to evaluate inter-coder reliability (e.g., Carletta 1996, Litman & Passonneau 1995; Moser & Moore 1995; Hirschberg & Nakatani 1996; Wiebe et al. 1997). However, in looking at agree ment among judges, we are often not as concerned with describing how well two particular judges sense 1 \"readiness to give attention)) sense 2 \"quality of causing attention to be givenn sense 3 ''activity~ subject, etc., which one gives","time and attention ton sense 4 \"advantage, advancement~ or favorn sense 5 \"a share (in a company, business, etc.)\" sense 6 \"money paid for the use of moneyn Figure 1: Noun Senses of Interest in LDOCE agree as in measuring how well any observer can distinguish the categories from one another. In other words, the issue is the precision of the clas sification pTocess.","In this paper, we present a study of a classi fication process. The section Agreement Among Judges presents an analysis of the patterns of agreement among the judges. Agreement is a function of the differences among the judges (i.e., their biases) and the distinguishability of thecate gories themselves. We study bias using the models for symmetry, marginal homogeneity, and quasi independence (in the subsection Observer DijjeT ences). We study category distinguishability us ing Darroch & McCloud's (1986) degree of distin guishability, O;J (in the subsection Category Dis tinguishability). Guided by these analyses, in tho section Modification of the Classification Process we investigate modifications to the classification process that improve reliability. We analyze the effects both of removing judges and collapsing cat egories. A technique is presented for formulating a tag set which can be automatically derived from the original tag set. The technique is successful in the study presented here: the derived tag set yields improved reliability, as measured by Cohen's \"· The Data The classification process performed in this study involved five human judges independently assign ing sense tags to 2369 instances of the noun interest taken from the Wall Street Journal Treebank Cor pus (Marcus et al. 1993). The senses given to the taggers, shown in Figure 1, are from the Longman's Dictionary of Contemporary English (LDOCE).","The annotation instructions were minimal. They were asked to usc their judgment in assigning to each usage of interest the single tag that best characterizes its meaning. It is likely that more 54 explicit tagging instructions including examples and default rules would improve agreement among judges. Indeed, an analysis of the classification process such as performed here could be used to formulate and interactively revise a set of tagging instructions, but this application is not considered here.","Five human judges, referred to as A through E, participated in the study. Two of the judges (judges C and D) were involved in the project and had participated in previous sense tagging exper iments. The remaining three judges (judges A, B and E) were not members of the project and had no previous background in NLP or linguistics."]},{"title":"Agreement Among Judges","paragraphs":["All of the techniques that we present for the analy sis of agreement are appropriate for category classi fications assigned to multiple objects (in this case, words) by two juclges. 1","We analyze t.he agreement among all five judges by evaluating the agreement between all pair-wise combinations of these judges. We exclusively use maximum likelihood estimates of model parameters. The Basics Tables 1-5 present half of the data, in con tingency table format:. Each table is for one pair-wise combination of the five judges. The rest of the data, for the other five combina tions, is available on the World Wiele Web at http:"]},{"title":"I I","paragraphs":["crl. nmsu. eduiResearchiProjectslgmphling. In each table, the rows correspond to the senses assigned by the first judge while the columns cor respond to those assigned by the second judge. Let nij denote the number of words that judge one clas sifies as i and judge two classifies as sense j. If we let Pii be the probability that the judges will agree that a randomly selected usage is sense i, then Lip;; is the total probability of agreement across all senses. Pii can be estin1ated as 2!i.i... (a 1naxirnum n++ likelihood estimate), and the total probability of agreement can be estimated as Li Pii = L; ~';~, where n++"]},{"title":"=","paragraphs":["Lij n;J"]},{"title":"=","paragraphs":["2369. 1 Several of these techniques are also applicable to the","analysis of multiple judges.","The simplest measure of agreement is the esti mated probability of agreement, i.e., L,;f!;;, where the possible values are afiected by the marginal totals (i.e., the row and column totals). Cohen's K. compares the total probability of agreement to that expected if the ratings were statistically inde pendent (i.e. 1 \"chance agreernent\"). That value is then normalized by the maximum possible level of agreement given the marginal distributions. The marginal distributions can be estimated from the marginal counts as:"]},{"title":"fh+ = ,nq. and fi-f-i","paragraphs":["!!±L n++ n++","The complete formulation of K. is:","K = L-di;;- L,;fi;+P+i 1 - L,; Pi+P+i"]},{"title":"(1)","paragraphs":["K is 0 when the agreement is that expected by chance, and is 1.0 when there is perfect agreement.","An extension of K. for the case of multiple judges (three or rnore) is presented in Davies and Fleiss (1982) and used in this study. Analyzing Patterns of Agreement In a classification experiment, the two judges are assumed to classify any given usage independently of each other, but it is clear in the formulation of K that we expect the data to exhibit depe,nllence, i.e., Ji;1"]},{"title":"i","paragraphs":["Pi+ x J3c.1· Where docs this dependence come from? II; ari;;es from three factors and their possible interactions: (1) the heterogeneity of the objects being classified (i.e., the usages of interest), (2) the heterogeneity of the judges, and (3) the distinctions made in the category definitions.","We focus on the latter two factors and their in teraction. Rather than simply measuring agree ment we measure the contributions to agreement made by these two factors and propose changes to the classification process based on the analy sis. Just as overall agreement can be assessed as a function of the counts in the pair-wise confusion matrices, so can the measures of observer· differ· ence (bias) and category distinguishability. 0 bserver Differences (Bias) The hypothesis of no difference between two judges is the hypoth esis of complete symmetry (Sym in Table 6), that is,"]},{"title":"Pi,. = f3 1·i or","paragraphs":["!~~ = 1 for all i, j. If this ratio equals . PJ' one for all i, j then it follows that the observers' in-terpretations are indistinguishable. 55","Complete symmetry implies marginal symmetry, that is, Pi+ = P+i· Bias of one judge relative to another is evidenced as a discrepancy between these marginal distributions. BiaB decreases as the marginal distributions becorne nwre nearly equiva lent. The measure of bias is the test for marginal homogeneity (M.H. in Table 6), Pi+= P+i for all"]},{"title":",,","paragraphs":["It is possible to access the similarity of two judges even when there is evidence of bias. The model for quasi-independence ( Q.I. in Table 6) (Bishop et a!. 1975) tests whether two judges' de cisions are independent if we consider only the of!~ diagonal counts--the counts corresponding to dis agreement (i.e., Pij = Pi+ x P+j for i"]},{"title":"i j).","paragraphs":["Quasi independence holds when, given that the judges disagree, there is no pattern of association in the categories they assign.","In the tests for symmetry, marginal homogene ity, and quasi-independence, a model is formu lated that enforces the hypothesized constraint, e:g., Pij = P.ii in the case of symmetry. The degree to which the data is approximated by a model is called the fit of the model. In this work, the flt of each model is reported in terms of the. likelihood ratio statistic, G2",", and its significance. The higher the G 2","value, the poorer the flt. The fit of a model is considered acceptable if its reference significance level is greater than 0.001 (i.e., if there is greater than a 0.001 probability that the data sample was randomly selected from a population described by the model). Category Distinguishability The ratio Tij = fi;; xfi;; referred to as the diagonal cross-product-Vii XPjj' · ratio, represents the odds for disagreement over agreement on categories i, j. Darroch and"]},{"title":"Mc","paragraphs":["Cloud (1986) define the degree of distinguishability, Oij, for categories i, j as:","Pij X Pji O;j"]},{"title":"=","paragraphs":["1 - Tij"]},{"title":"=","paragraphs":["1 - ' ' (2) Pii X PJJ If Oij = 1, we say that the categories are completely distinguishable, and, if li;j = 0, they are completely indistinguishable. Majority Consensus When multiple judges are involved in a study, it is possible to formulate a majority tag for each object, that is, the tag that the majority of the judges assign to each object. It represents majority opinion and is useful in iden tifying outlyers, as shown in the next section."]},{"title":"Results","paragraphs":["Table 6 presents the results of the tests for ob server differences and Table 7 presents the mea sures of category distinguishability. All evaluations are performed on each pair-wise confusion matrix. The eolumns labeled MIA through MI.E refer to similar tables comparing the majority tag to the assignments made by each judge (e.g., judge A, in the case of MIA). These tables are not included in the paper.","While the r; values in Table 6 are reasonably high, the judges display bias and cannot. be con sidered interchangeable. The only exception is the strong similarity between the majority tag and the assignments made by judge C (i.e., the column la beled MIG in Table 6); these tags are symmet ric and unbiased. Among the five judges, the most similar are judges C and D, the two ex perienced judges. While their scores for symme try and marginal homogeneity are not significant, indicating· a relative bias, their score for quasi independence is significance (i.e., 0.004"]},{"title":">","paragraphs":["0.001,","the cutoff we use to judge significance). This indi","cates that, although judges C and D are not indis","tinguishable, there is no systematic difference of","opinion between them. Judge D also shows some","similarity to the majority tag. The judge that. is least similar to the others is","judge E; this is particularly evident when judge E","is compared to the majority tag. The distinguishability, oi.i, of all pair-wise eombi","nat.ions of tags arc evaluated in Table 7. All scores","are at. or near the maximum of 1.0, with the ex","ception of those measuring the distinguishability","of tags 1 and 2. It. is particularly low in Table AlB","(i.e., Table 2)."]},{"title":"Modification of the Classification Process","paragraphs":["Based on the results presented above, we modified the classification process in two ways: (1) judge E is removed, and (2) sense tags 1 and 2 are conflat.ed 56 to form a single sense distinction. The poor marks for distinguishability between these senses seem to be reflected in a closeness in meaning (see in Figure 1), supporting the decision to conflate them.","Removing judge E from the study removes the tables with the lowest r; scores. As a result, the agreement among all judges inereases from 0.874 to 0.898, as measured by Davies and Fleiss' extension of r;.","The process of conflating two tags is accom plished using the latent class model (Goodman 1974)2","This procedure has historically been used to identify a set of latent categories that explain the interdependencies among the observable cat egories. In this case, the observable categories are the sense tags assigned by the remaining four judges, while the latent categories correspond to the unobservable true meanings of the noun inter est. Once the desired number of latent. categories has been specified, these categories are assigned via the EM algorithm as described in Goodman (1974) and applied in Pedersen & Bruce (1997) 3",".","Using the EM algorithm as described above, all usages of interest are assigned to one of five latent sense groupings. The mapping between the derived (i.e., latent.) categories and the observed senses is established to maximize the correlation between latent categories and observed senses. This corre lation for each judge, is estimated as part of the process of assigning latent. categories. As an ex ample, Table 10 presents the correlation for judge C. The values recorded in the table are the proba bilities of judge"]},{"title":"C","paragraphs":["assigning sense tag i and the EM algorithm assigning latent tag j. As can be seen, correlation is maximized when the map ping of observed tags to latent tags is as follows: 1 =? 1, 2 =? 1, 3 =? 2, 4 =? 3, 5 =? 4, and 6 =? 5. This mapping conflates senses 1 and 2 while leav ing all other senses intact. This corresponds to our expectations based on the study of agreement presented in the previous section. Using this map ping, the observer difference measures among the","2","Also referred to as the Naive Bayes model (Langley et","al. 1992).","3","This is a well known unsupervised learning alobserved","tagsgorithm; other notable references to this procedure are","Lazarfeld (1966), Pearl (1988), and AutoClass (Cheeseman","1990). 1 2","Judge 3"]},{"title":"c","paragraphs":["4 5 6 0.142 0.003 .000 0.001 0.001 0.000","Latent Tag","2 3 4 0.010 0.001 0.001 0.001 0 001 0.000 0.024 0.005 0.000 0.000 0.074 0.001 0.003"]},{"title":"o:ooo","paragraphs":[".206 0.000 0.000 0.000 Table 10: Tag Correlation for Judge C"]},{"title":"\"","paragraphs":["0 002 0.000 0.000 0.000 0.000 0.526 four judges for the latent tag set are presented in Table 8, and the distinguishability of latent tags is presented in Table 9. As compared to the origi nal classification process, the agreement among all judges increases from 0.874 to 0.916 for the revised tag set. with four judges.","Recent work has proposed various methods for pruning senses for word instances and tuning tag sets to a particular domain using corpus infor mation and existing linguistic knowledge sources (e.g., Yarowsky 1992, Jing et. al. 1997, Basili et al. 1997). We have presented an automatic method for refining a tag set. using an important additional source of information: the 1nanual annotations assigned by human judges. ."]},{"title":"f Conclusion There is increasing awareness of the need to Inan","paragraphs":["age the uncertainty inherent. in many classification systems. We have presented procedures that can be used to analyze and refine any classification sys tem that makes use of nominal categories. These techniques can be used to study and improve the reliability of human judges as well as refine catego rizations that can be applied automatically and, in the process, establish an upper bound on the accu racy of automatic classification, i.e., the agreement among the human judges. In future work, we will apply these techniques to the analysis and evalua tion of automated classification systems."]},{"title":"References [1]","paragraphs":["Bishop,"]},{"title":"Y.","paragraphs":["M., Fienbcrg, S., & Holland, P. (1975). DiscTete Multivar'iate Analysis: Theor·y and Practice. Cambridge: The MIT Press. [2] Basili,"]},{"title":"R.,","paragraphs":["Della Rocca, M,, Pazien,a, M. T. ( 1997). Toward a bootstrapping frame work for corpus semantic tagging. In PToc. SIGLEX Workshop on Tagging Text with Lexical Semantics, pp, 58-65.","[3] Carletta, J. (1996). Assessing agreement on classification taks: the kappa statistic. Com putational Linguistics 22 (2).","[4] Cheeseman, P. & Stutz, J. (1996). Bayesian Classification (AutoClass): Theory and Re sults. In Fayyad, Piatetsky-Shapiro, Smyth, & Uthurusamy editors, Advances in Knowl edge Discovery and Data Mining, AAAI Press/MIT Press. [5] Cohen,"]},{"title":"J.","paragraphs":["(1960). A coefficient of agreement for nominal scales, Ed'Ucational and Psych, Meas. 20: 37--46."]},{"title":"[6]","paragraphs":["Davies, M. & Fleiss, J. (1982). Mea suring Agreement for Multinomial Data BiometTics,38: 104 7-·1051.","[7] Darroch & McCloud. (1986). Category Dis","tinguishability and Observer Agreement. ' Austral. Jour·nal of Statistics, 28(3):371-388."]},{"title":"' [8]","paragraphs":["Goodman,"]},{"title":"L.","paragraphs":["(1974). Exploratory latent structure analysis using both identifi able and unidentifiable models. BiometTika, 61(2):215-231.","[9] Hirschberg, J. & Nakatani, C. (1996). A prosodic analysis of discourse segments in direction-giving monologues. In Proc. A CL-96, pp. 286-293.","[10] .ling, H., Hatzivassiloglou, V., Passonneau, R., and McKeown, Kathleen (1997). Inves tigating complementary methods for verb sense pruning. In Proc. SIGLEX Workshop on Tagging Text with Lexical Semantics, pp. 58-65.","[11] Langley, P., Iba, W. & Thompson, K. (1992). An analysis of bayesian classifiers. In Pro ceedings of the 10th National Conference on Artificial Intelligence, pp. 223·-228.","[12] Lazarfeld, P. (1966). Latent structure analy sis. In S. A. Stouffer, L. Guttman, E. Such man, P.Lazarfeld, S. Star, and J. Claussen (Ed.), Measurement and Prediction, New York: Wiley.","[13] Litman, D. & Passonneau, R. (1995). Com bining multiple knowledge sources for dis course segmentation. In Proc. 33rd Annual Meeting of the Assoc. for Computational Linguistics, MIT, pp. 130-143.","[14] Marcus, M., Santorini, B., & Marcinkiewicz, M. (1993). Building a large annotated cor pus of English: The Penn Treebank. Com putational Linguistics 19"]},{"title":"(2):","paragraphs":["313·330. [15] Moser, M. & Moore,"]},{"title":"J.","paragraphs":["(1995). Investigating cue selection and placement in tutorial dis courses. In Proc. 33rd Annual Meeting of the Assoc. for Computational Linguistics, MIT, pp. Ll0-143.","[16] Pearl, J. (1988). Probabilistic Reasoning In Intelligent Systems: Networks of Plausible Inference. San Mateo, Ca.: Morgan Kauf rnann.","[17] Pedersen, T. & Bruce, R. (1997). Distin guishing Word Senses in Untagged Text. Proceedings of the Second Conference on Empirical Methods in Natural Language Pro cessing {EMNLP-97}, August 1997.","[18] Wiebe, .J., O'Hara, T., McKeever, K., and Ohrstriim-Sandgren, T. (1997). An empirical approach to temporal reference resolution. Proc. 2nd Conference on Empirical Methods in Natur·al Langrwge Pmcessing (EMNLP-97}, Association for Computational Linguis tics, Brown University, August 1997, pp. 174-186.","[19] Yarowsky, D. (1992). Word-sense disam biguation using statistical models of Roget's categories trained on large corpora. In"]},{"title":"Proc.","paragraphs":["COLING-92. 58 sense1 sense2","Judge 1 sense3","= A sense4 senseS sertse6 sense1 nu 174 n21 -7 n31 -- 25 n41 - 3 not - 1 7l61 sense2","nl2 llo nn- 8 ns2 - 24 n42 - 1 ns2 - 1 n52- 0 Judge 2"]},{"title":"=","paragraphs":["E sense3 sense4 n\" 11 n14 8 n23- 1 n24 - 2 n33 - 40 ns4 - 12 n4s....:. 3 n44 - 156 nss- 6 ns4 - 12 n53 1 n64 2","sense5","n1s","nzs -1","n3s- 4","n4s- 8","ns5 - 474","-","n55 6 sense6 n16 2 nz6- 0 ns5- 3 n45 -1 ns6- 6","no, 1245 nl+"]},{"title":"=","paragraphs":["316 n2+ = 19 n3+ = 108 n4+ = 172 ns+"]},{"title":"=","paragraphs":["500 na+"]},{"title":"=","paragraphs":["1254 ', u"]},{"title":"'","paragraphs":["192 n+5 499 n+6 12o7 n++ = 2369 Table 1: Confusion Matrix for Judges A and E 1 2","Judge 1 3","=A 4 1 242 13 32 2","Judge 2 = B 2 3 4 5 37 21 7 8 2 1 1 1 5 53 i5 1 0 1 161 6 6 1 1 2 2 316 19 108 172","3 0"]},{"title":"zo","paragraphs":["16 458 3 5 500 0 0 1 1 6 1246 6 1254"]},{"title":"'","paragraphs":["292 44 97 201 480 J25o 2369 Table 2: Confusion Matrix for Judges A and B 1 2","Judge 1 3","= A 4 5 6 303","__;_~-42 4"]},{"title":"--;r","paragraphs":["5 368","Judge 2 = C 2 3 4 5 6 2 0 6 3 2 6 1 1 1 0 3 56 5 1 1 0 8 154 6"]},{"title":"·a--","paragraphs":["0 1 13 480 2 ·' 1 1 1 5 1241 12 67 18U 496- 1246 Table 3: Confusion Matrix for Judges A and C 1 2","Judge 1 3","="]},{"title":"c","paragraphs":["4 5 6","342","- 1","~----z-","~-----s-4 1 35~ .htdge 2 = D 2 3 4 5 6 1 2'- 2 12 9","10 0 0 0 1 1 48 12 3 1","- 1 0 160 7 1 u u u •~v 0 0 u 0 0 12_45","13 53 174 511 1260 Table 4: Confusion Matrix for Judges C and D 1 2","Judge 1 3","="]},{"title":"c","paragraphs":["4 5 6","1 i--206"]},{"title":"~~","paragraphs":["1 1 1 1 210","Judge 2 E 2 3 4 5 6 131 11 6 7 7 - 11 0 0 0 1","--6-- ~'42 13 2 3 1 5 164 8 1 0 4 7 481 3 0 0 2 1 1242 149 62 192 499 1257 316 19 108 172 500 1254 2369 368 12 67 180 496 1246 2369 368 12 67 180 496 1246 2369 Table 5: Confusion Matrix for Judges C and E 59 Test.","AlE AIC AID BIG BID BIE G]D C\\F ' lJII'"]},{"title":"'","paragraphs":["MIA MIB MIG WID ' MIE"]},{"title":"['''\".","paragraphs":["c{2","165 70 77 75 105 101 109 1G 226 214 81 84 22 39 212","f---..;\\~'9·- 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 o.ooo 0.000 0.000 0.102 0.001 0.000","lVi. Tl.:","cf2","150 30 47 58 69 79 90 37 213 210 64 42 15 39 206","Sig. 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 o.ooo 0.000 0.000 0.010 0.000 0.000","Q. 1 ..","c;2","151 143 79 61 94 81 186 12 135 120 67 82 34 25 120","Si_q. 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.004 0.000 0.000 0.000","0.~_;;~ 0.016 0.051 0.000","appa 0.825 0.866 1 0.903 0.882 0.873 .821 0. 51 . 85 0.819 02.2_9_ ONll .",".977 0.96 0.874 Table 6: Tests of Observer Differences (Bias) for Five Judges and Six Senses s~nscs","AlE AlB AIC AID BIG BID BIB 0","OlD GjE DIE 0","MIA MIB MIC MID MIE","1 2 0.422 0.006 0.989 0.986 0.765 0.662 0.183 1.000 1.000 1.000 0.990 0.675 1.000 1.00 1.000","1 3 0.960 0.948 1.000 0.997 0.959 0.964 0.950 1.000 .999 0.997 1.000 0.9(f8 1.000 1.000 0.999","1 • 4 0.999 0.999 1.0 0 0.999 0.999 0.999 0.999 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000"]},{"title":"r-l","paragraphs":["-- 5 1. 0 1. 00 1.000 1.0 1. 0 0 0 1.00 1.000 1. 00 1. 00 1. 1. 1. 00 1. 1.","-- (j","1. l. JOO 1.000 t.obo 1.000 1.00 1. 0 1.00 1. 00 l.O 1.00 1. 0 1.000 1.000 1. 00","2 - 3 0 J25 o:-%3 0.9!)1 0.978 0.0 0.979 1.000 1.00 1.( 00 l.QOO 0.988 0.966 1. 0 1.0 1.0 0","·-·-·","4 .998 1.00 1.000 1.0 1 0 0.999 1.000 1. 1.0 0 1.000 1.0 1.00 1. 0 1.0 0 2","~:~5}-7 5 1.000 1.000 1.000 - 1 OOl 1.000 ).0 1. 00 1.0 0 ).","00 1.000 1.0 0 1.000 1.000 1.00","..","2 - 6 1.000 1.000 1.000 1.000 1.000 1. 0 1.000 1.00 1. 1.000 1.000 1.000 1. 1. 0 1.","3 4. (j]J94 0.998 0.995 0.9U4 0.997 0.9~)4 0.986 0.995 0.991 0.993 0 .. 99 0.998 0.999 0.999 0.996","3 5 0.99\\) 0.99(1 1.000 1.000 1.000 1.000 O.ll94 1.000 1.000 0.(1(19 1.000 1.000 1.000 1.000 1.000","3 6 1.000 1.060 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000","4\" -- .\\ \"\"O.a9D .99\\l O.Sl9\\l o.9m 1.000 l\"'lTITif 0.999 ~ :~~6-","·o.H9' 1. 1 0 ~- \"\"\"\"T.(\"f\"O(f","1.0 0 1.000 4 ... (i"]},{"title":"::ogiT-","paragraphs":["1.0 1.000 Cooo 1.000 1.000 1 0 1--i.ooo 1.000 1.000 1.000 1.0 1 0 1.00","5 6 1.000 1.000 1.000 1. 00 I. 1.00 1.000 1.000 1. '0 ~ 1.0 1.000 1. 00 1.000",". -","Table 7: Measure of Cat~gory Distinguishability for Five .Judges and Six S~nses A IB AIC AID BIG BID CID MIA MIB MIG MID"]},{"title":"[f[--","paragraphs":["."]},{"title":"'","paragraphs":["56 63 63 72 70 44 72 72 17 36","S1g. 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.068 0.000","--~","c;'J 10 ;)9","52 38 53 37 57 43 7 29","Sig. 0.001 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.136 0.000","-·-----","(.(.G2.","72 08 50 46 23 37 00 37 20 19 Siy. 0.000 0.000 0.000 0.000 0.016 0.000 0.000 0.000 0.006 0.017 /{ tl]!l!!:....__ ~~· ~()8 _2-~l24 0.91 0.909 0.902 0.952 0.943 0.\\)26 0.978 0.964 Table 8: Tests of Obsnrver Differences (Bias) for Four Judges and Five Senses","AlB AIC >","AID BIG BID CID MIA MIB MIG MID","1 2 0.94 0.9H7 .994 0.957 O.H64 1. 1.0 0 0.968 1.000 1.000","1 3 1.000 0.999 0.999 .99 0.999 .. 1.000 1.000 1.000 1. ).","00","1"]},{"title":"'","paragraphs":["1 00 1.000 1.000 1.000 0.\\)99 1. 0 1. ).","0 -- 1. 0 1.000 1 5 1.000 1.00 1.000 ).","00 l.OOQ 1. 0 1.00 1.( 00 1. ). 2 3 0.998 O.\\l95 0.993 o.mn 0.004 0.995 0.999 0.998 1.000 0.997 2 4 0.>199 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1. 0 1.000 2 1.000 1.000 1.000 1.000 J .000 1.000 1. 0 1.000 1.000 1.000 3 .999 .\\199 O.>lH8 0.999 1. 1. 0 1.00 1.000 1.00 1. 0 3 ·- 5 1.0 1.0 1. 0 1.00 1. 00 1.00 1. 0 1. .00 1.000","·• 1.000 1.00 1 00 1.000 1. I. 0 1. 0 1. 0 1.0 0 1.","Table 9: Measure of Category Distinguishabilily for Fott!\" Judges and Five Senses"]},{"title":"60","paragraphs":[]}]}
