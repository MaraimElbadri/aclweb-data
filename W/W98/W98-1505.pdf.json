{"sections":[{"title":"Valence Induction with a Head-Lexicalized PCFG Glenn Carroll and Mats Rooth IMS, Universitiit Stuttgart","paragraphs":["{glenn,mats}@ims.uni-stuttgart.de"]},{"title":"Introduction","paragraphs":["Either directly or indirectly, the lexicon for a natu ral language specifies complementation frames or va lences for open-class words such as verbs and nouns. Constructing a lexicon of complementation fram<:~s for larg<-'! vocabularies constitutes a challenge of scale, with the further complication that frame usage, like vocabulary, varies with genre and undergoes ongo ing: innovation in a living language. This paper ad dresses this problem by means of a learning tech·· niquc baswi on probabilistic lexicalized context free grammars and the expectation-maximi~\";ation (EM) algorithm. Given a hand-written grammar and a text corpus, frequencies of a head word accompanied by a frame are estimated using the inside-outside al gorithm, and such frequencies are used to compute probability para.meters characterizing subcategoriza tion. The procedure can be iterated for improved models. \\Nc show that the scheme is practical for large vocabularies and accurate enough to capture differences in usage, such as those characteristic of different domains."]},{"title":"A grammar and formalism","paragraphs":["The core of t.he grammar is an"]},{"title":"X","paragraphs":["grammar (Jackend off [1977]) of phrases including noun phrases, preposi tional phrases, and verbal clusters. A representative verbal structure is given on the left in Figure 1. The symbol VF'C is read \"finite verb chunk''; similarly we work with noun chunks (Nc), prepositional chunks (Pc), and so forth. Our use of the ehunk concept follows Abney [1991], Abney [1995]. Categories are interpretable in terms of a feature decomposition, but are treated as atomic in the formalism. We depart from a standard context-free formalism in that heads are marked on the right hand sides of rules, using a prime (').","The grammar includes complementation rules for verbs, nouns, and adjectives. Complements are at tached at. a level above the chunk, which we call the phrasal level. For instance, the category VFP is ex panded as a finite verb chunk vrc and a sequence"]},{"title":"36","paragraphs":["of complements. This is illustrated on the right in Figure 1, where the VFC headed by decided takes a VTOP complement, the VTOC headed by emphasize takes an NP complement, and so forth.","Finally, the least standard part of the grammar is a large set of state or n-gram rules which form a parse without constructing a standard clause-level analy sis. Instead) phrasal categories are strung together with context-free rules modelling a finite state ma chine, \\vhcre the states are categories consisting of an ordered pair of phrasal categories. This results in right-branching structures, as illustrated Figure 2. Note that the entire tree on the right in Figure 1 could be substituted for the finite verb phrase VFP in the tree on the left in Figure 2. The st<:l.te rules allow almost all the sentences (about 97%) in the corpus to be parsed, at the price of not assigning linguistically realistic higher-level structure.","We now define headed context-free grammars in the sense employed here. Definition. A headed context free grammar is a tuple (N,T, W,L, R,s), where: (i) Nand Tare dis joint sets, interpreted as the non-terminal and ter minal categories respectively. (ii) VV is a set, in terpreted as the set of words. (iii) L is a relation between"]},{"title":"W","paragraphs":["and"]},{"title":"T,","paragraphs":["indicating the possible terminal categories {parts of speech) for a given word. {iv) The set of headed productions"]},{"title":"R","paragraphs":["is a finite subset of N x N' x (NUT) x N', such that each non-terminal occurs as the left hand side of some rule and each terminal occurs on the right hand side of some rule. (v) s E N, with the interpretation of a start symbol.","We typically use fi as a variable for mother cate gories, n for head daughter categories, and a and (3 for the category sequences flanking the head on the right hand side, so that (ii, a, n, (3) represents a rule. x is used as a variable for non-head categories.","A category"]},{"title":"n","paragraphs":["in N is a projection of a category n in NUT if there is some rule of the form {ii, a, n, (3). The set of lexicalized nonterminals N C W x N is the composition of .C with the transitive closure of the VFC VFP"]},{"title":"~-~ ~","paragraphs":["MD2 VBASE3 VFC VTOP"]},{"title":"/~ /~ L~ ~~~---","paragraphs":["ADV1 MD1 VHBASE2 VN3"]},{"title":"I has decided","paragraphs":["VTOC NP"]},{"title":"<~<~ to emphasize","paragraphs":["NC PP"]},{"title":"~-/\\ the festive spirit","paragraphs":["PC NP"]},{"title":"~ N1 of 6 I","paragraphs":["ADV"]},{"title":"I really","paragraphs":["MD VHBASE1 VN2"]},{"title":"shdu!d","paragraphs":["VHJASE"]},{"title":"A I","paragraphs":["ADV1 VN1"]},{"title":"I I","paragraphs":["VN"]},{"title":"I recovered I I have","paragraphs":["ADV"]},{"title":"I fully the times","paragraphs":["Figure 1: Illustrations of a finite verb chunk and complementation. projection relation. We have (w, n) eN if the word w can be the lexical head of the nontcrminal category n (in a complete or incomplete tree)."]},{"title":"Lexicalization and the probability model","paragraphs":["This section defines a parameterized family of proba bility distributions over the trees license_dfby a bead lexicalixed CFG. The main ideas on th'C parameter ization of a lexicalized context free grr-mmar which are employed here derive from Charniak [1995]; see also the remarks on lexic:alization in Charniak [1993 1 section 8.4].","The head marking on rules is used to project lexical items up a chain of categories. In the transitive verb phrase on the right in Figure 21 question is projected to the NP level) and asked is projectE~d to the VFP level. In this tree, the non-terminal nodes are lexi calized non-terminals) while the terminal nodes are members of £. The point of projecting head words is to make information which probabilistically condi tions rules and lexical choices available at the rele vant leveL At the top level in this example, the head asked is used to condition the choice of the phrase structure rule VFP"]},{"title":"-+","paragraphs":["YFC' NP as well as the choice of question, the head of the object.","We now define events which characterize choices of rules and of lexical beads. Definition. Given a grammar G = (N, T, W,"]},{"title":"£,","paragraphs":["R., s) with lexicalized non-terminals N, the set of rule events ER( G) is the set of tuples (w, n, a, n, f!) such that (w, n) is an el ement of"]},{"title":"N","paragraphs":["and (n, a, n, f!) is an element of R.. The"]},{"title":"37","paragraphs":["set of lexical choice events EL(G) is the set of tuples (w, n, x, v) such that (i) (w, n) and (v, x) are elements of"]},{"title":"N;","paragraphs":["1 (ii) in some rule of the form (h) a 1 n, {3), x is","an element of one or both of the category sequc_nces","o: and {3; and","By virtue of the length of tuples, ER(G) and","EL(G) are disjoint, and the union E(G) can be","formed without confusing lexical with rule events.","A head-lexicalized PCFG is represented as a func:","~tion mapping events to real numbers.","Definition. Let G be a headed context free gram","l:nar. A head-lexicalized probabilistic context free","grammar with signature"]},{"title":"G","paragraphs":["is a function p with do main E(G) and range [0, 1] satisfying the concli ticms:"]},{"title":"(i)","paragraphs":["Fixing any lexicalized non-terminal (1V, ft), \"i:.o:,n,fJPW,fl.,a,n,fJ::::: 1; (ii) Fixing any lexicalized non·· terminal (w, n) and possible non-head daughter :r,"]},{"title":"Lx,w","paragraphs":["Pw,11,x,w"]},{"title":"=","paragraphs":["l. Here the value of the function p on a rule event is written as Pw,n,a,n,J3, and on a lexical event as P~u,n,:c,w·","To assign probability weights to trees, we usc a tree-licensing and labelling interpretation of the grammar; a node in a tree analysis is labeled with event corresponding to the rule used to expand the node, and the list of lexical events for the non-head daughters of the node. Where r is a labeled tree li-","1","In the events, conditioning factors are ordered in the","way they are dropped off in the smoothing procedure de","scribed below. In a lexical event (w,"]},{"title":"n,","paragraphs":["x, v), the choice of the word v is conditioned on the parent lexical head w, the parent category"]},{"title":"n.,","paragraphs":["and the child category x. In the first smoothing distribution, the first conditioning factor, i.e. the parent head w, is dropped. START"]},{"title":"I","paragraphs":["(asked, VFP) NP:VFP (asked, VFC) (question, NP) NP"]},{"title":"~","paragraphs":["he VFP"]},{"title":"I","paragraphs":["VFP:COMP COMP:ADVP (has, VHF2)"]},{"title":"I","paragraphs":["(has, VHF1)"]},{"title":"I c=_--:c-~-;-;---'","paragraphs":["has sprained his ankle COMP","(has, VHF) ADVP:PERP"]},{"title":"I","paragraphs":["ADVP PERP"]},{"title":"I I L","paragraphs":["app:--a_r_m-crt7ly~"]},{"title":"I","paragraphs":["(question, NC) (asked, VN2)"]},{"title":"I","paragraphs":["(asked, VN1)"]},{"title":"I","paragraphs":["(asked, VN) (a, DETSG1)"]},{"title":"I","paragraphs":["(a, DETSG)"]},{"title":"I","paragraphs":["(question, NSGl)"]},{"title":"'I","paragraphs":["(reasonable, ADJ 1) {question, NSG( (perfectly, ADV1)"]},{"title":"I","paragraphs":["(perfectly, ADV)"]},{"title":"I l","paragraphs":["(question, NSG) (reasonable, ADJ 1)"]},{"title":"I","paragraphs":["(reasonable, ADJ) Figure 2: Left: finite-state structure; Right: Lexicalization. censed by G, we define e(-r) : E(G) -t IN to be a function counting occurrences of events as labels in -r. Algebraically, we think of e(r) as a monomial in the variables E(G); the exponent of a given variable (or event) z is the number of occurrences of z in 1, We denote the evaluation of a polynomial or mono mial <P in the variables E(G) by subscripting: </Jp is the value of <P at the vector of reals p. Relative to a parameter setting p,"]},{"title":"[e(r)]p","paragraphs":["is interpreted as the probabilistic weight of the labeled tree r 2","These notions are exemplified in Figure 3, which is a phrase structure tree for the N1 (read: N-bar) /Jig big problem in a grammar where N1 is the sen tence category. Each non-terminal is labeled with a phrase structure rule, and with lexical choice events for non-head daughters. In this case, the only non head daughters are the two A 1 's headed with head big. (problem,N1,A1,big) is a lexical choice event where big is selected as the head of an A1 with par ent category N1, and parent head pr-oblem. An event. monomial corresponding to the event tree is obtained as the symbolic product of the events labeling the tree,"]},{"title":"Parameter Estimation","paragraphs":["Given a grammar G) the inductive problem is to es timate a head-lexicalized PCFG with signature G. We work with the standard method for estimat ing PCFGs, based on the Expectation-Maximization","2","As with ordinary PCFGs) depending on the parame~ ters, the construction may or may not define a probability measure on the set of finite trees licensed by G. For the general case, infinite trees can be included in the sam ple space, This requires an extension in the definition of the measure but does not affect the probabilities of finite trees."]},{"title":"38","paragraphs":["framework (Baum & Sell [1968]; Dempster, Laird & Rubin [1977]).","Above, we defined the event polynomial e(r) for an event tree r licensed by G. The event polynomial for a sentence a is the sum of the event polynomi-· als for the event trees with yield a. Where corpus C is a sequences of sentences, the corpus event poly nomial"]},{"title":"e(C)","paragraphs":["is the (polynomial) product of the event polynomials for the sentences in"]},{"title":"C,","paragraphs":["In these terms, maximum likelihood estimation selects a parameter setting p such that the value"]},{"title":"[e(C)]p","paragraphs":["of the corpus polynomial is maximized; this corresponds to select ing a parameter setting which maximizes the proba bility of the corpus.","The E step of the EM algorithm computes an ex pected event count function which can be defined in terms of the corpus polynomiaL In the estimation of PCFGs using the inside-outside algorithm, event counts are computed iteratively, sentence by sen tence, The computation uses a packed parse forest, a compact and-or graph representing a set of trees and the sentence event polynomial, and which allows ef ficient computation"]},{"title":"of","paragraphs":["expected event counts, Some what more formally, we use the Inside-outside algo rithm (Baker [1979]). to compute"]},{"title":"Ep(zla):","paragraphs":["E(G) -t Dl where z ranges over events in the join rule and lexical event space E(G), defined earlier. c(a,p)(z) has the probabilistic interpretation of the expected number of occurrences of the event"]},{"title":"z","paragraphs":["in the set of trees with yield cr.","Given a parameter setting p, event counts are com puted and summed over the sentences in the corpus, In the algorithm of Baum and Sell, new parame ter values would be defined as relative frequencies of event counts, Le. maximum-likelihood estimation based on hidden data in the EM framework. We","(START-WORD ,START -CAT ,N 1 ,problem) problem,N1 -t A1 N1' (problem,N1 ,A1,big) (problem, N1)","big,A1 -t A' 1 problem,N1 -> A1 N1' (problem,N1 ,A 1 ,big) (big, A1)"]},{"title":"I","paragraphs":["(big, A) (problem, N1)","big,Al -t A' 1","problem,Nl -+ N' 1 (big, A1)"]},{"title":"I","paragraphs":["(big, A) (problem, N1)"]},{"title":"I","paragraphs":["(problem, N)","(problem,N1 -> A1 N1') 2","(START-WORD,STAitr-CAT,Nl,problem) 1","(big,A1 -> A')2 (problem,N1,A1,big)2","(problem,Nl -> N') 1 Figure 3: On the left, an event tree. On the right, the corresponding lexicalized tree. On the bottom, the event monomial obtained as a symbolic product of the event labels. The lexical choice event involving START-CAT chooses the head of the sentence, in this case problem. use instead a modified M step involving a smooth ing scheme in order to deal with the size of the pa rameter space and the resulting problems that (i) countt> are zero for the majority of events, and (ii) the parameter space is too large to be represented directly in computer memory. Lexicalized rules are smoothed against non-lexicalized rules in a standard back-off scheme (Katz [1980]). The smoothed proba bility is defined as a weighted sum of the maximum likelihood estimates for the lexicalized and unlexi calized rule probabilities. The smoothing weight is allowed to vary through five discrete value~ as a func tion of the frequency of the word-categol'y pair. The parameters give greater weight to the lexicalized dis tribution when enough data is present' to justify it. The smoothing parameters are set using the EM al gorithm on reserved data.","For the lexical choice distributions, an absolute dis counting scheme from Ney, Essen & Kneser [1994] is used, which is similar to Good-Turing, but somewhat simpler to work with."]},{"title":"The experiment","paragraphs":["We estimated a head-lexicalized PCFG from parts of the British National Corpus (BNC Consortium [1995]), using the grammar described in the first sec tion and the estimation method of the previous sec tion. A bootstrapping method was used, in which first a non-lexicalized probabilistic model was used to collect lexicalized event counts. On the next iter ation, counts were estimated based on a lexicalized weighting of parses, as described in the previous sec tion.","Analyses were restricted to those consistent with the part of speech tags specified in the BNC, which are produced with a tagger. In each lexicalized iter ation, event counts were collected over a contiguous"]},{"title":"39","paragraphs":["five million word segment of the corpus. Parameters were re-cornputed in the way described above, and the procedure was iterated on the next contiguous five-million word segment. Results from all iterations were pooled to form a single model estimated from 50M words. Table 1 illustrates lexical distributions in this model.","This training scheme allows the frame distribu tions for high-frequency words a chance to con verge on their true distributions, whereas a single 50M word iteration would not. The strategy de rives from a variant generalized EM algorithm pre sented in Neal & Hinton [1998]. In a nutshell, re estimating the parameters during the course of a sin gle training iteration will still lead to convergence On a maximum-likelihood estimate, provided certain conditions are met. Foremost among these is the re quirement that no parameter setting can be prema turely set to zero; this is met by our smoothing strat egy. This is not to say that precisely the same strat egy, pursued across multiple iterations, would pro duce a maximum-likelihood estimate; it would not. However, \"classical\" EM, requiring repeated itera tion over the entire training set, is both relatively inefficient and infeasible given our present computa tional resources."]},{"title":"Dictionary Evaluation","paragraphs":["The comparison to frames specified in a dictionary we use was introduced by Brent [1993] and subse quently used by Manning [1993], Ersan & Charniak [1995] and Briscoe & Carroll [1996]. The measure uses precision and recall to compare the set of in duced frames to those in the standard. Precision is the percentage of frames that the system proposes that are correct (i.e. in the standard). Recall is the percentage of frames in the standard that the system PNP satisfactonLADJP w PVFP address NP w adverb prob noun prob entirely 0.17 question 0.086 highly 0.11 issue 0.086 rnost 0.09 themselves 0.059 very 0.075 issues 0.031 quite 0.055 structure 0.031 wholly 0.032 argument 0.014 uncommonly 0.0037 questions 0.0043 especially 0.0037 electorate 0.0043","... ..."]},{"title":"--","paragraphs":["Table 1: On the left: the eight largest parameters in the lexical choice distribution describing modify ing adjectives selected by satisfactory. On the right: parallel information for the distribution describing heads of objects of the verb address. . proposes. If the results are broken down into true positives (TP), false positives (FP), true negatives (TN), and false negatives (FN), precision is defined as"]},{"title":"TP/(TP+FP)","paragraphs":["and recall is"]},{"title":"TP/(TP+FN).","paragraphs":["To pro duce measurements from our system, we must first reduce our distributions to set membership. Brent proposed a stoehastic filter for this reduction, consist ing of a set of per-frame probability cutoffs, which are applied independently of the lexical head. Although though the independence assumption is certainly du bious, we have adopted this method, without change, except for the introduction of a heuristic for finding the frame cutoffs.","The key property of cutoffs is that they control the tradeoff of precision versus recall. Raising the cutoff will generally produce a higher precision, but lower recall, and contrariwise. As we are neutral about this tradeoff, we set the cutoffs at the crossover point! where the difference in precision and recall changes sign. This is not entirely deterministic, as the mea sures may cross more than once; in that case, we optimize for the best precision.","For our dictionary, we used The Oxford Advanced Learner·'s Dictionary (Hornby [1985]), also used by Ersan/Charniak and Manning. We reduced our frame set and the dictionary1","S to a common set, map ping some frames and eliminating others. For evalu ation, we selected 200 verbs at random from among those that occurred more than 500 times in the train ing data; half were used to set the optimal cutoff parameters, and precision and recall were measured with the remainder.","Table shows results broken down by frame. The largest source of error is the intransitive frame. It is not hard to understand why: our robust parsing architecture resolves unparsable constructs as intran sitives. In addition to sentences where verbs are not"]},{"title":"40","paragraphs":["cutoff TP FP FN prec rec ..","~ns 0.15 20 24 12 0.6471 0.781 NP 0.021 3 5 1 0.9479 0.98!"]},{"title":"~-","paragraphs":["0.079 92 0 6 1 0.21 pp 0.045 27 15 6 0.7761 0.89( PART 0.027 60 5 14 0.8077"]},{"title":"o:ii","paragraphs":["VTOP 0.079 83 1 7 0.9 0.56~ NP PP 0.040 26 11 10 0.8281 0.84J NP PART 0.0099 68 6 12 0.7 0.53i NP NP 0.036 81 6 8 0.4545 0.38' NP VTOP 0.018 84 1 6 0.9 0.6 V!NG"]},{"title":"~VING","paragraphs":["0.019 86 3 6 0.625 0.45' 0.017 93 3 2 0.4 0.5","- NP VINF 0.019 99 1 0 0 - NP ADJ 0.016 85 1 12 0.6667 0.14; PP VTOP 0.014 97 1 1 0.5 0.5"]},{"title":"C ___","paragraphs":["_j_ _ __J_I_~31~0:._tl_:8:::::3:._ti_:1QLLO. 7888 1 o. 75c Table 2: Precision/recall broken down by frame. linked up with their complements because of interjec tions, complex conjunctions or ellipses, this includes frames such as SBAR and WH-complernents which are not included in the dnmk/phrase grammar. While it would be possible in principle to extract these from the present word collocation statistics, we plan in stead to pursue a solution involving extensions in the grammar.","A second major source of error is prepositional phrases. The complementation model embodied in the PCFG does not distinguish complements from adjuncts, and therefore adjunct prepositional phrases are a source of false positives. Thus the NP PP frame is scored as a false positive for the verb meet, be cause the OALD does not list the frame, although the combination appears often in the corpus data. While such frames lead to a loss of precision in the dictionary evaluation, we do not necessarily consider them a flaw in the information learned by the system, since the argument/adjunct distinction is often ten uous, and adjuncts are in many cases lexically con ditioned.","Lastly, there are many false negatives for the par ticle frame and noun plus particle. This is mainly due to disagreements between BNC particle tagging and particle markup in the OALD.","Despite these difficulties, the summary shown in table shows results that are on the whole favorable. In comparison with other work with a comparable number of frames (Manning, Ersan/Charniak), the system is well ahead on recall and well behind on precision. If one takes the sum of precision and re call to be the final performance indicator, than we are slightly ahead: 1.54 vs. 1.44 for Ersan and 1.33 precision ':Yo ·-","recall 'Yo of frarnes no. lex PCFG 79 75 15 Briscoe 66 36 159 Charniak 92 52 16 Manning"]},{"title":"90","paragraphs":["43 19' Table 3: Type precision/recall comparison. Some of Manning's frames are parameterized for a preposi tion. for Manning. Briscoe and Carroll's work, with ten times as many target frames, is so different that the numbers may be regarded as incomparable.","Obviously, precision and recall measured against a standard relies on the completeness and accuracy of that standard. In checking false positives, Ersan and Charniak found that the OALD was incomplete enough to have a serious impact on precision. Sym metrically, false negatives conflate deficiencies in the corpus with poor learning efficiency. It is impossible to say based on table which of the systems is more efficient at learning. While our system shows the best recall, this could be attributed to our having the best training data. Cha.rniak used 40M words of training data, comparable to our SOM, but his data was homo geneous, all taken from the Wall Street JournaL As we will show below, frame usage varies across genres, so the BNC, which includes texts from a wide vari ety of sources, shows more varied frame usage than the WSJ, and thus provides better dad for frame acquisition."]},{"title":"Cross entropy evaluation","paragraphs":["The information-theoretic notion of cross entropy provides a detailed measure of the similarity of the acquired probabilistic lexicon to the distribution of frames actually exhibited in the corpus (which we call the empirical distribution). The cross entropy of the estimated distribution q with the empirical dis tribution p obeys the identity"]},{"title":"CE(p, q)","paragraphs":["="]},{"title":"H(p) +","paragraphs":["D(pllq) where H is the usual entropy function and D is the rel1etive entropy, or Kullback-Leibler distance. The entropy of a distribution over frames can be con ceptualized as the average number of bits required to designate a frame in an ideal code based on the given distribution. In this context, entropy measures the complexity of the observed frame distribution. The relative entropy is the penalty paid in bits when the frame is chosen according to the empirical distri bution p, but the code is derived from the modeJls estimated distribution, q. Relative entropy is always non-negative, and reaches zero only when the two"]},{"title":"41","paragraphs":["obs freq est freq","1 imag natsci 1 frame imag 1 natsci 1 51 39 NP VTOP 40.4 34.2 21 43 NP 20.7 33.1 13 6 NP NP 8.8 3.9 6 1 NP PP 3.2 4.7 5 I NP PART 1.7 1.0 2 11 PP 1.8 10.2","I 0 SBAR 0 0","I 0 In trans 9.3 7.6","1 2130 1 1.913 1 entropy 1 2.476 1 2.423 1 Table 4: True and estimated frame frequencies for allow. distributions are identical. Our goal, then, is to min imize the relative entropy. For more in-depth dis cussion of entropy measures, see Cover & Thomas [1991], or any introductory information theory text.","For relative entropy to be finite, the estimated dis tribution q must be non-zero whenever p is. How ever, some observed frames are not present in the grammar, for one of two reasons. Some well-known frames such as SBAR require high-level constructs not available in the chunk/phrase grammar and un usual/unorthodox frames turn up in the data, e.g. PAH.T PP PP. Since the model lacks these frames, smoothing against the unlexicalized rules is insuffi cient. Instead, for all the estimated distributions, we smooth against a Poisson distribution over cat ~egories, which assigns non-zero probability to all frames, observed or not. This allows us to spell out the unknown frame using a known finite alphabet, the grammar categories, while retaining a reasonable average length over frames.","For our entropy measurements) we selected three verbs, allow, reach1 and suffer and extracted about 200 occurrences of each from portions of the BNC not used for training. Half of each sample was drawn from \"imaginative\" text and the other half from the natural or applied sciences, as indicated by BNC text mark-up. The true frame for each verb occurrence was marked by a human judge3",". The empirical dis tribution was taken as the maximum-likelihood esti mate from these frequencies. Tables 4 and 5 indicate t.he observed frequencies and the entropy of the re sulting distributions.","Alongside the observed frequencies, we indicate a set of estimated frequencies. These were generated by taking the 50M word model described above, pars ing the test sentences, and extracting the estimated frequencies. The sum of estimated frequencies is gen-","3For this judgment, the frame set was unrestricted, i.e. included frames not in the grammar."]},{"title":"I","paragraphs":["obs freq"]},{"title":"I","paragraphs":["est freq"]},{"title":"I","paragraphs":["imag natsci frame imag"]},{"title":"I","paragraphs":["natsci"]},{"title":"I","paragraphs":["63 88 NP 50.1 74.5 13 15 NP PP 5.9 10.9 9 I PART 5.9 0.8 6 0 PART PP 2.7 0 5 3 PP 6.7 3.4 4 I In trans 15.2 6.8 2 0 PART NP 0.5 0 I 0 NP PA!tr 0 0.1","--","2"]},{"title":"o 1 o","paragraphs":["979"]},{"title":"1","paragraphs":["ent10py"]},{"title":"I","paragraphs":["2 101"]},{"title":"I","paragraphs":["1 473"]},{"title":"I ,-","paragraphs":["obs freq"]},{"title":"I","paragraphs":["est freq-","imag natsci frame imag natsci","- 41 6 In trans 34.9 13.4 31 54 pp 27.4 50.5 21 36 NP 18.9 23.0","4 1 NP VTOP 2.1 0.7","3 4 NP PP 0.9 5.2","-","1 !.936 1 !.580 1 entropy 1 1.936 I 1.90:!] Table 5: True and estimated frame frequencies for reach (top) and s11jJer- (bottom). erallv less than the observed frequencies due to tag ging.errors, parse failures, and frequency assigned to frames not shown in the tables. However, an eyeball inspection of the tables shows that the parser does a good job of reproducing the target distribution.","One striking feature in the tables is the variation across genre. In particular, suffer used in the imagi native genre shows a very different distribution than suffer in the natural sciences. A chi-squared test ap plied to each pair indicates that the samples come from distinct distributions (confidence> 95%).","The column labeled \"50M lex\" in Table 6 provides a quantitative measure of the agreement between the 50M word combined model and the empirical distri butions for the three verbs in two genres in the form of relative entropy. The first column repeats the en tropy of the data distributions. For purposes of com parison, the second column indicates the relative en tropy of one data distribution with the other data distribution filling the role of the estimated distribu tion (i.e. q) in t.he discussion above. The relative entropy is lower when the estimated distribution is used for q than when the data distribution for the other genre is used for q in each case but one, where the figures are the same. This suggests the combined model contains fairly good overall distributions.","To numerically evaluate whether the system was abl(', to learn the distribution exhibited in a given col lection of sentences, we tuned the lexicon by parsing the test sentences for each genre separately with the"]},{"title":"42","paragraphs":["D(pjjq) for various q","other 50M 50M","head, genre H(p) genre lex unlex","allow imag 2.06 0.50 0.40 3.13 natsci 1.78 0.49 0.42 2.27","reach imag 1.99 0.91 0.35 1.07 natsci 0.90 0.37 0.37 1.36","suffer imag 1.86 0.87 0.24 0.70 na.tsci 1.51 0.59 0.37 1.19 mean 1.68 0.62 0.36 1.62 Table 6: Frame relative entropy for three verbs in two genres. The first column narnes the lexical head and genre, and the second the entropy (H) of the empirical distribution over frames, p. By empirical distribution we mean the relative frequencies from examples scored by a human judge. Columns three through five give the relative entropy D(pjjq) for var ious related distributions. In column three, q is the empirical frame distribution for the same head1 but with the complementary genre. In column four q is the (genre-independent) distribution derived from the 50M word lexicalbed model. Column five uses the unlexicalizecl frame distribution derived from the SOM model, i.e. a distribution insensitive to the head verb. Lower relative entropy is better. 50IVI word model, extracting the frequencies, and es timating the distribution from these. The results are the column 4 labeled ''SOM lexicalized extraction11","in 7. The following columns give the same figures for frcqency extraction with other models. Extraction with the large lexicalized model gives the best re sults, and gives better relative entropy than the 50M lexicalilazed model itself (in column 2). Notice that only the distributions estimated with the two 50M mo~lels are better than the 50M lexicalized model, though the unlexicalized one is only marginally bet ter. In this sense, only the 50M lexicalized parser proves to be a good enough parser for genre tuning. Notice that with this model, tuning in no case gives worse relative entropy1 and in five out of six cases give an improvement.","Notice also that relative entropy for the distribu tions obtained by tuning with the 50M model are a good deal lower than the cross-genre figures from Ta ble 6. This suggests that if we wanted to have a good probabilistic lexicon for, say, the imaginative genre, we would be better off using the automatic extrac tion procedure on data drawn from that. genre than using a perfect parser (or a lexicographer) on dat.a drawn from some other genre, such as the natural sciences. This provides a calibration of the accuracy of the lexicalized parser1","s estimates, and conversely demonstrates that words are not used in the same D(pllq)"]},{"title":"'50Ni","paragraphs":["50M 5M -","50M 5M","lex lex lex unl. unl.","head, genre mod extr extr extr extr","allow imag 0.40 0.32 1.32 0.47 1.32","natsci 0.42 0.28 0.28 0.52 0.86","reach imag 0.35 0.35 0.63 0.32 0.63","natsci 0.37 0.19 0.34 0.28 0.34","suffer imag 0.24 0.11 0.38 0.12 0.38","natsci 0.37 0.20 0.88 0.34 0.88","mean 0.36 0.24 0.64 0.34 0.74 Table 7: Relative entropy of distributions estimated by parsing the test sentences with various models, and using the Inside-outside algorithm to produce estimated distributions q. The first column names empirical distributions p. The second column repeats relative entropy for the 50M lexicalized model from the previous table. The third gives relative entropy where q is obtained by parsing and estimating fre quencies in the test sentences with the 50M lexical ized model. The following columns give the corre sponding figures for a q obtained by following the same procedure with a 5M word lexicalizcd model, a 50M word unlexicalized model, and a 5M word un lexicalized model. way in different genres."]},{"title":"Optimal parses . I","paragraphs":["Although identifying a unique parse does not play a role in our experiment, it is potentially useful for application!?. A simple criterion is to pick a parse with maximal probability; this is identified in a parse forest by iterating from terminal nodes, multiply ing child probabilities and the local node weight at and-nodes (chart edges), and choosing a child with maximal probability at or-nodes (chart constituents). Figures 1 and 4 give examples of maximal probability probability parses.","Other optimality criteria can be defined. Tlw structure on noun chunks is often highly ambiguous, because of bracketing and part of speech ambiguities among modifiers. I<Dr many purposes, the internal structure of an noun chunk is irrelevant; one just wants to identify the chunk. From this point of view, a probability estimate which considers just one anal ysis might underestimate the probability of a noun chunk. In what we call a sum-max parse, probabil ities are summed within chunks by the inside algo rithm. Above the chunk level, a highest-probability tree is computed, as described above."]},{"title":"43 Notes on the implementation and parsing times","paragraphs":["Software is implemented in C++. The parser used for the bootstrap phase is a vanilla CFG chart parser, operating bottom-up with top-down predictive filter ing. Chart entries are assigned probabilities using the unlexicalized PCFG, and the lexicalized frequencies are found by carrying out a modified inside-outside algorithm which simulates lexicalization of the chart.","In the iterative training phase, an unlexical ized context-free skeleton is found with the same parser. We transform this into its lexicalized form categories become ('W, n) pairs and rules acquire lexical heads ~-and carry out the standard inside outside using the more elaborate head-lexicalized PCFG model. Average speed of the parser during iterative training, including parsing, probability cal culation, and recording observations, is 10.4 words per second on a Sun SPARC-20. The memory re quirements for a model generated from a 5M word segment arc about 90Mbyte. The upshot of all this is that we can train about 1M words per day on one machine, and a single 5M word iteration requires one machine work week."]},{"title":"Discussion","paragraphs":["We believe the formalism and methodology described","here have the following advantages:","• The grammar is under the control of the compu tational linguist and is of a familiar kind, making it possible to incorporate standard linguistic anaJ-","1 yses, and making results interpretable in terrns of",", linguistic theory. In contrast, approaches where","' context free rules are learned are likely to produce structures which are uninterpretable in terms of linguistic theory and practice.","• Because of the context free framework, efficient parsing algorthims (chart parsing) and probabilis tic algorithms (the inside-outside algorithm) can be applied. With an efficient implementation, this makes it possible to construct representations of all the tree analyses for the sentences in corpora on the scale of ten to a hundred million words, and to map such a corpus to a probabilistic lexicon.","• With the robustness introduced by the state model, almost all sentences in the corpus can be parsed.","e The model assigns probabilities to sentences and trees, whieh is useful for applications independent of the lexicon-induction problem discussed here.","• The word-selection model, which threads a word bigram model through head relations in the syn tactic tree, allows a large body of word-word col locations to be learned from the corpus, and put to use in weighting of competing analyses. N_C-c anssa"]},{"title":"DETS~SGJ::clarissa","paragraphs":["DETsb .. -an mJsG"]},{"title":"I '\"","paragraphs":["I NSG_ -clarissa"]},{"title":"AD,I_-em~clarissa AbJ N~G embarr~ssed","paragraphs":["clarilsa S-clarissa N __ C:VFC1-sipped Figure 4: The first part of maximum probability parse. co -"]},{"title":"I","paragraphs":["COM_-,"]},{"title":"C~M I","paragraphs":["VFC1 :COM_C-giving COM_C:VGC1-giving VGC1-gavmg"]},{"title":"~.-----~~~~~~~~~------------~~","paragraphs":["VG_C-gaving N_C peregrine N_\"t!'-cue"]},{"title":"I I .","paragraphs":["VG=-glving NSG1-peregrane VG -gl o'vo'ng I _- NSG_ -peregrine"]},{"title":"vb N~G givi~g","paragraphs":["pereJrine oerscr=-an DETsb_-an oEfsa"]},{"title":"I","paragraphs":["an Figure 5: The second part."]},{"title":"44","paragraphs":["VFC1 :Cdiif_C-giving VGC EA_C-."]},{"title":"PER~","paragraphs":["C-."]},{"title":"P~R I","paragraphs":["• The valence information learned) rather than be ing simply a set of subcategorization frames) is a probability distribution which reflects the freqency of frames in a given training sample, and which can be plugged back into the parser and used to ana lyze further text.","Some of these benefits are purchased at the cost of a lack of sophistication in the grammar formal ism, compared to constraint-based· formalisms used in contemporary computational linguistics. This compromise is made in order to make large-scale ex periments achievable; our interest is in conducting scientific experiments---observational and modeling experiments---with large bodies of language use. It is natural that this should require incorporating ap proximations in computational models. Notably) the compromises made in our approach are not so se vere that the grammatical analyses identified and the probability parameters learned are out of touch with linguistic reality. This is in contrast to the situa tion with other approaches using similar mathemat ical methods, such as terminal-string n-gram rnodel ing."]},{"title":"Conclusion","paragraphs":["We have presented a statistically-based mc~thod for valence induction, b.::\\sed on the idea of automatic tuning of the probability parameters of a grammar. On the standard precision/recall measures1 our sys tem performs better on precision, worse!on recall 1 and on the whole somewhat better than·.- bther pub lished systems. We have provided a more precise evaluation via entropy measures) show\"ing that the model learns efficiently and builds accurate models of frame distributions. The cross-domain entropy of the data frame distributions provides numerical evi dence that frame usage varies across domains) similar to word usage. This, in turn, suggests that auto matic acquisition and stochastic tuning are a must for large-scale NLP applications and computational linguistic models."]},{"title":"Bibliography","paragraphs":["Abney, S. [1991L ~~Parsing by Chunks1)'","in Views on Phrase Stmcture, D. Bour.harcl & K. Leffel, eels., Kluwer Academic Publishers.","_____ [1995], \"Chunks and dependencies: Bringing processing evidence to bear on syn tax/) in Linguistics and Computation) Jen nifer S. Cole, Georgia M. Green & Jerry L. I'vforgan, cds., CSLI Publications.","BNC Consortium [1995], The British National Corpus, Oxford University) http:/ /info.ox.ac.uk/bncj."]},{"title":"45","paragraphs":["Baker, J. K. [1979], \"Trainable grammars for speech recognition1\"","Proceedings of the Spring Con ference of tlw Acoustical Society of America1 Cambridge, MA.","Baum, L. E. & Sell, G. R. [1968], \"Growth 11-ansfor mations for Functions on Manifolds,\" Pacific Journal of Mathematics 27.","Brent, M. R. [1993], \"From Grammar to Lexicon: Unsupervised Learning of Lexical Syntax,\" Computlltional Linguistics 19, 243-·262.","Briscoe, T. & Carroll, J. [1996], \"Automatic Extrac tion of Subcategorization from Corpora,n MS, http:/ /www.cl.cam.ac.ukjusers/ejb/.","Cbarniak, E. [1993], Statistical Language Learning, MIT, Cambridge, MA.","___ [1995], \"Parsing with Context-free Grammars and Word Statistics,\" Department of Computer Science, Brown University, Technical Report CS-95-28.","Cover, T. M. & Thomas, J. A. [1991], Elements of Infonnation Tlwmy, John Wiley and Sons 1 Inc., New York.","Dempster, A. P., Laird, N. M. & Rubin, D. B. [1977], \"Maximum likelihood from incomplete data via the EM algorithm,\" Journal of the Royal Statistics Society 39, 1-·38, Series B.","Ersan, M. & Charniak, E. [1995], \"A Statistical Syn tactic Disambiguation Program and what it learns,\" Brown CS Tech Report CS-95-29.","Hornby, A. S. [1985], Oxford Advanced Learner's Dictionary of Current English, Oxford Uni versity Press, Oxford, 4th Ed ..","Jackendoff, H.. [1977],"]},{"title":"X","paragraphs":["syntax: A study in phrase structure., MIT Press1 Cambridge, MA.","Katz, S. M. [1980], \"Estim<Ltion of probabilities from sparse data for the language model compo nent of a speech recognizer,\" IEEE Transac tions on Acoustics, Speech and Signnl Pro cessing 35, 400-401.","Manning, C. [1993], \"Automatic acquisition of a large subcategorization dictionary from corpora/1 Proceedings of the 31st Annual Meeting of the ACL.","Neal, R. M. & Hinton, G. E. [1998], \"A New View of the EM Algorithm that Justifies Incremental and Other Variants/1","in Learning in Graph ical Models, Michael I. Jordan, eel., Kluwer Academic Press.","Ney, H., Essen, U. & Kneser, R. [1994], \"On struc turing probabilistic dependences in stochas tic language modelling,)' Computer Speech and LanguageS, 1-·38."]}]}
