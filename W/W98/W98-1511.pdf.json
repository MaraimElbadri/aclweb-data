{"sections":[{"title":"Japanese Dependency Structure Analysis based on Lexicalized Statistics","paragraphs":["MASAKAZU"]},{"title":"Fujio NAIST 8916-5 Takayama, Ikorna Nara, 630-0101 JAPAN masaka-h@is.aist-nara.ac.jp Abstract","paragraphs":["\\Ve JH't'::wnt statistical rnodels of Japanese dependency analysis aud report results of some experiments to in vestigate the pnfonuancc of the models for the use fo a partical parsing system. The statistical modeb <:u'<' rather simple compared with the recent complex mod els <wd intesivcl,y usc lexical level information1 such a.s morplH:mcs1 and part-of-speech tags ..","\\Ve conducted several expcrimcllts to show the fol lowing properties of the modeb:","lb performance of the models according to feature selec tion",";{I performance of the models as a partial pa.rsiHg s:y:::; tclll.","The EDH .. [6] corpus was used for both tra.iuing a.ud evaluatioH of the syst(_'HL"]},{"title":"1. Introduction","paragraphs":["A HUilllwr of statistical parsing methods have lwe11 pro·· pOS('Cl. most of the systous focus on full parsing of scl\\ t<'Hces, and do not discuss the performance of partial parses, whkh b nucia.l for some application;;, such as i11forma.t.ion n'trieva.l or pre pron'ssing of corpus <HlllO ta.liou.","Early approa.clH'S of statistical pa.r,':>iHg [15, 10, 1~3] conditioned probabilitic::; on syHta(tic rules. To take lllOn: contextual iufonna.tioll into a.ccomtt, word collo cation i.':> applied to syntactk fonnalization, Buch as k'x ica.lized PCFG.lexica.li:zc'd tree adjoining gra.mmar1 and lcxicalir.:cd lillk gra.nuna.r.","The leugth of phrase::; or the distcuKe between head words were also cmtsidere<l in the severa.l mocleb [16, 8]","Th('re a.re parsing methods tlta.t do uot require a grammar. Colliils"]},{"title":"[a]","paragraphs":["propo.':\\c's a statistical pa.nwr bas('d on probabilities of d<'].H?llclcHcics bet.W('eH head-words in parse tn'<'S. Yasnha.ra. [18L co11stnH'ts a. syst<'m based"]},{"title":"88","paragraphs":["YUJI"]},{"title":"Matsumoto NAIST 8916-5 Takayama, Ikoma Nara, 630-0101 JAPAN matsu@is.aist-nara.ac.jp","paragraphs":["on collocation counts as the Oilly source of grammati cal iilfonna.tiOit."]},{"title":"He","paragraphs":["uses co-occurr<'Hce pa.th'nts of tlw POS tags of head-words. The method, how<'H'f, is not statistical, iu that it only accumulates correct pattems for direct. use.","Ma.genna.n [4] proposes a statistical parser based Oil a decision tree model, in which the probabilitiC':; arc conditioned on the derivation history of the parse tr<.'cs"]},{"title":"[4,","paragraphs":["10]. He compares the decision tiT(' model with the n-gra.m model, a.ud cla.ims that the a.mouut of parauw tcrs in t.h<' resulting model remaiHs relatively constant, depending mostly on the numher of traiuing exa.mph's.","Cha.ntiak [5] proposes a lH'W model alld compared it with Colli11s' 1 aud IVJa.germaiL's models and shows what aspects of these systems affect. their relative per fonnance.","In general, statistical models suffc'r from the probkut of data. spa.rs<'IH.'ss.","Instead of usiilg a complex sta.t.istiralinodd combi11nl with vnrions smoothillg techniq11es [1 1 2, I, 0], \\Ve stick to a. statistical model of simple scttillg aiming at au ('asy implt'mentatioll, and pursue a wa.y to select usdul information for achieving higlwr parse a.c·naa.cy.","T'he basic model is close to Colli11s' model[3] Ja.pa.HcS<~ dep<'ndeilcy structure are usually bct-':iNI Oil phrasa.l units (called '' b-un8ct.su1","'",").","A bnnsetsn b<:t.sica.lly COilSifits of OilC' (or a. sequ<'ucc of) content wonl(fi) and its sucn~cding function words (that forms the smallest phrase, such as a simple uoun phrase.).","\\Vc' consider the d<.'P<'nclell.C'Y structure such that C\\' cry lmnset5u. in a. :-:;euteuce except the right most one moclifi<'S one of its followiiLg bttnsef.s'l.t's iu the S<'nteuce and no two modifications llla.y cross each other.","The difference of our model to Collins' model priu cipall:y comes front the property of .Japanese seHtence structure. First, the type of modification rl'la.tioll ( dc pcndc'H<.'Y rel<:1..tions) is lmiqly determined by tlt<.' func tion words or tlte <.'IHling form of the modifler. Second, the modification <.tlways din:ct from left to right since .Japanese is a. lwad-fi.na.l language.","Tlt(•re an' various features that may aJfc:ct the parsiug pn'cision. \\Ve test a nmnber of possible setting ami try to find out the best combination of features. V1","h' a.lso test the p<~rfonmlUC(' of partial parsing iu several set~ tiugs. 200100 pa.rsed .Japanese sentences in EDR corpus is used for evaluatiou.","lu the uext. scctioH 1 the statistical model is described. Section 3 outlines the parsiug algorithm is outlined. sec tion 4 presents the evaluation method. Final section is for conclusion and future work."]},{"title":"2. The Statistical Model","paragraphs":["\\Ve propose a statistical model based on the feature-s of b-u.n8et8u's. Those fe•atures usually defined b:y the re~ suit of lllOrphological analysis1 such a.s part-of-speech (POS) tags1 inflection types, punetuations1 and other grannna.t.ical or surfac(' information. Some features a.re determined not directly from the modifier a11d mod ifiee lm .. nsets·u Js For inst.ann\\ the umubcr of bv.nsetsn l><'twt>en a. moclifi<'r and a. modifiee can be a feature.","\\Vc first introduce Hota.tiona.l conv<'ntions. S"]},{"title":"=","paragraphs":["w 1,,,,, Wn is a sentem·e, where wi is th<' i-tlt word."]},{"title":"T","paragraphs":["is a sequcHcc of -yvords and tag pairs, that is,"]},{"title":"T = <","paragraphs":["WJ,tl"]},{"title":">_. .. ,<","paragraphs":["Wn 1 t.n"]},{"title":">.","paragraphs":["F is a. seqncll('<' of lmn set.sn aud f<·aturc pairs, that is, F"]},{"title":"=<","paragraphs":["U1, f1"]},{"title":">, ... , <","paragraphs":["bm.fm"]},{"title":">.","paragraphs":["\\Ve· us<· the notation"]},{"title":".Dep(i) =","paragraphs":["j to iudicat:e that the ·i.-th lmnsetsv, iu tlH' sequence is a. modifier to the j-t.h bv:n .. <;d.<w .. H<.•n•, th<~ symbol \"it';, ti, an db; staml for word, tag, aHd bv:nM:tsu respectively, and fi reprc ocuto the oct of features a.osigucd to bu:n.<etsu. b;. 'rhc subscripts ·1n, and n sta.ll(l for the IlUmbcr of b·unsetsu )s a.Hd words, n'SJH.•ct.in•ly. Lis the sequ<'llC<' of dqwndeu cies: L = (Dep(l). Dep(2)., .. , Dep(rn -· 1)). ."]},{"title":"f","paragraphs":["In g<'IH'ra.L a statistical parsing model e.':itiuLttcs the <:oHditioua.l probability1"]},{"title":"P(P","paragraphs":["1"]},{"title":"I S'L","paragraphs":["for ea.ch v-<utdida.t.e• parse tree"]},{"title":"P","paragraphs":["1 for a. seuteuce"]},{"title":"S.","paragraphs":["ln .JapallCS<' dcpen d<'HC). ::;tructtu'(' analysis, the final goal is to identify"]},{"title":"L","paragraphs":["rather thau P1, and\\\\'<' try to maxiruiz<' the probability P(L,F,T IS).","Tlt<• mo::>t likely depeu<lency structure analysis u11der the model is the11:","Lb~, 81 = argmax P(L, F\\ TIS) L.F,T","Mgmax P(LJF, T, S) P(FIT, S) P(TIS) L. F.l'","\\Ve assume tha.t lmnsdsu. consttnction only dqH•ml 011 word/tag pairs, hmcc' P(F IT, S)"]},{"title":"=","paragraphs":["P(F IT), all(l assume that a. dependency structure can be dctermill('d only by b-u.nsds·u features, thuo P(L"]},{"title":"I","paragraphs":["F. T, S)"]},{"title":"=","paragraphs":["P(L"]},{"title":"I","paragraphs":["F). Th<' equation (1) is 11ow writt<.•u:"]},{"title":"L\"\"·''","paragraphs":["argmax P(LIF') l'(FIT) J'(TIS) L.F,T"]},{"title":"89","paragraphs":["For simplicity, \\\\'<' <:l.':>Sumc that the uwrphologica1 analysis and the b·unsetsn construction are both deter ministic. For the morphological analysis, we use the most likely output of the' .Japanese morphological ana lyzer C:lmScn [11].","For the b·un8dsu coustruct.ion, we usc a finite state trallsducer constructed from regular exprcssioHs of word/tag pairs.","\\Vhat we need to do therefor is to estimate P(L"]},{"title":"I","paragraphs":["F) 1 and fiud L for ca.ch 8 that maximizes the conditional prolmbilit.y P(L"]},{"title":"I F).","paragraphs":["YVe a.ssmne that dcpeudcncies are mutually indepen·· deut1 that is, m-1 P(L"]},{"title":"I","paragraphs":["F)="]},{"title":"IT","paragraphs":["P(Dep(i)=j"]},{"title":"I r, ...","paragraphs":[",fm) (1) i::::::l","a.nd no two modifications may cross each other. f1 1 ..","1 fm stands for th<• sequence of bnn.set81t features <:l.':>Signed to the bunset.<;·tt, Tln1sl P(L"]},{"title":"I","paragraphs":["P) can be defined <:t::; th(' product of the probability of dependency pairs.","Oue point that differs from the Colliusl model is that our model docs not estimate the type of dependency relations. It only estimate the exist(~nce of the depen dency relations. This is because• the type of dependency is detenniiH.'d uniquely by the modifier in Japall('Se sen tences. , The modC'l estimate th<' probability of each depen deJtcy pair directly by ma.ximunl likelihood estimation ba .. <:>cd on bztn.'ietsn f<•a.turcs. Head-words, POS tags, word classes, function words, pnnctua.tioHs, aml di::; tapce measure such as the numlwr of bnnsds·u's are used available for the probability cstimat.iou.","\\\\{c can expand each item of the equation ( 1) by us iug those fea.tun•s 1 and a.ssumiHg inckj)('IHh'IH'<' of tlH' co-o<.'TmTe'nc<· of some features. Ill the following, we discriminate the bnn5ct.m feature's that directory relat<• to the modifier a.ud modifi<'<' and the distance features that relate to relative positions of the rnodifkr aHd the modifi<'<'."]},{"title":"P","paragraphs":["(Dcp(i)=j"]},{"title":"I","paragraphs":["f1, ... ,fm) \"\" P,(Dep(i)=j"]},{"title":"I","paragraphs":["f1, ... , fm) (2) X Pd(Dep(i)=j"]},{"title":"I","paragraphs":["f1, ... , fm) (3)","lu the secoud equation, we assume indepcudence of two kiwis of probabilities. The first is the collocation probability ])('tween b·u:nsetsu j(:a.t·ures, and the second Oll<' is the distmln' fC'atnre between two b·unsct.su's. Tll<' iwlepeHekucy of these• two probabilities reduce the siz(' of thC' model.","\\Ve refer to the> probability (2) a.<:> the collocation probability, and the probability (3) as the distance probability.","The remaiuder of this s('(tion explains these proba bilities in <ktail."]},{"title":"Head Collocation Probability","paragraphs":[".Japanes<' language has dependency n'la.tions expn'ssed by the function words or the ending form, and they play a nuda} role in dctcrrnining the dependency structure. The relation name (type) is usua.ll:,' dd.ennincd by the functiou words.","If a b·unsetsn has no function words, we use POS tag (aud inflection type) of the right most content word of the bv:nsetstt.","Head word is basically defined by the right most cou tent word in the each bunsetsu.","By using these features, we define two models of head-collocation probabilities. The first is the gener ation probability of features and the second is the col location probability of features.","In the first model, -..ve assume J a.pa.ucsc dependency structure is the re::;ult of sclectiomt.l process of which each modifier selects a modifict'. The sclectiona.l prob ability is writteu as"]},{"title":"F.,r","paragraphs":["1(11j,'tj,JJj"]},{"title":"I","paragraphs":["h;,TidJi)· ht this","ex:pn•ssion, the modifieels features arc hj,Tj,fJj t~iven","that modifier's features are hi, 'l'i,Pi· The symbols","hi, 1'i,w!.d]Ji stand for head feature, rdation typc, and","punctuation~ respectively. \\Vith this seuing, we make","t.h(' following approxinmtion: dd","I'h<' ma.xiunun-likelilwod estimate of 1~ 1 i::; givc11 as","follows:"]},{"title":"1;,","paragraphs":["(l,;,l'j,Jlj 1 h;,l';,p;) C(Dcp(i)"]},{"title":"=","paragraphs":["j, h;, ,.,, p;, h;, l'j .. p;)"]},{"title":"----ern","paragraphs":["cp(i)"]},{"title":"=","paragraphs":["j'"]},{"title":"-:r;:,-,:7,","paragraphs":["J!"]},{"title":"i) C(Dcp(i)","paragraphs":["::::::j, hi,l'i,]Ji,llj,ThJJj) is the HuUlber of times tha.t. feature pairs of hi, 'l'i,JJ; and hj, Tj,}-Jj a.re ill a dependeiH'J' relation in th{' training da.t<-L","I11 tiH~ second model, we ddin(' the the t:><>le<\"tioua.J probability as F~.(Dep(i)=.i"]},{"title":"I","paragraphs":["h;,r;,p;,hj,l'j,J!j). This is t.lte probability that b\"Unsetstt b; modifies btwsets·a bj \\\\'heu those b-unsetsu.'s appear ia the sa.r11e s<?nteHcc. Ph (Dep(i)=j"]},{"title":"I","paragraphs":["f1, ... ,fm) def","Tll<' maximnm-likclilwod estilllat:e of F·c· is givc_'ll as follows: F,. (Dep(i)=.i"]},{"title":"I","paragraphs":["h;,r;,p;,hj,l'j,J!j) C'(Dq{i)=.i, h;,r;,p;,h1 ,rj,}Jj) --·~-C(hiJilPi,h~j,·!'j~Pj) .."]},{"title":"90","paragraphs":["Cs( h;, 'l'i 1 Pi 1 hj, rj ,JJ j) is the n umbcr of times h;, r i dJi and hjl ~'hPj appon in the saHH' senU'rtcc in tl1c traiu iHg data."]},{"title":"Cs(Dep('i)::::::j","paragraphs":["1"]},{"title":"h;,","paragraphs":["'~'i,Pi, hj, \"l'jdJj) is the mnn bcr of times h;, ri,]Ji awl hj, 'l'j,JJj are seeu in tlH' same sentence ill the trainiug data <Hid bi modifies bj with the relation 1'i·","For the head feature h;, we can usc the head word, as w<·ll as the POS tag or the word cla.ss of a head word. \\Ve use the .lapa.nesc thesa.urus ' Bunrui Coi Hyou'(l3GH)[l2J to define word dasses. DGH has a six-layered abstraction hierardty, iu which lllOH' than 80/)00 words arc assigHed at the leaves.","For each of those probabilities explained allOY<', we tested the followiag models for feature selection.","POS model uses POS tags for the hea.d","feature.","LEX model uses POS tags aud lexical forms","for the head feature.","DGH model uses POS tags~ lexical forms1 and","word classes for t.h<· head feature.","To acquire tlt<' statistics, we hav(' to resolve the fol lowing ambiguities:","e \\Vhich level of thesaurus hierarchy is appropriat<' as the rla.ss for htad-wonl","0 liow much infonnation from the function words should be <'Ollsidered to ddine the depcudency re lation narnes.","For t.ll(' liwitati011 of comput.<'l' resources) Wl' could HOt usc· all the combinatiou of word classes (the combi IW.tion of modifier and modifiee ). The collocation of word classes in th<' sa.nw lap'r i11 BGH wa~ h'arncd (from the 2nd to 6t.li la.yer) aud used separately.","In the current impleuwnt.atiou, we count tlH' statb tks for various kngth of dcpeH<kn<'y rdatiou muucs. Cowsider the ('Xampks in Table 1. Helation feature of nw<lifier in 3 __, 4 lll<tY bl· \"~"]},{"title":"-c'","paragraphs":["(,:\" or ''(,,: 1","'. Relation feature of modifice iu 3......, 4 may be ''-It"]},{"title":".0\"","paragraphs":["or empty.","Then, head collocation fcatUl'(' combinations defined fo 3 _, 4 a.re as follows (in th<' case of LEX model}:"]},{"title":"[:fL,","paragraphs":["ii]!"]},{"title":"[-t-tL i-], ['ll' ;t-c- 1:[:, ['icollli:","paragraphs":["~-\\J:~]-1 (I complete it until! this spring)","modifier1","s features rnodiflee1","s featun~s","'","relation name head head relation name","1 ->4 ;fl. ii (particle)"]},{"title":"'D!i:","paragraphs":["~-{):~ 2 ->4"]},{"title":"-z- fL","paragraphs":["(demonstrative"]},{"title":"t","paragraphs":["(case pmtidc)"]},{"title":"7GJlli:","paragraphs":["~ -1!_· ~ pronoun) ··-- 3 ->4"]},{"title":"'$ i ·c","paragraphs":["(particle )-1:"]},{"title":"7cllli:","paragraphs":["~ -1!_· ~"]},{"title":"( \"'\"e","paragraphs":["particle) Table 1: Example of dependency relations. Each square bracket represents a bunsets·u","modifier's feature modifiee\\ feature -- n:lation name head head relation name"]},{"title":":n-r:","paragraphs":["'","-~","~f!l( C' -tt· 0"]},{"title":":t ·c-r:","paragraphs":["'{~"]},{"title":"ifr&: -","paragraphs":["I'"]},{"title":"-","paragraphs":["~"]},{"title":"J\\:r&","paragraphs":["~-tl: -0"]},{"title":"r:","paragraphs":["~"]},{"title":"7tJOC","paragraphs":["\"'('.[: Noun"]},{"title":":fo&","paragraphs":["~ -li' 6"]},{"title":"'l' ·c--r:","paragraphs":["Noun XI& - [: Noun"]},{"title":"J\\:1&","paragraphs":["!'-\\!.' 0"]},{"title":"r:","paragraphs":["Noun"]},{"title":"Jtr&","paragraphs":["- \"'('.[: ~ Noun ~"]},{"title":"-ti- ;;:) 'l''C-l:","paragraphs":["~ Noun [: ~ Noun 0 -ti· 0 [: ~ Noun"]},{"title":"'l' ·c-r:","paragraphs":["Noun Noun ti- -tt· ;;:)"]},{"title":"'l''C-l:","paragraphs":["Noun Noun"]},{"title":"- r:","paragraphs":["Noun Noun 6"]},{"title":"-tt-","paragraphs":["0 [: Noun Noun","-------"]},{"title":"The Distance Probability","paragraphs":["Distance tn<'<lSure of dependency relations is an inlpor t.ant factor to clisarnbiguate d('pcndenry structure. For in::;taH('C1 relation type \"ha/pa.rticle1","'","has a t.en<~7-ncy to modify a distaut phra.c;al unit. ...","For the distance mc<:t.':itlH' of a pajr of bunsst.sn1","s, we use the Humlwrs of the lmnsets't~ '5 a.nd punctuations be tWN'H the1n.","Two types of probabilities are considered for the probabilities of head¥collocation d<'Scribed ahove.","Generation probability model of the distann' features is as follows:"]},{"title":"Pd(Dep(i)=j [","paragraphs":["fJ, ... ,fm) \"'"]},{"title":"F,;","paragraphs":["1 (r;,d;;,!Jij [ r;)","C'("]},{"title":"Dep{j_2=:J,","paragraphs":["r;~l_;;"]},{"title":",p;;)_ C(Dep(i)","paragraphs":["= j',"]},{"title":"r;)","paragraphs":["Collocation probability version of th<' distauce f<'a. tnn's is as follows:"]},{"title":"Pd(Dep(i)=j) \"' F;","paragraphs":["1"]},{"title":"(Dep(i)=j I","paragraphs":["r;,</;J,JJ;j)"]},{"title":"C(Dep( i)=j,","paragraphs":["r;, d;j"]},{"title":",JJ;;)","paragraphs":["C( r i 1 dij, Pij)","di}l and Pij indica.t.<' the number of lFunsets·u's and tlH' Humlwr of punct.uations1 rcspcctivd;y."]},{"title":"91","paragraphs":["Same a.s the case of estimation of head collocation probabilities, modification relations of various length wa .. s extracted from each modification pair."]},{"title":"3. The Algorithm Full Parse","paragraphs":["1. Tokcnization and POS-tagging is applied to the input 2. Construct bnnsetsu' a.nd defi11e its fe<.ttures,","3. Calculate the probabilities of every bnnsetsu pair, by using statistics derived from the EDH. corpus.","4. Compose the most likely (or n-best) de.peudeacy structure ba.<.;ed oH the statistical mod<'l describ<'d i11 , section 2.","For the first st.<'p, we usc the morphological analyz<~r, C:!taSeH[ll].","For tlt<' sccoad step, tokens arc a.mt.lized into bnn sets·u' based on pre-defined regular expres::;ions, and then bunsetsn fea.tun.•s ar<' extracted. The ba..,-:;ic rules for a:<.;signing features a.re a.c; follows:","• The right most content word in the lmnsetsn becomes the h<'ad fntturc.","• l'vlorplwlogica.l information (such a.s word, tag, aud iHflectiou form) of functiou words iu the b-unsetstt. de fitt<'S the depeud<'ll<\"J' relation.","Th<'re is a. room to customize the rules by a. user to cope with exceptional cases which do not fall into a general pattern, and to cope with conceptual differences between s:,rstem clesig11.s.","For the fourth step, we consid<'r tlH' <h:'IH'udcncy structure such that:","e Every bnnsdstt in S <'xcept tlt<~ right most one mod ifi.es Oll<' of its succeeding b~msdsu's iu the scutence","e No two modifications may cross ea.ch other (crossing constraiat) UndN tho:':>e constra.iuts, we use CYK algorithm to ef fectively select tlH~ most likely (n-best) combination of depeitdency relations."]},{"title":"Partial Parse","paragraphs":["\\Vc propose three types of partial pa.rHing, which focuses on the probabilities of each depcndcucy pairs (pO), the probabilities of whole dependency structure (pl), and sorne sp<:•cific dependency relatious (p2). (pO) Output depcmlency rclatious of which probabil ity is higher than a particular threshold. Th(' rc:·mlt is the set of dependcnci('S. (pl) N-best parses me firstly obtained. Then, the depcndcncks that are included in a.ll of the N-best parses are sdccted a.s the result. (p2) Only the dependencies of the sp(•cified rdaticms arc produced. Iu the pO algoritluu\\ we do not usc G)lJ( algorithm."]},{"title":"1f","paragraphs":["there arc more than two modifiees whose clepcmll'ncy probabilities are higher than the threshold, the highest one i:; choSl'II (in other words, do uot care about \"cross ing coustra.int''). Although t-his method i::> very sirnple1 it is useful1 for <'xampk, to help iutera.ctivc correction proc<~durc of trcc-ba1tk constructioit.","To use the"]},{"title":"p2","paragraphs":["algorithm) we must. evaluat<' tlw pre cisimt for each relation typcf. Some cxpcrimen t.s are givca iu the followillg section."]},{"title":"4. System Evaluation","paragraphs":["For thc training a.ml test corpora~ we used EDH .Japaue.sc bracketed corpus"]},{"title":"(GL","paragraphs":["·which contains about 208)000 scutcu<:c-s collected from articles of IW\\vspapl'l's a.nd ma.ga;;:ines.","\\\\.'c splittcd the :;entence.s into tweuty fill's. OJH' of th<'S\\' files is held out for c·valuation and others are used for t ra.iniug.","Full par!->e accuracy is cvaluaJed by the prccisiou of correct dependency pairs. Pa.rtia.l parse accuracy is evaluated by the precisioH and recall of correct dcpcll dcitcy pairs.","PH•cision aud l'('call are ddiHed as follows: Preci:;ion = Number of correct dependencies gt~nerated by th<:> systeu1 --~J-\\J~nnbe~f system's output. of dependencies- -· Re('(d/ :::: N nmher of correct dependencies generated by t-he system -------\"-·---T~tal numhel~-:;fdcpcnd~ucics"]},{"title":"92 Evaluation of Full Parse","paragraphs":["The precision of the numlwr of dependency pain; wa,<:> calculated uuder th<' followiag models. (a) Base-line (b) POS model"]},{"title":"(c)","paragraphs":["LEX model (d) BGH model The model (a) is used as the basc-liHe1 ill whicl1 all modifiers modify its immNliatc right b-ansetsu. ''POS model'' mea11s that POS tags of head-words are used as the lH'ad feature. \"LEX moclc-r' means that POS tags of head-words aud lexical items a,re used. \"BGH modcr' mea.ns that POS tag.s 1 lexica.l items, and word classes an.' used as the head feature. Thc level of the layer:-; i11 th(' thesaurus is altered from 2 to 6 (leaf htJ'l'r).","For e<tch of (b), (c), (d) models, we applied two prob ability models described in section 2 (generation prob ability and collocation probability) to each of head collocation probability and dista.uce probability. Theu e<tch"]},{"title":"(<t),","paragraphs":["(b), (c), a.ncl"]},{"title":"(c!)","paragraphs":["models has four different modeb. But. we only shows the result of the followiug t\\vo models) for the each POS1 LEX) and BGH model. e head-collocation (collocatimt HlOdd)"]},{"title":"+","paragraphs":["distaue<' (g('ll-l'ra.tion model) -+ model-1 0 head-collocation (collocation mod d)"]},{"title":"+","paragraphs":["dista.uce (collocation model) .......,. modcl-2 Since the other two models giv(' th<' performallC(' (pre cision) as ]ow as 70"]},{"title":"%,","paragraphs":["we will not go into ntorc dl'ta.il of those modds. T'hc amount of training data was cha11ged a.nd evaluated iu terms of thl' prccisiou of correct lk· peiidency relations.","Figure 1 shows the result of the precision for tlw inside and outside data uiider \"11HHld-r~ 1","Figure 2 shows the result of tlw precision for thC' in:::;idc and th(' outside data. uuder \"modcl-2\".","·'BGH:61",":","in the figmT nwaHs that the sixtl!··laycr of the t.hesa.urus is used for the word da:-:>s. It slightly outperforms other models that use higher layers in t.he thesaurus.","\\Vhen evalnating with outsid(• data1 we imposed cl'l' tain frequcllC.Y threshold on the stati.':ltical data. that is, the collocation data whose occurr<'HC<' frequency is less than i-tintes was dis<·a.rdcd, where"]},{"title":"i","paragraphs":["is a predetermined","threshold.","Figure 3 sllow the resultiug cltcutg<' of precisions under","the POS, LEX1 BGH modds. The vahH' of ·'i'. was","changed from 2 to 10.","1 I3y '·inside data\", w0 mean that. t.hc training data is used","also for t-he tesl da.ta, \\vhereas ·'outside data'· means that","th~ held-out data is used for the test data."]},{"title":"~ ' §","paragraphs":["[","I . .....,..........-.---....,.............,......-.....,-..,~-,~·-r-·•~,c:,;r.,::;,~=••=•;r~-~\"\"·~'·· ··~ 0.00 0.97 0.00 ·-~ ·-~ o.ro 0.92 0.91 o .• 0.89 0.00 0.87 0.00 0.85 O.M 0~ 0~ 0.8\\"]},{"title":"•.•","paragraphs":["0.79 0\" 0.77 0.76 0.75"]},{"title":"'","paragraphs":["0.00 0.00 ·-~ 0.92 0.0 0.00 0.00 O . .M 0.~"]},{"title":"•..","paragraphs":["0.78 0.76 0.74 0.72 0' 0.00 000 OM 0.~ 0","ln<ido:ki~·POS","···M···","ln>ido:l.kxloO.{IGit6","···~···","Ou\\Si<I<I:Mod&loLEX ·--<1·-","•","Out>klo:Modoi•POS - ..,. .•","OJ!<>do f.lo<!oO.OOH:6 -<>","1 2 3 4 5 B 7 8 9 10 11 12 13 14 IS 11 17 18 19 m t<o.,.,.(>:10.oo:Jsont.....co>) Figure 1: Precision under model-1.","0.6 -....J.........I---<-•• L......J...........l....--'.----'---......L.-~-..-.l.-L.-J.- •. J..-..l....-....L","0 I 2 3 4 5 G 7 8 9 10 \\1 12 13 14 15 16 17 18 19 20","''\"'\"J(•IO.oo:J •onl.....coo)"]},{"title":". I","paragraphs":["Figun' 2: Precision under rnodel-2.-' 0.85-5 0.85 ··\". 0.825","0.13.2 -··----~--~--..-...-..l.....~.....i-............1 1 2 3 • s 6","11\\r>.\"IK>~ POS mOd(ll _...,._. LEX mOd£< ···M··· (lGH","mod£<","• ··--'-·~·--··-'...--"]},{"title":"\"","paragraphs":["Figure 3: Precision of full parses. Trained from 190,000 sentences. Evaluated by 11000 sentences"]},{"title":"93","paragraphs":["From this experiment, we decided to set the value of i to 4.","The LEX model shows the highest performance in both cases, and the result of rnodel-1 outperforms that of model- 2 constantly.","Surprisingly, the BGH model shows poor perfor mance than the POS model. A part of the reason may comes from the fact that we only used one layer of word da..':lses for each experiments. Other reason may be that the hierarchy of \"Bunrui Goi Hyou'1","is not adequate for the syntactic analysis.","The graph shows tha..t the performance of the inside data decreases when the size of training data incrt~<t..<.>es.","The precision of the outside data in \"rnodel-1'1","con stantly close up to the precision of the inside data..","We use \"rnodel-11","l for further analysis."]},{"title":"Contribution of Head-Collocation Probability and Distance Probability","paragraphs":["To test which features of head-collocation and distance feature contribute to the accuracy of pa.rsing1 the fol lowing models arc tested. (e) Distmtcc probability"]},{"title":"(f)","paragraphs":["POS model without the distance probability (g) LEX without the distance probability (h) BGH without the distance probability Each model is trained by 190,000 scntences1 and evalu ated by 11000 sentences held out from the training data. model precision"]},{"title":"%","paragraphs":["correct/ total"]},{"title":"(c) 66.07 5087/7610","paragraphs":["(f)"]},{"title":"79.09 6019/7610 (g) 80.09 6095/7610","paragraphs":["(h)"]},{"title":"77.58 5819/7610","paragraphs":["Table 2: Precision for 1,000 sentences.","The distance probability makes little contribution to the parsing accuracy cornpared to the head collocation probability. This is because the features used for the distance probability is too sirnpl(~."]},{"title":"Sentence Level Evaluation","paragraphs":["\\Vc evaluate sentence level accuracy in this section. A sentence is regarded as correct if the correct structure is found in the n-best parse of the parser1 where n is a predet.ennined value.","Figure 4 shows the ra.t.c of correct parses appearing in the n-best parses, where n is changed from 1 to 10. The average number of b·unsetsu}s in a sentence is 7."]},{"title":"=","paragraphs":["'","~---'-------L___~-...1..-..----'--~--'------\"----'---","2 3 s 6 l 8 g","10","1\\-00>1 FigurE' 4: Distribution of correct parses (out of 10,000 sentences). Trained under LEX model by 190,000 sen teHces. When n is 5 the precision is 65.21"]},{"title":"%,","paragraphs":["and when n 1s 10, it becomes 73.40"]},{"title":"%. Evaluation of Each Relation Types","paragraphs":["\\iVc also check the precision of relation types. The re sults are shmvu in Table 5. The first column spccifi<~s the ty})e of dependency, \\vhich consists of a. word, a tag or an iufie<:tion form. The s<.~cond column iu Table 5 in dicates the ratio of correct dependencies over the total system output.","It is seen that the frequencies of relatio11 type, noun base-form-verb, aud ha-particle are high, and infiuC'ncc system's pcrfonnancc, siuce the precisions for th(;'oe relations are bad. The particle \"ha', \"v<:rb/renyou\", and \"verb/tckei'' ran construct subordinate clauses iH Japau<'S('1 and in some cas('S 1 it is difficult eveu for hu man to consistently detennin<~ its modifiee.","A uouu"]},{"title":"+","paragraphs":["punctuation patter11 is also a problematic case, because it can be a. part of coHjunction phrases. They behave like adverbs (temperaJ noun and adverbial noun) or form subordinate clauses.","In these cases, it is reasona.ble to leave these modifiees unsp<~cified. This doesn 1","t conflict the purpose of nsiug the :;ystern for practical fields or preprocessor of higher NLP, because it is favorable to output relia.bl<; partial parsen rath<~r than output unreliable full parocs."]},{"title":"Evaluation of Partial Parsing","paragraphs":["The results of full parsing accuracy show that model-1 under the LEX model outperforms other models.","For the rnodel1 W(' further examined partial parsing methods explained in section 3, and evalna.ted its pre cision and recall.","Table 3 shows the result of pO algorithm. The first colnmu iu Table 3 indicates the threshold 011 the prob-"]},{"title":"94 ,, ...","paragraphs":["0.7 ·----••. ______ )<_ ··········.\"- ·····-..... .. ............. ·· ......... .. ........... ,_"]},{"title":"••","paragraphs":["........... ~."]},{"title":":: L-:-----;--;----;--;--~:~J ' \"","paragraphs":["Figure 6: Evaluation of pi algorithm. LEX model learned from 190,000 sentences was used. ability of each dependency relation. The degree of the 0.5 8(i.l~--\\6:l;l~~~-:377) 8:l.52 (6:156/7610) 0.6 88.2:l (619:lj7019) 8Ll8 (6193/7610) 0.7 90.2,1 (5999/!Hi48) 78.8:l (5999/7610) o.s"]},{"title":"n.n","paragraphs":["(5705/6179) 74.97 (570.<:i/76to)","L_o\".\"\"-_1 ___ ,'0:g 5!c·c_J ::c9 ( !?..!~!.QLM 0 ~.L"]},{"title":"__ ,_ --''\"i","paragraphs":["7'-'. \"\"'\"\"-' 0( oe_'","''''\"\":Lic_7<\"1l'-'O\")_...J","Table 3: Evaluation of pO algorithm. LEX model","learned from 190,000 sentences was used. reliability (hence the degree of the precision) can be controlled by the value of the threshold ou the proba bilities.","Figure 5 shows the result of pi algoritlnn. The value of \"11 11","in the pl algorithm is varied from 2 to 10.","The degree of the precision can be controlled by the '~.ra.lue of \"u)). Figure 6 depicts the l'<'Sults in graphs.","Q:0t:.-h ·o.:- 'c:;s\"h ':c' ':::\"--+-''cc\"c::':c' ';;';::'o~n:,_'il:;\";;,( 'c;o;;'c',\"\"7'?'!,/;;to:.-t:-_n,_,l l--+_.:'.o.'o;'\";;';.-1 .;,%;.-;(<:or rcc t j to t.a I 2 88.7~ Fll1!l/5'Hl_:!~ 77.14 (5l•IH/7fil?.! :l 91.0:J (570f>/617ll) fi9.9l (570;1/7610)","92.53 (5!)99/{Hi48) 6.\"1.80 (59!19/7610) 5 ll:JA7 {6UJ:lj70Hl) 61.<16 (61H:J/7610)"]},{"title":"\"","paragraphs":["9:l.99 (H:lfl(i/7377) 58.fJ9 (6:JS6/7610)","7 fl-1.71 (fl:l56/7377) 5fi.'2:l ((\\;J;J6/7610) fl5.26 (6356/?a77) 51.11 (6:lf>6j7fll0)","9 95.78 (63fl(l/7:l77) fJ2.:!0 (6356/7610)","·~-_!_~~~~-~~- _IJ5.99 (6:356/7:177) .\"J0.38 (6:JS6j761(~- T'a.bl<' 4: Evaluation of pi algorithm. LEX model learned frorn 190,000 sentenc(~S \\Vas used.","Table 5 ~hows the result of p2 algorithm. p2 algo rithm achieves slightly better precision than full pars<', but. is 11ot a.s good a,<.;; pO and pi algorithms.","\\Vheu comparing three methods) pO algorithm shows highe~t perfonna.nc<', in tenus of the precision and re··"]},{"title":"r","paragraphs":["relation name (lexicon/POS/infiectiou form) predsion (%)"]},{"title":"I","paragraphs":["correct total"]},{"title":"I J","paragraphs":["adjectivc/rentai 95.41 1019 1068","/demonstrative/ 93.72 1329 1418 wo cp 93.32 7000 7501 nojp 92.15 11040 11980 nijcp 91.51 5769 6304","jadjectivejrcnyou 88.14 959 1088","-· gajcP( 87.94 5025 5714",";verbjbac;e 87.32 1344 1539 tojcpf 85.49 1585 1854 mojp/ 83.54 1680 2011 ·--··-","de7cP7 81.83 991 1211 ;verb jtekei 79.55 926 1164","/temporal noun/ -- 78.20 1155 1477 -","da/ declarative/tekei 77.96 902 1157 ha/p/","···-· 75.32 5790 7687 -","75.29 1182 1570 jnour:J_","jverblfenyou 72.43 796 1099 Figure 5: System's outputs were classified according to the right most constituent of relation type! and sorted with their precisions. The symbol cp, <:utcl p in the first column mean C<L'W-particle and particle. Renyou, rcntai tckei and base arc the mtmes of infkction forrns. rc l;;\\:tio~·tJ};-;<:> ·without \"ha'1 without \"verb/renyou,t.<,kci\" without ;;V(~rbf.~-cll)'ou,t.ckci, lm\"","precision% 86.21 (5904/6808) 85.56 (6333/7402) 86.~7 (5748/6640) ·Dthl<' 5: D<'pcndency relations without sorne types of relations. Trained by 100,000 scutenccs. Evaluated by other 1,000 scHtences. calL \\Vlu.'n pO and pi algorithm shows"]},{"title":"sam~nSrcci~ion,","paragraphs":["pO a.lgoritlmt slww~ higher n'calL","pO ami p.l algorithms ca.n be controll<.'d 'by a siugle parameter."]},{"title":"5. Conclusion and Future Works","paragraphs":["\\~;<' showed that the statistical method incorporating lexical kv('l iufonua.t.ion without any grammar rule is effective iu .Japanes<' dependeucy structure a.naJysb.","lu~tea.d of lexical it<'lll::>, W<' also tested word classes of tllt' thesaurus a.':> lH•ad features of phrasal uuits"]},{"title":"(BGH","paragraphs":["model). But. that. modd showed poor perfonuatLcc than the POS model (which uses part-of-SlH'ech ta.gs 1 a.s head features). This m<t:y be because that the hierarchy of applied th<'saurns is not appropriate for the syntactic <utal}·sio.","85% of precioion (the munlwr of correct d('pcndency relations) is achieved by using LEX model.","In those cxpcrinwuts, th<' combiuations of features a.rc dct.cnnin('(l manually by hmnau. There is a room to sdcct tlH' combinations of f<'a.tures automatica.ll,y."]},{"title":"95","paragraphs":["One reason of this comes from the fact that we ap plied various kinds of distance features, such as the mnnlwr of nou11 phrases, the Humber of case pa.rtides, the number of verbs and other kinds of gra.mmatica..l fea tures between t.vm bnnsetsn's, but finally it turned out that simple featnres1 such il$ the numl.K'r of b-unsetsv.\\ ·and punctuations bet\"w<'CH two b·unsdS1t\\) shows good performance. This ma.y imply the limit.aion of man ua.l sekction of combinations of features. Aut.ornatica.l sdection of appropriate features is one of our future works.",";\\\\-\"e also proposed several partial parse rncthods. /\\.mong them, pO algorithm is exhibit('d highest tWl'·· f01'mancc in terms of precision a.ud rcca.ll 1 in spite of its :-:;implicit.y of algorithm.","In pO algorithm, the d('grec of reliability (in other wor<L <h'gree of precision) is controllable by a. single pa.ntlll('tcr.","Partial p<.l.l\"S(' tnethod ca.n be used for other NLP ap plicatioHS1 such ::ts iufonnatiou retrieval or prepron'ss ing of corpus <umotation."]},{"title":"References (l]","paragraphs":["S.F. Chen aud. An empirical study of smoot.h iug t<•chuiqm's for language modeling. Proceedinqs of the 84th Ann1wl Meetinq of the Association fo-r Com-JFntat-ional Linq-uisti.cs, pp. 310·-3181 Jun 1906.","[2] l\\1. Joltll Collins aud Jcuues Brooks. Prepositional phrase at.tachnwut. through a. backed-off\" model. P.roceuli:nqs of the Third WoTkshop on Ve·ry Larqe CoTpora, pp. 27---38 1"]},{"title":".J","paragraphs":["un 1905.","[3] Ivlicha.el Jolm Colli11s. A new statistical parsn based on bigram kxical dcpt'IHleucies. Yroceedinqs of the 84th Annnal Meeting of the Associat.ion fo-r Comyntational Lingnistics1 pp. 184-·--191 1 .Juu 1996.","[4] D.Magennau. Statistical decision-tree model for parsing. Proceedi:ngs of the 83th Annual Meeting of the Association for CornJmf.a.tional"]},{"title":"Linrru:i8t1:cs,","paragraphs":["pp. 276 283, .Jun 1995.","[5] E.CliaTniak. Statistical parsing with a context-free grammar and word statistics. AAAI~ pp. pa.g('S 598 603., 1997.","[6] .Japan Electrouic Dictionary n('SCarch Institute, Ltd. BDR Elect·mnic D-ictiona:ry Techn.ico.l G-u.ide. 1990.","[7] F .. Jdinek and RL.Mcrcer. Interpolated estima tion of rna.rkov source parameters from sparse da.t.a.. Proceed-ings of the WoT.t<>hop on PatteTn Recoqn:i"]},{"title":"tion in","paragraphs":["Practice1 pp. 400-401, :tvlarch 1987."]},{"title":"[8] H.Li. A","paragraphs":["probabilistic disa.mhiguation method ba:;ed on psycltoliuguistic. Proceedinqs of the Forth VVoTkshop on Very Lmge Corpom1 Aug 1996.","[9] S. 1vi. Katz. Estimation of probabilit.ic~ from sparse data for the language model component of a i:ipeech rccoguizcr. IEEE Tra:n..<>a.ctions on Acon8tic.<>, Speech1 nnd Siqnal P.rocessi.nq, Vol. ASSP-35, No. S, pp. 400-401, March 1987.","[10] D. Ma.gerumn. ami M. Ma.rcus. Pearl: i\\ proba. bilist.ic chart parser. Procecdiugs of the 1991 Eu ropca.ll ACL Coufercnce, 1991. Berlin, Germany.","[11] Y. lv1at::>urnoto 1 0. Imaichi, T. Yama~hita, A Ki t<:wchi, and Tonwaki hua.rnura. Mo-rphological analysi8 system. ChaSen ·version J.Ob5 ·u5C'f' nw:n. ual. i'via.t::>nmoto la.b. Nara. Im:t.itnt.e of Scient<' and Technology. (in hpanese), 1996."]},{"title":"[12]","paragraphs":["Wonl List by Se-rna:ntic PTinciples. syuei syuppa.u. (in japanese·), 1()64,1993.","[13] F. Pcrdra. a.nd Y. Sdtabes. Insid<~-outside n· cstima.tiou from pa.rtially bracketed corpora. P.ro cenl-inqs of the 80th Annnal A1eeti.ng of the As80c-iati.on for Com.pu.tat-ional L·inguisti.cs, pp. pag<·s 128 135., 1992.","[lA] H B.ivci:iL Learning decision lists. Machine Learn ill.iJ, pp. 229 246, 1987.","[15) 'f.Briscoe a.nd .Jolm Carroll. G<·nna.lizcd prob·· a.bilistic lr parsing of natural language (corpora.) with 1mificatiou-bas<>d gra.mmars. Com.pntational"]},{"title":"96","paragraphs":["Linq·w:stics, Vol. Vol.191 No.1, pp. 25 .. ~29, i\\'lar 1993.","[16] W.ll.Hogmhout ancl Y.Matsumoto. TrainiHg stochastic gnunma.rs on scma.ntica.l categories. Connect·ion·ist, Statistical~ and Symbolic Ap pmaches to Learninq for Nat·ural La.ng·uaqe P.ro cessing~ 1996.","[17) D. Ya.TO\\\\'sky. D('cision lists for lexical ambigu ity rcsolutiou: Application to accent restoration in spanish and frcuch. P.roceed·i.ng8 of the 32th An nual Meetinq of the Association foT Cornptl.tati.onal L-inqttistics, pp. 88--·95, .Tun 1994.","[18] H Yasuhara. Kaka.ri-uke dependency analysis with lt'<trning fmtction based oH reduc('d type cooccur rellce n'lation. Jo·u'tnnl of ]u.pa:nesc As8oc-iat:ionfor Lnngtwge Processinq, pp. 87--101, Oct. 1996."]}]}
