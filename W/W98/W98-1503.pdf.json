{"sections":[{"title":"Aligning Clauses in Parallel Texts Sotii·is Boutsis and Stelios Piperidis Institute for Language and Speech Processing - ILSP Artemidos & Epidavrou 151-25, Athens, Greece tel:+301 6800959, fax:+301 6854270 email: {sboutsis, spip}@ilsp.gr National Technical University of Athens - NTUA Abstract","paragraphs":["This paper describes a method for the automatic align ment of parallel texts at clause level. The method fea tures statistical techniques coupled with shallow linguis tic processing. It presupposes a parallel bilingual corpus and identifies alignments between the clauses of the source and target language sides of the corpus. Parallel texts are first statistically aligned at sentence level and then tagged with their part-of-speech categories. Regular grammars functioning on tags, recognize clauses on both sides of the parallel text. A probabilistic model is ap plied next, operating on the basis of word occurrence and co-occurrence probabilities and character lengths. De pending on sentence size, possible alignments arc fed into a dynamic progranuning framework or a simulated annealing system in order to find or approxim~te the best alignment. 1he method has been tested on"]},{"title":"a","paragraphs":["Small Eng~ lish-Greek corpus consisting of texts relevant to software systems and has produced promising results in terms of correctly identified clause alignments."]},{"title":"Introduction","paragraphs":["The availability of large collections of texts in electronic fom1, has given rise to a wide range of applications aim~ ing at the elicitation of linguistic resources such as tTanslation dictionaries, transfer grammars and retTieval of translation examples (Dagan et al., 1991; Matsumoto et al., 1993), or even the building of fully-blown machine translation systems (Brown et al., 1990). The pmpose of this paper is to describe a technique for extracting trans lation correspondences at bellow sentence level by em ploying statistical techniques coupled with shallow lin guistic processing catering for the segmentation of sen tences into clauses.","Statistical processing has proved powerful for the exh·action of translation equivalences at sentence and intra-sentence level. Brown et al. (1991) described a method based on the number of words contained in sen tences. The general idea is that the closer in length two"]},{"title":"17","paragraphs":["sentences are, the most likely they are to align. Moreo ver, certain anchor points and paragraph markers are considered. Dynamic progra111111ing and HMMs are pipe lined to produce alignments at sentence level. The method has been applied to the Hansard-Corpus, achiev ing an accuracy of 96%-97%. Gale and Church ( 1991) proposed a method that relies on a simple statistical model of character lengths. The model is based on the observation that longer sentences in one language tend to be translated into longer sentences in the other language while shorter ones tend to be translated into shorter ones. A probabilistic score is assigned to each pair of proposed sentence pairs, and a dynamic programming framework calculates the most probable alignment. Although the apparent efficacy of the Gale-Church algorithm is unde niable and validated on different pairs of languages, rt faces problems when handling complex alignments(l-0, 1-2, 2-2).","'Simard et al. (1992) argue that a small amount of lin guistic information is necessary in order to overcome the inherited weaknesses of the purely statistical techniques. They proposed using cognates, which are pairs of tokens of different languages sharing 11","0bvioUS 11","phonological or orthographic and semantic properties, since these are likely to be used as mutual translations. Papageorgiou et al. (1994) proposed a generic alignment scheme invoking surface linguistic information coupled with information about possible unit delimiters depending on the level at which alignment is sought. Each unit, sentence, clause or phrase, is represented by the sum of its content part of speech (POS) tags. The results are then fed into a dy namic programming framework that computes the opti mum alignment of text units.","Brown (1988) uses a probabilistic measure to esti mate word similarity of two languages in the context of statistically-based machine translation. Kay and Ro escheisen (1993) present an algorithm for aligning bilin gual texts on the basis of internal evidence. Processing is pcrfom1ed in many iterations and each new iteration uses the results of the previous one in order to calculate more accurate word and sentence correspondences. In each iteration, processing consists of calculating corre spondences between sentences on the basis of their rela tive positions, and then calculating word correspon dences on the basis of word co-occunences in related sentences. The Dice coefficient is used as the similarity measure between words of two languages in an attempt to secure the conectness of the alignment of parallel texts at sentence level. Kitamura and Matsumoto (1995) have used the same Dice coefficient to calculate the word similarity between Japanese-English parallel corpora. Single word correspondences have also been investigated by Gale and Church (1991 b) using a statistical evaluation of contingency tables. Piperidis et al. ( 1997) and Boutsis and Piperidis ( 1996) describe methods for extracting sin gle and multi-word equivalences based on a parallel cor pus statistically aligned at sentence level and employing a similarity metric along the lines of the Dice coefficient with comparable performance.","Collocational conespondences have been studied by Smadja (1992) and Smadja et al. (1996), in an attempt to find h·anslation patterns for continuous and discontinuous collocations in English and French. Meaningful colloca tions are first extracted in the source language while their corresponding French ones are found by calculating the mutual information between instances of the English collocation and various single word candidates in Eng lish-French aligned corpora. Recent work has broad ened the scope identifying correspondences between word sequences. Kupiec (1993) proposes a method for extracting translation patterns of noun phrases from Eng lish-French parallel corpora. The corpus is tagged at part of~spcech (POS) level and then finite-state recognizers specified by regular expressions defined in tenns of POS categories detect noun phrases on either side. Probabili ties of correspondences are then calculated using an it erative EM-like algorithm. Kumano and Hirakawa (1994) presuppose an ordinary bilingual dictimmy and non-parallel corpora, attempting to find bilingual cone spondences in a Japanese-English setting at word, noun phrase and unknown word level. Extending previous work, Kitamura and Matsumoto (1996) apply the Dice coefficient on word sequence correspondence extraction.","This paper describes a method for the automatic alignment of parallel texts at clause level. Texts are first aligned at sentence level using statistical techniques. Part-of-speech tagging takes place next annotating each word form with the appropriate part of speech. Process ing in this step and the next one is monolingual, so each language side of the text is treated independently of the other. Surface syntactic analysis is performed next on the basis of regular grammars. Shallow parsing results in the recognition of clauses. Statistical processing follows taking into account different sources of information, aiming at identifying intra-sentence alignments formed by the clauses of the parallel sentences of the bitext. The"]},{"title":"18","paragraphs":["method caters for alignments of type 1-0, 1-1, 1-2, 2-1, and 2-2. A first pass through the text computes occur rence and co-occunence probabilities for content words on both language sides. A probabilistic score, expressing the probability that a clause (or a pair of clauses) of the source language is h·anslated into a clause (or a pair of clauses) of the target language, is computed on the basis of the previously calculated word probabilities, and a model of character lengths. Possible clause alignments are examined by a dynamic programming framework deciding on the best alignment. Avoiding combinatorial explosion requires that large sentences be channeled into a module that approximates the optimal alignment through simulated amrealing, operating in polynomial time. EM iterative training caters for the estimation of the model's parameters, given the lack of hand-aligned training material. The overview of the processing is pictured in Figure 1."]},{"title":"Test Corpus","paragraphs":["The cmpus used to develop and test the proposed algo rithms consists of text from the HP-VUE software plat form documentation set. The Greek text contains 35726 wordfomrs and the English text 28872. The number of different words is 4512 for the Greek text and 3219 for the English text. The richer mmphology of the Greek language accounts for the approximately 30% difference between these two figures."]},{"title":"Text Handling","paragraphs":["Recognizing and labeling surface phenomena in the text is a necessary prerequisite for most Natural Language Processing (NLP) systems. In order to be able to make full use of the corpus, texts should be rendered in an ap propriate fmm. To this end, parallel texts are normalized and handled. In the framework of the presented method, basic text handling is perfonned with the use of a Mu1text-like tokeniser, (Di Christo et al., 1995). Identifi cation of word boundaries, sentence boundaries, abbre viations etc. takes place. Following co1nn1on practice, the tokeniser makes use of a regular-expression based defi nition of words, coupled with downstream precompiled lists for the Greek and English language and simple heu ristics. This proves to be quite successful in recognizing sentences and words effectively."]},{"title":"Senteuce Alignment","paragraphs":["Alignment consists in establishing correspondence links between units in a bilingual text. At this stage, the method aligns input text at sentence level. Processing caters for sentence substitution (one sentence translates into one)) deletion (a sentence is not translated at all), insertion (a sentence with no equivalent in the source text Shallow Parsing & Clause Recognition Figure 1: Processing Overview is introduced by the translator), contraction (two con secutive sentences translate into one), expansion (one sentence translates into two) and merging (two sentences translate jointly into two).","The heart of the alignment scheme, employed at this stage, is a method for aligning sentences based on a sim ple statistical model of character lengths, (Gale and Church, 1991). The method relies on the assumption that longer sentences in the source language tend to be trans lated into longer sentences in the target and vice-versa. A probabilistic score is assigned to each pair of proposed sentence pairs, based on the ratio of lengths of the sen tences and the variance of this ratio. This probabilistic score is used in a dynamic programming framework in order to find the maximum likelihood alignment of sen tences. Additionally, following (Brown et al., 1991) certain points of the texts can be anchored thus dividing them into smaller sections that need to be aligned. Be sides anchors, paragraph markers are also_ /considered. Anchor points are specific to the text to"]},{"title":"be·","paragraphs":["aligned and they usually appear in both texts. They are divided into major and minor anchors and alignment proceeds in two steps, first aligning major anchor points and then minor anchor points, followed by sentence alignment. The alignment algorithm has been tested in the setting of a multilingual text processing system and has been re ported to yield accuracy between 96% and I 00%, (Piperidis, 1995)."]},{"title":"Part of speech tagging","paragraphs":["Both English and Greek texts are analyzed morphosyn tactically. The words in the patallel sentences are tagged with their corresponding POS categories. The corpus is thus represented as a bitext of tagged mutual sentence translations where every word is accompanied by its cor responding POS tag."]},{"title":"For Greek","paragraphs":["Tagging with part-of-speech information for Greek takes place in two steps. First, each word is endowed with all possible tags tlnough lexicon lookup, and then a disam biguation module decides on the most probable am1ota tion.","Lexicon lookup operates on a morphological lexicon of modern Greek. It endows the words of the text with the characteristics found in the lexicon. The tagset used has been devised for the morphological annotation of Greek corpora and conforms to the guidelines set up by EAGLES and PAROLE, trying, at the same time, to capture the morphological peculiarities of the Greek lan guage.","Text produced at the output of lexicon lookup is an notated with below POS information i.e. subcategorisa tion information for each POS category. Each wordform recognised as noun, for example, is annotated for case, number, gender etc. Ambiguous wordfmms are endowed with all possible annotations. However, not all available mmphological information is necessary for later proc essing. In addition, wordforms grammatically fully char acterized with below POS information are highly am biguous. Retaining all such information would impose a heavy burden on the disambiguation process. Experi mentation has proved that performance of next stages is not seriously affected by reducing the tagset. To this end, a simplified tagset has been used helping reduce ambigu ous wordforms notably. In addition, words not found in the lexicon are assigned possible tags on the basis of a probabilistic model operating on word suffixes. In case ·of multiple tagging, a disambiguator based on trigrams and contextual rules trained on Greek texts, suggests the"]},{"title":"19","paragraphs":["tag that is most likely to be the correct, (Papageorgiou, 1996). This stage produces around 95% correct results."]},{"title":"For English","paragraphs":["Tagging for English is based on mainstream statistical processing. A tagger implementing hidden markov model techniques is employed. The tagger has been trained on a large preannotated text collection and is then used to tag the HP-VUE test corpus. For training purposes, a set of technical texts annotated at POS level, drawn from the British National Corpus (BNC), has been used, (Burnard, 1995). Texts classified under the field codes: \"Written: Domain: Informative: Natural and pure sciences\" and \"Written: Domain: Infotmative: Applied Science\" have been selected. The size of the text collection is ca. 5,000,000 words. Text is annotated with POS tags ac cording to the BNC tagset (Leech, 1995). This text col lection is used to train the Acquilex HMM tagger (El worthy, 1997) and estimate model parameters. After training, the HP-VUE corpus is tagged by application of the Viterbi algorithm."]},{"title":"Clause recognition","paragraphs":["This stage) like the previous one, processes each lan guage side of the text independently of the other. It aims at breaking sentences of both languages into clauses with well-defined boundaries.","In order to recognise clauses, this stage takes advan tage of a shallow parser equipped with granunars for Greek and English. Syntactic analysis consists of parsing via finite state automata. Under this approach, a text can be analysed syntactically on the basis of granunars con taining non-recursive rules written in the form of regular expressions. Rules are numbered in order to be applied in a certain order. The grammar is translated into finite state automata with standard techniques (Aho ct a!., 1986) and automata are connected in a pipeline in order to form a cascade, which is used to annotate text in an incremental way. Each rule (regular expression) de scribes a specific phenomenon and higher-order mles can be expressed on the basis of the already described ones. Rules are designed to be reliable when they arc applied using longest match) in order to avoid the need for dis ambiguation between different length instances of the same constituent type.","A basic characteristic of this method is that parsing is deterministic and no backtracking takes place. No ambi guity is produced since each automaton takes a definite decision about a constituent's existence or non-existence. This doesn't mean that ambiguities are resolved but that they are enclosed inside syntactic chunks, whose bounda ries have been recognised, although their internal struc ture may have not been decided. Enclosure of ambiguity helps generate only one partial parse for each sentence, since ambiguity is kept local and does not cause the pro duction of multiple parses for the whole sentence.","It should be noted that the method does not depend on the exact method adopted for clause recognision. Another system performing clause recognition could be used in stead. This has also to do with the availability of the rele vant linguistic processing modules. On the other hand, being aware of the complete partial parse can be ve1y useful, if one is up to extend the method to cover other types of sub-sentence alignments (e.g. alignment of np's). It is also significant that the additional processing of shallow parsing does not impose serious speed over heads since the speed of analysis is measured in tens of hundreds of words/second. Clause boundaries for each analysed sentence are channelled into the next stages of processing. No distinction is made between different clause types. A sample output of this stage is shown in Figure 2."]},{"title":"20","paragraphs":["[c/ SEVERAL U71LI11ES HELP YOU elj [el DIAGNOSE CONFIGURA110N AND DATABASE ERRORS c/j fc/ IIOAAA BOHBH11KA llPOTPAMMATA BOliBOYN c/] [el NA d!AJWQEETE E<PAAMATA AJAMOP<PQI.HE KAI BALIJE AEtJOMENQN elj [c/ IF YOUR SYSTEM IS PROPERLY CONFIGURED clj [c/ TO AUTOMATICAJ.LY RUN liP VUE elj. [cl YOU WILL SEE 71!E liP VUE LOGIN SCREEN cl] [c/ WHEN YOUR SYSTEM IS BOOTED clj /clAN TO LTEFHMA DIE EJNAI EQl.TA AIAMOP<PQMENO elj fcl 17.4 NA EKTEAEI AYTOMATA 70 HP VUE elj [c/ BA AEJ11i THN 080NH EYNAEEH!.: TOY HP VUE elf [el OTAN 70 EYLIHMA DIE EKKJNEJ el/ /cl If YOU HAVE NO CONSOLE elf, [c/ YOU MUST J.OG IN FROM A REMOTE SYSTEM elf [elAN AEN YIIAPXEJ el] /cl llPE/lEI NA EILEABETE A/70 ENA AIIOMAKPYEMENO H2.71IMA elj","Figure 2: Parallel text with marked clause boundaries"]},{"title":"Translation model Part a","paragraphs":["In this section we present the basic translation model, which is used for the purposes of clause alignment. Let's consider two corresponding sentences of the parallel text which are translations of each other, the source sentence ~L \"\"'scil ~ci"]},{"title":"2","paragraphs":["... ~cil and its tTanslation into the target language ~L"]},{"title":"=","paragraphs":["tcil tc 12 ... tcirn where sci and tci are clauses identified during the previous stage. We approximate sentence h·anslation with the assumption that clauses can be tTanslated from the source into the target language in the following ways:","A. 1-0 and 0-1, when a clause of the source or the target sentence has no equivalent clause in the other language.","B. 1-1, when a clause of the source sentence is translated into one clause of the target sentence.","C. 1-2 and 2-1, when a clause of the source is translated into two clauses of the target or two clauses of the source translate into one of the target.","D. 2-2, when two clauses jointly translate into two clauses of the other language.","We view each group of aligned sentences of the par allel text as a sequence of clause-beads (after sentence beads in (Brown et al., 1991)) where a bead accounts for a group of clauses that align with each other according to one of the above mentioned ways. A clause-alignment Ai = { ail ai2 ... a in } for a given pair i of sentences is a set of clause-beads"]},{"title":"a","paragraphs":["ti covering all clauses of the source and target sentence under the condition that each clause participates to one and only one clause-bead. Figure 3 shows a schematic example of a clause alignment between two sentences containing four and three clauses each. Making the assumption that transla tion of clauses in a bead is independent of clauses be longing to other beads we seek the alignment that maxi mises the joint distribution: (!) and assuming that Pr(n) (where n is the number of beads in the alignment) is independent of"]},{"title":"s","paragraphs":["1, Ti and n we get: (2)"]},{"title":"s","paragraphs":["is ignored for the rest of the analysis, since it is a mul tiplicative constant factor having the same value for all clause-alignments."]},{"title":"Part b","paragraphs":["Finding the. correct alignment requires that we estimate clause-bead probabilities Pr(a;j) which express the probability for the source sentence clauses of the bead to be translated into the corresponding target sentence clauses. We consider a 1-1 bead covering the source and target clauses: scis = swisl sw1s 2 ... sw;.~p and","(where swisp is the p1","h word of the s1","h clause of the i1","h source sentence of the parallel text etc.) A first writing ofPr(aij) can be as follows:","Pr(aij) = P1_1 Pr(scis, tcit) -· --~ (3) where ?1_1 is the probability of a '1-l' clause alignment. Referring to the second factor of (3), in order to ap proximate Pr(scis, tcit) we take into account two pa rameters: a) the length of the source and target clauses and b) the source language and target language words contained in ~~cis· and ~~it . We model the probability that source text with character length l(scis) is h·ans-"]},{"title":"21","paragraphs":["Bead 1 Bead 2 Bead 3 ~····~·-~-\"~-.-.,.--~~"]},{"title":"[§>J 8 8 8 8 8 8 ______","paragraphs":["_)","Figure 3: An aligmnent with three beads (SC:Source sentence Clause TC:Target sentence Clause) lated into target text with length l(tcit) with a distribu tion"]},{"title":"Pr~(sc,,.","paragraphs":["), l(tcit) ). Under the assumption that the model used by the sentence aligner (\"Sentence Align ment\" section , (Gale and Church, 1991)) expressing sentence alignment probabilities on the basis of character lengths"]},{"title":"r","paragraphs":["valid when applied to clause-lengths, we estimate PrV(scis) l(tcit)) with the same model.","Furthermore, we approximate clauses by unordered sets focusing on content can·ying words i.e. content words, which are taken to be verbs, nouns, adjectives and adverbs. Thus, we assume that content words contribute the most to the examined probability. tcit and scis are represented by the unordered sets of the content words they contain. Following that, equation (3) can be written as: (4) where sew stands for source clause content word and tcw stands for target clause content word. To approxi mate the third factor of Eq. ( 4) we assume that the con tent words of the source clause are independent events and the same is valid for the words of the target clause. That is:"]},{"title":"Pr({~·cwisl","paragraphs":[",scwis2•····scwisv"]},{"title":"h = Pr(~cwitl","paragraphs":[",tcwit2 , ... ,tcwitw"]},{"title":"}J","paragraphs":["= Pr(tcwitl) Pr(tcwit 2 ) ... Pr(tcwitw) (5) (6)","Under this model each word of the target clause de pends on zero or one word of the source clause. To illustrate, let's consider the source clause","sc = { scw1, scw 2 , scw","3 } the target clause"]},{"title":"1~ ~ {","paragraphs":["tewl' tcw2' tew3 } and a word alignment wj so","that lcw1 depends on sew","1, lew","2 depends on scw","2","while tcw 3 and scw","3 are independent events. In this case,","Prw. ( { sew","1, scw","2 , scw","3 }, { tcw","1, tcw 2 , tcw","3"]},{"title":"}J ~","paragraphs":["J","Pr(tcw1, sew","1) Pr(lcw2 , sew 2 ) Pr(lcw3 ) Pr(scw3 ) (7)","given the computation of Figure 4. Consequently, when estimating bead probability Pr( aij) , we need to sum probabilities over aU possible word alignments Wj. This would require however to inspect an exponentiaUy large set of possible word alignments. Thus, we would like to approximate the sum with its biggest tem1. This is not feasible, eitl1er. So, a greedy-like technique is followed, which does not guar antee to find the best word alignment but usually comes up with a big enough value to distinguish between good and not so good clause alignments. The largest word pair probabilities are selected first while probabilities of any unmatched words are taken into account next. In order to select a pair of words for Eq. (7) two heuristic conditions must be met: 1) the occurrence frequencies of the two words should not differ more than 50%, 2) their co-occurrence frequency in the bitext should not differ more than 50% from their occurrence frequencies in the texts.","In case of a non '1-1' alignment between clauses, the","Prw . ( { sew!' scw","2 , scw","3 }, { lew!' tew 2 , tew","3"]},{"title":"}J ~","paragraphs":["J same model is used, where"]},{"title":"f;_","paragraphs":["1 is substituted by ~--"]},{"title":"2","paragraphs":[","]},{"title":"P","paragraphs":["2_ 1 ,"]},{"title":"P","paragraphs":["2_ 2 , ~-o and"]},{"title":"P","paragraphs":["0_ 1 . We take ~-"]},{"title":"2","paragraphs":["~P"]},{"title":"2","paragraphs":["_"]},{"title":"1","paragraphs":["and ~-o ~"]},{"title":"P","paragraphs":["0_","1 . The distribution on character lengths is also","taken to be independent of the alignment type."]},{"title":"Model Training","paragraphs":["In order to calculate clause-alignment probabilities, given the model presented in the previous section, estimations for several model parameters should be available. At this stage, parameters are estimated on the basis of simple corpus statistics. The probability of a single word of the source or target text is taken to be:"]},{"title":"f(w)","paragraphs":["Pr(w)=--- IJ(w') w' (8) where the denominator of Eq. (8) is the sum of the fre quencies of all words i.e. the lengtlr of the source or the target text in words. Conespondingly, the probability relating a word of the source text with a word of the tar get text is estimated by:","f(sw,lw) Pr(sw, tw) ~ _ __c:.__:c__cc_:___"]},{"title":"I","paragraphs":["f(sw' ,lw') (sw',tw') (9)","For the presented application of the method, these probabilities are computed over the whole corpus. In very large texts it is adequate to estimate the probabilities in a representative large portion of the text. It would be also possible to use pre-computed probabilities from an other text of the same domain, given that both texts share Prw."]},{"title":"c{","paragraphs":["lew","1, tcw","2 , tew","3 }/{ sew!' scw","2 , scw","3"]},{"title":"}J","paragraphs":["Pr( { scw 1, scw2 , scw","3"]},{"title":"}J ~","paragraphs":["J (Eq. (5), (6)) Prw. (tcw1"]},{"title":"j{","paragraphs":["scw1, scw","2 , scw","3","J","Pr(sew 1 ) Pr(scw 2",") Pr(scw 3 ) ~ Pr(lew1/scw1) Pr(lcw2 /scw2 ) Pr(lcw3 ) Pr(sew1) Pr(sew2 ) Pr(sew3 )"]},{"title":"~","paragraphs":["Pr(lcw1 , scw","1) Pr(lcw2 , sew 2 )","--\"-----~'-- -------=~-'~ Pr(lcw","3",") Pr(sew 1",") Pr(scw 2",") Pr(scw 3 ) ~","Pr(sew1) Pr(scw2 )","Pr(tcw1, scw","1) Pr(tcw2 , scw","2 ) Pr(tew3 ) Pr(.s·cw3 )","Figure 4: Computation of Prw. ( { scw","1, scw 2 , scw3 }, { tcw1, tcw","2 , tcw","3"]},{"title":"}J","paragraphs":["J"]},{"title":"22","paragraphs":["the same characteristics with respect to language use, coverage and translation. Estimating ~--r, !~"]},{"title":"__","paragraphs":["2 ,"]},{"title":"P","paragraphs":["2 __ 2 and"]},{"title":"P","paragraphs":["0 .","1 rs less straightforward. Given the lack of training material, that is marked-up text aligned at clause level, no safe set of values can be computed for these parameters. To work around this, we first make an educated guess and then apply the EM (Expectation-Maximization) algorithm. The EM algorithm consists of two major steps: an ex pectation step followed by a maximization step. The ex pectation uses the current estimates of the parameters to process input data and the maximization provides next a new estimate of these parameters. These two steps iter ate until convergence. EM is not guaranteed to converge to a global maximum; if many points of local conver gence exist, the point where the method will convergence will depend on the initial parameter estimations. The initial parameter values we used and the estimated ones after the process converged arc displayed in the Table I.","If an alignment type does not occur in the output (' 1-0' alignment in this case), the relevant probability takes a very small value (IE-4)."]},{"title":"Best Clause-Alignment Selection","paragraphs":["This stage aims at finding the best alignment between the clauses of two parallel sentences (or in the case of a non '1-1' sentence alignment e.g. '1-2', an alignment is sought between the clauses of the source sentence and the clauses of the two target sentences). Two schemes are considered, dynamic programming and simulated annealing.","Dynamic programming is a generalizat~on of the greedy technique. It can be used to solve problems, whose solutions can be considered as a sequence of deci sions. Usually dynamic programming is uSed to address an optimization problem, seeking the sequence of deci sions giving the optimal solution. In many problems, decisions taken on the basis of local data always lead to optimal solutions; this is the case of problems solved by greedy teclmiques. On the other hand, there are prob lems, including alignment, for which this doesn't hold true. In this case one would have to generate all possible decision sequences and evaluate them. Dynamic pro gramming can be used to exclude sub-optimal decision sequences so that they may not be considered. The prinM ciple of optimality governing dynamic programming is: \"Any sub-sequence of the optimal decision sequence is optimal for the sub-problem corresponding to this sub sequence of decisions\".","Although dynamic programming is successfully ap plied to sentence alignment, it comes close to its limits when dealing with sub-sentence alignments given that the assumption of the left-to-right translation made for sentence alignment, is not valid at the bellow sentence"]},{"title":"23","paragraphs":["Alignment Initial Probability Probability Type Estimation after Conver-","gence 1-0 0.05 0.0001 1-1 0.8 0.6986 1-2 0.1 0.2465 2-2 0.05 0.0548 Table I : Initial and estimated probabilities level, or in other words, the order of the clauses in the source language is not the same in the target language. To handle cases of clause-alignments involving a number of clauses in the order of ten or more, we use a simulated annealing framework to approximate the optimal align ment. Simulated annealing (Metropolis et aL, 1953), (Kirkpatrick et aL 1983), is a method for optimising functions depending on a large number of parameters. Annealing is a metallurgical term and the method is in spired by the controlled cooling of metals getting from the liquid to the solid state. The algorithm has been suc cessfully applied for optimization purposes, including the approximate solution of TSP (Traveling Salesman Prob lem). This algorithm does not guarantee to find the best solution, but it may come up with a good approximation of it in non-exponential time. Processing starts with a random clause-alig1m1ent A. Initial temperature setting is T~45 and after each iteration it is reduced by 0.9. Each iteration is performed tlu·ough 1000 steps. In each step, a random change in A is proposed and the cost function (negative logarithm of the clause-aligurnent probability) is computed. If the new aligurnent is better, the change is","liE adopted, if not, it is adopted with probability"]},{"title":"P = e","paragraphs":["T , wl1ere LJE is the change in the cost function. Once the loqp is computed with no change in the configuration, or 10 iterations have been performed, the best alignment that has been found till that time is proposed."]},{"title":"Results","paragraphs":["The method has been applied to the corpus presented in section 2. A sample output of the method is displayed hereunder. Each table contains a source sentence, a tar get sentence and the set of proposed clause alignments (underlined alignments are wrong): Aligurnent type·2-2 Dynamic Programming (DP)"]},{"title":"'","paragraphs":["{ciiF YOU HAVE NO CONSOLE cl], {cl YOU MUST LOG IN FROM A REMOTE SYSTEM cl] {clAN flEN YnAPXEI c/] {cl nPEnEI NA EliEIIGETE AnO ENA AnOMAKPYi:MENO i:Yi:THMA c/] IF YOU HAVE NO CONSOLE <->AN flEN YnAPXEI YOU MUST LOG IN FROM A REMOTE SYSTEM<-> nPEnEI NA Eli:EIIGETE AnO ENA AnOMAKPYi:MENO i:Yi:THMA Alignment type-3-3 DP"]},{"title":",","paragraphs":["{el THERE ARE SEVERAL REASONS elf [el THAT HP VUE MIGHT FAIL elf {el TO START elf {el YnAPXOYN nOMOI 110101 elf {cl 1/A TOYI: OnOIOYI: TO HP VUE MnOPEI NA AnOTYXEI elf {el NA =EKINHI:EI elf THERE ARE SEVERAL REASONS<-> YnAPXOYN n0/\\1101 110101 THAT HP VUE MIGHT FAIL<-> 1/A TOYI: OnOIOYI: TO HP VUE MnOPEI NA AnOTYXEI TO START<-> NA ooEKINHI:EI Alignment type:4-3, DP {el WHEN HP VUE FAILS elf [el TO BEHAVE elf [el AS EXPECTED elf, {el YOU SHOULD OPEN THE APPROPRIATE ERROR-MONITORING FILE elf {el OTAN TO HP VUE AnOTY/XANEI elf [cl NA I:YMnEPI<PEPGEI KA TA TO ANAMENOMENO elf [el GA nPEnEI NA ANOEETE TO KATAMH/\\0 APXEIO nAPAKO/\\OYGHI:HI:I:<PAIIMATDNelj WHEN HP VUE FAILS<-> OTAN TO HP VUE AnOTYIXANEI TO BEHAVE AS EXPECTED <-> NA I:YMnEPI<PEPGEI KA TA TO ANAMENOMENO YOU SHOULD OPEN THE APPROPRIATE ERROR-MONITORING FILE<-> GA nPEnEI NA ANOEETE TO KATA/\\1\\H/\\0 APXEIO nAPAKO/\\OYGHI:HI: I:<PAIIMA TDN","Alignment type:6-6, Simulated Annealing(SA)",",--~--~~~------------~~------- {eiiF YOU PREVIOUSLY USED SOFTBENCH elf {elAND HAVE A PERSONAL <DIR>!HOMEDIRECTORYI .SOFT/NIT <!DIR> FILE elj, [el YOU MAY NEED elf [el TO REMOVE THE FILE elj [el OR EDIT IT elf [el TO INCLUDE THE HP VUE TOOLS elf [elAN nPOHIOYMENDI: XPHI:IMOnOIHI:A TE TO SOFTBENCH elf [el KAI EXETE ENA nPOI:DntKO APXEIO <DIR>!HOMEDIRECTORYI.SOFTINIT<!DIR> elf [el MnOPEI NA XPEIAHEI elf [el NA A<PAIPEI:ETE TO APXEIO elf [cl H NA TO TPOnOnOIHI:ETE elf {cl DHE NA nEPIMMBANEI TA EPrAIIEIA HP VUE cl] IF YOU PREVIOUSLY USED SOFTBENCH <->AN nPOHIOYMENDI: XPHI:IMOnOIHI:A TE TO SOFTBENCH AND HAVE A PERSONAL <DIR>IHOMEDIRECTORY I.SOFTINIT<!DIR> FILE<-> KAI EXETE ENA nPOI:DntKO APXEIO <DIR>IHOMEDIRECTORYI.SOFTINIT<IDIR> YOU MAY NEED <-> MnOPEI NA XPEIAI:TEI TO REMOVE THE FILE<-> NA A<PAIPEI:ETE TO APXEIO 24 OR EDIT IT<-> H NA TO TPOnOnOJHI:ETE TO INCLUDE THE HP VUE TOOLS <-> DHE NA nEPIIIAMBANEI TA EP/AIIEIA HP VUE","The performance has been evaluated on a text pmiion containing ca. 250 sentences and overall precision of the output has been calculated to be 85. 7%. If we exclude cases of misalignments due to errors in stages of proc essing preceding clause-alignment, we can calculate the precision of the last stage. In this case, precision is higher than 96%, so the error-rate introduced during clause alignment is less than 4%. In addition to the low error rate, clause-alignment corrects some of the enors caused by the previous stages, as it is mentioned in the next sec tion."]},{"title":"Discussion Given the incremental and engineering approach","paragraphs":["adopted, the results obtained so far are quite encouraging. The accuracy of tl1e output lies around +85%, making the method quite reliable and suitable to be used in real world application systems.","Most of the errors were introduced by the first three primary processing stages, that is sentence-alignment) POS tagging and clause recognition. Major improve ments in performance will certainly require further opti mization of some or all of these stages along with any refinements to the statistical clause-alignment model used in the last stage. Regarding refinements to clause alignment, there are several sources of information that could be readily taken into account. For example, pre compiled bilingual dictionaries could be of help in order to establish reliable word associations in very short texts, which do not allow the safe estimation of the required word probabilities, while preference tules on clause types could be used to reduce search space, favoring align ments betvveen certain clause types and penal ising others. Future developments are believed to help improve accu racy and performance and broaden the coverage of the system in order to cover additional types of sub-sentence alignments. An interesting remark is that errors intro duced by preceding stages are sometimes repaired by clause-alignment. For example, it may happen that a sentence is mistakenly chunked into clauses due to tag ging or other CITors. Then '1-2' and '2-2' clause alignments may function in such a way that illegally separated sentence pieces are brought back together.","It is well understood that linguistic resources building is one of the important stumbling blocks in the localiza tion/internationalization exercise. Methods approximat ing the automatic generation of such resources prove to be effective on a cost/time basis. Besides gains in speed and efficiency) the data driven approach improves con sistency, which is an important requirement for systems operating in a multilingual setting. By adopting a data driven approach and exploiting existing linguistic proc essing modules, the method produces textual parallel data of high resolution which can give a competitive advan tage to multilingual processes and systems, such as semi automatic lexicon builders, machine aided translation systems and retrieval of multilingual material."]},{"title":"References","paragraphs":["Abo A., R. Sethi, and J. Ullman. 1986. Compilers, Prin ciples, Techniques and Tools. Reading, Masschusets: Addison Wesley","Burnard, L. 1995. Users Reference Guide for the British National Coqms, British National Corpus Consor tium Report, Oxford, England.","Boutsis, S., and S. Piperidis. 1996. Automatic Extraction of Bilingual Lexical Equivalences from Parallel Cor pora. In Proc. Multilinguality in Software Industry IECAI, 27-31.12 August, Budapest, Hungary.","Brown, P. 1988. A Statistical Approach to Language Translation. In Proc.l21","h International Conference on Computational Linguistics, vol. 1, 71-76. Budapest, Hungary.","Brown, P., J .. Cocke, S. Della Pietra, V. Della Pietra, F. Jelinek, J. Lafferty, R. Mercer, and P. Roosin. 1990. A Statistical Approach to Machine Translation. Computational Linguistics. June: 79-85.","Brown P., J. Lai, and R. Mercer. 1991. Aligning Sen tences in Parallel Corpora. In Proc. 291","h Annual Meeting of the ACL, 169-176. 18-21 J1n/e, Berkley, Calif.","Dagan, l., A. Itai, and U. Schwall.l99l. 'l;wo languages are more informative than one. In Proc. 291","h Annual Meeting of the Association for Computational Lin guistics, 130-137.18-21 June, Berkley, Calif.","Di Christo, P., S. Harie, C. De Loupy, N. Ide, and J. Veronis. 1995. Set of programs for segmentation and lexical look up, MULTEXT LRE 62-050 project Deliverable 2.2.1.","Elworthy, D. 1997. Tagger Suite User's Manual. Cam bridge University Computer Laboratory Report, Cambridge, England.","Gale, W.A., and K.W. Church. 1991. A Program for Aligning Sentences in Parallel Corpora. In Proc. of the 29th Annual Meeting of the Association for Computational Linguistics, 177-184. 18-21 June, Berkley, Calif.","Gale, W.A., and K.W. Church. l99lb. Identifying word correspondences in parallel texts. Proceedings of the"]},{"title":"25","paragraphs":["Fourth DARPA Speech and Natural Language Workshop, 152-157.","Kay, M., and M. Roescheisen. 1993. Text-translation Alignment. Computational Linguistics. March: 121-142.","Kirkpatrick, S., C. Gelatt, and M.P. Vecchi. 1983. Opti misation by Simulated A1111ealing. Science Vol 220. pp. 671-680.","Kitamura, M., and Y. Matsumoto. 1995. A Machine Translation System based on Translation Rules Ac quired from Parallel Texts. In Proc. Recent Advances in Natural Language Processing, 27-44. 14 - 16 September, Tzigov Chark, Bulgaria.","Kitamura, M., and Y. Matsumoto. 1996. Automatic Ex traction of Word Sequence Correspondences in Par allel Corpora. In Proc. 4'\" Workshop on Very Large Cmpora, 79-87. 4 August, Copenhagen, Denmark.","Kumano, A., and H. Hirakawa. 1994. Building an MT Dictionary from Parallel Texts Based on Linguistic and Statistical Infonnation. In Proc. 15 1","h Interna tional Conference on Computational Linguistics)6-8l. 5-9 August, Kyoto, Japan.","Kupiec, J. 1993. An algorithm for Finding Noun Phrase Correspondences in Bilingual Cmpora. In Proc. 31st Annual Meeting of the Association for Computa tional Linguistics, 17-22. 22-26 June, Columbus, Ohio.","Leech, G. 1995. A brief users' guide to the grammatical tagging of the British National Corpus. British Na tional Corpus Consortium Report, Oxford, England.","Matsumoto, Y., H.Ishimoto, T. Utsuro. 1993. Structural , Matching of Parallel Texts. In Proc. 31\" Annual Meeting of the Association for Computational Lin guistics, 23-30. 22-26 June, Columbus, Ohio.","Metropolis, N., A. Rosenbluth, M. Teller, A. Teller, and E. Teller. 1953. Journal Chem. Phys. Vol. 21. Pp 1087.","Papageorgiou, H., L. Cranias, and S. Piperidis. 1994. Automatic Allignment in Parallel Corpora. In Proc. 32\"' Annual Meeting of the Association for Com putational Linguistics, 334-336. 27-30 June, Las Cruses, New Mexico.","Papageorgiou H. 1996. Part of Speech Disambiguation. In Hybrid Techniques for Bilingual Corpus Proc essing 63-83. PhD thesis, National Technical Uni versity of Athens, Greece","Piperidis S. 1995. Interactive Corpus-based Translation Drafting Tool. Aslib Proceedings. March: 83-92.","Piperidis, S., S. Boutsis, and I. Demiros. 1997. Automatic Translation Lexicon Generation. In Proc. Multilin guality in Software Industry /IJCAI. 25 August, Na goya, Japan.","Simard, M., G. Foster, and P. Isabelle. 1992. Using cog nates to align sentences in bilingual corpora. Proc. TMI-92. Montreal, Quebec.","Smadja, F. 1992. How to compile a bilingual colloca tional lexicon automatically.ln Proc. AAAI Work shop on Statistically -based NLP Techniques, 67-71. San Jose, California.","Smadja, F., K.R. McKeown, and V. Hatzivassiloglou. 1996. Translating Collocations for Bilingual Lexi cons: A Statistical Approach. Computational Lin guistics. March: 1-38."]},{"title":"26","paragraphs":[]}]}
