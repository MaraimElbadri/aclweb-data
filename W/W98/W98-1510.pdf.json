{"sections":[{"title":"An Empirical Evaluation on Statistical Parsing of Japanese Sentences using Lexical Association Statistics","paragraphs":["SHIRAI Kiyoaki INUI Kentaro TOKUNAGA Takenobu TANAKA Hozumi","Departrnent of Computer Science) Graduate School of Information Science and Enginc~t~ring 1 Tokyo Institute of Technology"]},{"title":"Abstract","paragraphs":["\\Ve are proposing a new framework of statisti cal language modeling which integrates lexical .association statistics with syntactic preference, while maintaining the modularity of those differ ent statistics types, facilitating both training of the model and analysis of its behavior. In this paper, we report the result of an empirical evalu ation of our model, where the model is applied to disambiguation of dependency structures of Japanese sentences. We also discussed the room remained for further improvement based on our error analysis."]},{"title":"1 Introduction","paragraphs":["In the statistical parsing literature) it has alread:y been established that statistics of lexical associ ation have real potential for improvement of dis ambiguation performance. The question is how lexical association statistics should be incorpo rated into the overall statistical parsing frame work. In exploring this issue) we consider the following four basic requirements:","o Integration of difjeTent types of statistics: Lexical association statistics should be inte gra.ted with other types of statistics that are also expected to be effective in statistical pars ing1 such as short-term POS n-gnun statistics and long-term structural preferences over parse trees.","• Modularity of statistics types: The total score of a parse derivation should be decomposable into factors derived from differ ent. types of statistics) which would facilitate analysis of a modePs behavior in terms of each statistics type.","• Pmbabilistically well-fottnded semantics: The language model used in a statistical parser should have probabilistically well-founclecl"]},{"title":"se","paragraphs":["mantics) whieh \\vould a.lso facilitate the anal:,'·· sis of the model's behavior. ,"]},{"title":"80","paragraphs":["o Trainability: Since incorporation of lexical association statis tics would make the model prohibitively com plex, the model's complexity should be flexibly controllable depending on the amount of avail able training data. However) it seems to be the case that no existing framework of language modeling [2, 4, 12, 1:3, H. 17) 18] satisfies these basic requirements simulta·· neouslyl. In this context1 we newly designed a framework of statistical language modeling tak ing all of the above four requirements int.o ac count"]},{"title":"[8) 9].","paragraphs":["This paper reports on tlw n'sult.:) of our preliminary experiment where our f'rrmH' vmrk was applied to structural disambigu<Hion of Japanese sentences.","In what follows) we first briefly review our framework (Section 2). \\Ve next describe tlw sc:t ting of our experiment) including a brief intro duction of Japanese dependency struetures, t.hC' data sets1 the baseline of the perfonnanc.C', Nc. (Section 3). We then describe the results of the experinH:mt) which was designed to assess the lu1 .. pact of the the incorporation of lexical associ ation statistics (Section 4). \\Vc finall~· discuss the current problems revealed through our <T ror analysis, suggesting some possible solutions (Section 5)."]},{"title":"2 Overview of our framework","paragraphs":["As with the most statistical parsing frameworks. given an input string A) we rank its pars<~ dr:rlYa tions according to the joint distribution J'(H, lr). where H1","is a word sequence candidate for A, and"]},{"title":"R","paragraphs":["is a parse derivation candidate for H-- whos('","terminal symbols constitute a POS tag scquc-;nce","L (see Figure 12","). We first. decompose 1'( fl. lr)","1","For further discussion, see [8]. This is also tlH'","case with recent works such as [3] and [5] due to t.hc","lad< of modularity of statistical types."]},{"title":"' .","paragraphs":["-Although syntactic structure R is represented af' a dependency structure in this figure, our framework into two submodels, the syntactic model"]},{"title":"l'(R)","paragraphs":["and the lexical model"]},{"title":"P(W\\R): P(R, W)","paragraphs":["="]},{"title":"P(R) · P(W\\R)","paragraphs":["(1) The syntactic model, whic:h is lexically insen sitive, reflects bpth POS n-gram statistics and structural preference, whereas the lexical model reflects lexical association statistics. This divi sion of labor allows for distinct modularity be tween the syntactic--based statistics and lexically sensitive statistics, while maintaining the proba bilistically wcll-foundedness of the overall model. Fignre 1: A parse derivation for an input string"]},{"title":"\"11Ji!c/J\";'","paragraphs":["1"]},{"title":"ii:","paragraphs":["]t~t.: (She ate a pie)\""]},{"title":"2.1 The syntactic model","paragraphs":["The syntactic model P(R) can be estimated us ing a wide range of existing syntactic-based lan guage modeling frameworks, from simple PCFG models to more context-sensitive models includ ing those proposed in [2, 13, 19]. Am.olg these, we, at present, use probabilistic GLil (PGLH.) language modeling, which is given by incorpo~ rating probabilistic distributions into the GLR parsing framework [10, 21]. The advantages of PGLR modeling are (a) PGLR. models are mildly context~sensitive, compared with PCFG models, and (b) PGLR. models inherently capture both structural preferences and POS bigram statistics, which meets our integration requirement. For further discussion, see [10]."]},{"title":"2.2 The lexical model","paragraphs":["The lexical model"]},{"title":"P(WIR)","paragraphs":["is the product of the probability of each lexical derivation li ·-7 Wi, where 11 E L (L C R) is the POS tag of"]},{"title":"w;","paragraphs":["E W:"]},{"title":"P(WIR) =II","paragraphs":["l'(w;\\R,w1 , ... ,w1_1) (2)","The key idea for estimating each factor P(vJiiR, w1, ... , Wi-d (a lexical derivation prob ability) is in assuming that each lexical derivation does not impose any restriction on the representation of syntactic structures."]},{"title":"81","paragraphs":["depends only on a certain small part of its whole context. We first assume that syntactic struc ture R in P(wiiR,w1 , .. . ,wi_- 1) can always be reduced to l; ( E R), which allows us to deal with the lexical model separately from the syntactic model. The question then is which subset C of { ·uJI, ... , Wi-l} has the strongest influence on the derivation li"]},{"title":"-+","paragraphs":["Wi· VVe refer to a member of such a subset"]},{"title":"C","paragraphs":["as a"]},{"title":"lexical context","paragraphs":["of the derivation li"]},{"title":"-+","paragraphs":["'Wj. Let uB illustrate this through the previous ex ample shown in Figure L Suppose that th(-: derivation order for"]},{"title":"TV","paragraphs":["is head-driven, as givr;n below, to guarantee that, for each of the words subordinated by a head word, the context of the derivation of that subordinated word alwa.J·'S in cludes that head word."]},{"title":"ta","paragraphs":["(PAST) -;"]},{"title":"tabe","paragraphs":["(eat) -+"]},{"title":"ga","paragraphs":["(NO III) --t"]},{"title":"o (ACC)","paragraphs":["·-+ kanojo (she) -; pai (pie)","First, for each lcxieal item that we don't cou sider any lexical association, \\VC estimate• the probability"]},{"title":"of","paragraphs":["its derivation as follows."]},{"title":"P(ta\\R) \"'P(ta!Av.1')","paragraphs":["( :l)"]},{"title":"P(t.abe\\R, ta)\"' P(tabe\\V)","paragraphs":["(l) Second,"]},{"title":",·ve","paragraphs":["estimate the probability of d(:ri\\· \"ing each slot-marker, e.g. ((ga (NOl'v'I)'' and ··o (ACCf' l","by considering not only the dependency between the head word and each of it::? slot markers, but also the dependency between slot markers subordinated by the same hc~a.d:"]},{"title":"'l'(ga\\R, tabe, ta) \"' P(ga\\l'r[h(tabe,[Pr,Pz])])","paragraphs":["(5)"]},{"title":"P(o\\R, ga, tabe, ta) \"' P(o\\Pz[h(tabe,","paragraphs":["[l'r"]},{"title":":ga, Pz])])","paragraphs":["(G) where h(h,"]},{"title":"[s1, ... , sn])","paragraphs":["is a lexical context denot.~ ing a head word h that subordinates the set of slots"]},{"title":"s,, ...","paragraphs":[",sn, and P(w;\\l;[h(h,[st,···•\"\"])]) is the probability of a lexical derivation l; -~t 11!","1 , given that Wi functions as a slot-marker of lexical head h(h, [sr, ..."]},{"title":",snJl·","paragraphs":["Finally, we estimate the probability of deriY·· ing each slot-filler, e.g. ''kanojo (sheY and -·'pai (pie)\", in assuming that the derivation of a slot.·· filler depends only on its head word a.nd slot: P(kanojo\\R, ga, o,"]},{"title":"tabe,","paragraphs":["ta) co","P(kanojo\\N[s(tobe, go)])","P(pa.iiR, kanojo, ga, o, tabe, tG) ~"]},{"title":"l'(paiiN[s(tabe, o)])","paragraphs":["(7)"]},{"title":"(8)","paragraphs":["where s(hl s) is a lexical context denoting a slot"]},{"title":"s","paragraphs":["of a head word h, and P(w1\\l;[s(h,"]},{"title":"s)])","paragraphs":["is the probability of a lexical derivation li -f w.i given that w; functions as a filler of a slot s(h,s).","Combining equations (3), (4), (5), (6), (7) and (8), we produce (9):","P(WIR) \"' P(taiAu:r) · P(tabeW) · P(gajP[h(tabe,[P,P])]) · P(oiP[h(tabe,[P:ga,P])]) · P(kanojoiN[s(tabe, ga)]) · P(paiiN[s(tabe,"]},{"title":"o)])","paragraphs":["(9)"]},{"title":"2.3 Handling multiple lexical contexts","paragraphs":["Note"]},{"title":"that a","paragraphs":["le_xical derivation may be associa.ted with more than one lexical context (multiple lex ical contexts). Multiple lexical contexts appear typically in coordinate structures. :For example, in the sentence shown in Figure 2, \"kanojo~wa (she-TOP)\" functions as the case of both of the verbs \"tabe (eat)\" and \"dekake (Ieaver'. Coordination Figure 2: An example sentence containing a coor dinate structure: \"She ate breakfast and left for school\"","Let us first consider the lexical deriva tion probability for the slot-filler \"kanojo (she)'1",".","According to the assumption men tioned in Section 2.21 the lexical contexts of this slot-filler should be s(tabe, wa) and s(dekake,wa). Thus, the probability of deriving it is P(kanojoiN![s(tabe, wa), s(dekake, wa)J). IV1ore generally, if a slot-filler W-t is associated with two lexical contexts c1 and c2 , then the probabil ity of deriving Wi can be estimated as follows:","P(w;jl,[c1 , c2])","P(i,[cr, c,Jiw;) · P(w;)","= (1~","P(/;[~"]},{"title":"1","paragraphs":[", c2]) \"' P(l;[c,]lw;) · P(l;[c,]ll;, w;) · P(w;)"]},{"title":"(ll)","paragraphs":["P(l;[c,]) · P(i;[c2Jii;])"]},{"title":"=","paragraphs":["P(w;ll;). P(w;li;[cr]) . P(w;ll;[c2]) (","12)","P(w;ll;) P(w;ll;)","P(w;jl;) · D(w;jl;[c!]) · D(w;jl1[c 2"]},{"title":"J)","paragraphs":["(13) In (13), we assume that the two lexical contexts c1 and c2 are mutually independent given li (and"]},{"title":"82","paragraphs":["w;): P(l,[cz]ll;[c,])"]},{"title":"\"'P(l;[c","paragraphs":["2 JII;) P(/;[c2JII;[c,],w;) \"'P(l;[c,JII;, w;) ( 1 :) ) (Jo) D(w1ll;[c]) is what. we call a lexical dependency parameter~ which is given by:","P( w;jl;[c]) D(w;jl;[c]) = P(w;ll;) (16) JJ(w;ll;[c]) measures the degree of the depen dency between the lexical derivation li ~ Wi and its lexical context c. It is close to one if wi and (' are highly independent. It becomes greater than one if w.; and c are positively correlated: wlwn'as it becomes less than one and dose to zero if 11'","1 and care negatively correlated. Thus, if we set a lexical dependency parameter to one, that meaw; we create a model that neglects the depend(-:nc.\\· associated with t,ha.t parameter. For examplr., the probability of deriving \"kanojo (she)\" in Figure 2 is calculated as follows.","P( kanojoiN,[s(t.abe, wo), s( deknke, wa)])","\"' P(kanojoiN1) • D(kanojoiN1 [s(labc, wa)]) ·D(kanojoiN1 [s(dekake, wa)]) ( 1 I)","Let us then move to the estimation of the pro b .. ability of deriving the slot-markers ''wo (TOP)\" \"o (ACC)\", and \"e (for)\", where ''wa\" is associ ated with both \"tabe (eat)\" and \"dekake (lean:)\" while \"a)) is assoeiatecl only with ''tabe'', all< I ·'ni\" is associated only with ''dekake\". To be mod(-' general, let slot-marker wo is associated with L\\\\'O lexical contexts c1 and c2, and slot-m;_trkers u_: 1 and w2 are, respectively, associatc~d with c1 and c2 . Assuming that w1 and w2 arc mutually de pendent, being both dependent on w0 , and c1 and c2 are mutually independent, the joint probabil ity of the derivations of 'Wo, W1 a.nd -w'2 can be estimated as (20) in Figure 3, similar to (13). For example, the probability of deriving \"wa (TOP)\" ao (ACC)\", and \"e (for)\" in Figure 2 is calculat.ed"]},{"title":"as (21)","paragraphs":["in Figme 3. Summarizing equations (2), (13) and (Hi), the","lexical model P(WIR) can be estimated by the","product of the context-free distribution of the","lexical derivations P,t(WIL) and tlJ<>. degree of","the dependency between the lexical derivations","D(WIR): P(WIR) \"'P,t(WIL) · D(Will) (22) P,t(WIL)"]},{"title":"=IT","paragraphs":["P(w;jl;) (23) m","D(WIR)"]},{"title":"=IT IT","paragraphs":["D(w;jl;[c])"]},{"title":"(2~1)","paragraphs":["\\vhere C.w, is the set of the lexical contexts of '11! 1 •"]},{"title":"P( wo,","paragraphs":["w,,"]},{"title":"wz\\lo","paragraphs":["[h(h,, [io, i!]), h(h,, [io,"]},{"title":"lz])], z,","paragraphs":["[h(h, , [io, !!]) ],"]},{"title":"lz","paragraphs":["[h(hz, [io, !,]) ])"]},{"title":"\"\" P( wo","paragraphs":["llo [h(h,, [io, !!]) ,"]},{"title":"h(hz,","paragraphs":["[lo,"]},{"title":"lz])]) · P(","paragraphs":["w,ll![h(hL[io : wo, !!])]) ·"]},{"title":"P(wz\\lz","paragraphs":["[h(hz, [io :"]},{"title":"wo ,lz])]) (","paragraphs":["18)"]},{"title":"\"\" P(wollo).","paragraphs":["P(wo\\lo[h(h,, [io,l!])]). P(wolio[h(h2 ,"]},{"title":"[1","paragraphs":["0"]},{"title":",/z])]) P(wollo) P(wollo)","paragraphs":["P( w,ll![h(h,, [io :"]},{"title":"Wo ,l !]) ]) ·","paragraphs":["P("]},{"title":"wzllz","paragraphs":["[h(hz,"]},{"title":"[/o : Wo ,!,]) ]) (","paragraphs":["10)"]},{"title":"-· P(wo[lo) ·","paragraphs":["D(wo[io[h(h,, [io, !!])]) ·"]},{"title":"D(wo[lo[h(hz, [lo,lz])])·","paragraphs":["P(w,ll,) · D(w,[i![h(h,, [io :wo,id)]) ·"]},{"title":"P(w,[lz) · D(wz[lz[h(hz,","paragraphs":["[io :wo, 1,])]) (20)"]},{"title":"P( wa, o, e[P","paragraphs":["1 [h(tabe,"]},{"title":"[P","paragraphs":["1 ,"]},{"title":"P","paragraphs":["2 ]),"]},{"title":"h(dekake, [P","paragraphs":["1 , 1'3])],"]},{"title":"P","paragraphs":["2"]},{"title":"[h(talle,","paragraphs":["[1'1, 1'2])],"]},{"title":"P,[h(dekake, [P,,","paragraphs":["p,])])"]},{"title":"\"' P( wall\\) · D(wa[PJ[h(tabe,","paragraphs":["[1'1 ,"]},{"title":"P","paragraphs":["2])]) ·"]},{"title":"D(wa[P","paragraphs":["1"]},{"title":"[h( dekake, [P","paragraphs":["1 ,"]},{"title":"P,])]) P(oiP","paragraphs":["2 ) ·"]},{"title":"D(o!Pz[h(tabe,","paragraphs":["[P, :"]},{"title":"wa,","paragraphs":["!'2 ])]) ·"]},{"title":"P(eiP,) · D(e!P,[h(dekake,","paragraphs":["[!', :"]},{"title":"wa,","paragraphs":["!':,])]) (21) Figure 3: The joint probability of the derivations of slot-markers"]},{"title":"2.4 Summary of our model","paragraphs":["From equatious (l) and (22), the overall distribu tion P(R, W) can be decomposed as follows: P(R, W) \"\"P(R) · P,1(WIL) · D(WIR) (25) where the first term P(R) reflects part-of-speech bigram statistics and structural preference, the second term P,J(WIL) reflects the occurrence of each word, and the third term D(WIR) reflects lexical association. Thus, equation (25) suggests that our model integrates these types of statis tics, while maintaining modularity of lexical association. ! :"]},{"title":"f","paragraphs":["Figure 4 shows the factors of the P(R,W) for the sentence in Figure 1. In this figure: 1. P(R) reflects the syntactic pre.fcrence. 2. P,J(WIL), which consists of"]},{"title":"P(kanojo!N),","paragraphs":["P(gaiP) etc., reflects the occurrence of each word. 3. D(WIR), which consists of"]},{"title":"D(o!N[h(tabe, [])]), D(paiiN[s(tabc, ACC)])","paragraphs":["etc., reflects the lexi cal association statistics. In this way, our modeling maintains the modu larity of different statistics types.","The modularity of the lexical model facilitates parameter estimation. Although the syntactic model idmtlly requires fully bracketed training corpora, training it is expected to be manage able since the model's parameter space tends to be only a small part of t.he overall parameter space. The lexical assodation statistics, on the other hand, rnay have a much larger parameter space, and thus may require much larger amounts of training dat:-\\ as compared to the syntactic"]},{"title":"83","paragraphs":["modeL Howc:ver, since our lexical model can lw trained independently of syntactic preferenct\\ onP can train it using partially parsed tagged corpora. which ca.n he produc-ed at a lower cost (i.e. nuto matieally), as well a.s fully bracketed corpora. ln fact, we used both a full-bracketed corpus and n partially parsed corpus in our cxporiJucnt."]},{"title":"3 A preliminary experiment","paragraphs":["Let us f-irst briefly describe some fundamemnl features of Japanese syntax. A JapaneS(-' SPll·· tcncc can be analyzed as a. S(~qnence of so--called"]},{"title":"b\"unsettt.","paragraphs":["phrases (BPs, hereafter) as illustratc~d in Fir$ure 1. A BP is a chunk of words consisting of a content word (noun, verb, a.djcctive, etc.) accmu pari-ied by some function word(s) (postposition. auxiliary, etc.). For example, the BP \"ko:nojo-gr{ (Bl\\) in Figure 1 consists of the noun 11","kon.ojo (she)\" followed by the postposition \"ga (:\\Oil I)\"' which functions as a slot-marker. The BP 11","tahe"]},{"title":"ta\"","paragraphs":["(BP3 ), on the other hand, consists of the verb \"tabe (eat)\" follmved by the auxiliar:v 11","f.a (PAST)\"","Given a sequence of BPs) one can recognized<' pendency relations betvveen them as illustrated in Figure 1. In Ja.pa .. nese) if BPi precedes IJP_i, and BPi and BPj are in a. dependency relation) then BPi is always the modifier of BJJ.;, and we sa.v 11 BPi modifies BPj·:' For exa.mple 1 in Figure. 1. both BP1 and BPz modify BP3.","For the preliminary evaluation of our model, we restricted our focus only on the model's per formance for structural disambigua.tion excluding morphological disambiguation. Thus1 the task of the parser was restricted to detennina.tion of the dependency structure of an input sentence, ,.,_,hich is given together with the specification of word"]},{"title":"A~~","paragraphs":[")P(R) N 1 P1 N2 P~ V Aux"]},{"title":"- - Pik;n~j~l~f P(~oMIP) r -P(;a;l~)r P(~CCJPf P(;a~;IV) r -p(~1~~J -~~~(~1-L)","paragraphs":["kanojo ga pa1 o tabe ta","(she} (NOH) (pie) (ACC) (eat) {PAST)","--------;..---- -:+-----;....-----: jJ---- .;..1.;.'-------------","' 'I ' 'II •1.1",". ~---------- ~ ,---------- -~ ·-------- -~r r--- ----· c 1 : 1","D(kanojolN[s(tabe,NOM)J)1 : :1, 1:' P(WIR)","D(NOMIP[h(ea[[ACCJTI)T ___ --,-~----\"::","'----------'-,----------'• D(WIR)","D(paiiN[s(tabe,ACC)]),","D(ACCIP[h(tabe~[Jjj) Figure 4: The summary of our model segments) their POS tags, and the boundaries be tween BPs.","In developing the grammar used by our PGLR parser, we first established a categori2ation of BPs based on the POS of their constituents: post positional BPs, verbal BPs, nominal predicative BPs, etc. We then developed a modification con straint matrix that describes \\vhich BP category can modify which BP category, based on exam ples collected from the Kyoto University text cor pus"]},{"title":"[11].","paragraphs":["We finally transformed this matrix into a CFG; for instance, the constraint that a BP of category Ci can modify a BP of category CJ can be transformed into context-free rules such as (C;"]},{"title":"-+","paragraphs":["C; C;), (C; --> C; C;), etc., where X denotes a nontermina.l symbol.","For the text data, we used roughly 10,000 sen tences from the Kyoto University text corpus for training the syntactic model, and the \\Vhole EDR corpus"]},{"title":"[6]","paragraphs":["Mel the R.WC POS-taggecl cor pus"]},{"title":"[16]","paragraphs":["for training the lexical model. For test ing, we used 500 sentences collected from the Kyoto University text corpus with the average sentence length being 8.7 BPs. The data sets used for training and testing are mutually ex clusive. The grammar used by our probabilis tic G LR parser was a CFG automatic.ally ac quired from the training sentences) consisting of 967 context-free rules containing 50 nontermina.J symbols and 43 terminal symbols (i.e. BP cate gories).","The~ baseline of the disambiguation perfor mance was assessed by way of a naive strategy which selects the nearest possible modifiee (simi larly to the right association principle in English) under the non-crossing constraint. The perfor mance of this naive strategy was 62.4% in BP hased accuracy: where BP-based accuracy is the ratio of the number of the BPs whose modifiee"]},{"title":"84","paragraphs":["is correctly identified to the total numbc!r of BPs (excluding the tv·.ro rightmost BPs for each S('ll tence). On the other hand, the syntactic model P(R) achieved 72.1% in BP-bascd <tccun\\C)'- 9.7 points above the baseline."]},{"title":"4 The contribution of the lexical model","paragraphs":["In our experiment, we considered the following three lexical dependency parameters in the lPxical modeL","First, we considered the depenclcnde,s lwt\\YCC'll slot-markers and their lexical hca,cl by using tlw lexical dependency parameter (26). D(piP[h(h, [s 1 , _. _,"]},{"title":"s,])])","paragraphs":["(26) (26) can be computed from P(p''II\"'[h(h. [])]). the distribution of n post. positions (slot-Jnarkers) given that all of them are suborclinnted ])~· a single lexical head h. \\Ve trained this distribution using 1501000 instances of p11","- {verb,adj ective,nom?.n(tl_pred1.cate} colloca t:ion collected from the EDR full-bracketed corpus. For parameter estimation) we used the 1naximwu entropy estimation technique"]},{"title":"[1,","paragraphs":["15). For furtlwr details of this estimation process, see"]},{"title":"[20]-","paragraphs":["Next, \\Ve considered depench:.ncies het\\Yecn slot-fillers and their head verh coupled with tlw corresponding slot-markers by using the lexical dependency parameter (27). D(niN[s(v,p)]) (27) (27) was trained using 6.7 million instances of noun-postposition-verb collocation eollectr;(l from both the EDR and RWC corpora. For parameter estimation, we used 115 non-hien.trchical seman tic noun classes derived from the N'fT semantic dictionary [7] to reduce the parameter space:"]},{"title":"D(niN[s(v,p)]);:, L,","paragraphs":["P(cniN[s(v,p)]) ·"]},{"title":"P(nlcn) P(niN)","paragraphs":["(28)"]},{"title":"P(cniN[s(v,p)])","paragraphs":["was estimated using a simple back-off smoothing technique: for any given lexi cal verb v and postposition p, if the frequency of"]},{"title":"s(v,p)","paragraphs":["is less than a certain threshold ,\\ (in our experiment,,\\= 100), then"]},{"title":"P(cniN[s(v,p)])","paragraphs":["was approximated to be P(c,,!N[s(c,p)]) where"]},{"title":"c,","paragraphs":["is a class of v whose frequency is more than ,\\,","Finally, we considered the occurrence of post positions by using the lexical dependency param eter (29)."]},{"title":"D(piP[head_type])","paragraphs":["(29) In .Japanese, the distribution of the lexical deriva tion of postpositions,"]},{"title":"P(piP),","paragraphs":["is quite differ ent depending on whether they function as slot markers of verbs, adjectives and nominal precli catcs such as \"ga"]},{"title":"(NOM)\"","paragraphs":["and \"o (ACC)\" in Fig","ure 1, or they function as slot-markers of nouns","sueh as ((no (oft in the following sentence.","hana no syashin3","(flower) (of) (picture) For such a rea.son, we introduced the lexical de pendency pararneter (29L \\vherc head .. type de notes whether the postposition"]},{"title":"P","paragraphs":["functions as a slot-marker of a predicate or a nomL \\Vc esti matl~d this dependency parameter using about 950,000 postpositions collected from the EDR corpuR. ."]},{"title":"f","paragraphs":["Table 1 summarizes the results of.~-fhc experi ment. The lexical model achieved 76.5% in BP bascd accuracy) and the model using both the syntactic and lexical model achieved"]},{"title":"82.8%","paragraphs":["in BP-based accuracy. According to these results1 the contribution of lexical statistics for disam biguation is as great as that of syntactic statistics in our framework.","The bottom three lines in Table 1 denotes the setting where the only lexical dependency param eter (26), (27) and (29) are considered in the lexi cal model. Among these, the contribution of (29) was greatest."]},{"title":"5 Error analysis","paragraphs":["In the test set, there were 574 BPs whose rnocl ifiec was not correctly identified by the system. Among these errors) we particularly explored 290 errors that were associated with postpositional I3Ps functioning as a ease of either a verb) adjec tive) or nominal predicate, since, for lexical asso ciation statistics in the lexical model, we took the","3","This sentence means ~'a picture of a flower.\""]},{"title":"85","paragraphs":["Table 1: The contribution of the lexical model"]},{"title":"'b_a_s_e'h~.","paragraphs":["n-e-------+"]},{"title":"a~;~~·~)y","paragraphs":["syntactic model only 72.1"]},{"title":"%","paragraphs":["lexical model only 76.5 % syntactic"]},{"title":"+","paragraphs":["lexical model 82.8"]},{"title":"%","paragraphs":["syntactic model"]},{"title":"+","paragraphs":["(26) syntactic model"]},{"title":"+","paragraphs":["(27) syntactic model"]},{"title":"+","paragraphs":["(29)"]},{"title":"7:l:;r-:%-","paragraphs":["783% 81.3% dependencies between slots (i.e. slot-markers and slot-fillers) and their heads into account. In this exploration) we identified three major error types: (a) errors associated with a coordinate clause, (b) errors associated with relative clauses, (c) errors associated with the lack of the consideration of dependency between slot-fillers."]},{"title":"5.1 Coordinate structures","paragraphs":["One of the typical error types iR associated with coordinate structures. The sentence in Figure 2 has at least three alternative interpretations in terms of which J3P is modific~d by the left most BP"]},{"title":"\"kano}o-wa","paragraphs":["(she-TOP)\": (a)"]},{"title":"\"i.abc-l.a","paragraphs":["(cat-PAST)\", (b)"]},{"title":"\"dckake-ta","paragraphs":["(leave-PAST)\", (c)","· both \"tabe-ta (ea.t-PASTf) and ''dekake~tn (leaH'·· PASTr. Among these alternatives, the most rea sonable interpretation is obviously (c), where the two predicative BPs constitute a coordinate structure."]},{"title":"In","paragraphs":["our experim<.mt) however, neither the train"]},{"title":"ir~g","paragraphs":["data nor the test data indicates such coordi nate structures. Thus, in the above sentence, for example, the system was required to choose oue of two alternatives"]},{"title":"(>t)","paragraphs":["and (b), where (b) is the pre ferred candidate according to the structura.l pol-· icy underlying our corpora. However, this choice is not really meaningful. F'urtlwrmore1 the system systematically prefers (aL the wrong choiCl\\ since (i) the syntactic model tends to pref(-;r shorter distance modification relations (similarly to th(-' right association principle in English): and (ii) the lexical model is expected to support both can didates because both"]},{"title":"D(kanojoiN[s(tabc, wa)])","paragraphs":["in (a) and"]},{"title":"D(kanojo!N[s(dekakc, wa)])","paragraphs":["in (b) should be high. This problem malws the per fonnance of our model lower than \\vhat it should be.","Obviously, the first step to resolving this prob lem is to enhance our corpora and grammar t.o enable the parser to generate the third interpre tation, i.e. to explicitly generate a coordinate' structure such as (c) if needed. Once such a set ting is established, \\Ve then need to consider the lexical contexts of each of the constituents modi fying a coordinate structure, such as \"kanojo-wa (she-Torr' in the above sentence."]},{"title":"In","paragraphs":["interpreta tion (c), since \"kanojo-wa (she-TOP)\" modifies both predicative BPs, it is reasonable to asso ciate it with two lexical contexts,"]},{"title":"s(tabe, wa)","paragraphs":["and"]},{"title":"s(dekake, wa).","paragraphs":["As mentioned in Section 2, our framework allows"]},{"title":"us","paragraphs":["to deal with such multiple lexical contexts, namely:","D(kanojo[N[s(tabe, wa), s(dekake, wa)]) \"'D(kanojo[N[s(tabe,"]},{"title":"wa)]) ·","paragraphs":["D(kanojo[N[s(dekake,"]},{"title":"wa)])","paragraphs":["(30) The correct interpretation (c) would assigned higher probability than (a) or (b), since the two lexical dependency parameters in (30), D(kanojo[ N[s(tabe, wa)]) and D(kanojo[N[s(dekake, wa)]) are both expected to be sufficiently large."]},{"title":"5.2 Treatment of correference","paragraphs":["One may have already noticed that the issue dis cussed above can be generalized as an issue asso ciated with the treatment of correference in de pendency structures. Narnely, if a prepositional BP i:s correferred to by more than one clause as a participant~ a naive treatment of this cmTef ercncc relation eould require the parser to make a meaningless choice: which clause subordinates that BP. This problem in the treatment of corref erence is considered to cause a significant propor tion of errors associated with relative/adverbial clauses or compound predicates. Such errors are expected to be resolvable through an extension of the model, as discussed in Section 5 .1.","Let us briefly look at another example in Figure 51 1vhere the matrix clause and relative clause correfm· to the leftrnost BP ''kanojo-wa (she-TOP)\", i.e. interpretation (c). \\Vithout any refined treatment of this correference relation, the parser would be required to make a meaningless choice between (a) and (b).","N, P,","I I","kanojo wa","{o.l1~) (TOP) Adv I kinou","{yoslo!d~\\') v, Aux, N, P I I I 1' kat ta hon o (b\"y) !PAST) (book) (!ICC) v, Aux, I I yon (Ia {rottd) (PASl) Figure 5: An example sentence containing a rela tin; clause: ''She read the book which she bought yestercla.y1","'"]},{"title":"5.3 Dependency between slot fillers","paragraphs":["According to the results summarized in Table 1, the contribution of the dependency between"]},{"title":"86","paragraphs":["slot-fillers a11d their heads seems to be negligibl~· small."]},{"title":"Vvre","paragraphs":["can enumerate several possible rea sons including th<.1.t the estimation of these types of dependency pa.rarneters 'vas not sufficiently so pllisticated.","In addition to these reasons, \\VC also found that the lack of the consideration of dependency b(: tween slot-fillers was also problematic iu sonw cases; there are particular patterns where depen dency between slot-fillers seems to be highly sig nificant. For example) in the clause \"knnojo-wa (she-TOP) ishu-ni (doctor-DAT) nat-ta (become PAST)\" (she became a doctor), the distrilm· tion of the ftller of the \"wa (TOP)\" slot is considered to be highly dependent on the filler of the \"ni (DAT)\" slot, \"isha (doctor)\", since its distribution would be markedly different if ~'isha. (doctor)'1","was replaced with ';m:i::::u (\\Yil ter))). Similar patterns include, for ex;Jmpl(:\\ ;, A wo (ACC) B-ni (DAT) s·ur·u (make)\", where .I and Bare highly dependent, and \"A-ga (NO'Il) B-wo (ACC) suTn (do)\", where noun D iudicat. .. ing an action strongly influences the (listribltl im1 of A.","In our framework, this type of problem ('all ])(' treated by means of controlling the choicC' of h:xi cal contexts. \\Ve arc ncn:v conducting anot.lwr ('X periment in \\Vhich the dependencies between .slot. flllers arc additionally considered in particular patterns. Note that the ref-inement of our mod('l in this manner illustrates that the: modularity of lexical association statistics fa.cilitate;.; rulc-\\)ased control in choosing the locations where lexical as sociation is considered. This rulc-bas(~d control allows us to incorporate qualitative kuowh'dg(' such as linguistic insights and heuristics IH'\\\\·l~· obtained from experiments based on t.lle mod(']."]},{"title":"6 Conclusion","paragraphs":["In this paper, we first presented a new franH' work of language modeling for statistical pars ing, which incorporates lexical association statis tics while maintaining modularity. \\Vc then re ported on the results of our preliminary evrdu ation of the model's performance, showing that both the syntactic and lexical models made a con siderable contribution to structural disambigua tion, and that the division of labor between those two models thus seemed to be working well to date.","IV1any issues remain unclear. Fir::;t:, we need to conduct experiments on the combination of the morphological and syntactic clisa .. mbiguation tasks: which our framework intrinsica.ll.Y is ck signed for. Second, empirical compa .. risou with other lexically sensitive models is also strongly required. One interesting issue is whether the division of labor between the syntactic and lex ical models presented in this paper works well language-independently) or conversely) whether the existing models designed for English are equally applicable to languages like Japanese."]},{"title":"Acknowledgements","paragraphs":["The authors would like to thank the staff of NTT for making available their considerable electronic resources."]},{"title":"References","paragraphs":["[1] A. L. Berger, S. A. Della Pietra, and V. J. Della Pietra. A maximum entropy approach to natural language processing. Computa tional Linguistics, 22(1):39 71, 1996.","[2] E. Black, F. Jelinek, J. Lafferty, D. M. Magerman, R. Mercer, and S. Roukos. To wards history-based grammars: Using richer models for probabilistic parsing. In Proceed ings of the ACL, pages 31··37, 1993.","[3] E. Charniak. Statistical parsing with a context-free grammar and word statistics. In Proceedings of the AAAI, 1997."]},{"title":"[4]","paragraphs":["J:vl. Collins."]},{"title":"A","paragraphs":["new statistical parser based on bigram lexical dependencies. In Pmceedings of t.he ACL, 1996.","[5] M. Collins. Three generative, le1icalised models for statistical parsing. In TVoceedings of the ACL, 1997."]},{"title":"[G]","paragraphs":["EDR. The EDR electronic dictionary tech nical guide (second edition). Technical Re· port TR·-045, Japan Electronic Dictionary Research Institute, 1995. [7] S. Ikehara, tv!. Miyazaki, S. Shirai, A. Yokoo,"]},{"title":"I-1.","paragraphs":["Nakaiwa,"]},{"title":"K. Ogura, Y.","paragraphs":["Ooyama) and Y. Hayashi. A"]},{"title":"Japanese","paragraphs":["Lexicon. hvanami Shoten, 1997. (In Japanese).","[8] K. Inui, K. Shirai, H. Tanaka, and T. Toku· naga. Integrated probabilistie language modeling for statistical parsing. Technical Report TR97-0005, Dept. of Computer Sci ence, Tokyo Institute of Technology, 1997. ftp://ftp.cs.titech.ac.jp/lab/tanaka /papers/97/inui97b.ps.gz.","[9] K. Inui, K. Shirai, T. Tokunaga, and H. Tanaka. Integration of statistical tech niques for parsing. In summary collection of the IJCAI'97 poster· session, 1997."]},{"title":"87","paragraphs":["[10] K. Inui, V. Sornlertlamvanich, H. Tanaka, and T. Tokunaga. A new formalization of probabilistic GLR parsing. In Proceedings of the IWPT, 1997."]},{"title":"[11]","paragraphs":["S. Kurohashi and M. Nagao. Kyoto univer sity text corpus project. In Proceedings of the 11th Annual Conference of JSAI, pages 58··61, 1997. (In Japanese).","[12] H. Li. A probabilistic disambiguation method based on psycholinguistie principles. In Proceedings of WVLC-4, 1996.","[13] D. M. Magerman and M. Marcus. Pearl: A probabilistic chart parser. In Proceedings of the EACL, pages 15 20, 1991.","[14] D. M. Magerman. Statistical decision-tree models for parsing. In Proceedings of the ACL, pages 276 283, 1995.","[15] A. Ratnaparkhi, J. Reyner, and S. Roukos. A maximum entropy model for prepositionnl phrase attachment. In Proceedings of the fht. man Language Technology Wo1'kshop 1 page~ 250-255, 1994.","[16] Real World Computing Partnership. HWC text. database. http: I /wmr. rwcp. or. j p/ WS\"I-lg.html) 1995.","[17] P. Resnik. Probabilistic tree-adjoining gram mar as a framev·mrk for statistical natural language processing. In Proceedings of !:he CO LING, pages 418 424, 1!)92."]},{"title":",","paragraphs":["[18] Y. Schabes. Stochastic lcxic:alized t.rcc- ·. adjoining grammars. In Proceedings of UH_:","CO LING, pages 425·432, 1992.","[1.9] S. Sekinc and R. Grishman. A corpus-based probabilistic grammar with only two non terminals. In Pr·oceedings of the IWI'T, 19%.","[20) K. Shirai1 K. Inui 1 T. Toln.maga 1 and H. Tanaka. Learning dependencies lw tween case frames using maximum (-:ntropy method. In Proceedings of Ann'IWl lvfeding of the Japan Association for No.tu.ral Lo.n· guage Pmcessing, 1997. (In Japanese).","[21] V. Sornlcrtlamvanic:h, K. lnui, K. Shirai, H. Tanaka1 T. 'I'okunaga) and T. Take:~-a.'''<-l. Empirieal evaluation of probabilistic glr parsing. In Pr·oceedings of the NLPRS, 1997."]}]}
