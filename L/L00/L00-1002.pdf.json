{"sections":[{"title":"Collocations as Word Co-$$$$ccu rren ce Restriction Data - An A pplication to Japanese Word P rocessor - Kosho Shudo*, Masahito Takahashi*, Yasuo K oyam a**, Kenji Yoshim ura*","paragraphs":["*F aculty of E ngineering, Fukuoka University *Fukuoka 814-0180, Japan **aiso ft co. **2-1-27, C huou, Matsu m o to, N agano 390-0811, Japan *shudo@ tl.fukuoka-u.ac.jp, *takahasi@helio.tl.fukuoka-u.ac.jp,","**koyam a@ aiso ft.co.jp, *yosimura@ tl.fukuoka-u.ac.jp Abstract  Collocations, the com bination of specific words are quite useful linguistic resources for NLP in general. The purpose of this paper is to show their usefuln ess, exem plifying an application to K anji character decision pro cesses for Jap an ese w o rd pro cesso rs. U n like recent trials of autom atic extraction, our collocations were collected m anually through many years of intensive investigation of corpus. Our collection procedure consists of (1) finding a proper com bination of words in a corpus and (2) recollecting similar com binations of words, incited by it. This procedure, which depends on hum an judgm ent and the enrichm ent of data by asso ciation, is effective for rem edying the sparsen ess o f data problem , although the arbitrariness of hum an judgm ent is inevitable. A pproximately seventy two thousand and four hundred collocations w ere used as w ord co-occurrence restriction data for deciding K anji characters in the pro cessing of Japanese w ord processo res. E xperiments have show n that the collocation data yield 8.9% higher fraction of Kana-to-Kanji character conversion accuracy than the system w hich uses no collocation data and 7.0% higher, than a com m ercial word pro cesso r so ftware of average perform ance. "]},{"title":"1.In troduction","paragraphs":["The m eanings of words in a natural sen tence are mutually bound. The most crucial problem for any kind of NLP is to disam b iguate the m eanings of each w ord in the sen tence w ith clues from w ord co-occurrence. It is, how ever, well know n that it's very difficult to construct a rule sy stem w hich disam b iguates them effectively for unrestricted or less-restricted natural sen tences. Authors have pointed out the im portance of lexicon-based linguistic resources for prescribing the co-occurrence of words such as collocations, idiom s etc., as w ell as rule-based resources, e.g., the set of case patterns com bined with a thesaurus or a set of sem an tic features. (Shudo,1989) In particular, collocations, which are the com bination of specific words are quite im portant linguistic resources for NLP in general. The purpose of this paper is to show their usefuln ess, exem plifying their application to K anji character decision pro cesses for Japanese word processors."]},{"title":"2.Collocation Data","paragraphs":["Our collocations w ere collected m anually through m any years o f intensive investigation of corpus. The extraction procedure consists of (1) finding a proper com bination of words in a corpus and (2) recollecting similar com binations of words, being incited by it. This p rocedure, which depends on hum an judgm ent and enrichm ent of data by association, seem s effective both to rem edy the data sparsen ess p roblem and to reduce the noise o f the data, although the arbitrarin ess o f hum an judgm ent to so m e ex tent is inevitable. No autom atized, stochastic method for the current state of the art assu res the necessity and sufficiency of data, in spite of recent am bitious trials.( Shinnoh, 1995, Ikehara, 1996) The collocations w hich w e collected are w ord strings w hose specific com ponent word implies strongly the occurrence of the rest. For exam ple, \"Ylb g u ssu ri nem uru(sleep soundly)\" is co llected as a collocation since the occurrence of \"Ylb g u ssu ri(soundly)\" strongly implies the occurrence of \" nem uru(sleep)\" in the close position, nam ely the conditional probability p(\" sleep\"|\" Ylb soundly\") of occurren ce seem s significantly larger than the case for ordinary verb-adverb pairs. Presum ably sev eral million sentences or phrases in printed articles such as new spapers, school textbooks, journals an d dictionaries w hich w ere investigated. We believe that in sp ite of so m e arbitrariness, the ability of the educated adult to recollect expressions of his/her native language and to estimate their frequency is more superior than any current autom atized sy stem based on n-gram m odels et al. Most Japanese idiom s and proverbs are included in the data. The collocations are classified into two classes b y their sy n tactic functions; one is a class of functional collocations which work like functional words. The other is a class o f con cep tual collocations which have conceptual contents like nouns, verbs, adverbs, adjectives, adjective verbs."]},{"title":"3. Functional Collocations","paragraphs":["We have two kinds of functional words; one is the particle (postpositional) which is u sed to m ark a certain relationship betw een concepts in the sentence, and the other is the auxiliary verb w hich is u sed to give the sen tence tense, aspect, m ood, m ode, sp eak er's judgm ents, etc. Accordingly, functional collocations are classified into two types, as well; one is relational collocation and the other is auxiliary p red icative collocations. (Shudo,1979) The former is exem plified by expressions in E nglish su ch as \"in order to\", \"b ased on \", \"according to\" ,\"in proportion to\", etc. and the latter, by \"is able to\", \"have to\", \"is obliged to\", etc."]},{"title":"3.1 Relational Collocations","paragraphs":["A pproximately one thousand relational collocations w ere collected. T able 1 show s exam ples of them . \"/\" in the table show s the word boundary in the collocation. t/mMo ni/tuite (about), t/lo, t/SZ, t/,nMo, w/Ot, pQ̂, srq/Mlh, iZp/sX, t/TTc, qx/tQ, t/0‘o, yTp/sX, w/Łt  Table 1: Exam ples of the relational collocation"," sZy/ssM nakereba/naranai (must), T/‘sM, o/M, hOU/M, Vp/K, t/§MsM, SfU/K, b/sM, qx/vsM, ctx/MsM, tx/tysM"," Table 2: Exam ples of the auxiliary predictive collocation "]},{"title":"3.2 Auxiliary P red ictive Collocation","paragraphs":["A pproximately one thousand four hundred auxiliary predictive collocations w ere collected. Table 2 sh o w s so m e exam ples of them ."]},{"title":"4. C on cep tual Collocation","paragraphs":["A pproximately seventy thousand conceptual collocations were collected. Table 3. show s exam ples of conceptual collocations."]},{"title":"5. A pplication to Kana-to-Kanji C on version for Japanese word P rocessor 5.1. Japanese Word P rocessor","paragraphs":["A Japanese w ord processor receives w ords, phrases or sen tences w ritten exclusively in K ana (phonetic) characters through keyboard strokes and outputs their equivalents w ritten in K ana and K anji (ideographic) characters. It is a k ind of M achine T ranslatio n sy stem which converts so u rce Kana character strings into targets of K ana and K anji mixed strings. U su ally, th ese strings have no space betw een w ords. Input strings are analyzed morphologically and segm ented into w ords (or morphem es). Then, words which sh o u ld be converted into Kanji strings and their equivalent Kanji strings are chosen. The m ajor issue for this technology is concerned w ith raising the accuracy of the seg m en tation into words and of the selection of Kanji am ong m any hom ophonic candidates. Besides the sem antic fram ew ork, e.g., thesaurus-based or sem an tic feature-based restrictions on word co-occurrences, case fram e, neural net, etc., which have been adopted in recent works(Oshima, 1986; Kobayashi, 1987; Yamamoto, 1992), we attem pted to raise the accuracy of Kanji selection by adopting surface level resources, i.e., collocation data explained in Chapter 2.-4."]},{"title":"5.2 E xp eriment","paragraphs":["We adopted the minimum cost method(Yoshimura, 1987) com bined w ith V iterbi's algorithm (Viterbi, 1967) in the disam b iguation of seg m en tations. Figure 1 illustrates the seg m en tation and conversion pro cess of input string","kind E xam ple","Nominal zw/ akano/tanin (a perfect stranger), Łw/, Go, M/}V, ¿/̋¿, qw/CM, ¥w/, „t/, S:/STZ","Verbal SmU/R otsu riga/ kuru (be enough to make change), s, +/(b, G|, /, ¶/HZ, œU/, >/AX, 2/, ./KZ","Adjectival ju‘M uraganashii (m ournful), h7M, Q[msM, MM/sM, 6SU/pTM , >U/OM Adjective -verbal ;O/ gokigen/nanam e (in a bad m ood), ‘[, {U/T futokoroga/ atataka (be rich), jU/IT","A dverbial w/ anno/jou (as expected), Mmt/sX, Ł/KTdo, /Œlo","A dnom inal q‘V ashiki (bad), Kas","Four-Kanji","-com pound f>+ gaden’insui (every m iller draw s water to his ow n mill), ŏ̧","su ru ‹t/b ooyakeni/su ru (make public), t/b","Others z/MTu toshiham o/ikanu (young), ?d, t/øu| œ~/T‘ ","Table 3: Exam ples of conceptual collocations    \"korehanegatotem ofukai(This has a very deep cause.)\". A line denotes p o ssible concatenation of candidate w ords (unitary expression - bunsetsu ). The num ber attached to each w ord candidate m eans the partial-minimum-cost of the string of words w hich begins at the top of the input string and ends w ith the w ord. A collocation,\"U/M nega/fukai\" interrupted by an adverb \"qo totem o\" and succession of adverb-adjective, i.e.,\"qo\" -\"M\" cau ses the addition of cost -3 and -1, respectively, to the basic cost, +8 of the string \"\\x/U/qo/M\" . Thus, the minimum cost (+4) of \"\\x/U/qo/ M\" is obtained as the result."]},{"title":"5.3 Prototype System A","paragraphs":["We first developed a prototype Kana-to-Kanji conversion sy stem nam ed system A w hich is eq u ipped w ith no collocation data but with only an ordinary word dictionary."]},{"title":"5.4 System B, C and D","paragraphs":["W e next reinforced S ystem A to obtain sy stem B by adding collocation data. S y stem C is a slightly m odified commercial sy stem , i.e. WXG Ver2.05 (made by a.i. so ft co.) adjusted to our experimental fram ew ork. System D is sy stem C, additionally equipped with the collocation data.   \\ x v U q o ~ T M  Ł ̧  x v N ̨ „· qqqq oooo ̆  \\ ̇C  ‰ UUUU Y = K U MMMM  \\\\\\\\ xxxx ¤ T M  ¤ q =  \\ x v h J  f  h q q  f q  h q o £  f q o    "]},{"title":"5.5 Input Data for E valuation","paragraphs":["We prepared approximately twenty three thousand pairs of input Kana strings and their equivalent Kana-Kanji mixed strings to be the expected output for our experiment. The latter includes additional information about each w ord boundary w ith the acceptable tolerance. T he average num ber of Kana for each input string is ap p roximately twenty eight."]},{"title":"5.6 E xp erimental Results","paragraphs":["The experimental sy stem performs the m orphological analysis along w ith th e co st calculation, selects the least cost seg m en tation and K anji candidate as the conversion result and then checks w hether it matches the acceptable answ er given w ith the input string. The system gives the statistical data after pro cessing all input strings, calculating inputs successfully segm ented and converted. The major results are given in Table 4. Com paring System A with B , w e conclude that the introduction of approximately 72,400 collocations cause an 8.9% rise of accuracy rate. In addition, the collocation data provide a 7.0% higher accuracy rate for System D than System C."]},{"title":"5.7 Input Length vs. A ccu racy ","paragraphs":["System A System","B","System C","System D 10,978 /22,923 13,013 /22,923 12,424 /22,923 14,027 /22,923 C onver-","sion 47.9% 56.8% 54.2% 61.2% 18,424 /22,923 19,555 /22,923 18,620 /22,923 19,682 /22,923 Segm en","-tation 80.4% 85.3% 81.2% 85.9%"," Table 4: Results of the experiment   Figure 2 show s the relationship betw een the length of input string and the accuracy of the conversion of System D. More than 520,000 different seg m en tations are mathem atically p o ssible in situations where the length of input is twenty K ana and the m inimum and maximum length of a seg m en t is one and eighteen, respectively. On the other hand, Figure 2 show s that the segm entation accuracy rate for a twenty K ana input is 9 0 .7% . Our figures sh o w that com binatorial explosion of am biguity is well prevented by the linguistic and technological fram ew ork of our sy stem , despite longer inputs an d greater dem ands on the sy stem . Figure 1: An exam ple of pro cessing Figure 2: A ccuracy vs. length of input (System D)"]},{"title":"6. Concluding Rem arks","paragraphs":["T echniques using extensive surface linguistic resources are quite im portant for the future evolution of NLP. The prospect of the availability of large scale collocation data for the reduction of am biguity raised for various kinds of NLP is p resented in this p ap er, show ing an experimental exam ple of their application to the Japanese w ord processor. In fact, they work as an effective restriction for word co-occurrence w hereas the w ord-class level prescription tends tow ards various failures. Collocations have been collected m anually for more than ten years b y the authors in order to improve, not only a high precision w ord processor, but also m o re general Japanese language pro cessin g sy stem s. Many resources, e.g. school textbooks, new spapers, novels, journals, dictionaries, etc., were referred to by w orkers, who then proceeded to judge likely candidates for the collocation. This p ro cess is based on our contention that linguistic expressions are extracted far more accurately and far more extensively through hum ans than by com puter program , despite certain arbitrarin ess inhered in hum an judgem ent and their selection of collocations. T he sparsen ess problem of expression data is crucial in the autom atic and stochastic approach. On the other hand, the brain of the educated adult is q u ite pow erful in storing and retrieving expressions of his/her mother tongue by asso ciation. We believe the next attractive and prom ising field of application for our data w ill be large vocabulary continuous speech recognition."]},{"title":"Referen ces","paragraphs":["Shudo,K. et al. (1979). A Structural M odel of B unsetsu for M achine Pro cessing of Japanese. In Trans. IECE, 62-D-12, (in Japanese).  Shudo,K. (1989). Japanese Collocations. T echnical R eport of Grant-in-Aid for Scientific R esearch, No.63101005, M ESSC , (in Japanese).  Shinnou,H & Isah ara, H. (1995). Autom atic A cquisition of Idiom s on Lexical P eculiarity. In Trans. of IPSJ, 36-8, (in Japanese).  Ikehara,S. et al. (1996). A Statistical Method for Extracting Uninterrupted and Interrupted Collocations from Very Large Corpora. In Proc. of 16th Internat. C onf. on C om putational Linguistics (COLING 96).  Oshima,Y. et al. (1986). A Disam b iguation Method in Kana-to-Kanji C onversion Using Case Fram e Grammar. In Trans. of IPSJ, 27-7, (in Japanese).  K obayashi,T. et al. (1987). R ealization of Kana-to-Kanji C onversion Using Neural Networks. In Toshiba Review , 47-11, (in Japanese).  Yamamoto,K. et al. (1992). Kana-to-Kanji C onversion Using Co-occurrence Groups. In Proc. of 44th C onf. of IPSJ, (in Japanese).  Yoshimura,K. et al. (1987). Morphological Analysis of Japanese Sentences using the L east Cost Method\", In IPSJ SIG NL-60, (in Japanese).  Viterbi,A.,J. (1967). Error B ounds for C onvolutional C odes and an Asymptotically Optimal D ecoding Algorithm . In IEEE Trans. on Information Theory, 13.  C hurch,K.W. et al. (1990). Word A sso ciation Norms, Mutual Information, and Lexicography. In C om putational Linguistics, 16.    [%] 100 90 80 70 60 50 40 30 20 10 Segm entation Number of Kana characters in input C onversion 10 20 30 40 50 60"]}]}
