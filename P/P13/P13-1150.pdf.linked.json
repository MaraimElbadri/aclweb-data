{"sections":[{"title":"","paragraphs":["Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1527–1536, Sofia, Bulgaria, August 4-9 2013. c⃝2013 Association for Computational Linguistics"]},{"title":"Unsupervised Consonant-Vowel Prediction over Hundreds of Languages Young-Bum Kim and Benjamin Snyder University of Wisconsin-Madison {ybkim,bsnyder}@cs.wisc.edu Abstract","paragraphs":["In this paper, we present a solution to one aspect of the decipherment task: the prediction of consonants and vowels for an unknown language and alphabet. Adopting a classical Bayesian perspective, we performs posterior inference over hundreds of languages, leveraging knowledge of known languages and alphabets to un-cover general linguistic patterns of typologically coherent language clusters. We achieve average accuracy in the unsupervised consonant/vowel prediction task of 99% across 503 languages. We further show that our methodology can be used to predict more fine-grained phonetic distinctions. On a three-w ay classification task between vowels, nasals, and non-nasal consonants, our model yields unsupervised accuracy of 89% across the same set of languages."]},{"title":"1 Introduction","paragraphs":["Over the past centuries, dozens of lost languages have been deciphered through the painstaking work of scholars, often after decades of slow progress and dead ends. However, several important writing systems and languages remain undeciphered to this day.","In this paper, we present a successful solution to one aspect of the decipherment puzzle: automatically identifying basic phonetic properties of letters in an unknown alphabetic writing system. Our key idea is to use knowledge of the phonetic regularities encoded in known language vocabularies to automatically build a universal probabilistic model to successfully decode new languages.","Our approach adopts a classical Bayesian perspective. We assume that each language has an unobserved set of parameters explaining its observed vocabulary. We further assume that each language-specific set of parameters was itself drawn from an unobserved common prior, shared across a cluster of typologically related languages. In turn, each cluster derives its parameters from a universal prior common to all language groups. This approach allows us to mix together data from languages with various levels of observations and perform joint posterior inference over unobserved variables of interest.","At the bottom layer (see Figure 1), our model assumes a language-speci fic data generat-ing HMM over words in the language vocabulary. Each word is modeled as an emitted sequence of characters, depending on a corresponding Markov sequence of phonetic tags. Since individual letters are highly constrained in their range of phonetic values, we make the assumption of one-tag-per - observation-type (e.g. a single letter is constrained to be always a consonant or always a vowel across all words in a language).","Going one layer up, we posit that the language-specific HMM parameters are themselves drawn from informative, non-symmetric distributions representing a typologically coherent language grouping. By applying the model to a mix of languages with observed and unobserved phonetic sequences, the cluster-le vel distributions can be in-ferred and help guide prediction for unknown languages and alphabets.","We apply this approach to two small decipherment tasks:","1. predicting whether individual characters in an unknown alphabet and language represent vowels or consonants, and","2. predicting whether individual characters in an unknown alphabet and language represent vowels, nasals, or non-nasal consonants. For both tasks, our approach yields considerable 1527 success. We experiment with a data set consist-ing of vocabularies of 503 languages from around the world, written in a mix of Latin, Cyrillic, and Greek alphabets. In turn for each language, we consider it and its alphabet “unobserved” — we hide the graphic and phonetic properties of the symbols — while treating the vocabularies of the remaining languages as fully observed with phonetic tags on each of the letters.","On average, over these 503 leave-one-language-out scenarios, our model predicts consonant/vowel distinctions with 99% accuracy. In the more challenging task of vowel/nasal/non-nasal prediction, our model achieves average accuracy over 89%."]},{"title":"2 Related Work","paragraphs":["The most direct precedent to the present work is a section in Knight et al. (2006) on universal phonetic decipherment. They build a trigram HMM with three hidden states, corresponding to consonants, vowels, and spaces. As in our model, individual characters are treated as the observed emissions of the hidden states. In contrast to the present work, they allow letters to be emitted by multiple states.","Their experiments show that the HMM trained with EM successfully clusters Spanish letters into consonants and vowels. They further design a more sophisticated finite-state model, based on linguistic universals regarding syllable structure and sonority. Experiments with the second model indicate that it can distinguish sonorous consonants (such as n, m, l, r) from non-sonorous consonants in Spanish. An advantage of the linguistically structured model is that its predictions do not require an additional mapping step from uninterpreted hidden states to linguistic categories, as they do with the HMM.","Our model and experiments can be viewed as complementary to the work of Knight et al., while also extending it to hundreds of languages. We use the simple HMM with EM as our baseline. In lieu of a linguistically designed model structure, we choose an empirical approach, allowing posterior inference over hundreds of known languages to guide the model’ s decisions for the unknown script and language.","In this sense, our model bears some similarity to the decipherment model of Snyder et al. (2010), which used knowledge of a related language (Hebrew) in an elaborate Bayesian framework to decipher the ancient language of Ugaritic. While the aim of the present work is more modest (discover-ing very basic phonetic properties of letters) it is also more widely applicable, as we don’ t required detailed analysis of a known related language.","Other recent work has employed a similar perspective for tying learning across languages. Naseem et al. (2009) use a non-parametric Bayesian model over parallel text to jointly learn part-of-speech taggers across 8 languages, while Cohen and Smith (2009) develop a shared logistic normal prior to couple multilingual learning even in the absence of parallel text. In similar veins, Berg-Kirkpatrick and Klein (2010) develop hierarchically tied grammar priors over languages within the same family, and Bouchard-Côté et al. (2013) develop a probabilistic model of sound change using data from 637 Austronesian languages.","In our own previous work, we have developed the idea that supervised knowledge of some number of languages can help guide the unsupervised induction of linguistic structure, even in the absence of parallel text (Kim et al., 2011; Kim and Snyder, 2012)1",". In the latter work we also tackled the problem of unsupervised phonemic prediction for unknown languages by using textual regularities of known languages. However, we assumed that the target language was written in a known (Latin) alphabet, greatly reducing the difficulty of the prediction task. In our present case, we assume no knowledge of any relationship between the writing system of the target language and known languages, other than that they are all alphabetic in nature.","Finally, we note some similarities of our model to some ideas proposed in other contexts. We make the assumption that each observation type (letter) occurs with only one hidden state (consonant or vowel). Similar constraints have been developed for part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011), and the power of type-based sampling has been demonstrated, even in the absence of explicit model constraints (Liang et al., 2010)."]},{"title":"3 Model","paragraphs":["Our generative Bayesian model over the observed vocabularies of hundreds of languages is 1 We note that similar ideas were simultaneously proposed","by other researchers (Cohen et al., 2011). 1528 1529 For example, the cluster Poisson parameter over vowel observation types might be λ = 9 (indicating 9 vowel letters on average for the cluster), while the parameter over consonant observation types might be λ = 20 (indicating 20 consonant letters on average). These priors will be distinct for each language cluster and serve to characterize its general linguistic and typological properties.","We pause at this point to review the Dirichlet distribution in more detail. A k−dimensional Dirichlet with parameters α1 ... αk defines a distribution over the k − 1 simplex with the following density: f (θ1 ... θk|α1 ... αk) ∝ ∏ i θαi−1 i where αi > 0, θi > 0, and","∑","i θi = 1. The Dirichlet serves as the conjugate prior for the Multinomial, meaning that the posterior θ1...θk|X1...Xn is again distributed as a Dirichlet (with updated parameters). It is instructive to reparameterize the Dirichlet with k + 1 parameters:","f (θ1 ... θk|α0, α′ 1 ... α′","k) ∝ ∏ i θ","α0α′ i−1 i where α0 =","∑ i αi, and α′","i = αi/α0. In this","parameterization, we have E[θi] = α′","i. In other words, the parameters α′","i give the mean of the distribution, and α0 gives the precision of the distribution. For large α0 ≫ k, the distribution is highly peaked around the mean (conversely, when α0 ≪ k, the mean lies in a valley).","Thus, the Dirichlet parameters of a language cluster characterize both the average HMMs of individual languages within the cluster, as well as how much we expect the HMMs to vary from the mean. In the case of emission distributions, we assume symmetric Dirichlet priors — i.e. one-parameter Dirichlets with densities given by f (θ1 ...θk|β) ∝","∏ θ","(β−1)","i . This assumption is necessary, as we have no way to identify characters across languages in the decipherment scenario, and even the number of consonants and vowels (and thus multinomial/Dirichlet dimensions) can vary across the languages of a cluster. Thus, the mean of these Dirichlets will always be a uniform emission distribution. The single Dirichlet emission parameter per cluster will specify whether this mean is on a peak (large β) or in a valley (small β). In other words, it will control the expected sparsity of the resulting per-language emission multinomials.","In contrast, the transition Dirichlet parameters may be asymmetric, and thus very specific and informative. For example, one cluster may have the property that CCC consonant clusters are exceedingly rare across all its languages. This property would be expressed by a very small mean α′ CCC ≪ 1 but large precision α0. Later we shall see examples of learned transition Dirichlet parameters. 3.3 Cluster Generation The generation of the cluster parameters (Algorithm 1) defines the highest layer of priors for our model. As Dirichlets lack a standard conjugate prior, we simply use uniform priors over the in-terval [0, 500]. For the cluster Poisson parameters, we use conjugate Gamma distributions with vague priors.3"]},{"title":"4 Inference","paragraphs":["In this section we detail the inference procedure we followed to make predictions under our model. We run the procedure over data from 503 languages, assuming that all languages but one have observed character and tag sequences: w1, w2, . . . , t1, t2, . . . Since each character type w is assumed to have a single tag category, this is equivalent to observing the character token sequence along with a character-type-to-ta g mapping tw. For the target language, we observe only character token sequence w1, w2, . . .","We assume fixed and known parameter values only at the cluster generation level. Unobserved variables include (i) the cluster parameters α, β, λ, (ii) the cluster assignments z, (iii) the per-language HMM parameters θ, φ for all languages, and (iv) for the target language, the tag tokens t1, t2, . . . — or equivalently the character-type-to-tag mappings tw — along with the observation type-counts Nt. 4.1 Monte Carlo Approximation Our goal in inference is to predict the most likely tag tw,l for each character type w in our target language l according to the posterior: f (tw,l | w, t−l) = ˆ f (tl, z, α, β | w, t−l) d Θ (1) 3 (1,19) for consonants, (1,10) for vowels, (0.2, 15) for","nasals, and (1,16) for non-nasal consonants. 1530 where Θ = (t−w,l, z, α, β), w are the observed character sequences for all languages, t−l are the character-to-tag mappings for the observed languages, z are the language-to-cluster assignments, and α and β are all the cluster-le vel transition and emission Dirichlet parameters.","Sampling values (tl, z, α, β)N","n=1 from the integrand in Equation 1 allows us to perform the standard Monte Carlo approximation: f (tw,l = t | w, t−l) ≈ N −1 N ∑ n=1 I (tw,l = t in sample n) (2) To maximize the Monte Carlo posterior, we simply take the most commonly sampled tag value for character type w in language l. Note that we leave out the language-le vel HMM parameters (θ, φ) as well as the cluster-le vel Poisson parameters λ from Equation 1 (and thus our sample space), as we can analytically integrate them out in our sampling equations. 4.2 Gibbs Sampling To sample values (tl, z, α, β) from their posterior (the integrand of Equation 1), we use Gibbs sampling, a Monte Carlo technique that constructs a Markov chain over a high-dimensional sample space by iteratively sampling each variable conditioned on the currently drawn sample values for the others, starting from a random initialization. The Markov chain converges to an equilibrium distribution which is in fact the desired joint density (Geman and Geman, 1984). We now sketch the sampling equations for each of our sampled variables. Sampling tw,l To sample the tag assignment to character w in language l, we need to compute: f (tw,l | w, t−w,l, t−l, z, α, β) (3) ∝ f (wl, tl, Nl | αk, βk, Nk−l) (4) where Nl are the types-per -tag counts implied by the mapping tl, k is the current cluster assignment for the target language (zl = k), αk and βk are the cluster parameters, and Nk−l are the types-per -tag counts for all languages currently assigned to the cluster, other than language l.","Applying the chain rule along with our model’ s conditional independence structure, we can further re-write Equation 4 as a product of three terms: f (Nl|Nk−l) (5) f (t1, t2, . . . |αk) (6) f (w1, w2, . . . |Nl, t1, t2, . . . , βk) (7) The first term is the posterior predictive distribution for the Poisson-Gamma compound distribution and is easy to derive. The second term is the tag transition predictive distribution given Dirichlet hyperparameters, yielding a familiar Polya urn scheme form. Removing terms that don’ t depend on the tag assignment tl,w gives us:","∏ t,t′ (","αk,t,t′ + n(t, t′",")) [n′","(t,t′",")]","∏ t","( ∑ t′ αk,t,t′ + n(t)",") [n′ (t)] where n(t) and n(t, t′",") are, respectively, unigram and bigram tag counts excluding those containing character w. Conversely, n′","(t) and n′","(t, t′",") are, respectively, unigram and bigram tag counts only including those containing character w. The no-tation a[n]","denotes the ascending factorial: a(a + 1) · · · (a+n−1). Finally, we tackle the third term, Equation 7, corresponding to the predictive distribution of emission observations given Dirichlet hyperparameters. Again, removing constant terms gives us: β [n(w)] k,t","∏ t′ Nl,t′β [n(t′",")] k,t′ where n(w) is the unigram count of character w, and n(t′",") is the unigram count of tag t, over all characters tokens (including w). Sampling αk,t,t′ To sample the Dirichlet hyperparameter for cluster k and transition t → t′",", we need to compute: f (αk,t,t′|t, z) ∝ f (t, z|αz,t,t′) = f (tk|αz,t,t′) where tk are the tag sequences for all languages currently assigned to cluster k. This term is a predictive distribution of the multinomial-Dirichlet compound when the observations are grouped into multiple multinomials all with the same prior. Rather than inefficiently computing a product of Polya urn schemes (with many repeated ascending 1531","factorials with the same base), we group common","terms together and calculate: ∏","j=1(αk,t,t′ + k)n(j,k,t,t′",")","∏ j=1(","∑","t′′ αk,t,t′′ + k)n(j,k,t) where n(j, k, t) and n(j, k, t, t′",") are the numbers of languages currently assigned to cluster k which have more than j occurrences of unigram (t) and bigram (t, t′","), respectively.","This gives us an efficient way to compute unnormalized posterior densities for α. However, we need to sample from these distributions, not just compute them. To do so, we turn to slice sampling (Neal, 2003), a simple yet effective auxiliary variable scheme for sampling values from unnormalized but otherwise computable densities.","The key idea is to supplement the variable x, distributed according to unnormalized density p̃(x), with a second variable u with joint density defined as p(x, u) ∝ I(u < p̃(x)). It is easy to see that p̃(x) ∝ ́","p(x, u)du. We then iteratively sample u|x and x|u, both of which are distributed uniformly across appropriately bounded intervals. Our implementation follows the pseudocode given in Mackay (2003). Sampling βk,t To sample the Dirichlet hyperparameter for cluster k and tag t we need to compute: f (βk,t|t, w, z, N) ∝ f (w|t, z, βk,t, N) ∝ f (wk|tk, βk,t, Nk) where, as before, tk are the tag sequences for languages assigned to cluster k, Nk are the tag observation type-counts for languages assigned to the cluster, and likewise wk are the character sequences of all languages in the cluster. Again, we have the predictive distribution of the multinomial-Dirichlet compound with multiple grouped observations. We can apply the same trick as above to group terms in the ascending factorials for efficient computation. As before, we use slice sampling for obtaining samples. Sampling zl Finally, we consider sampling the cluster assignment zl for each language l. We calculate: f (zl = k|w, t, N, z−l, α, β) ∝ f (wl, tl, Nl|αk, βk, Nk−l) = f (Nl|Nk−l)f (tl|αk)f (wl|tl, Nl, βk) The three terms correspond to (1) a standard predictive distributions for the Poisson-g amma compound and (2) the standard predictive distributions for the transition and emission multinomial-Dirichlet compounds."]},{"title":"5 Experiments","paragraphs":["To test our model, we apply it to a corpus of 503 languages for two decipherment tasks. In both cases, we will assume no knowledge of our target language or its writing system, other than that it is alphabetic in nature. At the same time, we will assume basic phonetic knowledge of the writing systems of the other 502 languages. For our first task, we will predict whether each character type is a consonant or a vowel. In the second task, we further subdivide the consonants into two major categories: the nasal consonants, and the non-nasal consonants. Nasal consonants are known to be perceptually very salient and are unique in being high frequency consonants in all known languages. 5.1 Data Our data is drawn from online electronic translations of the Bible (http://www.bible.is, http://www.crosswire.org/index. jsp, and http://www.biblegateway. com). We have identified translations covering 503 distinct languages employing alphabetic writing systems. Most of these languages (476) use variants of the Latin alphabet, a few (26) use Cyrillic, and one uses the Greek alphabet. As Table 1 indicates, the languages cover a very diverse set of families and geographic regions, with Niger-Congo languages being the largest represented family.4","Of these languages, 30 are either language isolates, or sole members of their language family in our data set.","For our experiments, we extracted unique word types occurring at least 5 times from the downloaded Bible texts. We manually identified vowel, nasal, and non-nasal character types. Since the letter “y” can frequently represent both a consonant and vowel, we exclude it from our evaluation. On average, the resulting vocabularies contain 2,388 unique words, with 19 consonant characters, two 2 nasal characters, and 9 vowels. We include the data as part of the paper.","4","In fact, the Niger-Congo grouping is often considered the largest language family in the world in terms of distinct member languages. 1532 Language Family #lang Niger-Congo 114 Austronesian 67 Oto-Manguean 41 Indo-European 39 Mayan 34 Quechuan 17 Afro-Asiatic 17 Uto-Aztecan 16 Altaic 16 Trans-Ne w Guinea 15 Nilo-Saharan 14 Sino-T ibetan 13 Tucanoan 9 Creole 8 Chibchan 6 Maipurean 5 Tupian 5 Nakh-Daghestanian 4 Uralic 4 Cariban 4 Totonacan 4 Mixe-Zoque 3 Jivaroan 3 Choco 3 Guajiboan 2 Huavean 2 Austro-Asiatic 2 Witotoan 2 Jean 2 Paezan 2 Other 30 Table 1: Language families in our data set. The Other category includes 9 language isolates and 21 language family singletons. 5.2 Baselines and Model Variants As our baseline, we consider the trigram HMM model of Knight et al. (2006), trained with EM. In all experiments, we run 10 random restarts of EM, and pick the prediction with highest likelihood. We map the induced tags to the gold-standard tag categories (1-1 mapping) in the way that maximizes accuracy.","We then consider three variants of our model. The simplest version, SYMM, disregards all in-formation from other languages, using simple symmetric hyperparameters on the transition and emission Dirichlet priors (all hyperparameters set to 1). This allows us to assess the performance of Model Cons vs Vowel C vs V vs N All EM 93.37 74.59 SYMM 95.99 80.72 MERGE 97.14 86.13 CLUST 98.85 89.37 Isolates EM 94.50 74.53 SYMM 96.18 78.13 MERGE 97.66 86.47 CLUST 98.55 89.07 Non-Latin EM 92.93 78.26 SYMM 95.90 79.04 MERGE 96.06 83.78 CLUST 97.03 85.79 Table 2: Average accuracy for EM baseline and model variants across 503 languages. First panel: results on all languages. Second panel: results for 30 isolate and singleton languages. Third panel: results for 27 non-Latin alphabet languages (Cyrillic and Greek). Standard Deviations across languages are about 2%. our Gibbs sampling inference method for the type-based HMM, even in the absence of multilingual priors.","We next consider a variant of our model, MERGE, that assumes that all languages reside in a single cluster. This allows knowledge from the other languages to affect our tag posteriors in a generic, language-neutral way.","Finally, we consider the full version of our model, CLUST, with 20 language clusters. By allowing for the division of languages into smaller groupings, we hope to learn more specific parameters tailored for typologically coherent clusters of languages."]},{"title":"6 Results","paragraphs":["The results of our experiments are shown in Table 2. In all cases, we report token-le vel accuracy (i.e. frequent characters count more than infrequent characters), and results are macro-a veraged over the 503 languages. Variance across languages is quite low: the standard deviations are about 2 percentage points.","For the consonant vs. vowel prediction task, all tested models perform well. Our baseline, the EM-based HMM, achieves 93.4% accuracy. Simply using our Gibbs sampler with symmetric priors boosts the performance up to 96%. Performance 1533 1534 Figure 4: Inferred Dirichlet transition hyperparameters for bigram CLUST on three-w ay classification task with four latent clusters. Row gives starting state, column gives target state. Size of red blobs are proportional to magnitude of corresponding hyperparameters. Language Family Portion #langs Ent. Indo-European","0.38 26 2.26","0.24 41 3.19","0.21 38 3.77 Quechuan 0.89 18 0.61 Mayan 0.64 33 1.70 Oto-Manguean 0.55 31 1.99 Maipurean 0.25 8 2.75 Tucanoan 0.2 45 3.98 Uto-Aztecan 0.4 25 2.85 Altaic 0.44 27 2.76 Niger-Congo 1 2 0.00 0.78 23 1.26 0.74 27 1.05 0.68 22 1.22 0.67 33 1.62 0.5 18 2.21 0.24 25 3.27 Austronesian 0.91 22 0.53 0.71 21 1.51 0.24 17 3.06 Table 3: Plurality language families across 20 clusters. The columns indicate portion of languages in the plurality family, number of languages, and entropy over families. with a bigram HMM with four language clusters. Examining just the first row, we see that the languages are partially grouped by their preference for the initial tag of words. All clusters favor languages which prefer initial consonants, though this preference is most weakly expressed in cluster 3. In contrast, both clusters 2 and 4 have very dominant tendencies towards consonant-initial languages, but differ in the relative weight given to languages preferring either vowels or nasals initially.","Finally, we examine the relationship between the induced clusters and language families in Table 3, for the trigram consonant vs. vowel CLUST model with 20 clusters. We see that for about half the clusters, there is a majority language family, most often Niger-Congo. We also observe distinctive clusters devoted to Austronesian and Quechuan languages. The largest two clusters are rather indistinct, without any single language family achieving more than 24% of the total."]},{"title":"8 Conclusion","paragraphs":["In this paper, we presented a successful solution to one aspect of the decipherment task: the prediction of consonants and vowels for an unknown language and alphabet. Adopting a classical Bayesian perspective, we develop a model that performs posterior inference over hundreds of languages, leveraging knowledge of known languages to un-cover general linguistic patterns of typologically coherent language clusters. Using this model, we automatically distinguish between consonant and vowel characters with nearly 99% accuracy across 503 languages. We further experimented on a three-w ay classification task involving nasal characters, achieving nearly 90% accuracy.","Future work will take us in several new directions: first, we would like to move beyond the assumption of an alphabetic writing system so that we can apply our method to undeciphered syllabic scripts such as Linear A. We would also like to extend our methods to achieve finer-grained resolution of phonetic properties beyond nasals, consonants, and vowels. Acknowledgments The authors thank the reviewers and acknowledge support by the NSF (grant IIS-1116676) and a research gift from Google. Any opinions, findings, or conclusions are those of the authors, and do not necessarily reflect the views of the NSF. 1535"]},{"title":"References","paragraphs":["Taylor Berg-Kirkpatrick and Dan Klein. 2010. Phylogenetic grammar induction. In Proceedings of the ACL, pages 1288–1297. Association for Computational Linguistics.","Alexandre Bouchard-Côté, David Hall, Thomas L Griffiths, and Dan Klein. 2013. Automated reconstruction of ancient languages using probabilistic models of sound change. Proceedings of the National Academy of Sciences, 110(11):4224–4229.","Christos Christodoulopoulos, Sharon Goldwater, and Mark Steedman. 2011. A Bayesian mixture model for part-of-speech induction using multiple features. In Proceedings of EMNLP, pages 638–647. Association for Computational Linguistics.","Shay B Cohen and Noah A Smith. 2009. Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction. In Proceedings of NAACL, pages 74– 82. Association for Computational Linguistics.","Shay B Cohen, Dipanjan Das, and Noah A Smith. 2011. Unsupervised structure prediction with non-parallel multilingual guidance. In Proceedings of EMNLP, pages 50–61. Association for Computational Linguistics.","Stuart Geman and Donald Geman. 1984. Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images. Pattern Analysis and Machine Intelligence, IEEE Transactions on, (6):721–741.","Young-Bum Kim and Benjamin Snyder. 2012. Universal grapheme-to-phoneme prediction over latin alphabets. In Proceedings of EMNLP, pages 332–343, Jeju Island, South Korea, July. Association for Computational Linguistics.","Young-Bum Kim, João V Graça, and Benjamin Snyder. 2011. Universal morphological analysis using structured nearest neighbor prediction. In Proceedings of EMNLP, pages 322–332. Association for Computational Linguistics.","Kevin Knight, Anish Nair, Nishit Rathod, and Kenji Yamada. 2006. Unsupervised analysis for decipherment problems. In Proceedings of COLING/ACL, pages 499–506. Association for Computational Linguistics.","Yoong Keok Lee, Aria Haghighi, and Regina Barzilay. 2010. Simple type-le vel unsupervised POS tagging. In Proceedings of EMNLP, pages 853–861. Association for Computational Linguistics.","Percy Liang, Michael I Jordan, and Dan Klein. 2010. Type-based MCMC. In Proceedings of NAACL, pages 573–581. Association for Computational Linguistics.","David JC MacKay. 2003. Information Theory, Inference and Learning Algorithms. Cambridge University Press.","Tahira Naseem, Benjamin Snyder, Jacob Eisenstein, and Regina Barzilay. 2009. Multilingual part-of-speech tagging: Two unsupervised approaches. Journal of Artificial Intelligence Research, 36(1):341–385.","Radford M Neal. 2003. Slice sampling. Annals of statistics, 31:705–741.","Benjamin Snyder, Regina Barzilay, and Kevin Knight. 2010. A statistical model for lost language decipherment. In Proceedings of the ACL, pages 1048–1057. Association for Computational Linguistics. 1536"]}],"references":[{"authors":[{"first":"Taylor","last":"Berg-Kirkpatrick"},{"first":"Dan","last":"Klein"}],"year":"2010","title":"Phylogenetic grammar induction","source":"Taylor Berg-Kirkpatrick and Dan Klein. 2010. Phylogenetic grammar induction. In Proceedings of the ACL, pages 1288–1297. Association for Computational Linguistics."},{"authors":[{"first":"Alexandre","last":"Bouchard-Côté"},{"first":"David","last":"Hall"},{"first":"Thomas","middle":"L","last":"Griffiths"},{"first":"Dan","last":"Klein"}],"year":"2013","title":"Automated reconstruction of ancient languages using probabilistic models of sound change","source":"Alexandre Bouchard-Côté, David Hall, Thomas L Griffiths, and Dan Klein. 2013. Automated reconstruction of ancient languages using probabilistic models of sound change. Proceedings of the National Academy of Sciences, 110(11):4224–4229."},{"authors":[{"first":"Christos","last":"Christodoulopoulos"},{"first":"Sharon","last":"Goldwater"},{"first":"Mark","last":"Steedman"}],"year":"2011","title":"A Bayesian mixture model for part-of-speech induction using multiple features","source":"Christos Christodoulopoulos, Sharon Goldwater, and Mark Steedman. 2011. A Bayesian mixture model for part-of-speech induction using multiple features. In Proceedings of EMNLP, pages 638–647. Association for Computational Linguistics."},{"authors":[{"first":"Shay","middle":"B","last":"Cohen"},{"first":"Noah","middle":"A","last":"Smith"}],"year":"2009","title":"Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction","source":"Shay B Cohen and Noah A Smith. 2009. Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction. In Proceedings of NAACL, pages 74– 82. Association for Computational Linguistics."},{"authors":[{"first":"Shay","middle":"B","last":"Cohen"},{"first":"Dipanjan","last":"Das"},{"first":"Noah","middle":"A","last":"Smith"}],"year":"2011","title":"Unsupervised structure prediction with non-parallel multilingual guidance","source":"Shay B Cohen, Dipanjan Das, and Noah A Smith. 2011. Unsupervised structure prediction with non-parallel multilingual guidance. In Proceedings of EMNLP, pages 50–61. Association for Computational Linguistics."},{"authors":[{"first":"Stuart","last":"Geman"},{"first":"Donald","last":"Geman"}],"year":"1984","title":"Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images","source":"Stuart Geman and Donald Geman. 1984. Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images. Pattern Analysis and Machine Intelligence, IEEE Transactions on, (6):721–741."},{"authors":[{"first":"Young-Bum","last":"Kim"},{"first":"Benjamin","last":"Snyder"}],"year":"2012","title":"Universal grapheme-to-phoneme prediction over latin alphabets","source":"Young-Bum Kim and Benjamin Snyder. 2012. Universal grapheme-to-phoneme prediction over latin alphabets. In Proceedings of EMNLP, pages 332–343, Jeju Island, South Korea, July. Association for Computational Linguistics."},{"authors":[{"first":"Young-Bum","last":"Kim"},{"first":"João V","last":"Graça"},{"first":"Benjamin","last":"Snyder"}],"year":"2011","title":"Universal morphological analysis using structured nearest neighbor prediction","source":"Young-Bum Kim, João V Graça, and Benjamin Snyder. 2011. Universal morphological analysis using structured nearest neighbor prediction. In Proceedings of EMNLP, pages 322–332. Association for Computational Linguistics."},{"authors":[{"first":"Kevin","last":"Knight"},{"first":"Anish","last":"Nair"},{"first":"Nishit","last":"Rathod"},{"first":"Kenji","last":"Yamada"}],"year":"2006","title":"Unsupervised analysis for decipherment problems","source":"Kevin Knight, Anish Nair, Nishit Rathod, and Kenji Yamada. 2006. Unsupervised analysis for decipherment problems. In Proceedings of COLING/ACL, pages 499–506. Association for Computational Linguistics."},{"authors":[{"first":"Yoong","middle":"Keok","last":"Lee"},{"first":"Aria","last":"Haghighi"},{"first":"Regina","last":"Barzilay"}],"year":"2010","title":"Simple type-le vel unsupervised POS tagging","source":"Yoong Keok Lee, Aria Haghighi, and Regina Barzilay. 2010. Simple type-le vel unsupervised POS tagging. In Proceedings of EMNLP, pages 853–861. Association for Computational Linguistics."},{"authors":[{"first":"Percy","last":"Liang"},{"first":"Michael I","last":"Jordan"},{"first":"Dan","last":"Klein"}],"year":"2010","title":"Type-based MCMC","source":"Percy Liang, Michael I Jordan, and Dan Klein. 2010. Type-based MCMC. In Proceedings of NAACL, pages 573–581. Association for Computational Linguistics."},{"authors":[{"first":"David","middle":"JC","last":"MacKay"}],"year":"2003","title":"Information Theory, Inference and Learning Algorithms","source":"David JC MacKay. 2003. Information Theory, Inference and Learning Algorithms. Cambridge University Press."},{"authors":[{"first":"Tahira","last":"Naseem"},{"first":"Benjamin","last":"Snyder"},{"first":"Jacob","last":"Eisenstein"},{"first":"Regina","last":"Barzilay"}],"year":"2009","title":"Multilingual part-of-speech tagging: Two unsupervised approaches","source":"Tahira Naseem, Benjamin Snyder, Jacob Eisenstein, and Regina Barzilay. 2009. Multilingual part-of-speech tagging: Two unsupervised approaches. Journal of Artificial Intelligence Research, 36(1):341–385."},{"authors":[{"first":"Radford","middle":"M","last":"Neal"}],"year":"2003","title":"Slice sampling","source":"Radford M Neal. 2003. Slice sampling. Annals of statistics, 31:705–741."},{"authors":[{"first":"Benjamin","last":"Snyder"},{"first":"Regina","last":"Barzilay"},{"first":"Kevin","last":"Knight"}],"year":"2010","title":"A statistical model for lost language decipherment","source":"Benjamin Snyder, Regina Barzilay, and Kevin Knight. 2010. A statistical model for lost language decipherment. In Proceedings of the ACL, pages 1048–1057. Association for Computational Linguistics. 1536"}],"cites":[{"style":0,"text":"Knight et al. (2006)","origin":{"pointer":"/sections/3/paragraphs/0","offset":62,"length":20},"authors":[{"last":"Knight"},{"last":"al."}],"year":"2006","references":["/references/8"]},{"style":0,"text":"Snyder et al. (2010)","origin":{"pointer":"/sections/3/paragraphs/3","offset":76,"length":20},"authors":[{"last":"Snyder"},{"last":"al."}],"year":"2010","references":["/references/14"]},{"style":0,"text":"Naseem et al. (2009)","origin":{"pointer":"/sections/3/paragraphs/4","offset":90,"length":20},"authors":[{"last":"Naseem"},{"last":"al."}],"year":"2009","references":["/references/12"]},{"style":0,"text":"Cohen and Smith (2009)","origin":{"pointer":"/sections/3/paragraphs/4","offset":232,"length":22},"authors":[{"last":"Cohen"},{"last":"Smith"}],"year":"2009","references":["/references/3"]},{"style":0,"text":"Berg-Kirkpatrick and Klein (2010)","origin":{"pointer":"/sections/3/paragraphs/4","offset":382,"length":33},"authors":[{"last":"Berg-Kirkpatrick"},{"last":"Klein"}],"year":"2010","references":["/references/0"]},{"style":0,"text":"Bouchard-Côté et al. (2013)","origin":{"pointer":"/sections/3/paragraphs/4","offset":502,"length":27},"authors":[{"last":"Bouchard-Côté"},{"last":"al."}],"year":"2013","references":["/references/1"]},{"style":0,"text":"Kim et al., 2011","origin":{"pointer":"/sections/3/paragraphs/5","offset":212,"length":16},"authors":[{"last":"Kim"},{"last":"al."}],"year":"2011","references":["/references/7"]},{"style":0,"text":"Kim and Snyder, 2012","origin":{"pointer":"/sections/3/paragraphs/5","offset":230,"length":20},"authors":[{"last":"Kim"},{"last":"Snyder"}],"year":"2012","references":["/references/6"]},{"style":0,"text":"Lee et al., 2010","origin":{"pointer":"/sections/3/paragraphs/7","offset":273,"length":16},"authors":[{"last":"Lee"},{"last":"al."}],"year":"2010","references":["/references/9"]},{"style":0,"text":"Christodoulopoulos et al., 2011","origin":{"pointer":"/sections/3/paragraphs/7","offset":291,"length":31},"authors":[{"last":"Christodoulopoulos"},{"last":"al."}],"year":"2011","references":["/references/2"]},{"style":0,"text":"Liang et al., 2010","origin":{"pointer":"/sections/3/paragraphs/7","offset":436,"length":18},"authors":[{"last":"Liang"},{"last":"al."}],"year":"2010","references":["/references/10"]},{"style":0,"text":"Cohen et al., 2011","origin":{"pointer":"/sections/4/paragraphs/1","offset":22,"length":18},"authors":[{"last":"Cohen"},{"last":"al."}],"year":"2011","references":["/references/4"]},{"style":0,"text":"Geman and Geman, 1984","origin":{"pointer":"/sections/5/paragraphs/4","offset":981,"length":21},"authors":[{"last":"Geman"},{"last":"Geman"}],"year":"1984","references":["/references/5"]},{"style":0,"text":"Neal, 2003","origin":{"pointer":"/sections/5/paragraphs/32","offset":195,"length":10},"authors":[{"last":"Neal"}],"year":"2003","references":["/references/13"]},{"style":0,"text":"Mackay (2003)","origin":{"pointer":"/sections/5/paragraphs/34","offset":182,"length":13},"authors":[{"last":"Mackay"}],"year":"2003","references":[]},{"style":0,"text":"Knight et al. (2006)","origin":{"pointer":"/sections/6/paragraphs/4","offset":766,"length":20},"authors":[{"last":"Knight"},{"last":"al."}],"year":"2006","references":["/references/8"]}]}
