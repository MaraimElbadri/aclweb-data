{"sections":[{"title":"","paragraphs":["Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1754–1763, Sofia, Bulgaria, August 4-9 2013. c⃝2013 Association for Computational Linguistics"]},{"title":"Syntactic Patterns versus Word Alignment: Extracting Opinion Targets from Online Reviews Kang Liu, Liheng Xu and Jun Zhao National Laboratory of Pattern Recognition Institute of Automation, Chinese Academy of Sciences {kliu, lhxu, jzhao}@nlpr.ia.ac.cn Abstract","paragraphs":["Mining opinion targets is a fundamental and important task for opinion mining from online reviews. To this end, there are usually two kinds of methods: syntax based and alignment based methods. Syntax based methods usually exploited syntactic patterns to extract opinion targets, which were however prone to suffer from parsing errors when dealing with online informal texts. In contrast, alignment based methods used word alignment model to fulfill this task, which could avoid parsing errors without using parsing. However, there is no research focusing on which kind of method is more better when given a certain amount of reviews. To fill this gap, this paper empirically studies how the performance of these two kinds of methods vary when chang-ing the size, domain and language of the corpus. We further combine syntactic patterns with alignment model by using a partially supervised framework and investigate whether this combination is useful or not. In our experiments, we verify that our combination is effective on the corpus with small and medium size."]},{"title":"1 Introduction","paragraphs":["With the rapid development of Web 2.0, huge amount of user reviews are springing up on the Web. Mining opinions from these reviews become more and more urgent since that customers expect to obtain fine-grained information of products and manufacturers need to obtain immediate feedbacks from customers. In opinion mining, extracting opinion targets is a basic subtask. It is to extract a list of the objects which users express their opinions on and can provide the prior information of targets for opinion mining. So this task has attracted many attentions. To extract opinion targets, pervious approaches usually relied on opinion words which are the words used to express the opinions (Hu and Liu, 2004a; Popescu and Etzioni, 2005; Liu et al., 2005; Wang and Wang, 2008; Qiu et al., 2011; Liu et al., 2012). In-tuitively, opinion words often appear around and modify opinion targets, and there are opinion relations and associations between them. If we have known some words to be opinion words, the words which those opinion words modify will have high probability to be opinion targets.","Therefore, identifying the aforementioned opinion relations between words is important for extracting opinion targets from reviews. To fulfill this aim, previous methods exploited the words co-occurrence information to indicate them (Hu and Liu, 2004a; Hu and Liu, 2004b). Obviously, these methods cannot obtain precise extraction because of the diverse expressions by reviewers, like long-span modified relations between words, etc. To handle this problem, several methods exploited syntactic information, where several heuristic patterns based on syntactic parsing were designed (Popescu and Etzioni, 2005; Qiu et al., 2009; Qiu et al., 2011). However, the sentences in online reviews usually have informal writing styles including grammar mistakes, typos, improper punctuation etc., which make parsing prone to generate mistakes. As a result, the syntax-based methods which heavily depended on the parsing performance would suffer from parsing errors (Zhang et al., 2010). To improve the extraction performance, we can only employ some exquisite high-precision patterns. But this strategy is likely to miss many opinion targets and has lower recall with the increase of corpus size. To resolve these problems, Liu et al. (2012) formulated identifying opinion relations between words as an monolingual alignment process. A word can find its corresponding modifiers by using a word alignment 1754 Figure 1: Mining Opinion Relations between Words using Partially Supervised Alignment Model model (WAM). Without using syntactic parsing, the noises from parsing errors can be effectively avoided. Nevertheless, we notice that the alignment model is a statistical model which needs sufficient data to estimate parameters. When the data is insufficient, it would suffer from data sparseness and may make the performance decline.","Thus, from the above analysis, we can observe that the size of the corpus has impacts on these two kinds of methods, which arises some important questions: how can we make selection between syntax based methods and alignment based method for opinion target extraction when given a certain amount of reviews? And which kind of methods can obtain better extraction performance with the variation of the size of the dataset? Although (Liu et al., 2012) had proved the effectiveness of WAM, they mainly performed experiments on the dataset with medium size. We are still curious about that when the size of dataset is larger or smaller, can we obtain the same conclusion? To our best knowledge, these problems have not been studied before. Moreover, opinions may be expressed in different ways with the variation of the domain and language of the corpus. When the domain or language of the corpus is changed, what conclusions can we obtain? To answer these questions, in this paper, we adopt a unified framework to extract opinion targets from reviews, in the key component of which we vary the methods between syntactic patterns and alignment model. Then we run the whole framework on the corpus with different size (from #500 to #1, 000, 000), domain (three domains) and language (Chinese and English) to empirically assess the performance variations and discuss which method is more effective.","Furthermore, this paper naturally addresses an-other question: is it useful for opinion targets extraction when we combine syntactic patterns and word alignment model into a unified model? To this end, we employ a partially supervised alignment model (PSWAM) like (Gao et al., 2010; Liu et al., 2013). Based on the exquisitely designed high-precision syntactic patterns, we can obtain some precisely modified relations between words in sentences, which provide a portion of links of the full alignments. Then, these partial alignment links can be regarded as the constrains for a standard unsupervised word alignment model. And each target candidate would find its modifier under the partial supervision. In this way, the errors generated in standard unsupervised WAM can be corrected. For example in Figure 1, “kindly” and “courteous” are incorrectly regarded as the modifiers for “foods” if the WAM is performed in an whole unsupervised framework. However, by using some high-precision syntactic patterns, we can assert “courteous” should be aligned to “services”, and “delicious” should be aligned to “foods”. Through combination under partial supervision, we can see “kindly” and “courteous” are correctly linked to “services”. Thus, it’s reasonable to expect to yield better performance than traditional methods. As mentioned in (Liu et al., 2013), using PSWAM can not only inherit the advantages of WAM: effectively avoiding noises from syntactic parsing errors when dealing with informal texts, but also can improve the mining performance by using partial supervision. However, is this kind of combination always useful for opinion target extraction? To access this problem, we also make comparison between PSWAM based method and the aforementioned methods in the same corpora with different size, language and domain. The experimental results show the combination by using PSWAM can be effective on dataset with small and medium size. 1755"]},{"title":"2 Related Work","paragraphs":["Opinion target extraction isn’t a new task for opinion mining. There are much work focusing on this task, such as (Hu and Liu, 2004b; Ding et al., 2008; Li et al., 2010; Popescu and Etzioni, 2005; Wu et al., 2009). Totally, previous studies can be divided into two main categories: supervised and unsupervised methods.","In supervised approaches, the opinion target extraction task was usually regarded as a sequence labeling problem (Jin and Huang, 2009; Li et al., 2010; Ma and Wan, 2010; Wu et al., 2009; Zhang et al., 2009). It’s not only to extract a lexicon or list of opinion targets, but also to find out each opinion target mentions in reviews. Thus, the contextual words are usually selected as the features to indicate opinion targets in sentences. And classical sequence labeling models are used to train the extractor, such as CRFs (Li et al., 2010), HMM (Jin and Huang, 2009) etc.. Jin et al. (2009) proposed a lexicalized HMM model to perform opinion mining. Both Li et al. (2010) and Ma et al. (2010) used CRFs model to extract opinion targets in reviews. Specially, Li et al. proposed a Skip-Tree CRF model for opinion target extraction, which exploited three structures including linear-chain structure, syntactic structure, and conjunction structure. However, the main limitation of these supervised methods is the need of labeled training data. If the labeled training data is insufficient, the trained model would have unsatisfied extraction performance. Labeling sufficient training data is time and labor consuming. And for different domains, we need label data independently, which is obviously impracticable.","Thus, many researches focused on unsupervised methods, which are mainly to extract a list of opinion targets from reviews. Similar to ours, most approaches regarded opinion words as the indicator for opinion targets. (Hu and Liu, 2004a) regarded the nearest adjective to an noun/noun phrase as its modifier. Then it exploited an association rule mining algorithm to mine the associations between them. Finally, the frequent explicit product features can be extracted in a bootstrapping process by further combining item’s frequency in dataset. Only using nearest neighbor rule to mine the modifier for each candidate cannot obtain precise results. Thus, (Popescu and Etzioni, 2005) used syntax information to extract opinion targets, which designed some syntactic patterns to capture the modified relations between words. The experimental results showed that their method had better performance than (Hu and Liu, 2004a). Moreover, (Qiu et al., 2011) proposed a Double Propagation method to expand sentiment words and opinion targets iteratively, where they also exploited syntactic relations between words. Specially, (Qiu et al., 2011) didn’t only design syntactic patterns for capturing modified relations, but also designed patterns for capturing relations among opinion targets and relations among opinion words. However, the main limitation of Qiu’s method is that the patterns based on dependency parsing tree may miss many targets for the large corpora. Therefore, Zhang et al. (2010) extended Qiu’s method. Besides the patterns used in Qiu’s method, they adopted some other special designed patterns to increase recall. In addition they used the HITS (Kleinberg, 1999) algorithm to compute opinion target confidences to improve the precision. (Liu et al., 2012) formulated identifying opinion relations between words as an alignment process. They used a completely unsupervised WAM to capture opinion relations in sentences. Then the opinion targets were extracted in a standard random walk framework where two factors were considered: opinion relevance and target importance. Their experimental results have shown that WAM was more effective than traditional syntax-based methods for this task. (Liu et al., 2013) extend Liu’s method, which is similar to our method and also used a partially supervised alignment model to extract opinion targets from reviews. We notice these two methods ((Liu et al., 2012) and (Liu et al., 2013)) only performed experiments on the corpora with a medium size. Although both of them proved that WAM model is better than the methods based on syntactic patterns, they didn’t discuss the performance variation when dealing with the corpora with different sizes, especially when the size of the corpus is less than 1,000 and more than 10,000. Based on their conclusions, we still don’t know which kind of methods should be selected for opinion target extraction when given a certain amount of reviews."]},{"title":"3 Opinion Target Extraction Methodology","paragraphs":["To extract opinion targets from reviews, we adopt the framework proposed by (Liu et al., 2012), which is a graph-based extraction framework and 1756 has two main components as follows.","1) The first component is to capture opinion relations in sentences and estimate associations between opinion target candidates and potential opinion words. In this paper, we assume opinion targets to be nouns or noun phrases, and opinion words may be adjectives or verbs, which are usually adopted by (Hu and Liu, 2004a; Qiu et al., 2011; Wang and Wang, 2008; Liu et al., 2012). And a potential opinion relation is comprised of an opinion target candidate and its corresponding modified word.","2) The second component is to estimate the confidence of each candidate. The candidates with higher confidence scores than a threshold will be extracted as opinion targets. In this procedure, we formulate the associations between opinion target candidates and potential opinion words in a bipartite graph. A random walk based algorithm is employed on this graph to estimate the confidence of each target candidate.","In this paper, we fix the method in the second component and vary the algorithms in the first component. In the first component, we respectively use syntactic patterns and unsupervised word alignment model (WAM) to capture opinion relations. In addition, we employ a partially supervised word alignment model (PSWAM) to incorporate syntactic information into WAM. In experiments, we run the whole framework on the different corpora to discuss which method is more effective. In the following subsections, we will present them in detail.","3.1 The First Component: Capturing Opinion Relations and Estimating Associations between Words 3.1.1 Syntactic Patterns To capture opinion relations in sentences by using syntactic patterns, we employ the manual designed syntactic patterns proposed by (Qiu et al., 2011). Similar to Qiu, only the syntactic patterns based on the direct dependency are employed to guarantee the extraction qualities. The direct dependency has two types. The first type indicates that one word depends on the other word without any additional words in their dependency path. The second type denotes that two words both depend on a third word directly. Specifically, we employ Minipar1","to parse sentences. To further make syn-1 http://webdocs.cs.ualberta.ca/lindek/minipar.htm tactic patterns precisely, we only use a few dependency relation labels outputted by Minipar, such as mod, pnmod, subj, desc etc. To make a clear explanation, we give out some syntactic pattern examples in Table 1. In these patterns, OC is a potential opinion word which is an adjective or a verb. T C is an opinion target candidate which is a noun or noun phrase. The item on the arrows means the dependency relation type. The item in parenthesis denotes the part-of-speech of the other word. In these examples, the first three patterns are based on the first direct dependency type and the last two patterns are based on the second direct dependency type. Pattern#1: <OC> mod −−−→<TC>","Example: This phone has an amazing design Pattern#2: <TC> obj −−→<OC>","Example: I like this phone very much Pattern#3: <OC> pnmod −−−−→<TC>","Example: the buttons easier to use Pattern#4: <OC> mod −−−→(NN) subj ←−−−<TC>","Example: IPhone is a revolutionary smart phone Pattern#5: <OC> pred −−−→(VBE) subj ←−−−<TC>","Example: The quality of LCD is good Table 1: Some Examples of Used Syntactic Patterns 3.1.2 Unsupervised Word Alignment Model In this subsection, we present our method for capturing opinion relations using unsupervised word alignment model. Similar to (Liu et al., 2012), every sentence in reviews is replicated to generate a parallel sentence pair, and the word alignment algorithm is applied to the monolingual scenario to align a noun/noun phase with its modifiers. We select IBM-3 model (Brown et al., 1993) as the alignment model. Formally, given a sentence S = {w1, w2, ..., wn}, we have Pibm3(A|S) ∝ N ∏ i=1 n(φi|wi) N ∏ j=1 t(wj|waj )d(j|aj, N ) (1) where t(wj|waj ) models the co-occurrence information of two words in dataset. d(j|aj, n) models word position information, which describes the probability of a word in position aj aligned with a word in position j. And n(φi|wi) describes the ability of a word for modifying (being modified by) several words. φi denotes the number of words 1757 that are aligned with wi. In our experiments, we set φi = 2.","Since we only have interests on capturing opinion relations between words, we only pay at-tentions on the alignments between opinion target candidates (nouns/noun phrases) and potential opinion words (adjectives/verbs). If we directly use the alignment model, a noun (noun phrase) may align with other unrelated words, like prepositions or conjunctions and so on. Thus, we set constrains on the model: 1) Alignment links must be assigned among nouns/noun phrases, adjectives/verbs and null words. Aligning to null words means that this word has no modifier or modifies nothing; 2) Other unrelated words can only align with themselves.","3.1.3 Combining Syntax-based Method with Alignment-based Method In this subsection, we try to combine syntactic information with word alignment model. As mentioned in the first section, we adopt a partially supervised alignment model to make this combination. Here, the opinion relations obtained through the high-precision syntactic patterns (Section 3.1.1) are regarded as the ground truth and can only provide a part of full alignments in sentences. They are treated as the constrains for the word alignment model. Given some partial alignment links Â = {(k, ak)|k ∈ [1, n], ak ∈ [1, n]}, the optimal word alignment A∗","= {(i, ai)|i ∈ [1, n], ai ∈ [1, n]} can be obtained as A∗","= argmax","A P (A|S, Â), where (i, ai) means that a noun (noun phrase) at position i is aligned with its modifier at position ai.","Since the labeled data provided by syntactic patterns is not a full alignment, we adopt a EM-based algorithm, named as constrained hill-climbing algorithm(Gao et al., 2010), to estimate the parameters in the model. In the training process, the constrained hill-climbing algorithm can ensure that the final model is marginalized on the partial alignment links. Particularly, in the E step, their method aims to find out the alignments which are consistent to the alignment links provided by syntactic patterns, where there are main two steps involved.","1) Optimize towards the constraints. This step aims to generate an initial alignments for alignment model (IBM-3 model in our method), which can be close to the constraints. First, a simple alignment model (IBM-1, IBM-2, HMM etc.) is trained. Then, the evidence being inconsistent to the partial alignment links will be got rid of by using the move operator operator mi,j which changes aj = i and the swap operator sj1,j2 which exchanges aj1 and aj2. The alignment is updated iteratively until no additional inconsistent links can be removed.","2) Towards the optimal alignment under the constraints. This step aims to optimize towards the optimal alignment under the constraints which starts from the aforementioned initial alignments. Gao et.al. (2010) set the corresponding cost value of the invalid move or swap operation in M and S to be negative, where M and S are respectively called Moving Matrix and Swapping Matrix, which record all possible move and swap costs between two different alignments. In this way, the invalid operators will never be picked which can guarantee that the final alignment links to have high probability to be consistent with the partial alignment links provided by high-precision syntactic patterns.","Then in M-step, evidences from the neighbor of final alignments are collected so that we can produce the estimation of parameters for the next iteration. In the process, those statistics which come from inconsistent alignment links aren’t be picked up. Thus, we have P (wi|wai, Â) = {","λ, otherwise","P (wi|wai) + λ, inconsistent with Â","(2) where λ means that we make soft constraints on the alignment model. As a result, we expect some errors generated through high-precision patterns (Section 3.1.1) may be revised in the alignment process. 3.2 Estimating Associations between Words After capturing opinion relations in sentences, we can obtain a lot of word pairs, each of which is comprised of an opinion target candidate and its corresponding modified word. Then the conditional probabilities between potential opinion target wt and potential opinion word wo can be estimated by using maximum likelihood estimation. Thus, we have P (wt|wo) =","Count(wt,wo)","Count(wo) , where Count(·) means the item’s frequency information. P (wt|wo) means the conditional probabilities between two words. At the same time, we can obtain conditional probability P (wo|wt). Then, 1758 similar to (Liu et al., 2012), the association between an opinion target candidate and its modifier is estimated as follows. Association(wt, wo) = (α × P (wt|wo) + (1 − α) × P (wo|wt))−1",", where α is the harmonic factor. We set α = 0.5 in our experiments.","3.3 The Second Component: Estimating Candidate Confidence In the second component, we adopt a graph-based algorithm used in (Liu et al., 2012) to compute the confidence of each opinion target candidate, and the candidates with higher confidence than the threshold will be extracted as the opinion targets. Here, opinion words are regarded as the important indicators. We assume that two target candidates are likely to belong to the similar category, if they are modified by similar opinion words. Thus, we can propagate the opinion target confidences through opinion words.","To model the mined associations between words, a bipartite graph is constructed, which is defined as a weighted undirected graph G = (V, E, W ). It contains two kinds of vertex: opinion target candidates and potential opinion words, respectively denoted as vt ∈ V and vo ∈ V . As shown in Figure 2, the white vertices represent opinion target candidates and the gray vertices represent potential opinion words. An edge evt,vo ∈ E between vertices represents that there is an opinion relation, and the weight w on the edge represents the association between two words. Figure 2: Modeling Opinion Relations between Words in a Bipartite Graph","To estimate the confidence of each opinion target candidate, we employ a random walk algorithm on our graph, which iteratively computes the weighted average of opinion target confidences from neighboring vertices. Thus we have","Ci+1","= (1 − β) × M × M T","× Ci","+ β × I (3) where Ci+1","and Ci","respectively represent the opinion target confidence vector in the (i + 1)th and ith","iteration. M is the matrix of word associations, where Mi,j denotes the association between the opinion target candidate i and the potential opinion word j. And I is defined as the prior confidence of each candidate for opinion target. Similar to (Liu et al., 2012), we set each item in Iv =","tf(v)idf(v)","∑","v tf(v)idf(v) , where tf (v) is the term frequency of v in the corpus, and df (v) is computed by using the Google n-gram corpus2",". β ∈ [0, 1] represents the impact of candidate prior knowl-edge on the final estimation results. In experiments, we set β = 0.4. The algorithm run until convergence which is achieved when the confidence on each node ceases to change in a tolerance value."]},{"title":"4 Experiments 4.1 Datasets and Evaluation Metrics","paragraphs":["In this section, to answer the questions mentioned in the first section, we collect a large collection named as LARGE, which includes reviews from three different domains and different languages. This collection was also used in (Liu et al., 2012). In the experiments, reviews are first segmented into sentences according to punctuation. The detailed statistical information of the used collection is shown in Table 2, where Restaurant is crawled from the Chinese Web site: www.dianping.com. The Hotel and MP3 are used in (Wang et al., 2011), which are respectively crawled from www.tripadvisor.com and www.amazon.com. For each dataset, we perform random sampling to generate testing set with different sizes, where we use sampled subsets with #sentences = 5 × 102",", 103",", 5 × 103",", 104",", 5 × 104",", 105","and 106","sentences respectively. Each Domain Language Sentence Reviews Restaurant Chinese 1,683,129 395,124 Hotel English 1,855,351 185,829 MP3 English 289,931 30,837 Table 2: Experimental Dataset sentence is tokenized, part-of-speech tagged by using Stanford NLP tool3",", and parsed by using Minipar toolkit. And the method of (Zhu et al., 2009) is used to identify noun phrases. 2 http://books.google.com/ngrams/datasets 3 http://nlp.stanford.edu/software/tagger.shtml 1759","We select precision and recall as the metrics. Specifically, to obtain the ground truth, we manually label all opinion targets for each subset. In this process, three annotators are involved. First, every noun/noun phrase and its contexts in review sentences are extracted. Then two annotators were required to judge whether every noun/noun phrase is opinion target or not. If a conflict happens, a third annotator will make judgment for final results. The average inter-agreements is 0.74. We also perform a significant test, i.e., a t-test with a default significant level of 0.05. 4.2 Compared Methods We select three methods for comparison as follows.","• Syntax: It uses syntactic patterns mentioned in Section 3.1.1 in the first component to capture opinion relations in reviews. Then the associations between words are estimated and the graph based algorithm proposed in the second component (Section 3.3) is performed to extract opinion targets.","• WAM: It is similar to Syntax, where the only difference is that WAM uses unsupervised WAM (Section 3.1.2) to capture opinion relations.","• PSWAM is similar to Syntax and WAM, where the difference is that PSWAM uses the method mentioned in Section 3.1.3 to capture opinion relations, which incorporates syntactic information into word alignment model by using partially supervised framework. The experimental results on different domains are respectively shown in Figure 3, 4 and 5.","4.3 Syntax based Methods vs. Alignment based Methods Comparing Syntax with WAM and PSWAM, we can obtain the following observations: Figure 3: Experimental results on Restaurant Figure 4: Experimental results on Hotel Figure 5: Experimental results on MP3","1) When the size of the corpus is small, Syntax has better precision than alignment based methods (WAM and PSWAM). We believe the reason is that the high-precision syntactic patterns employed in Syntax can effectively capture opinion relations in a small amount of texts. In contrast, the methods based on word alignment model may suffer from data sparseness for parameter estimation, so the precision is lower.","2) However, when the size of the corpus in-creases, the precision of Syntax decreases, even worse than alignment based methods. We believe it’s because more noises were introduced from parsing errors with the increase of the size of the corpus , which will have more negative impacts on extraction results. In contrast, for estimating the parameters of alignment based methods, the data is more sufficient, so the precision is better compared with syntax based method.","3) We also observe that recall of Syntax is worse than other two methods. It’s because the human expressions of opinions are diverse and the manual designed syntactic patterns are limited to capture all opinion relations in sentences, which may miss an amount of correct opinion targets.","4) It’s interesting that the performance gap between these three methods is smaller with the increase of the size of the corpus (more than 50,000). We guess the reason is that when the data is sufficient enough, we can obtain sufficient statistics for each opinion target. In such situation, the graph-based ranking algorithm in the second component will be apt to be affected by the frequency information, so the final performance could not be sensitive to the performance of opinion relations iden-1760 tification in the first component. Thus, in this situation, we can get conclusion that there is no obviously difference on performance between syntax-based approach and alignment-based approach.","5) From the results on dataset with different languages and different domains, we can obtain the similar observations. It indicates that choosing either syntactic patterns or word alignment model for extracting opinion targets can take a few consideration on the language and domain of the corpus.","Thus, based on the above observations, we can draw the following conclusions: making chooses between different methods is only related to the size of the corpus. The method based on syntactic patterns is more suitable for small corpus (#sentences < 5 × 103","shown in our experiments). And word alignment model is more suitable for medium corpus (5 × 103","< #sentences < 5 × 104","). Moreover, when the size of the corpus is big enough, the performance of two kinds of methods tend to become the same (#sentences ≥ 105","shown in our experiments).","4.4 Is It Useful Combining Syntactic Patterns with Word Alignment Model In this subsection, we try to see whether combining syntactic information with alignment model by using PSWAM is effective or not for opinion target extraction. From the results in Figure 3, 4 and 5, we can see that PSWAM has the similar recall compared with WAM in all datasets. PSWAM outperforms WAM on precision in all dataset. But the precision gap between PSWAM and WAM decreases when the size of the corpus increases. When the size is larger than 5 × 104",", the performance of these two methods is almost the same. We guess the reason is that more noises from parsing errors will be introduced by syntactic patterns with the increase of the size of corpus , which have negative impacts on alignment performance. At the same time, as mentioned above, a great deal of reviews will bring sufficient statistics for estimating parameters in alignment model, so the roles of partial supervision from syntactic information will be covered by frequency information used in our graph based ranking algorithm.","Compared with State-of-the-art Methods. However, it’s not say that this combination is not useful. From the results, we still see that PSWAM outperforms WAM in all datasets on precision when size of corpus is smaller than 5 × 104",". To further prove the effectiveness of our combination, we compare PSWAM with some state-of-the-art methods, including Hu (Hu and Liu, 2004a), which extracted frequent opinion target words based on association mining rules, DP (Qiu et al., 2011), which extracted opinion targets through syntactic patterns, and LIU (Liu et al., 2012), which fulfilled this task by using unsupervised WAM. The parameter settings in these baselines are the same as the settings in the original papers. Because of the space limitation, we only show the results on Restaurant and Hotel, as shown in Figure 6 and 7. Figure 6: Compared with the State-of-the-art Methods on Restaurant Figure 7: Compared with the State-of-the-art Methods on Hotel","From the experimental results, we can obtain the following observations. PSWAM outperforms other methods in most datasets. This indicates that our method based on PSWAM is effective for opinion target extraction. Especially compared PSWAM with LIU, both of which are based on word alignment model, we can see PSWAM identifies opinion relations by performing WAM under partial supervision, which can effectively improve the precision when dealing with small and medium corpus. However, these improvements are limited when the size of the corpus increases, which has the similar observations obtained above.","The Impact of Syntactic Information on Word Alignment Model. Although we have prove the effectiveness of PSWAM in the corpus with small and medium size, we are still curious about how the performance varies when we incor-1761 porate different amount of syntactic information into WAM. In this experiment, we rank the used syntactic patterns mentioned in Section 3.1.1 according to the quantities of the extracted alignment links by these patterns. Then, to capture opinion relations, we respectively use top N syntactic patterns according to frequency mentioned above to generate partial alignment links for PSWAM in section 3.1.3. We respectively define N=[1,7]. The larger is N , the more syntactic information is in-corporated. Because of the space limitation, only the average performance of all dataset is shown in Figure 8. Figure 8: The Impacts of Different Syntactic Information on Word Alignment Model","In Figure 8, we can observe that the syntactic information mainly have effect on precision. When the size of the corpus is small, the opinion relations mined by high-precision syntactic patterns are usually correct, so incorporating more syntactic information can improve the precision of word alignment model more. However, when the size of the corpus increases, incorporating more syntactic information has little impact on precision."]},{"title":"5 Conclusions and Future Work","paragraphs":["This paper discusses the performance variation of syntax based methods and alignment based methods on opinion target extraction task for the dataset with different sizes, different languages and different domains. Through experimental results, we can see that choosing which method is not related with corpus domain and language, but strongly associated with the size of the corpus . We can conclude that syntax-based method is likely to be more effective when the size of the corpus is small, and alignment-based methods are more useful for the medium size corpus. We further verify that incorporating syntactic information into word alignment model by using PSWAM is effective when dealing with the corpora with small or medium size. When the size of the corpus is larger and larger, the performance gap between syntax based, WAM and PSWAM will decrease.","In future work, we will extract opinion targets based on not only opinion relations. Other semantic relations, such as the topical associations between opinion targets (or opinion words) should also be employed. We believe that considering multiple semantic associations will help to improve the performance. In this way, how to model heterogenous relations in a unified model for opinion targets extraction is worthy to be studied."]},{"title":"Acknowledgement","paragraphs":["This work was supported by the National Natural Science Foundation of China (No. 61070106, No. 61272332 and No. 61202329), the National High Technology Development 863 Program of China (No. 2012AA011102), the National Basic Research Program of China (No. 2012CB316300), Tsinghua National Laboratory for Information Science and Technology (TNList) Cross-discipline Foundation and the Opening Project of Beijing Key Laboratory of Internet Culture and Digital Dissemination Research (ICDD201201)."]},{"title":"References","paragraphs":["Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: parameter estimation. Comput. Linguist., 19(2):263– 311, June.","Xiaowen Ding, Bing Liu, and Philip S. Yu. 2008. A holistic lexicon-based approach to opinion mining. In Proceedings of the Conference on Web Search and Web Data Mining (WSDM).","Qin Gao, Nguyen Bach, and Stephan Vogel. 2010. A semi-supervised word alignment algorithm with partial manual alignments. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, pages 1–10, Uppsala, Sweden, July. Association for Computational Linguistics. 1762","Mingqin Hu and Bing Liu. 2004a. Mining opinion features in customer reviews. In Proceedings of Conference on Artificial Intelligence (AAAI).","Minqing Hu and Bing Liu. 2004b. Mining and summarizing customer reviews. In Proceedings of the tenth ACM SIGKDD international conference on Knowl-edge discovery and data mining, KDD ’04, pages 168–177, New York, NY, USA. ACM.","Wei Jin and Hay Ho Huang. 2009. A novel lexicalized hmm-based learning framework for web opinion mining. In Proceedings of International Conference on Machine Learning (ICML).","Jon M. Kleinberg. 1999. Authoritative sources in a hyperlinked environment. J. ACM, 46(5):604–632, September.","Fangtao Li, Chao Han, Minlie Huang, Xiaoyan Zhu, Yingju Xia, Shu Zhang, and Hao Yu. 2010. Structure-aware review mining and summarization. In Chu-Ren Huang and Dan Jurafsky, editors, COLING, pages 653–661. Tsinghua University Press.","Bing Liu, Minqing Hu, and Junsheng Cheng. 2005. Opinion observer: analyzing and comparing opinions on the web. In Allan Ellis and Tatsuya Hagino, editors, WWW, pages 342–351. ACM.","Kang Liu, Liheng Xu, and Jun Zhao. 2012. Opinion target extraction using word-based translation model. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1346–1356, Jeju Island, Korea, July. Association for Computational Linguistics.","Kang Liu, Liheng Xu, Yang Liu, and Jun Zhao. 2013. Opinion target extraction using partially supervised word alignment model.","Tengfei Ma and Xiaojun Wan. 2010. Opinion target extraction in chinese news comments. In Chu-Ren Huang and Dan Jurafsky, editors, COLING (Posters), pages 782–790. Chinese Information Processing Society of China.","Ana-Maria Popescu and Oren Etzioni. 2005. Extracting product features and opinions from reviews. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT ’05, pages 339–346, Stroudsburg, PA, USA. Association for Computational Linguistics.","Guang Qiu, Bing Liu, Jiajun Bu, and Chun Che. 2009. Expanding domain sentiment lexicon through double propagation.","Guang Qiu, Bing Liu 0001, Jiajun Bu, and Chun Chen. 2011. Opinion word expansion and target extraction through double propagation. Computational Linguistics, 37(1):9–27.","Bo Wang and Houfeng Wang. 2008. Bootstrapping both product features and opinion words from chinese customer reviews with cross-inducing.","Hongning Wang, Yue Lu, and ChengXiang Zhai. 2011. Latent aspect rating analysis without aspect key-word supervision. In Chid Apt, Joydeep Ghosh, and Padhraic Smyth, editors, KDD, pages 618–626. ACM.","Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu. 2009. Phrase dependency parsing for opinion mining. In EMNLP, pages 1533–1541. ACL.","Qi Zhang, Yuanbin Wu, Tao Li, Mitsunori Ogihara, Joseph Johnson, and Xuanjing Huang. 2009. Mining product reviews based on shallow dependency parsing. In Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’09, pages 726–727, New York, NY, USA. ACM.","Lei Zhang, Bing Liu, Suk Hwan Lim, and Eamonn O’Brien-Strain. 2010. Extracting and ranking product features in opinion documents. In Chu-Ren Huang and Dan Jurafsky, editors, COLING (Posters), pages 1462–1470. Chinese Information Processing Society of China.","Jingbo Zhu, Huizhen Wang, Benjamin K. Tsou, and Muhua Zhu. 2009. Multi-aspect opinion polling from textual reviews. In David Wai-Lok Cheung, Il-Yeol Song, Wesley W. Chu, Xiaohua Hu, and Jimmy J. Lin, editors, CIKM, pages 1799–1802. ACM. 1763"]}],"references":[{"authors":[{"first":"Peter","middle":"F.","last":"Brown"},{"first":"Vincent","middle":"J. Della","last":"Pietra"},{"first":"Stephen","middle":"A. Della","last":"Pietra"},{"first":"Robert","middle":"L.","last":"Mercer"}],"year":"1993","title":"The mathematics of statistical machine translation: parameter estimation","source":"Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: parameter estimation. Comput. Linguist., 19(2):263– 311, June."},{"authors":[{"first":"Xiaowen","last":"Ding"},{"first":"Bing","last":"Liu"},{"first":"Philip","middle":"S.","last":"Yu"}],"year":"2008","title":"A holistic lexicon-based approach to opinion mining","source":"Xiaowen Ding, Bing Liu, and Philip S. Yu. 2008. A holistic lexicon-based approach to opinion mining. In Proceedings of the Conference on Web Search and Web Data Mining (WSDM)."},{"authors":[{"first":"Qin","last":"Gao"},{"first":"Nguyen","last":"Bach"},{"first":"Stephan","last":"Vogel"}],"year":"2010","title":"A semi-supervised word alignment algorithm with partial manual alignments","source":"Qin Gao, Nguyen Bach, and Stephan Vogel. 2010. A semi-supervised word alignment algorithm with partial manual alignments. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, pages 1–10, Uppsala, Sweden, July. Association for Computational Linguistics. 1762"},{"authors":[{"first":"Mingqin","last":"Hu"},{"first":"Bing","last":"Liu"}],"year":"2004a","title":"Mining opinion features in customer reviews","source":"Mingqin Hu and Bing Liu. 2004a. Mining opinion features in customer reviews. In Proceedings of Conference on Artificial Intelligence (AAAI)."},{"authors":[{"first":"Minqing","last":"Hu"},{"first":"Bing","last":"Liu"}],"year":"2004b","title":"Mining and summarizing customer reviews","source":"Minqing Hu and Bing Liu. 2004b. Mining and summarizing customer reviews. In Proceedings of the tenth ACM SIGKDD international conference on Knowl-edge discovery and data mining, KDD ’04, pages 168–177, New York, NY, USA. ACM."},{"authors":[{"first":"Wei","last":"Jin"},{"first":"Hay","middle":"Ho","last":"Huang"}],"year":"2009","title":"A novel lexicalized hmm-based learning framework for web opinion mining","source":"Wei Jin and Hay Ho Huang. 2009. A novel lexicalized hmm-based learning framework for web opinion mining. In Proceedings of International Conference on Machine Learning (ICML)."},{"authors":[{"first":"Jon","middle":"M.","last":"Kleinberg"}],"year":"1999","title":"Authoritative sources in a hyperlinked environment","source":"Jon M. Kleinberg. 1999. Authoritative sources in a hyperlinked environment. J. ACM, 46(5):604–632, September."},{"authors":[{"first":"Fangtao","last":"Li"},{"first":"Chao","last":"Han"},{"first":"Minlie","last":"Huang"},{"first":"Xiaoyan","last":"Zhu"},{"first":"Yingju","last":"Xia"},{"first":"Shu","last":"Zhang"},{"first":"Hao","last":"Yu"}],"year":"2010","title":"Structure-aware review mining and summarization","source":"Fangtao Li, Chao Han, Minlie Huang, Xiaoyan Zhu, Yingju Xia, Shu Zhang, and Hao Yu. 2010. Structure-aware review mining and summarization. In Chu-Ren Huang and Dan Jurafsky, editors, COLING, pages 653–661. Tsinghua University Press."},{"authors":[{"first":"Bing","last":"Liu"},{"first":"Minqing","last":"Hu"},{"first":"Junsheng","last":"Cheng"}],"year":"2005","title":"Opinion observer: analyzing and comparing opinions on the web","source":"Bing Liu, Minqing Hu, and Junsheng Cheng. 2005. Opinion observer: analyzing and comparing opinions on the web. In Allan Ellis and Tatsuya Hagino, editors, WWW, pages 342–351. ACM."},{"authors":[{"first":"Kang","last":"Liu"},{"first":"Liheng","last":"Xu"},{"first":"Jun","last":"Zhao"}],"year":"2012","title":"Opinion target extraction using word-based translation model","source":"Kang Liu, Liheng Xu, and Jun Zhao. 2012. Opinion target extraction using word-based translation model. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1346–1356, Jeju Island, Korea, July. Association for Computational Linguistics."},{"authors":[{"first":"Kang","last":"Liu"},{"first":"Liheng","last":"Xu"},{"first":"Yang","last":"Liu"},{"first":"Jun","last":"Zhao"}],"year":"2013","title":"Opinion target extraction using partially supervised word alignment model","source":"Kang Liu, Liheng Xu, Yang Liu, and Jun Zhao. 2013. Opinion target extraction using partially supervised word alignment model."},{"authors":[{"first":"Tengfei","last":"Ma"},{"first":"Xiaojun","last":"Wan"}],"year":"2010","title":"Opinion target extraction in chinese news comments","source":"Tengfei Ma and Xiaojun Wan. 2010. Opinion target extraction in chinese news comments. In Chu-Ren Huang and Dan Jurafsky, editors, COLING (Posters), pages 782–790. Chinese Information Processing Society of China."},{"authors":[{"first":"Ana-Maria","last":"Popescu"},{"first":"Oren","last":"Etzioni"}],"year":"2005","title":"Extracting product features and opinions from reviews","source":"Ana-Maria Popescu and Oren Etzioni. 2005. Extracting product features and opinions from reviews. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT ’05, pages 339–346, Stroudsburg, PA, USA. Association for Computational Linguistics."},{"authors":[{"first":"Guang","last":"Qiu"},{"first":"Bing","last":"Liu"},{"first":"Jiajun","last":"Bu"},{"first":"Chun","last":"Che"}],"year":"2009","title":"Expanding domain sentiment lexicon through double propagation","source":"Guang Qiu, Bing Liu, Jiajun Bu, and Chun Che. 2009. Expanding domain sentiment lexicon through double propagation."},{"authors":[{"first":"Guang","last":"Qiu"},{"first":"Bing","middle":"Liu","last":"0001"},{"first":"Jiajun","last":"Bu"},{"first":"Chun","last":"Chen"}],"year":"2011","title":"Opinion word expansion and target extraction through double propagation","source":"Guang Qiu, Bing Liu 0001, Jiajun Bu, and Chun Chen. 2011. Opinion word expansion and target extraction through double propagation. Computational Linguistics, 37(1):9–27."},{"authors":[{"first":"Bo","last":"Wang"},{"first":"Houfeng","last":"Wang"}],"year":"2008","title":"Bootstrapping both product features and opinion words from chinese customer reviews with cross-inducing","source":"Bo Wang and Houfeng Wang. 2008. Bootstrapping both product features and opinion words from chinese customer reviews with cross-inducing."},{"authors":[{"first":"Hongning","last":"Wang"},{"first":"Yue","last":"Lu"},{"first":"ChengXiang","last":"Zhai"}],"year":"2011","title":"Latent aspect rating analysis without aspect key-word supervision","source":"Hongning Wang, Yue Lu, and ChengXiang Zhai. 2011. Latent aspect rating analysis without aspect key-word supervision. In Chid Apt, Joydeep Ghosh, and Padhraic Smyth, editors, KDD, pages 618–626. ACM."},{"authors":[{"first":"Yuanbin","last":"Wu"},{"first":"Qi","last":"Zhang"},{"first":"Xuanjing","last":"Huang"},{"first":"Lide","last":"Wu"}],"year":"2009","title":"Phrase dependency parsing for opinion mining","source":"Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu. 2009. Phrase dependency parsing for opinion mining. In EMNLP, pages 1533–1541. ACL."},{"authors":[{"first":"Qi","last":"Zhang"},{"first":"Yuanbin","last":"Wu"},{"first":"Tao","last":"Li"},{"first":"Mitsunori","last":"Ogihara"},{"first":"Joseph","last":"Johnson"},{"first":"Xuanjing","last":"Huang"}],"year":"2009","title":"Mining product reviews based on shallow dependency parsing","source":"Qi Zhang, Yuanbin Wu, Tao Li, Mitsunori Ogihara, Joseph Johnson, and Xuanjing Huang. 2009. Mining product reviews based on shallow dependency parsing. In Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’09, pages 726–727, New York, NY, USA. ACM."},{"authors":[{"first":"Lei","last":"Zhang"},{"first":"Bing","last":"Liu"},{"first":"Suk","middle":"Hwan","last":"Lim"},{"first":"Eamonn","last":"O’Brien-Strain"}],"year":"2010","title":"Extracting and ranking product features in opinion documents","source":"Lei Zhang, Bing Liu, Suk Hwan Lim, and Eamonn O’Brien-Strain. 2010. Extracting and ranking product features in opinion documents. In Chu-Ren Huang and Dan Jurafsky, editors, COLING (Posters), pages 1462–1470. Chinese Information Processing Society of China."},{"authors":[{"first":"Jingbo","last":"Zhu"},{"first":"Huizhen","last":"Wang"},{"first":"Benjamin","middle":"K.","last":"Tsou"},{"first":"Muhua","last":"Zhu"}],"year":"2009","title":"Multi-aspect opinion polling from textual reviews","source":"Jingbo Zhu, Huizhen Wang, Benjamin K. Tsou, and Muhua Zhu. 2009. Multi-aspect opinion polling from textual reviews. In David Wai-Lok Cheung, Il-Yeol Song, Wesley W. Chu, Xiaohua Hu, and Jimmy J. Lin, editors, CIKM, pages 1799–1802. ACM. 1763"}],"cites":[{"style":0,"text":"Hu and Liu, 2004a","origin":{"pointer":"/sections/2/paragraphs/0","offset":689,"length":17},"authors":[{"last":"Hu"},{"last":"Liu"}],"year":"2004a","references":["/references/3"]},{"style":0,"text":"Popescu and Etzioni, 2005","origin":{"pointer":"/sections/2/paragraphs/0","offset":708,"length":25},"authors":[{"last":"Popescu"},{"last":"Etzioni"}],"year":"2005","references":["/references/12"]},{"style":0,"text":"Liu et al., 2005","origin":{"pointer":"/sections/2/paragraphs/0","offset":735,"length":16},"authors":[{"last":"Liu"},{"last":"al."}],"year":"2005","references":["/references/8"]},{"style":0,"text":"Wang and Wang, 2008","origin":{"pointer":"/sections/2/paragraphs/0","offset":753,"length":19},"authors":[{"last":"Wang"},{"last":"Wang"}],"year":"2008","references":["/references/15"]},{"style":0,"text":"Qiu et al., 2011","origin":{"pointer":"/sections/2/paragraphs/0","offset":774,"length":16},"authors":[{"last":"Qiu"},{"last":"al."}],"year":"2011","references":["/references/14"]},{"style":0,"text":"Liu et al., 2012","origin":{"pointer":"/sections/2/paragraphs/0","offset":792,"length":16},"authors":[{"last":"Liu"},{"last":"al."}],"year":"2012","references":["/references/9"]},{"style":0,"text":"Hu and Liu, 2004a","origin":{"pointer":"/sections/2/paragraphs/1","offset":234,"length":17},"authors":[{"last":"Hu"},{"last":"Liu"}],"year":"2004a","references":["/references/3"]},{"style":0,"text":"Hu and Liu, 2004b","origin":{"pointer":"/sections/2/paragraphs/1","offset":253,"length":17},"authors":[{"last":"Hu"},{"last":"Liu"}],"year":"2004b","references":["/references/4"]},{"style":0,"text":"Popescu and Etzioni, 2005","origin":{"pointer":"/sections/2/paragraphs/1","offset":582,"length":25},"authors":[{"last":"Popescu"},{"last":"Etzioni"}],"year":"2005","references":["/references/12"]},{"style":0,"text":"Qiu et al., 2009","origin":{"pointer":"/sections/2/paragraphs/1","offset":609,"length":16},"authors":[{"last":"Qiu"},{"last":"al."}],"year":"2009","references":["/references/13"]},{"style":0,"text":"Qiu et al., 2011","origin":{"pointer":"/sections/2/paragraphs/1","offset":627,"length":16},"authors":[{"last":"Qiu"},{"last":"al."}],"year":"2011","references":["/references/14"]},{"style":0,"text":"Zhang et al., 2010","origin":{"pointer":"/sections/2/paragraphs/1","offset":955,"length":18},"authors":[{"last":"Zhang"},{"last":"al."}],"year":"2010","references":["/references/19"]},{"style":0,"text":"Liu et al. (2012)","origin":{"pointer":"/sections/2/paragraphs/1","offset":1213,"length":17},"authors":[{"last":"Liu"},{"last":"al."}],"year":"2012","references":["/references/9"]},{"style":0,"text":"Liu et al., 2012","origin":{"pointer":"/sections/2/paragraphs/2","offset":432,"length":16},"authors":[{"last":"Liu"},{"last":"al."}],"year":"2012","references":["/references/9"]},{"style":0,"text":"Gao et al., 2010","origin":{"pointer":"/sections/2/paragraphs/3","offset":265,"length":16},"authors":[{"last":"Gao"},{"last":"al."}],"year":"2010","references":["/references/2"]},{"style":0,"text":"Liu et al., 2013","origin":{"pointer":"/sections/2/paragraphs/3","offset":283,"length":16},"authors":[{"last":"Liu"},{"last":"al."}],"year":"2013","references":["/references/10"]},{"style":0,"text":"Liu et al., 2013","origin":{"pointer":"/sections/2/paragraphs/3","offset":1335,"length":16},"authors":[{"last":"Liu"},{"last":"al."}],"year":"2013","references":["/references/10"]},{"style":0,"text":"Hu and Liu, 2004b","origin":{"pointer":"/sections/3/paragraphs/0","offset":115,"length":17},"authors":[{"last":"Hu"},{"last":"Liu"}],"year":"2004b","references":["/references/4"]},{"style":0,"text":"Ding et al., 2008","origin":{"pointer":"/sections/3/paragraphs/0","offset":134,"length":17},"authors":[{"last":"Ding"},{"last":"al."}],"year":"2008","references":["/references/1"]},{"style":0,"text":"Li et al., 2010","origin":{"pointer":"/sections/3/paragraphs/0","offset":153,"length":15},"authors":[{"last":"Li"},{"last":"al."}],"year":"2010","references":["/references/7"]},{"style":0,"text":"Popescu and Etzioni, 2005","origin":{"pointer":"/sections/3/paragraphs/0","offset":170,"length":25},"authors":[{"last":"Popescu"},{"last":"Etzioni"}],"year":"2005","references":["/references/12"]},{"style":0,"text":"Wu et al., 2009","origin":{"pointer":"/sections/3/paragraphs/0","offset":197,"length":15},"authors":[{"last":"Wu"},{"last":"al."}],"year":"2009","references":["/references/17"]},{"style":0,"text":"Jin and Huang, 2009","origin":{"pointer":"/sections/3/paragraphs/1","offset":114,"length":19},"authors":[{"last":"Jin"},{"last":"Huang"}],"year":"2009","references":["/references/5"]},{"style":0,"text":"Li et al., 2010","origin":{"pointer":"/sections/3/paragraphs/1","offset":135,"length":15},"authors":[{"last":"Li"},{"last":"al."}],"year":"2010","references":["/references/7"]},{"style":0,"text":"Ma and Wan, 2010","origin":{"pointer":"/sections/3/paragraphs/1","offset":152,"length":16},"authors":[{"last":"Ma"},{"last":"Wan"}],"year":"2010","references":["/references/11"]},{"style":0,"text":"Wu et al., 2009","origin":{"pointer":"/sections/3/paragraphs/1","offset":170,"length":15},"authors":[{"last":"Wu"},{"last":"al."}],"year":"2009","references":["/references/17"]},{"style":0,"text":"Zhang et al., 2009","origin":{"pointer":"/sections/3/paragraphs/1","offset":187,"length":18},"authors":[{"last":"Zhang"},{"last":"al."}],"year":"2009","references":["/references/18"]},{"style":0,"text":"Li et al., 2010","origin":{"pointer":"/sections/3/paragraphs/1","offset":525,"length":15},"authors":[{"last":"Li"},{"last":"al."}],"year":"2010","references":["/references/7"]},{"style":0,"text":"Jin and Huang, 2009","origin":{"pointer":"/sections/3/paragraphs/1","offset":548,"length":19},"authors":[{"last":"Jin"},{"last":"Huang"}],"year":"2009","references":["/references/5"]},{"style":0,"text":"Jin et al. (2009)","origin":{"pointer":"/sections/3/paragraphs/1","offset":575,"length":17},"authors":[{"last":"Jin"},{"last":"al."}],"year":"2009","references":[]},{"style":0,"text":"Li et al. (2010)","origin":{"pointer":"/sections/3/paragraphs/1","offset":658,"length":16},"authors":[{"last":"Li"},{"last":"al."}],"year":"2010","references":["/references/7"]},{"style":0,"text":"Ma et al. (2010)","origin":{"pointer":"/sections/3/paragraphs/1","offset":679,"length":16},"authors":[{"last":"Ma"},{"last":"al."}],"year":"2010","references":[]},{"style":0,"text":"Hu and Liu, 2004a","origin":{"pointer":"/sections/3/paragraphs/2","offset":218,"length":17},"authors":[{"last":"Hu"},{"last":"Liu"}],"year":"2004a","references":["/references/3"]},{"style":0,"text":"Popescu and Etzioni, 2005","origin":{"pointer":"/sections/3/paragraphs/2","offset":655,"length":25},"authors":[{"last":"Popescu"},{"last":"Etzioni"}],"year":"2005","references":["/references/12"]},{"style":0,"text":"Hu and Liu, 2004a","origin":{"pointer":"/sections/3/paragraphs/2","offset":901,"length":17},"authors":[{"last":"Hu"},{"last":"Liu"}],"year":"2004a","references":["/references/3"]},{"style":0,"text":"Qiu et al., 2011","origin":{"pointer":"/sections/3/paragraphs/2","offset":932,"length":16},"authors":[{"last":"Qiu"},{"last":"al."}],"year":"2011","references":["/references/14"]},{"style":0,"text":"Qiu et al., 2011","origin":{"pointer":"/sections/3/paragraphs/2","offset":1119,"length":16},"authors":[{"last":"Qiu"},{"last":"al."}],"year":"2011","references":["/references/14"]},{"style":0,"text":"Zhang et al. (2010)","origin":{"pointer":"/sections/3/paragraphs/2","offset":1472,"length":19},"authors":[{"last":"Zhang"},{"last":"al."}],"year":"2010","references":["/references/19"]},{"style":0,"text":"Kleinberg, 1999","origin":{"pointer":"/sections/3/paragraphs/2","offset":1660,"length":15},"authors":[{"last":"Kleinberg"}],"year":"1999","references":["/references/6"]},{"style":0,"text":"Liu et al., 2012","origin":{"pointer":"/sections/3/paragraphs/2","offset":1752,"length":16},"authors":[{"last":"Liu"},{"last":"al."}],"year":"2012","references":["/references/9"]},{"style":0,"text":"Liu et al., 2013","origin":{"pointer":"/sections/3/paragraphs/2","offset":2205,"length":16},"authors":[{"last":"Liu"},{"last":"al."}],"year":"2013","references":["/references/10"]},{"style":0,"text":"Liu et al., 2012","origin":{"pointer":"/sections/3/paragraphs/2","offset":2399,"length":16},"authors":[{"last":"Liu"},{"last":"al."}],"year":"2012","references":["/references/9"]},{"style":0,"text":"Liu et al., 2013","origin":{"pointer":"/sections/3/paragraphs/2","offset":2422,"length":16},"authors":[{"last":"Liu"},{"last":"al."}],"year":"2013","references":["/references/10"]},{"style":0,"text":"Liu et al., 2012","origin":{"pointer":"/sections/4/paragraphs/0","offset":77,"length":16},"authors":[{"last":"Liu"},{"last":"al."}],"year":"2012","references":["/references/9"]},{"style":0,"text":"Hu and Liu, 2004a","origin":{"pointer":"/sections/4/paragraphs/1","offset":303,"length":17},"authors":[{"last":"Hu"},{"last":"Liu"}],"year":"2004a","references":["/references/3"]},{"style":0,"text":"Qiu et al., 2011","origin":{"pointer":"/sections/4/paragraphs/1","offset":322,"length":16},"authors":[{"last":"Qiu"},{"last":"al."}],"year":"2011","references":["/references/14"]},{"style":0,"text":"Wang and Wang, 2008","origin":{"pointer":"/sections/4/paragraphs/1","offset":340,"length":19},"authors":[{"last":"Wang"},{"last":"Wang"}],"year":"2008","references":["/references/15"]},{"style":0,"text":"Liu et al., 2012","origin":{"pointer":"/sections/4/paragraphs/1","offset":361,"length":16},"authors":[{"last":"Liu"},{"last":"al."}],"year":"2012","references":["/references/9"]},{"style":0,"text":"Qiu et al., 2011","origin":{"pointer":"/sections/4/paragraphs/4","offset":253,"length":16},"authors":[{"last":"Qiu"},{"last":"al."}],"year":"2011","references":["/references/14"]},{"style":0,"text":"Liu et al., 2012","origin":{"pointer":"/sections/4/paragraphs/10","offset":253,"length":16},"authors":[{"last":"Liu"},{"last":"al."}],"year":"2012","references":["/references/9"]},{"style":0,"text":"Brown et al., 1993","origin":{"pointer":"/sections/4/paragraphs/10","offset":492,"length":18},"authors":[{"last":"Brown"},{"last":"al."}],"year":"1993","references":["/references/0"]},{"style":0,"text":"Gao et al., 2010","origin":{"pointer":"/sections/4/paragraphs/16","offset":155,"length":16},"authors":[{"last":"Gao"},{"last":"al."}],"year":"2010","references":["/references/2"]},{"style":0,"text":"Then, 1758","origin":{"pointer":"/sections/4/paragraphs/24","offset":199,"length":10},"authors":[{"last":"Then"}],"year":"1758","references":[]},{"style":0,"text":"Liu et al., 2012","origin":{"pointer":"/sections/4/paragraphs/24","offset":222,"length":16},"authors":[{"last":"Liu"},{"last":"al."}],"year":"2012","references":["/references/9"]},{"style":0,"text":"Liu et al., 2012","origin":{"pointer":"/sections/4/paragraphs/26","offset":125,"length":16},"authors":[{"last":"Liu"},{"last":"al."}],"year":"2012","references":["/references/9"]},{"style":0,"text":"Liu et al., 2012","origin":{"pointer":"/sections/4/paragraphs/35","offset":248,"length":16},"authors":[{"last":"Liu"},{"last":"al."}],"year":"2012","references":["/references/9"]},{"style":0,"text":"Liu et al., 2012","origin":{"pointer":"/sections/5/paragraphs/0","offset":230,"length":16},"authors":[{"last":"Liu"},{"last":"al."}],"year":"2012","references":["/references/9"]},{"style":0,"text":"Wang et al., 2011","origin":{"pointer":"/sections/5/paragraphs/0","offset":523,"length":17},"authors":[{"last":"Wang"},{"last":"al."}],"year":"2011","references":["/references/16"]},{"style":0,"text":"Zhu et al., 2009","origin":{"pointer":"/sections/5/paragraphs/8","offset":58,"length":16},"authors":[{"last":"Zhu"},{"last":"al."}],"year":"2009","references":["/references/20"]},{"style":0,"text":"Hu and Liu, 2004a","origin":{"pointer":"/sections/5/paragraphs/27","offset":124,"length":17},"authors":[{"last":"Hu"},{"last":"Liu"}],"year":"2004a","references":["/references/3"]},{"style":0,"text":"Qiu et al., 2011","origin":{"pointer":"/sections/5/paragraphs/27","offset":229,"length":16},"authors":[{"last":"Qiu"},{"last":"al."}],"year":"2011","references":["/references/14"]},{"style":0,"text":"Liu et al., 2012","origin":{"pointer":"/sections/5/paragraphs/27","offset":317,"length":16},"authors":[{"last":"Liu"},{"last":"al."}],"year":"2012","references":["/references/9"]}]}
