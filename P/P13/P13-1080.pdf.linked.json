{"sections":[{"title":"","paragraphs":["Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 811–821, Sofia, Bulgaria, August 4-9 2013. c⃝2013 Association for Computational Linguistics"]},{"title":"Shallow Local Multi Bottom-up Tree Transducers in Statistical Machine Translation Fabienne Braune and Nina Seemann and Daniel Quernheim and Andreas Maletti Institute for Natural Language Processing, University of Stuttgart Pfaffenwaldring 5b, 70569 Stuttgart, Germany {braunefe,seemanna,daniel,maletti}@ims.uni-stuttgart.de Abstract","paragraphs":["We present a new translation model in-tegrating the shallow local multi bottom-up tree transducer. We perform a large-scale empirical evaluation of our obtained system, which demonstrates that we significantly beat a realistic tree-to-tree baseline on the WMT 2009 English → German translation task. As an additional contribu-tion we make the developed software and complete tool-chain publicly available for further experimentation."]},{"title":"1 Introduction","paragraphs":["Besides phrase-based machine translation systems (Koehn et al., 2003), syntax-based systems have become widely used because of their ability to handle non-local reordering. Those systems use synchronous context-free grammars (Chiang, 2007), synchronous tree substitution grammars (Eisner, 2003) or even more powerful for-malisms like synchronous tree-sequence substitution grammars (Sun et al., 2009). However, those systems use linguistic syntactic annotation at different levels. For example, the systems proposed by Wu (1997) and Chiang (2007) use no linguistic information and are syntactic in a structural sense only. Huang et al. (2006) and Liu et al. (2006) use syntactic annotations on the source language side and show significant improvements in translation quality. Using syntax exclusively on the target language side has also been successfully tried by Galley et al. (2004) and Galley et al. (2006). Nowadays, open-source toolkits such as Moses (Koehn et al., 2007) offer syntax-based components (Hoang et al., 2009), which allow experiments without expert knowledge. The improvements observed for systems using syntactic annotation on either the source or the target language side naturally led to experiments with models that use syntactic annotations on both sides. However, as noted by Lavie et al. (2008), Liu et al. (2009), and Chiang (2010), the integration of syntactic information on both sides tends to decrease translation quality because the systems become too restrictive. Several strategies such as (i) using parse forests instead of single parses (Liu et al., 2009) or (ii) soft syntactic constraints (Chiang, 2010) have been developed to alleviate this problem. Another successful approach has been to switch to more powerful formalisms, which allow the extraction of more general rules. A particularly powerful model is the non-contiguous version of synchronous tree-sequence substitution grammars (STSSG) of Zhang et al. (2008a), Zhang et al. (2008b), and Sun et al. (2009), which allows sequences of trees on both sides of the rules [see also (Raoult, 1997)]. The multi bottom-up tree transducer (MBOT) of Arnold and Dauchet (1982) and Lilin (1978) offers a middle ground between traditional syntax-based models and STSSG. Roughly speaking, an MBOT is an STSSG, in which all the discontinuities must occur on the target language side (Maletti, 2011). This restriction yields many algorithmic advantages over both the traditional models as well as STSSG as demonstrated by Maletti (2010). For-mally, they are expressive enough to express all sensible translations (Maletti, 2012)1",". Figure 2 displays sample rules of the MBOT variant, called lMBOT, that we use (in a graphical representation of the trees and the alignment).","In this contribution, we report on our novel statistical machine translation system that uses an lMBOT-based translation model. The theoretical foundations of lMBOT and their integration into our translation model are presented in Sec-tions 2 and 3. In order to empirically evaluate the lMBOT model, we implemented a machine trans-1","A translation is sensible if it is of linear size increase and can be computed by some (potentially copying) top-down tree transducer. 811 Sε NP1 JJ11 Official111 NNS12 forecasts121 VP2 VBD21 predicted211 NP22 QP221 RB2211 just22111 CD2212 322121 NN222 %2221 Figure 1: Example tree t with indicated positions. We have t(21) = VBD and t|221 is the subtree marked in red. lation system that we are going to make available to the public. We implemented lMBOT inside the syntax-based component of the Moses open source toolkit. Section 4 presents the most important algorithms of our lMBOT decoder. We evaluate our new system on the WMT 2009 shared translation task English → German. The translation quality is automatically measured using BLEU scores, and we confirm the findings by providing linguistic evidence (see Section 5). Note that in contrast to several previous approaches, we perform large scale experiments by training systems with approx. 1.5 million parallel sentences."]},{"title":"2 Theoretical Model","paragraphs":["In this section, we present the theoretical generative model used in our approach to syntax-based machine translation. Essentially, it is the local multi bottom-up tree transducer of Maletti (2011) with the restriction that all rules must be shallow, which means that the left-hand side of each rule has height at most 2 (see Figure 2 for shallow rules and Figure 4 for rules including non-shallow rules). The rules extracted from the training example of Figure 3 are displayed in Figure 4. Those extracted rules are forcibly made shallow by removing internal nodes. The application of those rules is illustrated in Figures 5 and 6.","For those that want to understand the inner workings, we recall the principal model in full detail in the rest of this section. Since we utilize syntactic parse trees, let us introduce trees first. Given an alphabet Σ of labels, the set TΣ of all Σ-trees is the smallest set T such that σ(t1, . . . , tk) ∈ T for all σ ∈ Σ, integer k ≥ 0, and t1, . . . , tk ∈ T . In-tuitively, a tree t consists of a labeled root node σ followed by a sequence t1, . . . , tk of its children. A tree t ∈ TΣ is shallow if t = σ(t1, . . . , tk) with σ ∈ Σ and t1, . . . , tk ∈ Σ. NP QP NN → ( PP von AP NN ) S NP VBD NP → ( S NP VAFIN PP VVPP ) Figure 2: Sample lMBOT rules.","To address a node inside a tree, we use its position, which is a word consisting of positive in-tegers. Roughly speaking, the root of a tree is addressed with the position ε (the empty word). The position iw with i ∈ N addresses the position w in the ith","direct child of the root. In this way, each node in the tree is assigned a unique position. We illustrate this notion in Figure 1. Formally, the positions pos(t) ⊆ N∗","of a tree t = σ(t1, . . . , tk) are inductively defined by pos(t) = {ε} ∪ pos(k)","(t 1, . . . , tk), where","pos(k) (t1, . . . , tk) = ⋃ 1≤i≤k {iw | w ∈ pos(ti)} . Let t ∈ TΣ and w ∈ pos(t). The label of t at position w is t(w), and the subtree rooted at position w is t|w. These notions are also illustrated in Figure 1. A position w ∈ pos(t) is a leaf (in t) if w1 /∈ pos(t). In other words, leaves do not have any children. Given a subset N ⊆ Σ, we let leafN (t) = {w ∈ pos(t) | t(w) ∈ N, w leaf in t} be the set of all leaves labeled by elements of N . When N is the set of nonterminals, we call them leaf nonterminals. We extend this notion to sequences t1, . . . , tk ∈ TΣ by leaf (k) N (t1, . . . , tk) = ⋃ 1≤i≤k {iw | w ∈ leafN (ti)}. Let w1, . . . , wn ∈ pos(t) be (pairwise prefixincomparable) positions and t1, . . . , tn ∈ TΣ. Then t[wi ← ti]1≤i≤n denotes the tree that is obtained from t by replacing (in parallel) the subtrees at wi by ti for every 1 ≤ i ≤ n.","Now we are ready to introduce our model, which is a minor variation of the local multi bottom-up tree transducer of Maletti (2011). Let Σ and ∆ be the input and output symbols, respectively, and let N ⊆ Σ ∪ ∆ be the set of nonterminal symbols. Essentially, the model works on pairs ⟨t, (u1, . . . , uk)⟩ consisting of an input tree t ∈ TΣ 812 and a sequence u1, . . . , uk ∈ T∆ of output trees. Such pairs are pre-translations of rank k. The pre-translation ⟨t, (u1, . . . , uk)⟩ is shallow if all trees t, u1, . . . , uk in it are shallow.","Together with a pre-translation we typically have to store an alignment. Given a pre-translation ⟨t, (u1, . . . , uk)⟩ of rank k and 1 ≤ i ≤ k, we call ui the ith","translation of t. An alignment for this pre-translation is an injective mapping ψ : leaf","(k)","N (u1, . . . , uk) → leafN (t)×N such","that if (w, j) ∈ ran(ψ), then also (w, i) ∈ ran(ψ)","for all 1 ≤ j ≤ i.2","In other words, if an alignment","requests the ith","translation, then it should also re-","quest all previous translations. Definition 1 A shallow local multi bottom-up tree transducer (lMBOT) is a finite setR of rules to-gether with a mapping c : R → R such that every rule, written t →ψ (u1, . . . , uk), contains a shallow pre-translation ⟨t, (u1, . . . , uk)⟩ and an alignment ψ for it.","The components t, (u1, . . . , uk), ψ, and c(ρ) are called the left-hand side, the right-hand side, the alignment, and the weight of the rule ρ = t →ψ (u1, . . . , uk). Figure 2 shows two example lMBOT rules (without weights). Overall, the rules of an lMBOT are similar to the rules of an SCFG (synchronous context-free grammar), but our right-hand sides contain a sequence of trees instead of just a single tree. In addition, the alignments in an SCFG rule are bijective between leaf nonterminals, whereas our model permits multiple alignments to a single leaf nonterminal in the left-hand side (see Figure 2).","Our lMBOT rules are obtained automatically from data like that in Figure 3. Thus, we (word) align the bilingual text and parse it in both the source and the target language. In this manner we obtain sentence pairs like the one shown in Figure 3. To these sentence pairs we apply the rule extraction method of Maletti (2011). The rules extracted from the sentence pair of Figure 3 are shown in Figure 4. Note that these rules are not necessarily shallow (the last two rules are not). Thus, we post-process the extracted rules and make them shallow. The shallow rules corresponding to the non-shallow rules of Figure 4 are shown in Figure 2.","Next, we define how to combine rules to form derivations. In contrast to most other models, we","2","ran(f) for a mapping f : A → B denotes the range of f, which is {f(a) | a ∈ A}. S NP JJ Official NNS forecasts VP VBD predicted NP QP RB just CD 3 NN % S NP ADJA Offizielle NN Prognosen VAFIN sind VP PP APPR von AP ADV nur CARD 3 NN % VVPP ausgegangen Figure 3: Aligned parsed sentences. only introduce a derivation semantics that does not collapse multiple derivations for the same input-output pair.3","We need one final notion. Let ρ = t →ψ (u1, . . . , uk) be a rule and w ∈ leafN (t) be a leaf nonterminal (occurrence) in the left-hand side. The w-rank rk(ρ, w) of the rule ρ is rk(ρ, w) = max {i ∈ N | (w, i) ∈ ran(ψ)} . For example, for the lower rule ρ in Figure 2 we have rk(ρ, 1) = 1, rk(ρ, 2) = 2, and rk(ρ, 3) = 1. Definition 2 The set τ (R, c) of weighted pre-translations of an lMBOT (R, c) is the smallest set T subject to the following restriction: If there exist","• a rule ρ = t →ψ (u1, . . . , uk) ∈ R,","• a weighted pre-translation","⟨tw, cw, (uw 1 , . . . , uw","kw )⟩ ∈ T","for every w ∈ leafN (t) with – rk(ρ, w) = kw,4 – t(w) = tw(ε),5","and – for every iw′","∈ leaf(k)","N (u1, . . . , uk),6","ui(w′",") = uv","j (ε) with ψ(iw′",") = (v, j),","then ⟨t′",", c′ , (u′","1, . . . , u′","k)⟩ ∈ T is a weighted pre-","translation, where • t′","= t[w ← t","w | w ∈ leafN (t)], 3 A standard semantics is presented, for example,","in (Maletti, 2011). 4 If w has n alignments, then the pre-translation selected","for it has to have suitably many output trees. 5 The labels have to coincide for the input tree. 6 Also the labels for the output trees have to coincide. 813 JJ Official → ( ADJA Offizielle ) NNS forecasts → ( NN Prognosen ) VBD predicted → ( VAFIN sind , VVPP ausgegangen ) RB just → ( ADV nur ) CD 3 → ( CARD 3 ) NN % → ( NN % ) NP JJ NNS → ( NP ADJA NN ) QP RB CD → ( AP ADV CARD ) NP QP NN → ( PP APPR von AP NN ) S NP VP VBD NP → ( S NP VAFIN VP PP VVPP ) Figure 4: Extracted (even non-shallow) rules. We obtain our rules by making those rules shallow.","• c′ = c(ρ) · ∏","w∈leafN (t) cw, and","• u′","i = ui[iw′ ← uv","j | ψ(iw′",") = (v, j)] for every 1 ≤ i ≤ k.","Rules that do not contain any nonterminal leaves are automatically weighted pre-translations with their associated rule weight. Otherwise, each nonterminal leaf w in the left-hand side of a rule ρ must be replaced by the input tree tw of a pre-translation ⟨tw, cw, (uw","1 , . . . , uw","kw )⟩, whose root is labeled by the same nonterminal. In addition, the rank rk(ρ, w) of the replaced nonterminal should match the number kw of components in the selected weighted pre-translation. Finally, the nonterminals in the right-hand side that are aligned to w should be replaced by the translation that the alignment requests, provided that the nonterminal matches with the root symbol of the requested translation. The weight of the new pre-translation is obtained simply by multiplying the rule weight and the weights of the selected weighted pre-translations. The overall process is illustrated in Figures 5 and 6."]},{"title":"3 Translation Model","paragraphs":["Given a source language sentence e, our translation model aims to find the best corresponding target language translation ĝ;7","i.e., ĝ = arg maxg p(g|e) . We estimate the probability p(g|e) through a log-linear combination of component models with parameters λm scored on the pre-translations ⟨t, (u)⟩ such that the leaves of t concatenated read e.8 p(g|e) ∝ 7 ∏ m=1 hm ( ⟨t, (u)⟩)λm Our model uses the following features hm(⟨t, (u1, . . . , uk)⟩) for a general pre-translation τ = ⟨t, (u1, . . . , uk)⟩: 7","Our main translation direction is English to German. 8","Actually, t must embed in the parse tree of e; see Section 4. (1) The forward translation weight using the rule","weights as described in Section 2 (2) The indirect translation weight using the rule","weights as described in Section 2 (3) Lexical translation weight source → target (4) Lexical translation weight target → source (5) Target side language model (6) Number of words in the target sentences (7) Number of rules used in the pre-translation (8) Number of target side sequences; here k times","the number of sequences used in the pre-","translations that constructed τ (gap penalty)","The rule weights required for (1) are relative frequencies normalized over all rules with the same left-hand side. In the same fashion the rule weights required for (2) are relative frequencies normalized over all rules with the same right-hand side. Additionally, rules that were extracted at most 10 times are discounted by multiplying the rule weight by 10−2",". The lexical weights for (2) and (3) are obtained by multiplying the word translations w(gi|ej) [respectively, w(ej|gi)] of lexically aligned words (gi, ej) accross (possibly discontiguous) target side sequences.9","Whenever a source word ej is aligned to multiple target words, we average over the word translations.10 h3(⟨t, (u1, . . . , uk)⟩) = ∏ lexical item e occurs in t average {w(g|e) | g aligned to e}","The computation of the language model estimates for (6) is adapted to score partial translations consisting of discontiguous units. We explain the details in Section 4. Finally, the count c of target sequences obtained in (7) is actually used as a score 1001−c",". This discourages rules with many target sequences.","9","The lexical alignments are different from the alignments used with a pre-translation.","10","If the word ej has no alignment to a target word, then it is assumed to be aligned to a special NULL word and this alignment is scored. 814 Combining a rule with pre-translations: NP JJ NNS → ( NP ADJA NN ) JJ Official → ( ADJA Offizielle ) NNS forecasts → ( NN Prognosen ) Obtained new pre-translation: NP JJ Official NNS forecasts → ( NP ADJA Offizielle NN Prognosen ) Figure 5: Simple rule application. Combining a rule with pre-translations: S NP VBD NP → ( S NP VAFIN PP VVPP ) NP JJ Official NNS forecasts → ( NP ADJA Offizielle NN Prognosen ) VBD predicted → ( VAFIN sind , VVPP ausgegangen ) NP QP RB just CD 3 NN % → ( PP von AP ADV nur CARD 3 NN % ) Obtained new pre-translation: S NP JJ Official NNS forecasts VBD predicted NP QP RB just CD 3 NN % → ( S NP ADJA Offizielle NN Prognosen VAFIN sind PP von AP ADV nur CARD 3 NN % VVPP ausgegangen ) Figure 6: Complex rule application. S NP VAFIN PP VVPP Offizielle Prognosen ( sind , ausgegangen ) von nur 3 % Figure 7: Illustration of LM scoring. 815"]},{"title":"4 Decoding","paragraphs":["We implemented our model in the syntax-based component of the Moses open-source toolkit by Koehn et al. (2007) and Hoang et al. (2009). The standard Moses syntax-based decoder only handles SCFG rules; i.e, rules with contiguous components on the source and the target language side. Roughly speaking, SCFG rules are lMBOT rules with exactly one output tree. We thus had to extend the system to support our lMBOT rules, in which arbitrarily many output trees are allowed.","The standard Moses syntax-based decoder uses a CYK+ chart parsing algorithm, in which each source sentence is parsed and contiguous spans are processed in a bottom-up fashion. A rule is applicable11","if the left-hand side of it matches the nonterminal assigned to the full span by the parser and the (non-)terminal assigned to each subspan.12","In order to speed up the decoding, cube pruning (Chiang, 2007) is applied to each chart cell in order to select the most likely hypotheses for subspans. The language model (LM) scoring is directly in-tegrated into the cube pruning algorithm. Thus, LM estimates are available for all considered hypotheses. To accommodate lMBOT rules, we had to modify the Moses syntax-based decoder in several ways. First, the rule representation itself is adjusted to allow sequences of shallow output trees on the target side. Naturally, we also had to ad-just hypothesis expansion and, most importantly, language model scoring inside the cube pruning algorithm. An overview of the modified pruning procedure is given in Algorithm 1.","The most important modifications are hidden in lines 5 and 8. The expansion in Line 5 involves matching all nonterminal leaves in the rule as defined in Definition 2, which includes matching all leaf nonterminals in all (discontiguous) output trees. Because the output trees can remain discontiguous after hypothesis creation, LM scoring has to be done individually over all output trees. Algorithm 2 describes our LM scoring in detail. In it we use k strings w1, . . . , wk to collect the lexical information from the k output com-","11","Note that our notion of applicable rules differs from the default in Moses.","12","Theoretically, this allows that the decoder ignores unary parser nonterminals, which could also disappear when we make our rules shallow; e.g., the parse tree left in the pre-translation of Figure 5 can be matched by a rule with left-hand side NP(Official, forecasts). Algorithm 1 Cube pruning with lMBOT rules Data structures: - r[i, j]: list of rules matching span e[i . . . j] - h[i, j]: hypotheses covering span e[i . . . j] - c[i, j]: cube of hypotheses covering span e[i . . . j] 1: for all lMBOT rules ρ covering span e[i . . . j] do 2: Insert ρ into r[i, j] 3: Sort r[i, j] 4: for all (l →ψ r) ∈ r[i, j] do 5: Create h[i, j] by expanding all nonterminals in l with","best scoring hypotheses for subspans 6: Add h[i, j] to c[i, j] 7: for all hypotheses h ∈ c[i, j] do 8: Estimate LM score for h // see Algorithm 2 9: Estimate remaining feature scores 10: Sort c[i, j] 11: Retrieve firstα elements from c[i, j] // we use α = 103 ponents (u1, . . . , uk) of a rule. These strings can later be rearranged in any order, so we LM-score all of them separately. Roughly speaking, we obtain wi by traversing ui depth-first left-to-right. If we meet a lexical element (terminal), then we add it to the end of wi. On the other hand, if we meet a nonterminal, then we have to consult the best pre-translation τ ′","= ⟨t′",", (u′","1, . . . , u′","k′)⟩, which will contribute the subtree at this position. Suppose that u′","j will be substituted into the nonterminal in question. Then we first LM-score the pre-translation τ ′","to obtain the string w′","j corresponding to u′","j. This string w′","j is then appended to wi. Once all the strings are built, we score them using our 4-gram LM. The overall LM score for the pre-translation is obtained by multiplying the scores for w1, . . . , wk. Clearly, this treats w1, . . . , wk as k separate strings, although they eventually will be combined into a single string. Whenever such a concatenation happens, our LM scoring will automatically compute n-gram LM scores based on the concatenation, which in particular means that the LM scores get more accurate for larger spans. Finally, in the final rule only one component is allowed, which yields that the LM indeed scores the complete output sentence.","Figure 7 illustrates our LM scoring for a pre-translation involving a rule with two (discontiguous) target sequences (the construction of the pre-translation is illustrated in Figure 6). When processing the rule rooted at S, an LM estimate is computed by expanding all nonterminal leaves. In our case, these are NP, VAFIN, PP, and VVPP. However, the nodes VAFIN and VVPP are assembled from a (discontiguous) tree sequence. This means that those units have been considered as in-816 Algorithm 2 LM scoring Data structures: - (u1, . . . , uk): right-hand side of a rule - (w1, . . . , wk): k strings all initially empty 1: score = 1 2: for all 1 ≤ i ≤ k do 3: for all leaves l in ui (in lexicographic order) do 4: if l is a terminal then 5: Append l to wi 6: else 7: LM score the best hypothesis for the subspan 8: Expand wi by the corresponding w′","j 9: score = score · LM(wi) dependent until now. So far, the LM scorer could only score their associated unigrams. However, we also have their associated strings w′","1 and w′","2, which can now be used. Since VAFIN and VVPP now become parts of a single tree, we can perform LM scoring normally. Assembling the string we obtain","Offizielle Prognosen sind von nur 3 % ausgegangen which is scored by the LM. Thus, we first score the 4-grams “Offizielle Prognosen sind von”, then “Prognosen sind von nur”, etc."]},{"title":"5 Experiments 5.1 Setup","paragraphs":["The baseline system for our experiments is the syntax-based component of the Moses open-source toolkit of Koehn et al. (2007) and Hoang et al. (2009). We use linguistic syntactic annotation on both the source and the target language side (tree-to-tree). Our contrastive system is the lMBOT-based translation system presented here. We provide the system with a set of SCFG as well as lMBOT rules. We do not impose any maximal span restriction on either system.","The compared systems are evaluated on the English-to-German13","news translation task of WMT 2009 (Callison-Burch et al., 2009). For both systems, the used training data is from the 4th version of the Europarl Corpus (Koehn, 2005) and the News Commentary corpus. Both translation models were trained with approximately 1.5 million bilingual sentences after length-ratio filtering. The word alignments were generated by GIZA++ (Och and Ney, 2003) with the growdiag-final-and heuristic (Koehn et al., 2005). The 13 Note that our lMBOT-based system can be applied to any","language pair as it involves no language-specific engineering. System BLEU Baseline 12.60 lMBOT ∗","13.06","Moses t-to-s 12.72 Table 1: Evaluation results. The starred results are statistically significant improvements over the Baseline (at confidencep < 0.05). English side of the bilingual data was parsed using the Charniak parser of Charniak and Johnson (2005), and the German side was parsed using BitPar (Schmid, 2004) without the function and morphological annotations. Our German 4-gram language model was trained on the German sentences in the training data augmented by the Stuttgart SdeWaC corpus (Web-as-Corpus Consortium, 2008), whose generation is detailed in (Baroni et al., 2009). The weights λm in the log-linear model were trained using minimum error rate training (Och, 2003) with the News 2009 development set. Both systems use glue-rules, which allow them to concatenate partial translations without performing any reordering. 5.2 Results We measured the overall translation quality with the help of 4-gram BLEU (Papineni et al., 2002), which was computed on tokenized and lowercased data for both systems. The results of our evaluation are reported in Table 1. For comparison, we also report the results obtained by a system that utilizes parses only on the source side (Moses tree-to-string) with its standard features.","We can observe from Table 1 that our lMBOT-based system outperforms the baseline. We obtain a BLEU score of 13.06, which is a gain of 0.46 BLEU points over the baseline. This improvement is statistically significant at confidence p < 0.05, which we computed using the pairwise bootstrap resampling technique of Koehn (2004). Our system is also better than the Moses tree-to-string system. However this improvement (0.34) is not statistically significant. In the next section, we confirm the result of the automatic evaluation through a manual examination of some translations generated by our system and the baseline.","In Table 2, we report the number of lMBOT rules used by our system when decoding the test set. By lex we denote rules containing only lexical 817","lex non-term total contiguous 23,175 18,355 41,530","discontiguous 315 2,516 2,831 Table 2: Number of rules used in decoding test (lex: only lexical items; non-term: at least one nonterminal). 2-dis 3-dis 4-dis 2,480 323 28 Table 3: Number of k-discontiguous rules. items. The label non-term stands for rules containing at least one leaf nonterminal. The results show that approx. 6% of all rules used by our lMBOT-system have discontiguous target sides. Further-more, the reported numbers show that the system also uses rules in which lexical items are combined with nonterminals. Finally, Table 3 presents the number of rules with k target side components used during decoding. 5.3 Linguistic Analysis In this section we present linguistic evidence supporting the fact that the lMBOT-based system significantly outperforms the baseline. All examples are taken from the translation of the test set used for automatic evaluation. We show that when our system generates better translations, this is directly related to the use of lMBOT rules.","Figures 8 and 9 show the ability of our system to correctly reorder multiple segments in the source sentence where the baseline translates those segments sequentially. An analysis of the generated derivations shows that our system produces the correct translation by taking advantage of rules with discontiguous units on target language side. The rules used in the presented derivations are displayed in Figures 10 and 11. In the first example (Figure 8), we begin by translating “(( smuggle)VB (eight projectiles)NP (into the kingdom)PP)VP” into the discontiguous sequence composed of (i) “( acht geschosse)NP” ; (ii) “( in das königreich)PP” and (iii) “( schmuggeln)VP”. In a second step we as-semble all sequences in a rule with contiguous target language side and, at the same time, insert the word “( zu)PTKZU” between “( in das königreich)PP” and “( schmuggeln)VP”.","The second example (Figure 9) illustrates a more complex reordering. First, we trans-VP VB NP PP → ( NP NP , PP PP , VVINF VVINF ) S TO VP → ( VP NP PP PTKZU VVINF ) Figure 10: Used lMBOT rules for verbal reorder-ing VP ADV commented on NP → ( NP NP , ADV ADV , VPP kommentiert ) VP VBZ VP → ( NP NP , VAFIN VAFIN , ADV ADV , VPP VPP ) TOP NP VP → ( TOP NP VAFIN NP ADV VVPP ) Figure 11: Used lMBOT rules for verbal reorder-ing late “(( again)ADV commented on (the problem of global warming)NP)VP” into the discontiguous sequence composed of (i) “( das problem der globalen erwärmung)NP”; (ii) “( wieder)ADV” and (iii) “( kommentiert)VPP”. In a second step, we translate the auxiliary “( has)VBZ” by in-serting “( hat)VAFIN” into the sequence. We thus obtain, for the input segment “(( has)VBZ (again)ADV commented on (the problem of global warming)NP)VP”, the sequence (i) “( das problem der globalen erwärmung)NP”; (ii) “( hat)VAFIN”; (iii) “( wieder)ADV”; (iv) “( kommentiert)VVPP”. In a last step, the constituent “( president václav klaus)NP” is inserted between the discontiguous units “( hat)VAFIN” and “( wieder)ADV” to form the contiguous sequence “(( das problem der globalen erwärmung)NP (hat)VAFIN (präsident václav klaus)NP (wieder)ADV (kommentiert)VVPP)TOP”.","Figures 12 and 13 show examples where our system generates complex words in the target language out of a simple source language word. Again, an analysis of the generated derivation shows that lMBOT takes advantage of rules hav-ing several target side components. Examples of such rules are given in Figure 14. Through its ability to use these discontiguous rules, our system correctly translates into reflexive or particle verbs such as “ konzentriert sich” (for the English “ focuses”) or “ besteht darauf ” (for the English “ insist”). Another phenomenon well handled by our system are relative pronouns. Pronouns such as “that” or “whose” are systematically translated 818 . . . geplant hatten 8 geschosse in das königreich zu schmuggeln . . . had planned to smuggle 8 projectiles into the kingdom . . . vorhatten zu schmuggeln 8 geschosse in das königreich Figure 8: Verbal Reordering (top: our system, bottom: baseline) das problem der globalen erwärmung hat präsident václav klaus wieder kommentiert president václav klaus has again commented on the problem of global warming präsident václav klaus hat wieder kommentiert das problem der globalen erwärmung Figure 9: Verbal Reordering (top: our system, bottom: baseline) . . . die serbische delegation bestand darauf , dass jede entscheidung . . . . . . the serbian delegation insisted that every decision . . . . . . die serbische delegation bestand , jede entscheidung . . . Figure 12: Relative Clause (top: our system, bottom: baseline) . . . die roadmap von bali , konzentriert sich auf die bemühungen . . . . . . the bali roadmap that focuses on efforts . . . . . . die bali roadmap , konzentriert auf bemühungen . . . Figure 13: Reflexive Pronoun (top: our system, bottom: baseline) into both both, “ ,” and “ dass” or “ ,” and “ deren” (Figure 12)."]},{"title":"6 Conclusion and Future Work","paragraphs":["We demonstrated that our lMBOT-based machine translation system beats a standard tree-to-tree system (Moses tree-to-tree) on the WMT 2009 translation task English → German. To achieve this we implemented the formal model as described in Section 2 inside the Moses machine translation toolkit. Several modifications were necessary to obtain a working system. We publicly release all our developed software and our complete tool-chain to allow independent experiments and evaluation. This includes our lMBOT decoder IN that → ( $, ,",", KOUS dass ) VBZ focuses → ( VVFIN konzentriert , PRF sich ) Figure 14: lMBOT rules generating a relative clause/reflexive pronoun presented in Section 4 and a separate C++ module that we use for rule extraction (see Section 3).","Besides the automatic evaluation, we also performed a small manual analysis of obtained translations and show-cased some examples (see Section 5.3). We argue that our lMBOT approach can adequately handle discontiguous phrases, which occur frequently in German. Other languages that exhibit such phenomena include Czech, Dutch, Russian, and Polish. Thus, we hope that our system can also successfully be applied for other language pairs, which we plan to pursue as well.","In other future work, we want to investigate full backwards application of lMBOT rules, which would be more suitable for the converse translation direction German → English. The current independent LM scoring of components has some negative side-effects that we plan to circumvent with the use of lazy LM scoring."]},{"title":"Acknowledgement","paragraphs":["The authors thank Alexander Fraser for his ongo-ing support and advice. All authors were financially supported by the German Research Founda-tion (DFG) grant MA 4959 / 1-1. 819"]},{"title":"References","paragraphs":["André Arnold and Max Dauchet. 1982. Morphismes et bimorphismes d’arbres. Theoret. Comput. Sci., 20(1):33–93.","Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and Eros Zanchetta. 2009. The WaCky Wide Web: A collection of very large linguistically processed web-crawled corpora. Language Resources and Evaluation, 43(3):209–226.","Chris Callison-Burch, Philipp Koehn, Christof Monz, and Josh Schroeder. 2009. Findings of the 2009 Workshop on Statistical Machine Translation. In Proc. 4th Workshop on Statistical Machine Translation, pages 1–28.","Eugene Charniak and Mark Johnson. 2005. Coarse-to-fine n-best parsing and MaxEnt discriminative reranking. In Proc. 43rd ACL, pages 173–180.","David Chiang. 2007. Hierarchical phrase-based translation. Computat. Linguist., 33(2):201–228.","David Chiang. 2010. Learning to translate with source and target syntax. In Proc. 48th ACL, pages 1443– 1452.","Jason Eisner. 2003. Learning non-isomorphic tree mappings for machine translation. In Proc. 41st ACL, pages 205–208.","Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In Proc. HLT-NAACL, pages 273–280.","Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve Deneefe, Wei Wang, and Ignacio Thayer. 2006. Scalable inference and training of context-rich syntactic translation models. In Proc. 44th ACL, pages 961–968.","Hieu Hoang, Philipp Koehn, and Adam Lopez. 2009. A unified framework for phrase-based, hierarchical, and syntax-based statistical machine translation. In Proc. 6th Int. Workshop Spoken Language Translation, pages 152–159.","Liang Huang, Kevin Knight, and Aravind Joshi. 2006. Statistical syntax-directed translation with extended domain of locality. In Proc. 7th Conf. Association for Machine Translation of the Americas, pages 66– 73.","Philip Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proc. HLT-NAACL, pages 127–133.","Philipp Koehn, Amittai Axelrod, Alexandra Birch Mayne, Chris Callison-Burch, Miles Osborne, and David Talbot. 2005. Edinburgh system description for the 2005 IWSLT Speech Translation Evaluation. In Proc. 2nd Int. Workshop Spoken Language Translation.","Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proc. ACL, pages 177–180.","Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proc. EMNLP, pages 388–395.","Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In Proc. 10th Machine Translation Summit, pages 79–86.","Alon Lavie, Alok Parlikar, and Vamshi Ambati. 2008. Syntax-driven learning of sub-sentential translation equivalents and translation rules from parsed parallel corpora. In Proc. 2nd ACL Workshop on Syntax and Structure in Statistical Translation, pages 87–95.","Eric Lilin. 1978. Une généralisation des transducteurs d’états finis d’arbres: les S-transducteurs. Thèse 3ème cycle, Université de Lille.","Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-string alignment template for statistical machine translation. In Proc. 44th ACL, pages 609–616.","Yang Liu, Yajuan Lü, and Qun Liu. 2009. Improving tree-to-tree translation with packed forests. In Proc. 47th ACL, pages 558–566.","Andreas Maletti. 2010. Why synchronous tree substitution grammars? In Proc. HLT-NAACL, pages 876–884.","Andreas Maletti. 2011. How to train your multi bottom-up tree transducer. In Proc. 49th ACL, pages 825–834.","Andreas Maletti. 2012. Every sensible extended top-down tree transducer is a multi bottom-up tree transducer. In Proc. HLT-NAACL, pages 263–273.","Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computat. Linguist., 29(1):19–51.","Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proc. 41st ACL, pages 160–167.","Kishore Papineni, Salim Roukos, Todd Ward, and Wei jing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proc. 40th ACL, pages 311–318.","Jean-Claude Raoult. 1997. Rational tree relations. Bull. Belg. Math. Soc. Simon Stevin, 4(1):149–176.","Helmut Schmid. 2004. Efficient parsing of highly ambiguous context-free grammars with bit vectors. In Proc. 20th COLING, pages 162–168. 820","Jun Sun, Min Zhang, and Chew Lim Tan. 2009. A non-contiguous tree sequence alignment-based model for statistical machine translation. In Proc. 47th ACL, pages 914–922.","Web-as-Corpus Consortium. 2008. SDeWaC — a 0.88 billion word corpus for german. Website: http: //wacky.sslmit.unibo.it/doku.php.","Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computat. Linguist., 23(3):377–403.","Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li, Chew Lim Tan, and Sheng Li. 2008a. A tree sequence alignment-based tree-to-tree translation model. In Proc. 46th ACL, pages 559–567.","Min Zhang, Hongfei Jiang, Haizhou Li, Aiti Aw, and Sheng Li. 2008b. Grammar comparison study for translational equivalence modeling and statistical machine translation. In Proc. 22nd International Conference on Computational Linguistics, pages 1097–1104. 821"]}],"references":[{"authors":[{"first":"André","last":"Arnold"},{"first":"Max","last":"Dauchet"}],"year":"1982","title":"Morphismes et bimorphismes d’arbres","source":"André Arnold and Max Dauchet. 1982. Morphismes et bimorphismes d’arbres. Theoret. Comput. Sci., 20(1):33–93."},{"authors":[{"first":"Marco","last":"Baroni"},{"first":"Silvia","last":"Bernardini"},{"first":"Adriano","last":"Ferraresi"},{"first":"Eros","last":"Zanchetta"}],"year":"2009","title":"The WaCky Wide Web: A collection of very large linguistically processed web-crawled corpora","source":"Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and Eros Zanchetta. 2009. The WaCky Wide Web: A collection of very large linguistically processed web-crawled corpora. Language Resources and Evaluation, 43(3):209–226."},{"authors":[{"first":"Chris","last":"Callison-Burch"},{"first":"Philipp","last":"Koehn"},{"first":"Christof","last":"Monz"},{"first":"Josh","last":"Schroeder"}],"year":"2009","title":"Findings of the 2009 Workshop on Statistical Machine Translation","source":"Chris Callison-Burch, Philipp Koehn, Christof Monz, and Josh Schroeder. 2009. Findings of the 2009 Workshop on Statistical Machine Translation. In Proc. 4th Workshop on Statistical Machine Translation, pages 1–28."},{"authors":[{"first":"Eugene","last":"Charniak"},{"first":"Mark","last":"Johnson"}],"year":"2005","title":"Coarse-to-fine n-best parsing and MaxEnt discriminative reranking","source":"Eugene Charniak and Mark Johnson. 2005. Coarse-to-fine n-best parsing and MaxEnt discriminative reranking. In Proc. 43rd ACL, pages 173–180."},{"authors":[{"first":"David","last":"Chiang"}],"year":"2007","title":"Hierarchical phrase-based translation","source":"David Chiang. 2007. Hierarchical phrase-based translation. Computat. Linguist., 33(2):201–228."},{"authors":[{"first":"David","last":"Chiang"}],"year":"2010","title":"Learning to translate with source and target syntax","source":"David Chiang. 2010. Learning to translate with source and target syntax. In Proc. 48th ACL, pages 1443– 1452."},{"authors":[{"first":"Jason","last":"Eisner"}],"year":"2003","title":"Learning non-isomorphic tree mappings for machine translation","source":"Jason Eisner. 2003. Learning non-isomorphic tree mappings for machine translation. In Proc. 41st ACL, pages 205–208."},{"authors":[{"first":"Michel","last":"Galley"},{"first":"Mark","last":"Hopkins"},{"first":"Kevin","last":"Knight"},{"first":"Daniel","last":"Marcu"}],"year":"2004","title":"What’s in a translation rule? In Proc","source":"Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In Proc. HLT-NAACL, pages 273–280."},{"authors":[{"first":"Michel","last":"Galley"},{"first":"Jonathan","last":"Graehl"},{"first":"Kevin","last":"Knight"},{"first":"Daniel","last":"Marcu"},{"first":"Steve","last":"Deneefe"},{"first":"Wei","last":"Wang"},{"first":"Ignacio","last":"Thayer"}],"year":"2006","title":"Scalable inference and training of context-rich syntactic translation models","source":"Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve Deneefe, Wei Wang, and Ignacio Thayer. 2006. Scalable inference and training of context-rich syntactic translation models. In Proc. 44th ACL, pages 961–968."},{"authors":[{"first":"Hieu","last":"Hoang"},{"first":"Philipp","last":"Koehn"},{"first":"Adam","last":"Lopez"}],"year":"2009","title":"A unified framework for phrase-based, hierarchical, and syntax-based statistical machine translation","source":"Hieu Hoang, Philipp Koehn, and Adam Lopez. 2009. A unified framework for phrase-based, hierarchical, and syntax-based statistical machine translation. In Proc. 6th Int. Workshop Spoken Language Translation, pages 152–159."},{"authors":[{"first":"Liang","last":"Huang"},{"first":"Kevin","last":"Knight"},{"first":"Aravind","last":"Joshi"}],"year":"2006","title":"Statistical syntax-directed translation with extended domain of locality","source":"Liang Huang, Kevin Knight, and Aravind Joshi. 2006. Statistical syntax-directed translation with extended domain of locality. In Proc. 7th Conf. Association for Machine Translation of the Americas, pages 66– 73."},{"authors":[{"first":"Philip","last":"Koehn"},{"first":"Franz","middle":"Josef","last":"Och"},{"first":"Daniel","last":"Marcu"}],"year":"2003","title":"Statistical phrase-based translation","source":"Philip Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proc. HLT-NAACL, pages 127–133."},{"authors":[{"first":"Philipp","last":"Koehn"},{"first":"Amittai","last":"Axelrod"},{"first":"Alexandra","middle":"Birch","last":"Mayne"},{"first":"Chris","last":"Callison-Burch"},{"first":"Miles","last":"Osborne"},{"first":"David","last":"Talbot"}],"year":"2005","title":"Edinburgh system description for the 2005 IWSLT Speech Translation Evaluation","source":"Philipp Koehn, Amittai Axelrod, Alexandra Birch Mayne, Chris Callison-Burch, Miles Osborne, and David Talbot. 2005. Edinburgh system description for the 2005 IWSLT Speech Translation Evaluation. In Proc. 2nd Int. Workshop Spoken Language Translation."},{"authors":[{"first":"Philipp","last":"Koehn"},{"first":"Hieu","last":"Hoang"},{"first":"Alexandra","last":"Birch"},{"first":"Chris","last":"Callison-Burch"},{"first":"Marcello","last":"Federico"},{"first":"Nicola","last":"Bertoldi"},{"first":"Brooke","last":"Cowan"},{"first":"Wade","last":"Shen"},{"first":"Christine","last":"Moran"},{"first":"Richard","last":"Zens"},{"first":"Chris","last":"Dyer"},{"first":"Ondrej","last":"Bojar"},{"first":"Alexandra","last":"Constantin"},{"first":"Evan","last":"Herbst"}],"year":"2007","title":"Moses: Open source toolkit for statistical machine translation","source":"Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proc. ACL, pages 177–180."},{"authors":[{"first":"Philipp","last":"Koehn"}],"year":"2004","title":"Statistical significance tests for machine translation evaluation","source":"Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proc. EMNLP, pages 388–395."},{"authors":[{"first":"Philipp","last":"Koehn"}],"year":"2005","title":"Europarl: A parallel corpus for statistical machine translation","source":"Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In Proc. 10th Machine Translation Summit, pages 79–86."},{"authors":[{"first":"Alon","last":"Lavie"},{"first":"Alok","last":"Parlikar"},{"first":"Vamshi","last":"Ambati"}],"year":"2008","title":"Syntax-driven learning of sub-sentential translation equivalents and translation rules from parsed parallel corpora","source":"Alon Lavie, Alok Parlikar, and Vamshi Ambati. 2008. Syntax-driven learning of sub-sentential translation equivalents and translation rules from parsed parallel corpora. In Proc. 2nd ACL Workshop on Syntax and Structure in Statistical Translation, pages 87–95."},{"authors":[{"first":"Eric","last":"Lilin"}],"year":"1978","title":"Une généralisation des transducteurs d’états finis d’arbres: les S-transducteurs","source":"Eric Lilin. 1978. Une généralisation des transducteurs d’états finis d’arbres: les S-transducteurs. Thèse 3ème cycle, Université de Lille."},{"authors":[{"first":"Yang","last":"Liu"},{"first":"Qun","last":"Liu"},{"first":"Shouxun","last":"Lin"}],"year":"2006","title":"Tree-to-string alignment template for statistical machine translation","source":"Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-string alignment template for statistical machine translation. In Proc. 44th ACL, pages 609–616."},{"authors":[{"first":"Yang","last":"Liu"},{"first":"Yajuan","last":"Lü"},{"first":"Qun","last":"Liu"}],"year":"2009","title":"Improving tree-to-tree translation with packed forests","source":"Yang Liu, Yajuan Lü, and Qun Liu. 2009. Improving tree-to-tree translation with packed forests. In Proc. 47th ACL, pages 558–566."},{"authors":[{"first":"Andreas","last":"Maletti"}],"year":"2010","title":"Why synchronous tree substitution grammars? In Proc","source":"Andreas Maletti. 2010. Why synchronous tree substitution grammars? In Proc. HLT-NAACL, pages 876–884."},{"authors":[{"first":"Andreas","last":"Maletti"}],"year":"2011","title":"How to train your multi bottom-up tree transducer","source":"Andreas Maletti. 2011. How to train your multi bottom-up tree transducer. In Proc. 49th ACL, pages 825–834."},{"authors":[{"first":"Andreas","last":"Maletti"}],"year":"2012","title":"Every sensible extended top-down tree transducer is a multi bottom-up tree transducer","source":"Andreas Maletti. 2012. Every sensible extended top-down tree transducer is a multi bottom-up tree transducer. In Proc. HLT-NAACL, pages 263–273."},{"authors":[{"first":"Franz","middle":"Josef","last":"Och"},{"first":"Hermann","last":"Ney"}],"year":"2003","title":"A systematic comparison of various statistical alignment models","source":"Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computat. Linguist., 29(1):19–51."},{"authors":[{"first":"Franz","middle":"Josef","last":"Och"}],"year":"2003","title":"Minimum error rate training in statistical machine translation","source":"Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proc. 41st ACL, pages 160–167."},{"authors":[{"first":"Kishore","last":"Papineni"},{"first":"Salim","last":"Roukos"},{"first":"Todd","last":"Ward"},{"first":"Wei","middle":"jing","last":"Zhu"}],"year":"2002","title":"BLEU: a method for automatic evaluation of machine translation","source":"Kishore Papineni, Salim Roukos, Todd Ward, and Wei jing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proc. 40th ACL, pages 311–318."},{"authors":[{"first":"Jean-Claude","last":"Raoult"}],"year":"1997","title":"Rational tree relations","source":"Jean-Claude Raoult. 1997. Rational tree relations. Bull. Belg. Math. Soc. Simon Stevin, 4(1):149–176."},{"authors":[{"first":"Helmut","last":"Schmid"}],"year":"2004","title":"Efficient parsing of highly ambiguous context-free grammars with bit vectors","source":"Helmut Schmid. 2004. Efficient parsing of highly ambiguous context-free grammars with bit vectors. In Proc. 20th COLING, pages 162–168. 820"},{"authors":[{"first":"Jun","last":"Sun"},{"first":"Min","last":"Zhang"},{"first":"Chew","middle":"Lim","last":"Tan"}],"year":"2009","title":"A non-contiguous tree sequence alignment-based model for statistical machine translation","source":"Jun Sun, Min Zhang, and Chew Lim Tan. 2009. A non-contiguous tree sequence alignment-based model for statistical machine translation. In Proc. 47th ACL, pages 914–922."},{"authors":[{"first":"Web-as-Corpus","last":"Consortium"}],"year":"2008","title":"SDeWaC — a 0","source":"Web-as-Corpus Consortium. 2008. SDeWaC — a 0.88 billion word corpus for german. Website: http: //wacky.sslmit.unibo.it/doku.php."},{"authors":[{"first":"Dekai","last":"Wu"}],"year":"1997","title":"Stochastic inversion transduction grammars and bilingual parsing of parallel corpora","source":"Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computat. Linguist., 23(3):377–403."},{"authors":[{"first":"Min","last":"Zhang"},{"first":"Hongfei","last":"Jiang"},{"first":"Aiti","last":"Aw"},{"first":"Haizhou","last":"Li"},{"first":"Chew","middle":"Lim","last":"Tan"},{"first":"Sheng","last":"Li"}],"year":"2008a","title":"A tree sequence alignment-based tree-to-tree translation model","source":"Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li, Chew Lim Tan, and Sheng Li. 2008a. A tree sequence alignment-based tree-to-tree translation model. In Proc. 46th ACL, pages 559–567."},{"authors":[{"first":"Min","last":"Zhang"},{"first":"Hongfei","last":"Jiang"},{"first":"Haizhou","last":"Li"},{"first":"Aiti","last":"Aw"},{"first":"Sheng","last":"Li"}],"year":"2008b","title":"Grammar comparison study for translational equivalence modeling and statistical machine translation","source":"Min Zhang, Hongfei Jiang, Haizhou Li, Aiti Aw, and Sheng Li. 2008b. Grammar comparison study for translational equivalence modeling and statistical machine translation. In Proc. 22nd International Conference on Computational Linguistics, pages 1097–1104. 821"}],"cites":[{"style":0,"text":"Koehn et al., 2003","origin":{"pointer":"/sections/2/paragraphs/0","offset":50,"length":18},"authors":[{"last":"Koehn"},{"last":"al."}],"year":"2003","references":["/references/11"]},{"style":0,"text":"Chiang, 2007","origin":{"pointer":"/sections/2/paragraphs/0","offset":226,"length":12},"authors":[{"last":"Chiang"}],"year":"2007","references":["/references/4"]},{"style":0,"text":"Eisner, 2003","origin":{"pointer":"/sections/2/paragraphs/0","offset":281,"length":12},"authors":[{"last":"Eisner"}],"year":"2003","references":["/references/6"]},{"style":0,"text":"Sun et al., 2009","origin":{"pointer":"/sections/2/paragraphs/0","offset":383,"length":16},"authors":[{"last":"Sun"},{"last":"al."}],"year":"2009","references":["/references/28"]},{"style":0,"text":"Wu (1997)","origin":{"pointer":"/sections/2/paragraphs/0","offset":519,"length":9},"authors":[{"last":"Wu"}],"year":"1997","references":["/references/30"]},{"style":0,"text":"Chiang (2007)","origin":{"pointer":"/sections/2/paragraphs/0","offset":533,"length":13},"authors":[{"last":"Chiang"}],"year":"2007","references":["/references/4"]},{"style":0,"text":"Huang et al. (2006)","origin":{"pointer":"/sections/2/paragraphs/0","offset":623,"length":19},"authors":[{"last":"Huang"},{"last":"al."}],"year":"2006","references":["/references/10"]},{"style":0,"text":"Liu et al. (2006)","origin":{"pointer":"/sections/2/paragraphs/0","offset":647,"length":17},"authors":[{"last":"Liu"},{"last":"al."}],"year":"2006","references":["/references/18"]},{"style":0,"text":"Galley et al. (2004)","origin":{"pointer":"/sections/2/paragraphs/0","offset":866,"length":20},"authors":[{"last":"Galley"},{"last":"al."}],"year":"2004","references":["/references/7"]},{"style":0,"text":"Galley et al. (2006)","origin":{"pointer":"/sections/2/paragraphs/0","offset":891,"length":20},"authors":[{"last":"Galley"},{"last":"al."}],"year":"2006","references":["/references/8"]},{"style":0,"text":"Koehn et al., 2007","origin":{"pointer":"/sections/2/paragraphs/0","offset":959,"length":18},"authors":[{"last":"Koehn"},{"last":"al."}],"year":"2007","references":["/references/13"]},{"style":0,"text":"Hoang et al., 2009","origin":{"pointer":"/sections/2/paragraphs/0","offset":1010,"length":18},"authors":[{"last":"Hoang"},{"last":"al."}],"year":"2009","references":["/references/9"]},{"style":0,"text":"Lavie et al. (2008)","origin":{"pointer":"/sections/2/paragraphs/0","offset":1303,"length":19},"authors":[{"last":"Lavie"},{"last":"al."}],"year":"2008","references":["/references/16"]},{"style":0,"text":"Liu et al. (2009)","origin":{"pointer":"/sections/2/paragraphs/0","offset":1324,"length":17},"authors":[{"last":"Liu"},{"last":"al."}],"year":"2009","references":["/references/19"]},{"style":0,"text":"Chiang (2010)","origin":{"pointer":"/sections/2/paragraphs/0","offset":1347,"length":13},"authors":[{"last":"Chiang"}],"year":"2010","references":["/references/5"]},{"style":0,"text":"Liu et al., 2009","origin":{"pointer":"/sections/2/paragraphs/0","offset":1576,"length":16},"authors":[{"last":"Liu"},{"last":"al."}],"year":"2009","references":["/references/19"]},{"style":0,"text":"Chiang, 2010","origin":{"pointer":"/sections/2/paragraphs/0","offset":1630,"length":12},"authors":[{"last":"Chiang"}],"year":"2010","references":["/references/5"]},{"style":0,"text":"Zhang et al. (2008a)","origin":{"pointer":"/sections/2/paragraphs/0","offset":1939,"length":20},"authors":[{"last":"Zhang"},{"last":"al."}],"year":"2008a","references":["/references/31"]},{"style":0,"text":"Zhang et al. (2008b)","origin":{"pointer":"/sections/2/paragraphs/0","offset":1961,"length":20},"authors":[{"last":"Zhang"},{"last":"al."}],"year":"2008b","references":["/references/32"]},{"style":0,"text":"Sun et al. (2009)","origin":{"pointer":"/sections/2/paragraphs/0","offset":1987,"length":17},"authors":[{"last":"Sun"},{"last":"al."}],"year":"2009","references":["/references/28"]},{"style":0,"text":"Raoult, 1997","origin":{"pointer":"/sections/2/paragraphs/0","offset":2076,"length":12},"authors":[{"last":"Raoult"}],"year":"1997","references":["/references/26"]},{"style":0,"text":"Arnold and Dauchet (1982)","origin":{"pointer":"/sections/2/paragraphs/0","offset":2138,"length":25},"authors":[{"last":"Arnold"},{"last":"Dauchet"}],"year":"1982","references":["/references/0"]},{"style":0,"text":"Lilin (1978)","origin":{"pointer":"/sections/2/paragraphs/0","offset":2168,"length":12},"authors":[{"last":"Lilin"}],"year":"1978","references":["/references/17"]},{"style":0,"text":"Maletti, 2011","origin":{"pointer":"/sections/2/paragraphs/0","offset":2367,"length":13},"authors":[{"last":"Maletti"}],"year":"2011","references":["/references/21"]},{"style":0,"text":"Maletti (2010)","origin":{"pointer":"/sections/2/paragraphs/0","offset":2504,"length":14},"authors":[{"last":"Maletti"}],"year":"2010","references":["/references/20"]},{"style":0,"text":"Maletti, 2012","origin":{"pointer":"/sections/2/paragraphs/0","offset":2596,"length":13},"authors":[{"last":"Maletti"}],"year":"2012","references":["/references/22"]},{"style":0,"text":"Maletti (2011)","origin":{"pointer":"/sections/3/paragraphs/0","offset":183,"length":14},"authors":[{"last":"Maletti"}],"year":"2011","references":["/references/21"]},{"style":0,"text":"Maletti (2011)","origin":{"pointer":"/sections/3/paragraphs/7","offset":116,"length":14},"authors":[{"last":"Maletti"}],"year":"2011","references":["/references/21"]},{"style":0,"text":"Maletti (2011)","origin":{"pointer":"/sections/3/paragraphs/19","offset":309,"length":14},"authors":[{"last":"Maletti"}],"year":"2011","references":["/references/21"]},{"style":0,"text":"Maletti, 2011","origin":{"pointer":"/sections/3/paragraphs/43","offset":4,"length":13},"authors":[{"last":"Maletti"}],"year":"2011","references":["/references/21"]},{"style":0,"text":"Koehn et al. (2007)","origin":{"pointer":"/sections/5/paragraphs/0","offset":91,"length":19},"authors":[{"last":"Koehn"},{"last":"al."}],"year":"2007","references":["/references/13"]},{"style":0,"text":"Hoang et al. (2009)","origin":{"pointer":"/sections/5/paragraphs/0","offset":115,"length":19},"authors":[{"last":"Hoang"},{"last":"al."}],"year":"2009","references":["/references/9"]},{"style":0,"text":"Chiang, 2007","origin":{"pointer":"/sections/5/paragraphs/3","offset":49,"length":12},"authors":[{"last":"Chiang"}],"year":"2007","references":["/references/4"]},{"style":0,"text":"Koehn et al. (2007)","origin":{"pointer":"/sections/6/paragraphs/0","offset":106,"length":19},"authors":[{"last":"Koehn"},{"last":"al."}],"year":"2007","references":["/references/13"]},{"style":0,"text":"Hoang et al. (2009)","origin":{"pointer":"/sections/6/paragraphs/0","offset":130,"length":19},"authors":[{"last":"Hoang"},{"last":"al."}],"year":"2009","references":["/references/9"]},{"style":0,"text":"Callison-Burch et al., 2009","origin":{"pointer":"/sections/6/paragraphs/2","offset":35,"length":27},"authors":[{"last":"Callison-Burch"},{"last":"al."}],"year":"2009","references":["/references/2"]},{"style":0,"text":"Koehn, 2005","origin":{"pointer":"/sections/6/paragraphs/2","offset":154,"length":11},"authors":[{"last":"Koehn"}],"year":"2005","references":["/references/15"]},{"style":0,"text":"Och and Ney, 2003","origin":{"pointer":"/sections/6/paragraphs/2","offset":363,"length":17},"authors":[{"last":"Och"},{"last":"Ney"}],"year":"2003","references":["/references/23"]},{"style":0,"text":"Koehn et al., 2005","origin":{"pointer":"/sections/6/paragraphs/2","offset":421,"length":18},"authors":[{"last":"Koehn"},{"last":"al."}],"year":"2005","references":["/references/12"]},{"style":0,"text":"Charniak and Johnson (2005)","origin":{"pointer":"/sections/6/paragraphs/5","offset":229,"length":27},"authors":[{"last":"Charniak"},{"last":"Johnson"}],"year":"2005","references":["/references/3"]},{"style":0,"text":"Schmid, 2004","origin":{"pointer":"/sections/6/paragraphs/5","offset":303,"length":12},"authors":[{"last":"Schmid"}],"year":"2004","references":["/references/27"]},{"style":0,"text":"Consortium, 2008","origin":{"pointer":"/sections/6/paragraphs/5","offset":515,"length":16},"authors":[{"last":"Consortium"}],"year":"2008","references":["/references/29"]},{"style":0,"text":"Baroni et al., 2009","origin":{"pointer":"/sections/6/paragraphs/5","offset":567,"length":19},"authors":[{"last":"Baroni"},{"last":"al."}],"year":"2009","references":["/references/1"]},{"style":0,"text":"Och, 2003","origin":{"pointer":"/sections/6/paragraphs/5","offset":676,"length":9},"authors":[{"last":"Och"}],"year":"2003","references":["/references/24"]},{"style":0,"text":"Papineni et al., 2002","origin":{"pointer":"/sections/6/paragraphs/5","offset":926,"length":21},"authors":[{"last":"Papineni"},{"last":"al."}],"year":"2002","references":["/references/25"]},{"style":0,"text":"Koehn (2004)","origin":{"pointer":"/sections/6/paragraphs/6","offset":311,"length":12},"authors":[{"last":"Koehn"}],"year":"2004","references":["/references/14"]}]}
