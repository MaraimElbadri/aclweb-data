{"sections":[{"title":"","paragraphs":["Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1568–1576, Sofia, Bulgaria, August 4-9 2013. c⃝2013 Association for Computational Linguistics"]},{"title":"Beam Search for Solving Substitution Ciphers Malte Nuhn and Julian Schamper and Hermann Ney Human Language Technology and Pattern Recognition Computer Science Department, RWTH Aachen University, Aachen, Germany <surname>@cs.rwth-aachen.de Abstract","paragraphs":["In this paper we address the problem of solving substitution ciphers using a beam search approach. We present a conceptually consistent and easy to implement method that improves the current state of the art for decipherment of substitution ciphers and is able to use high order n-gram language models. We show experiments with 1:1 substitution ciphers in which the guaranteed optimal solution for 3-gram language models has 38.6% decipherment error, while our approach achieves 4.13% decipherment error in a fraction of time by using a 6-gram language model. We also apply our approach to the famous Zodiac-408 cipher and obtain slightly better (and near to optimal) results than previously published. Unlike the previous state-of-the-art approach that uses additional word lists to evaluate possible decipherments, our approach only uses a letter-based 6-gram language model. Further-more we use our algorithm to solve large vocabulary substitution ciphers and improve the best published decipherment error rate based on the Gigaword corpus of 7.8% to 6.0% error rate."]},{"title":"1 Introduction","paragraphs":["State-of-the-art statistical machine translation (SMT) systems use large amounts of parallel data to estimate translation models. However, parallel corpora are expensive and not available for every domain.","Recently different works have been published that train translation models using only non-parallel data. Although first practical applications of these approaches have been shown, the overall decipherment accuracy of the proposed algorithms is still low. Improving the core decipherment algorithms is an important step for making decipherment techniques useful for practical applications.","In this paper we present an effective beam search algorithm which provides high decipherment accuracies while having low computational requirements. The proposed approach allows using high order n-gram language models, is scal-able to large vocabulary sizes and can be adjusted to account for a given amount of computational resources. We show significant improvements in decipherment accuracy in a variety of experiments while being computationally more effective than previous published works."]},{"title":"2 Related Work","paragraphs":["The experiments proposed in this paper touch many of previously published works in the decipherment field.","Regarding the decipherment of 1:1 substitution ciphers various works have been published: Most older papers do not use a statistical approach and instead define some heuristic measures for scoring candidate decipherments. Approaches like (Hart, 1994) and (Olson, 2007) use a dictionary to check if a decipherment is useful. (Clark, 1998) defines other suitability measures based on n-gram counts and presents a variety of optimization techniques like simulated annealing, genetic algorithms and tabu search.","On the other hand, statistical approaches for 1:1 substitution ciphers were published in the natural language processing community: (Ravi and Knight, 2008) solve 1:1 substitution ciphers optimally by formulating the decipherment problem as an integer linear program (ILP) while (Corlett and Penn, 2010) solve the problem using A∗","search. We use our own implementation of these methods to report optimal solutions to 1:1 substitution ci-1568 phers for language model orders n = 2 and n = 3.","(Ravi and Knight, 2011a) report the first automatic decipherment of the Zodiac-408 cipher. They use a combination of a 3-gram language model and a word dictionary. We run our beam search approach on the same cipher and report better results without using an additional word dictionary—just by using a high order n-gram language model.","(Ravi and Knight, 2011b) report experiments on large vocabulary substitution ciphers based on the Transtac corpus. (Dou and Knight, 2012) improve upon these results and provide state-of-the-art results on a large vocabulary word substitution cipher based on the Gigaword corpus. We run our method on the same corpus and report improvements over the state of the art.","(Ravi and Knight, 2011b) and (Nuhn et al., 2012) have shown that—even for larger vocabulary sizes—it is possible to learn a full translation model from non-parallel data. Even though this work is currently only able to deal with substitution ciphers, phenomena like reordering, insertions and deletions can in principle be included in our approach."]},{"title":"3 Definitions","paragraphs":["In the following we will use the machine translation notation and denote the ciphertext with f N 1 = f1 . . . fj . . . fN which consists of cipher tokens fj ∈ Vf . We denote the plaintext with eN 1 = e1 . . . ei . . . eN (and its vocabulary Ve respectively). We define e0 = f0 = eN+1 = fN+1 = $ (1) with “ $” being a special sentence boundary token. We use the abbreviations V e = Ve ∪ {$} and V f respectively.","A general substitution cipher uses a table s(e|f ) which contains for each cipher token f a probability that the token f is substituted with the plaintext token e. Such a table for substituting cipher tokens {A, B, C, D} with plaintext tokens {a, b, c, d} could for example look like","a b c d A 0.1 0.2 0.3 0.4 B 0.4 0.2 0.1 0.3 C 0.4 0.1 0.2 0.3 D 0.3 0.4 0.2 0.1","The 1:1 substitution cipher encrypts a given plaintext into a ciphertext by replacing each plaintext token with a unique substitute: This means that the table s(e|f ) contains all zeroes, except for one “ 1.0” per f ∈ Vf and one “ 1.0” per e ∈ Ve. For example the text abadcab would be enciphered to BCBADBC when using the substitution","a b c d A 0 0 0 1 B 1 0 0 0 C 0 1 0 0 D 0 0 1 0","In contrast to the 1:1 substitution cipher, the homophonic substitution cipher allows multiple cipher tokens per plaintext token, which means that the table s(e|f ) is all zero, except for one “ 1.0” per f ∈ Vf . For example the above plaintext could be enciphered to ABCDECF when using the homophonic substitution","a b c d A 1 0 0 0 B 0 1 0 0 C 1 0 0 0 D 0 0 0 1 E 0 0 1 0 F 0 1 0 0 We will use the definition","nmax = max e ∑ f s(e|f ) (2) to characterize the maximum number of different cipher symbols allowed per plaintext symbol.","We formalize the 1:1 substitutions with a bijective function φ : Vf → Ve and homophonic substitutions with a general function φ : Vf → Ve.","Following (Corlett and Penn, 2010), we call cipher functions φ, for which not all φ(f )’s are fixed, partial cipher functions . Further, φ′","is said to extend φ, if for all f that are fixed inφ, it holds that f is also fixed inφ′","with φ′","(f ) = φ(f ). 1569 The cardinality of φ counts the number of fixed f ’s in φ.","When talking about partial cipher functions we use the notation for relations, in which φ ⊆ Vf × Ve. For example with","φ = {(A, a)} φ′ = {(A, a), (B, b)}","it follows that φ ⊆1 φ′","and","|φ| = 1 |φ′","| = 2","φ(A) = a φ′","(A) = a","φ(B) = undefined φ′","(B) = b","The general decipherment goal is to obtain a mapping φ such that the probability of the deciphered text is maximal:","φ̂ = arg max φ p(φ(f1)φ(f2)φ(f3)...φ(fN )) (3)","Here p(. . . ) denotes the language model. Depending on the structure of the language model Equation 3 can be further simplified."]},{"title":"4 Beam Search","paragraphs":["In this Section we present our beam search approach to solving Equation 3. We first present the general algorithm, containing many higher level functions. We then discuss possible instances of these higher level functions. 4.1 General Algorithm Figure 1 shows the general structure of the beam search algorithm for the decipherment of substitution ciphers. The general idea is to keep track of all partial hypotheses in two arrays Hs and Ht. During search all possible extensions of the partial hypotheses in Hs are generated and scored. Here, the function EXT ORDER chooses which cipher symbol is used next for extension, EXT LIMITS decides which extensions are allowed, and SCORE scores the new partial hypotheses. PRUNE then selects a subset of these hypotheses which are stored to Ht. Afterwards the array Hs is copied to Ht and the search process continues with the updated array Hs.","Due to the structure of the algorithm the cardinality of all hypotheses in Hs increases in each step. Thus only hypotheses of the same cardinality 1 shorthand notation for φ′","extends φ 1: function BEAM SEARCH(EXT ORDER,","EXT LIMITS, PRUNE) 2: init sets Hs, Ht 3: CARDINALITY = 0 4: Hs.ADD((∅, 0)) 5: while CARDINALITY < |Vf | do 6: f = EXT ORDER[CARDINALITY] 7: for all φ ∈ Hs do 8: for all e ∈ Ve do 9: φ′",":= φ ∪ {(e, f )} 10: if EXT LIMITS(φ′",") then 11: Ht.ADD(φ′",",SCORE (φ′",")) 12: end if 13: end for 14: end for 15: PRUNE(Ht) 16: CARDINALITY = CARDINALITY + 1 17: Hs = Ht 18: Ht.CLEAR() 19: end while 20: return best scoring cipher function in Hs 21: end function Figure 1: The general structure of the beam search algorithm for decipherment of substitution ciphers. The high level functions SCORE, EXT ORDER, EXT LIMITS and PRUNE are described in Section 4. are compared in the pruning step. When Hs contains full cipher relations, the cipher relation with the maximal score is returned.2","Figure 2 illustrates how the algorithm explores the search space for a homophonic substitution cipher. In the following we show several instances of EXT ORDER, EXT LIMITS, SCORE, and PRUNE. 4.2 Extension Limits (EXT LIMITS) In addition to the implicit constraint of φ being a function Vf → Ve, one might be interested in functions of a specific form:","For 1:1 substitution ciphers (EXT LIMITS SIMPLE) φ must fulfill that the number of cipher letters f ∈ Vf that map to any e ∈ Ve is at most one. Since partial hypotheses violating this condition can never “recover” when being extended, it becomes clear that these partial hypotheses can be left out from search.","2","n-best output can be implemented by returning the n best scoring hypotheses in the final arrayHs. 1570 ∅ a b c d a b c d a b c d a b c d a b c d . . . . . . . . . . . . . . . a b c d a b c d a b c d a b c d . . . . . . . . . . . . a b c d a b c d a b c d a b c d B C A D Figure 2: Illustration of the search space explored by the beam search algorithm with cipher vocabulary Vf = {A, B, C, D}, plaintext vocabulary Ve = {a, b, c, d}, EXT ORDER = (B, C, A, D), homophonic extension limits (EXT LIMITS HOMOPHONIC) with nmax = 4, and histogram pruning with nkeep = 4. Hypotheses are visualized as nodes in the tree. The x-axis represents the extension order. At each level only those 4 hypotheses that survived the histogram pruning process are extended.","Homophonic substitution ciphers can be handled by the beam search algorithm, too. Here the condition that φ must fulfill is that the number of cipher letters f ∈ Vf that map to any e ∈ Ve is at most nmax (which we will call EXT LIMITS HOMOPHONIC). As soon as this condition is violated, all further extensions will also violate the condition. Thus, these partial hypotheses can be left out. 4.3 Score Estimation (SCORE) The score estimation function needs to predict how good or bad a partial hypothesis (cipher function) might become. We propose simple heuristics that use the n-gram counts rather than the original ciphertext. The following formulas consider the 2-gram case. Equations for higher n-gram orders can be obtained analogously.","With Equation 3 in mind, we want to estimate the best possible score N+1 ∏ j=1","p(φ′ (fj)|φ′","(fj−1)) (4) which can be obtained by extensions φ′","⊇ φ. By defining counts3 Nff′ = N+1 ∑ i=1","δ(f, fi−1)δ(f ′ , fi) (5) 3 δ denotes the Kronecker delta.","we can equivalently use the scores ∑","f,f′ ∈V f","Nff′ log p(φ′ (f ′ )|φ′","(f )) (6) Using this formulation it is easy to propose a whole class of heuristics: We only present the simplest heuristic, which we call TRIV-IAL HEURISTIC. Its name stems from the fact that it only evaluates those parts of a given φ′","that are already fixed, and thus does not estimate any future costs. Its score is calculated as ∑","f,f′ ∈φ′","Nff′ log p(φ′ (f ′ )|φ′","(f )). (7) Here f, f ′","∈ φ′","denotes that f and f ′","need to be covered in φ′",". This heuristic is optimistic since we implicitly use “ 0” as estimate for the non fixed parts of the sum, for which Nff′ log p(·|·) ≤ 0 holds. It should be noted that this heuristic can be implemented very efficiently. Given a partial hypothesis φ with given SCORE(φ) the score of an extension φ′","can be calculated as","SCORE(φ′",") = SCORE(φ) + NEWLY FIXED(φ, φ′",")","(8) where NEWLY FIXED only includes scores for","n-grams that have been newly fixed in φ′","during","the extension step from φ to φ′",". 1571 4.4 Extension Order (EXT ORDER) For the choice which ciphertext symbol should be fixed next during search, several possibilities exist: The overall goal is to choose an extension order that leads to an overall low error rate. Intuitively it seems a good idea to first try to decipher higher frequent words rather than the lowest frequent ones. It is also clear that the choice of a good extension order is dependent on the score estimation function SCORE: The extension order should lead to informative scores early on so that mislead-ing hypotheses can be pruned out early.","In most of our experiments we will make use of a very simple extension order: HIGHEST UNIGRAM FREQUENCY simply fixes the most frequent symbols first.","In case of the Zodiac-408, we use another strategy that we call HIGHEST NGRAM COUNT extension order. In each step it greedily chooses the symbol that will maximize the number of fixed ciphertext n-grams. This strategy is useful because the SCORE function we use is TRIV-IAL HEURISTIC, which is not able to provide informative scores if only few full n-grams are fixed. 4.5 Pruning (PRUNE) We propose two pruning methods: HISTOGRAM PRUNING sorts all hypotheses according to their score and then keeps only the best nkeep hypotheses.","THRESHOLD PRUNING keeps only those hypotheses φkeep for which SCORE(φkeep) ≥ SCORE(φbest) − β (9) holds for a given parameter β ≥ 0. Even though THRESHOLD PRUNING has the advantage of not needing to sort all hypotheses, it has proven difficult to choose proper values for β. Due to this, all experiments presented in this paper only use HISTOGRAM PRUNING."]},{"title":"5 Iterative Beam Search","paragraphs":["(Ravi and Knight, 2011b) propose a so called “iterative EM algorithm”. The basic idea is to run a decipherment algorithm—in their case an EM algorithm based approach—on a subset of the vocabulary. After having obtained the results from the restricted vocabulary run, these results are used to initialize a decipherment run with a larger vocabulary. The results from this run will then be used for a further decipherment run with an even larger vocabulary and so on. In our large vocabulary word substitution cipher experiments we it-eratively increase the vocabulary from the 1000 most frequent words, until we finally reach the 50000 most frequent words."]},{"title":"6 Experimental Evaluation","paragraphs":["We conduct experiments on letter based 1:1 substitution ciphers, the homophonic substitution cipher Zodiac-408, and word based 1:1 substitution ciphers.","For a given reference mapping φref , we evaluate candidate mappings φ using two error measures: Mapping Error Rate MER(φ, φref ) and Symbol Error Rate SER(φ, φref ). Roughly speaking, SER reports the fraction of symbols in the deciphered text that are not correct, while MER reports the fraction of incorrect mappings in φ.","Given a set of symbols Veval with unigram counts N (v) for v ∈ Veval, and the total amount of running symbols Neval = ∑ v∈Veval N (v) we define MER = 1 − ∑ v∈Veval","1 |Veval| · δ(φ(v), φref (v)) (10) SER = 1 − ∑ v∈Veval N (v) Neval · δ(φ(v), φref (v)) (11) Thus the SER can be seen as a weighted form of the MER, emphasizing errors for frequent words. In decipherment experiments, SER will often be lower than MER, since it is often easier to decipher frequent words. 6.1 Letter Substitution Ciphers As ciphertext we use the text of the English Wikipedia article about History4",", remove all pictures, tables, and captions, convert all letters to lowercase, and then remove all non-letter and non-space symbols. This corpus forms the basis for shorter cryptograms of size 2, 4, 8, 16, 32, 64, 128, and 256—of which we generate 50 each. We make sure that these shorter cryptograms do not end or start in the middle of a word. We create the ciphertext using a 1:1 substitution cipher in which we fix the mapping of the space symbol ’ ’. This 4 http://en.wikipedia.org/wiki/History 1572 Order Beam MER [%] SER [%] RT [s] 3 10 33.15 25.27 0.01 3 100 12.00 6.95 0.06 3 1k 7.37 3.06 0.53 3 10k 5.10 1.42 5.33 3 100k 4.93 1.31 47.70 3 ∞∗","4.93 1.31 19 700.00 4 10 55.97 48.19 0.02 4 100 18.15 14.41 0.10 4 1k 5.13 3.42 0.89 4 10k 1.55 1.00 8.57 4 100k 0.39 0.06 81.34 5 10 69.19 60.13 0.02 5 100 35.57 29.02 0.14 5 1k 10.89 8.47 1.29 5 10k 0.38 0.06 11.91 5 100k 0.38 0.06 120.38 6 10 74.65 64.77 0.03 6 100 40.26 33.38 0.17 6 1k 13.53 10.08 1.58 6 10k 2.45 1.28 15.77 6 100k 0.09 0.05 151.85 Table 1: Symbol error rates (SER), Mapping error rates (MER) and runtimes (RT) in dependence of language model order (ORDER) and histogram pruning size (BEAM) for decipherment of letter substitution ciphers of length 128. Runtimes are reported on a single core machine. Results for beam size “ ∞” were obtained using A∗","search. makes our experiments comparable to those conducted in (Ravi and Knight, 2008). Note that fixing the ’ ’ symbol makes the problem much easier: The exact methods show much higher computational demands for lengths beyond 256 letters when not fixing the space symbol.","The plaintext language model we use a letter based (Ve = {a, . . . , z, }) language model trained on a subset of the Gigaword corpus (Graff et al., 2007).","We use extension limits fitting the 1:1 substitution cipher nmax = 1 and histogram pruning with different beam sizes.","For comparison we reimplemented the ILP approach from (Ravi and Knight, 2008) as well as the A∗","approach from (Corlett and Penn, 2010).","Figure 3 shows the results of our algorithm for different cipher length. We use a beam size of 100k for the 4, 5 and 6-gram case. Most remarkably our 6-gram beam search results are significantly better than all methods presented in the literature. For the cipher length of 32 we obtain a symbol error rate of just 4.1% where the optimal solution (i.e. without search errors) for a 3-gram 2 4 8 16 32 64 128 256 0 10 20 30 40 50 60 70 80 90 100 Cipher Length Symbol Error Rate (%) Exact 2gram Exact 3gram Beam 3gram Beam 4gram Beam 5gram Beam 6gram Figure 3: Symbol error rates for decipherment of letter substitution ciphers of different lengths. Error bars show the 95% confidence interval based on decipherment on 50 different ciphers. Beam search was performed with a beam size of “100k”. language model has a symbol error rate as high as 38.3%.","Table 1 shows error rates and runtimes of our algorithm for different beam sizes and language model orders given a fixed ciphertext length of128 letters. It can be seen that achieving close to optimal results is possible in a fraction of the CPU time needed for the optimal solution: In the 3-gram case the optimal solution is found in 1","400 th of the time needed using A∗","search. It can also be seen that increasing the language model order does not increase the runtime much while provid-ing better results if the beam size is large enough: If the beam size is not large enough, the decipherment accuracy decreases when increasing the language model order: This is because the higher order heuristics do not give reliable scores if only few n-grams are fixed.","To summarize: The beam search method is significantly faster and obtains significantly better results than previously published methods. Further-more it offers a good trade-off between CPU time and decipherment accuracy. 1573 i l i k e k i l l i n g p e o p l e b e c a u s e i t i s s o m u c h f u n i t i n m o r e f u n t h a n k i l l i n g w i l d g a m e i n t h e f o r r e s t b e c a u s e m a n i s t h e m o a t r a n g e r o u e a n a m a l o f a l l t o k i l l s o m e t h i n g g i Figure 4: First 136 letters of the Zodiac-408 cipher and its decipherment. 6.2 Zodiac-408 Cipher As ciphertext we use a transcription of the Zodiac-408 cipher. It consists of 54 different symbols and has a length of 408 symbols.5","The cipher has been deciphered by hand before. It contains some mistakes and ambiguities: For example, it contains misspelled words like forrest (vs. forest), experence (vs. experience), or paradice (vs. paradise). Furthermore, the last 17 letters of the cipher do not form understandable English when applying the same homophonic substitution that deciphers the rest of the cipher. This makes the Zodiac-408 a good candidate for testing the robustness of a decipherment algorithm.","We assume a homophonic substitution cipher, even though the cipher is not strictly homophonic: It contains three cipher symbols that correspond to two or more plaintext symbols. We ignore this fact for our experiments, and count—in case of the MER only—the decipherment for these symbols as correct when the obtained mapping is contained in the set of reference symbols. We use extension limits with nmax = 8 and histogram pruning with beam sizes of 10k up to 10M .","The plaintext language model is based on the same subset of Gigaword (Graff et al., 2007) data as the experiments for the letter substitution ciphers. However, we first removed all space sym-5 hence its name Order Beam MER [%] SER [%] RT [s] 4 10k 71.43 67.16 222 4 100k 66.07 61.52 1 460 4 1M 39.29 34.80 12 701 4 10M 19.64 16.18 125 056 5 10k 94.64 96.57 257 5 100k 10.71 5.39 1 706 5 1M 8.93 3.19 14 724 5 10M 8.93 3.19 152 764 6 10k 87.50 84.80 262 6 100k 94.64 94.61 1 992 6 1M 8.93 2.70 17 701 6 10M 7.14 1.96 167 181 Table 2: Symbol error rates (SER), Mapping error rates (MER) and runtimes (RT) in dependence of language model order (ORDER) and histogram pruning size (BEAM) for the decipherment of the Zodiac-408 cipher. Runtimes are reported on a 128-core machine. bols from the training corpus before training the actual letter based 4-gram, 5-gram, and 6-gram language model on it. Other than (Ravi and Knight, 2011a) we do not use any word lists and by that avoid any degrees of freedom in how to in-tegrate it into the search process: Only an n-gram language model is used.","Figure 4 shows the first parts of the cipher and our best decipherment. Table 2 shows the results of our algorithm on the Zodiac-408 cipher for different language model orders and pruning settings.","To summarize: Our final decipherment—for which we only use a 6-gram language model—has a symbol error rate of only 2.0%, which is slightly better than the best decipherment reported in (Ravi and Knight, 2011a). They used an n-gram language model together with a word dictionary and obtained a symbol error rate of 2.2%. We thus obtain better results with less modeling. 6.3 Word Substitution Ciphers As ciphertext, we use parts of the JRC corpus (Steinberger et al., 2006) and the Gigaword corpus (Graff et al., 2007). While the full JRC corpus contains roughly 180k word types and consists of approximately 70M running words, the full Gigaword corpus contains around 2M word types and roughly 1.5G running words.","We run experiments for three different setups: The “JRC” and “Gigaword” setups use the first half of the respective corpus as ciphertext, while the plaintext language model of order n = 3 was 1574 Setup Top MER [%] SER [%] RT [hh:mm] Gigaword 1k 81.91 27.38 03h 10m Gigaword 10k 30.29 8.55 09h 21m Gigaword 20k 21.78 6.51 16h 25m Gigaword 50k 19.40 5.96 49h 02m JRC 1k 73.28 15.42 00h 32m JRC 10k 15.82 2.61 13h 03m JRC-Shuf 1k 76.83 19.04 00h 31m JRC-Shuf 10k 15.08 2.58 13h 03m Table 3: Word error rates (WER), Mapping error rates (MER) and runtimes (RT) for iterative decipherment run on the (TOP) most frequent words. Error rates are evaluated on the full vocabulary. Runtimes are reported on a 128-core machine. trained on the second half. The “JRC-Shuf” setup is created by randomly selecting half of the sentences of the JRC corpus as ciphertext, while the language model was trained on the complementary half of the corpus.","We encrypt the ciphertext using a 1:1 substitution cipher on word level, imposing a much larger vocabulary size. We use histogram pruning with a beam size of 128 and use extension limits of nmax = 1. Different to the previous experiments, we use iterative beam search with iterations as shown in Table 3.","The results for the Gigaword task are directly comparable to the word substitution experiments presented in (Dou and Knight, 2012). Their final decipherment has a symbol error rate of 7.8%. Our algorithm obtains 6.0% symbol error rate. It should be noted that the improvements of 1.8% symbol error rate correspond to a larger improvement in terms of mapping error rate. This can also be seen when looking at Table 3: An improvement of the symbol error rate from 6.51% to 5.96% corresponds to an improvement of mapping error rate from 21.78% to 19.40%.","To summarize: Using our beam search algorithm in an iterative fashion, we are able to improve the state-of-the-art decipherment accuracy for word substitution ciphers."]},{"title":"7 Conclusion","paragraphs":["We have presented a simple and effective beam search approach to the decipherment problem. We have shown in a variety of experiments—letter substitution ciphers, the Zodiac-408, and word substitution ciphers—that our approach outperforms the current state of the art while being conceptually simpler and keeping computational demands low.","We want to note that the presented algorithm is not restricted to 1:1 and homophonic substitution ciphers: It is possible to extend the algorithm to solve n:m mappings. Along with more sophisticated pruning strategies, score estimation functions, and extension orders, this will be left for future research."]},{"title":"Acknowledgements","paragraphs":["This work was partly realized as part of the Quaero Programme, funded by OSEO, French State agency for innovation. Experiments were performed with computing resources granted by JARA-HPC from RWTH Aachen University un-der project “jara0040”."]},{"title":"References","paragraphs":["Andrew J. Clark. 1998. Optimisation heuristics for cryptology. Ph.D. thesis, Faculty of Information Technology, Queensland University of Technology.","Eric Corlett and Gerald Penn. 2010. An exact A* method for deciphering letter-substitution ciphers. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL), pages 1040–1047, Uppsala, Sweden, July. The Association for Computer Linguistics.","Qing Dou and Kevin Knight. 2012. Large scale decipherment for out-of-domain machine translation. In Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 266–275, Jeju Island, Korea, July. Association for Computational Linguistics.","David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda. 2007. English Gigaword Third Edition. Linguistic Data Consortium, Philadelphia.","George W. Hart. 1994. To decode short cryptograms. Communications of the Association for Computing Machinery (CACM), 37(9):102–108, September.","Malte Nuhn, Arne Mauser, and Hermann Ney. 2012. Deciphering foreign language by combining language models and context vectors. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL), pages 156–164, Jeju, Republic of Korea, July. Association for Computational Linguistics.","Edwin Olson. 2007. Robust dictionary attack of short simple substitution ciphers. Cryptologia, 31(4):332–342, October. 1575","Sujith Ravi and Kevin Knight. 2008. Attacking decipherment problems optimally with low-order n-gram models. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 812–819, Honolulu, Hawaii. Association for Computational Linguistics.","Sujith Ravi and Kevin Knight. 2011a. Bayesian inference for Zodiac and other homophonic ciphers. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL), pages 239–247, Portland, Oregon, June. Association for Computational Linguistics.","Sujith Ravi and Kevin Knight. 2011b. Deciphering foreign language. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT), pages 12–21, Portland, Oregon, USA, June. Association for Computational Linguistics.","Ralf Steinberger, Bruno Pouliquen, Anna Widiger, Camelia Ignat, Tomaž Erjavec, and Dan Tufis.̧ 2006. The JRC-Acquis: A multilingual aligned parallel corpus with 20+ languages. In In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC), pages 2142–2147. European Language Resources Association. 1576"]}],"references":[{"authors":[{"first":"Andrew","middle":"J.","last":"Clark"}],"year":"1998","title":"Optimisation heuristics for cryptology","source":"Andrew J. Clark. 1998. Optimisation heuristics for cryptology. Ph.D. thesis, Faculty of Information Technology, Queensland University of Technology."},{"authors":[{"first":"Eric","last":"Corlett"},{"first":"Gerald","last":"Penn"}],"year":"2010","title":"An exact A* method for deciphering letter-substitution ciphers","source":"Eric Corlett and Gerald Penn. 2010. An exact A* method for deciphering letter-substitution ciphers. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL), pages 1040–1047, Uppsala, Sweden, July. The Association for Computer Linguistics."},{"authors":[{"first":"Qing","last":"Dou"},{"first":"Kevin","last":"Knight"}],"year":"2012","title":"Large scale decipherment for out-of-domain machine translation","source":"Qing Dou and Kevin Knight. 2012. Large scale decipherment for out-of-domain machine translation. In Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 266–275, Jeju Island, Korea, July. Association for Computational Linguistics."},{"authors":[{"first":"David","last":"Graff"},{"first":"Junbo","last":"Kong"},{"first":"Ke","last":"Chen"},{"first":"Kazuaki","last":"Maeda"}],"year":"2007","title":"English Gigaword Third Edition","source":"David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda. 2007. English Gigaword Third Edition. Linguistic Data Consortium, Philadelphia."},{"authors":[{"first":"George","middle":"W.","last":"Hart"}],"year":"1994","title":"To decode short cryptograms","source":"George W. Hart. 1994. To decode short cryptograms. Communications of the Association for Computing Machinery (CACM), 37(9):102–108, September."},{"authors":[{"first":"Malte","last":"Nuhn"},{"first":"Arne","last":"Mauser"},{"first":"Hermann","last":"Ney"}],"year":"2012","title":"Deciphering foreign language by combining language models and context vectors","source":"Malte Nuhn, Arne Mauser, and Hermann Ney. 2012. Deciphering foreign language by combining language models and context vectors. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL), pages 156–164, Jeju, Republic of Korea, July. Association for Computational Linguistics."},{"authors":[{"first":"Edwin","last":"Olson"}],"year":"2007","title":"Robust dictionary attack of short simple substitution ciphers","source":"Edwin Olson. 2007. Robust dictionary attack of short simple substitution ciphers. Cryptologia, 31(4):332–342, October. 1575"},{"authors":[{"first":"Sujith","last":"Ravi"},{"first":"Kevin","last":"Knight"}],"year":"2008","title":"Attacking decipherment problems optimally with low-order n-gram models","source":"Sujith Ravi and Kevin Knight. 2008. Attacking decipherment problems optimally with low-order n-gram models. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 812–819, Honolulu, Hawaii. Association for Computational Linguistics."},{"authors":[{"first":"Sujith","last":"Ravi"},{"first":"Kevin","last":"Knight"}],"year":"2011a","title":"Bayesian inference for Zodiac and other homophonic ciphers","source":"Sujith Ravi and Kevin Knight. 2011a. Bayesian inference for Zodiac and other homophonic ciphers. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL), pages 239–247, Portland, Oregon, June. Association for Computational Linguistics."},{"authors":[{"first":"Sujith","last":"Ravi"},{"first":"Kevin","last":"Knight"}],"year":"2011b","title":"Deciphering foreign language","source":"Sujith Ravi and Kevin Knight. 2011b. Deciphering foreign language. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT), pages 12–21, Portland, Oregon, USA, June. Association for Computational Linguistics."},{"authors":[{"first":"Ralf","last":"Steinberger"},{"first":"Bruno","last":"Pouliquen"},{"first":"Anna","last":"Widiger"},{"first":"Camelia","last":"Ignat"},{"first":"Tomaž","last":"Erjavec"},{"first":"Dan","last":"Tufis.̧"}],"year":"2006","title":"The JRC-Acquis: A multilingual aligned parallel corpus with 20+ languages","source":"Ralf Steinberger, Bruno Pouliquen, Anna Widiger, Camelia Ignat, Tomaž Erjavec, and Dan Tufis.̧ 2006. The JRC-Acquis: A multilingual aligned parallel corpus with 20+ languages. In In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC), pages 2142–2147. European Language Resources Association. 1576"}],"cites":[{"style":0,"text":"Hart, 1994","origin":{"pointer":"/sections/3/paragraphs/1","offset":239,"length":10},"authors":[{"last":"Hart"}],"year":"1994","references":["/references/4"]},{"style":0,"text":"Olson, 2007","origin":{"pointer":"/sections/3/paragraphs/1","offset":256,"length":11},"authors":[{"last":"Olson"}],"year":"2007","references":["/references/6"]},{"style":0,"text":"Clark, 1998","origin":{"pointer":"/sections/3/paragraphs/1","offset":325,"length":11},"authors":[{"last":"Clark"}],"year":"1998","references":["/references/0"]},{"style":0,"text":"Ravi and Knight, 2008","origin":{"pointer":"/sections/3/paragraphs/2","offset":133,"length":21},"authors":[{"last":"Ravi"},{"last":"Knight"}],"year":"2008","references":["/references/7"]},{"style":0,"text":"Corlett and Penn, 2010","origin":{"pointer":"/sections/3/paragraphs/2","offset":279,"length":22},"authors":[{"last":"Corlett"},{"last":"Penn"}],"year":"2010","references":["/references/1"]},{"style":0,"text":"Ravi and Knight, 2011a","origin":{"pointer":"/sections/3/paragraphs/4","offset":1,"length":22},"authors":[{"last":"Ravi"},{"last":"Knight"}],"year":"2011a","references":["/references/8"]},{"style":0,"text":"Ravi and Knight, 2011b","origin":{"pointer":"/sections/3/paragraphs/5","offset":1,"length":22},"authors":[{"last":"Ravi"},{"last":"Knight"}],"year":"2011b","references":["/references/9"]},{"style":0,"text":"Dou and Knight, 2012","origin":{"pointer":"/sections/3/paragraphs/5","offset":116,"length":20},"authors":[{"last":"Dou"},{"last":"Knight"}],"year":"2012","references":["/references/2"]},{"style":0,"text":"Ravi and Knight, 2011b","origin":{"pointer":"/sections/3/paragraphs/6","offset":1,"length":22},"authors":[{"last":"Ravi"},{"last":"Knight"}],"year":"2011b","references":["/references/9"]},{"style":0,"text":"Nuhn et al., 2012","origin":{"pointer":"/sections/3/paragraphs/6","offset":30,"length":17},"authors":[{"last":"Nuhn"},{"last":"al."}],"year":"2012","references":["/references/5"]},{"style":0,"text":"Corlett and Penn, 2010","origin":{"pointer":"/sections/4/paragraphs/9","offset":11,"length":22},"authors":[{"last":"Corlett"},{"last":"Penn"}],"year":"2010","references":["/references/1"]},{"style":0,"text":"Ravi and Knight, 2011b","origin":{"pointer":"/sections/6/paragraphs/0","offset":1,"length":22},"authors":[{"last":"Ravi"},{"last":"Knight"}],"year":"2011b","references":["/references/9"]},{"style":0,"text":"Ravi and Knight, 2008","origin":{"pointer":"/sections/7/paragraphs/6","offset":64,"length":21},"authors":[{"last":"Ravi"},{"last":"Knight"}],"year":"2008","references":["/references/7"]},{"style":0,"text":"Graff et al., 2007","origin":{"pointer":"/sections/7/paragraphs/7","offset":134,"length":18},"authors":[{"last":"Graff"},{"last":"al."}],"year":"2007","references":["/references/3"]},{"style":0,"text":"Ravi and Knight, 2008","origin":{"pointer":"/sections/7/paragraphs/9","offset":55,"length":21},"authors":[{"last":"Ravi"},{"last":"Knight"}],"year":"2008","references":["/references/7"]},{"style":0,"text":"Corlett and Penn, 2010","origin":{"pointer":"/sections/7/paragraphs/10","offset":15,"length":22},"authors":[{"last":"Corlett"},{"last":"Penn"}],"year":"2010","references":["/references/1"]},{"style":0,"text":"Graff et al., 2007","origin":{"pointer":"/sections/7/paragraphs/18","offset":70,"length":18},"authors":[{"last":"Graff"},{"last":"al."}],"year":"2007","references":["/references/3"]},{"style":0,"text":"Ravi and Knight, 2011a","origin":{"pointer":"/sections/7/paragraphs/18","offset":906,"length":22},"authors":[{"last":"Ravi"},{"last":"Knight"}],"year":"2011a","references":["/references/8"]},{"style":0,"text":"Ravi and Knight, 2011a","origin":{"pointer":"/sections/7/paragraphs/20","offset":186,"length":22},"authors":[{"last":"Ravi"},{"last":"Knight"}],"year":"2011a","references":["/references/8"]},{"style":0,"text":"Steinberger et al., 2006","origin":{"pointer":"/sections/7/paragraphs/20","offset":447,"length":24},"authors":[{"last":"Steinberger"},{"last":"al."}],"year":"2006","references":["/references/10"]},{"style":0,"text":"Graff et al., 2007","origin":{"pointer":"/sections/7/paragraphs/20","offset":498,"length":18},"authors":[{"last":"Graff"},{"last":"al."}],"year":"2007","references":["/references/3"]},{"style":0,"text":"Dou and Knight, 2012","origin":{"pointer":"/sections/7/paragraphs/23","offset":109,"length":20},"authors":[{"last":"Dou"},{"last":"Knight"}],"year":"2012","references":["/references/2"]}]}
