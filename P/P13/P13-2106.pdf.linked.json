{"sections":[{"title":"","paragraphs":["Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 597–603, Sofia, Bulgaria, August 4-9 2013. c⃝2013 Association for Computational Linguistics"]},{"title":"Nonparametric Bayesian Inference and Efficient Parsing for Tree-adjoining Grammars Elif Yamangil and Stuart M. Shieber Harvard University Cambridge, Massachusetts, USA {elif, shieber}@seas.harvard.edu Abstract","paragraphs":["In the line of research extending statistical parsing to more expressive grammar formalisms, we demonstrate for the first time the use of tree-adjoining grammars (TAG). We present a Bayesian nonparametric model for estimating a probabilistic TAG from a parsed corpus, along with novel block sampling methods and approximation transformations for TAG that allow efficient parsing. Our work shows performance improvements on the Penn Treebank and finds more compact yet linguistically rich representations of the data, but more importantly provides techniques in grammar transformation and statistical inference that make practical the use of these more expressive systems, thereby enabling further experimentation along these lines."]},{"title":"1 Introduction","paragraphs":["There is a deep tension in statistical modeling of grammatical structure between providing good expressivity — to allow accurate modeling of the data with sparse grammars — and low complexity — making induction of the grammars (say, from a treebank) and parsing of novel sentences computationally practical. Tree-substitution grammars (TSG), by expanding the domain of locality of context-free grammars (CFG), can achieve better expressivity, and the ability to model more contextual dependencies; the payoff would be better modeling of the data or smaller (sparser) models or both. For instance, constructions that go across levels, like the predicate-argument structure of a verb and its arguments can be modeled by TSGs (Goodman, 2003).","Recent work that incorporated Dirichlet process (DP) nonparametric models into TSGs has provided an efficient solution to the daunting model selection problem of segmenting training data trees into appropriate elementary fragments to form the grammar (Cohn et al., 2009; Post and Gildea, 2009). The elementary trees combined in a TSG are, intuitively, primitives of the language, yet certain linguistic phenomena (notably various forms of modification) “split them up”, preventing their reuse, leading to less sparse grammars than might be ideal (Yamangil and Shieber, 2012; Chiang, 2000; Resnik, 1992).","TSGs are a special case of the more flexible grammar formalism of tree adjoining grammar (TAG) (Joshi et al., 1975). TAG augments TSG with an adjunction operator and a set of auxiliary trees in addition to the substitution operator and initial trees of TSG, allowing for “splicing in” of syntactic fragments within trees. This functionality allows for better modeling of linguistic phenomena such as the distinction between modifiers and arguments (Joshi et al., 1975; XTAG Research Group, 2001). Unfortunately, TAG’s expressivity comes at the cost of greatly increased complexity. Parsing complexity for unconstrained TAG scales as O(n6","), impractical as compared to CFG and TSG’s O(n3","). In addition, the model selection problem for TAG is significantly more complicated than for TSG since one must reason about many more combinatorial options with two types of derivation operators. This has led researchers to resort to manual (Doran et al., 1997) or heuristic techniques. For example, one can consider “outsourcing” the auxiliary trees (Shieber, 2007), use template rules and a very small number of grammar categories (Hwa, 1998), or rely on head-words and force lexicalization in order to constrain the problem (Xia et al., 2001; Chiang, 597 2000; Carreras et al., 2008). However a solution has not been put forward by which a model that maximizes a principled probabilistic objective is sought after.","Recent work by Cohn and Blunsom (2010) argued that under highly expressive grammars such as TSGs where exponentially many derivations may be hypothesized of the data, local Gibbs sampling is insufficient for effective inference and global blocked sampling strategies will be necessary. For TAG, this problem is only more severe due to its mild context-sensitivity and even richer combinatorial nature. Therefore in previous work, Shindo et al. (2011) and Yamangil and Shieber (2012) used tree-insertion grammar (TIG) as a kind of expressive compromise between TSG and TAG, as a substrate on which to build nonparametric inference. However TIG has the constraint of disallowing wrapping adjunction (coordination between material that falls to the left and right of the point of adjunction, such as parentheticals and quotations) as well as left adjunction along the spine of a right auxiliary tree and vice versa.","In this work we formulate a blocked sampling strategy for TAG that is effective and efficient, and prove its superiority against the local Gibbs sampling approach. We show via nonparametric inference that TAG, which contains TSG as a subset, is a better model for treebank data than TSG and leads to improved parsing performance. TAG achieves this by using more compact grammars than TSG and by providing the ability to make finer-grained linguistic distinctions. We explain how our parameter refinement scheme for TAG allows for cubic-time CFG parsing, which is just as efficient as TSG parsing. Our presentation as-sumes familiarity with prior work on block sampling of TSG and TIG (Cohn and Blunsom, 2010; Shindo et al., 2011; Yamangil and Shieber, 2012)."]},{"title":"2 Probabilistic Model","paragraphs":["In the basic nonparametric TSG model, there is an independent DP for every grammar category (such as c = NP), each of which uses a base distribution P0 that generates an initial tree by making stepwise decisions and concentration parameter αc that controls the level of sparsity (size) of the generated grammars: Gc ∼ DP(αc, P0(· | c)) We extend this model by adding specialized DPs for auxiliary trees Gaux","c ∼ DP(αaux","c , P aux","0 (· | c)) Therefore, we have an exchangeable process for generating auxiliary tree aj given j − 1 auxiliary trees previously generated p(aj | a<j) =","nc,aj + αaux c P aux","0 (aj | c)","j − 1 + αaux","c (1) as for initial trees in TSG (Cohn et al., 2009).","We must define base distributions for initial trees and auxiliary trees. P0 generates an initial tree with root label c by sampling rules from a CFG P̃ and making a binary decision at every node generated whether to leave it as a frontier node or further expand (with probability βc) (Cohn et al., 2009). Similarly, our P aux","0 generates an auxiliary tree with root label c by sampling a CFG rule from P̃ , flipping an unbiased coin to decide the direction of the spine (if more than a unique child was generated), making a binary decision at the spine whether to leave it as a foot node or further expand (with probability γc), and recurring into P0 or P aux","0 appropriately for the off-spine and spinal children respectively.","We glue these two processes together via a set of adjunction parameters μc. In any derivation for every node labeled c that is not a frontier node or the root or foot node of an auxiliary tree, we determine the number (perhaps zero) of simultaneous adjunctions (Schabes and Shieber, 1994) by sampling a Geometric(μc) variable; thus k simultaneous adjunctions would have probability (μc)k","(1 − μ","c). Since we already provide simultaneous adjunction we disallow adjunction at the root of auxiliary trees."]},{"title":"3 Inference","paragraphs":["Given this model, our inference task is to explore posterior derivations underlying the data. Since TAG derivations are highly structured objects, we design a blocked Metropolis-Hastings sampler that samples derivations per entire parse trees all at once in a joint fashion (Cohn and Blunsom, 2010; Shindo et al., 2011; Yamangil and Shieber, 2012). As in previous work, we use a Goodman-transformed TAG as our proposal distribution (Goodman, 2003) that incorporates additional CFG rules to account for the possibility of backing off to the infinite base distribution P aux 0 , and use the parsing algorithm described by Shieber et al. (1995) for computing inside probabilities under this TAG model.","The algorithm is illustrated in Table 1 along with Figure 1. Inside probabilities are computed in a bottom-up fashion and a TAG derivation is sampled top-down (Johnson et al., 2007). The 598 N α N γ N N Ni ... β δN0 α N1 γ N2 N3 N4 ... β δ Nj α Nk N∗ β Nl γ Nm N∗ δ Figure 1: Example used for illustrating blocked sampling with TAG. On the left hand side we have a partial training tree where we highlight the particular nodes (with node labels 0, 1, 2, 3, 4) that the sampling algorithm traverses in post-order. On the right hand side is the TAG grammar fragment that is used to parse these particular nodes: one initial tree and two wrapping auxiliary trees where one adjoins into the spine of the other for full general-ity of our illustration. Grammar nodes are labeled with their Goodman indices (letters i, j, k, l, m). Greek letters α, β, γ, δ denote entire subtrees. We assume that a subtree in an auxiliary tree (e.g., α) parses the same subtree in a training tree. sampler visits every node of the tree in post-order (O(n) operations, n being the number of nodes), visits every node below it as a potential foot (an-other O(n) operations), visits every mid-node in the path between the original node and the potential foot (if spine-adjunction is allowed) (O(log n) operations), and forms the appropriate chart items. The complexity is O(n2","log n) if spine-adjunction is allowed, O(n2",") otherwise."]},{"title":"4 Parameter Refinement","paragraphs":["During inference, adjunction probabilities are treated simplistically to facilitate convergence. Only two parameters guide adjunction: μc, the probability of adjunction; and p(aj | a<j, c) (see Equation 1), the probability of the particular auxiliary tree being adjoined given that there is an adjunction. In all of this treatment, c, the context of an adjunction, is the grammar category label such as S or NP, instead of a unique identifier for the node at which the adjunction occurs as was originally the case in probabilistic TAG literature. However it is possible to experiment with further refinement schemes at parsing time. Once the sampler converges on a grammar, we can reestimate its adjunction probabilities. Using the O(n6",") parsing algorithm (Shieber et al., 1995) we experimented with various refinements schemes — ranging from full node identifiers, to Goodman Chart item Why made? Inside probability","Ni[4] By assumption. −","Nk[3-4] N∗[4] and β (1 − μc) × π(β)","Nm[2-3] N∗[3] and δ (1 − μc) × π(δ)","Nl[1-3] γ and Nm[2-3] (1 − μc) × π(γ) ×π(Nm[2-3])","Naux[1-3] Nl[1-3] nc,a l/(nc + αaux","c ) ×π(Nl[1-3])","Nk[1-4] Naux[1-3] and Nk[3-4] μc × π(Naux[1-3]) ×π(Nk[3-4])","Nj[0-4] α and Nk[1-4] (1 − μc) × π(α) ×π(Nk[1-4])","Naux[0-4] Nj[0-4] nc,a j /(nc + αaux","c ) ×π(Nj[0-4])","Ni[0] Naux[0-4] and Ni[4] μc × π(Naux[0-4]) ×π(Ni[4]) Table 1: Computation of inside probabilities for TAG sampling. We create two types of chart items: (1) per-node, e.g., Ni[ν] denoting the probability of starting at an initial subtree that has Goodman index i and generating the subtree rooted at node ν, and (2) per-path, e.g., Nj[ν-η] denoting the probability of starting at an auxiliary subtree that has Goodman index j and generating the subtree rooted at ν minus the subtree rooted at η. Above, c denotes the context of adjunction, which is the nonterminal label of the node of adjunction (here, N), μc is the probability of adjunction, nc,a is the count of the auxiliary tree a, and nc =","∑","a nc,a is total number of adjunctions at","context c. The function π(·) retrieves the inside","probability corresponding to an item. index identifiers of the subtree below the adjunction (Hwa, 1998), to simple grammar category labels — and find that using Goodman index identifiers as c is the best performing option.","Interestingly, this particular refinement scheme also allows for fast cubic-time parsing, which we achieve by approximating the TAG by a TSG with little loss of coverage (no loss of coverage under special conditions which we find that are often satisfied) and negligible increase in grammar size, as discussed in the next section."]},{"title":"5 Cubic-time parsing","paragraphs":["MCMC training results in a list of sufficient statistics of the final derivation that the TAG sampler converges upon after a number of iterations. Basically, these are the list of initial and auxiliary trees, their cumulative counts over the training data, and their adjunction statistics. An adjunction statistic is listed as follows. If α is any elementary tree, and β is an auxiliary tree that adjoins n times at node ν of α that is uniquely reachable at path p, we write α p ← β (n times). We denote ν alternatively as","α[p]. 599 * q ! p \" n m k # * p \" i i i q !i k # * mi \"i #i i i #j j j q !i j i j !ij i (1) (2) (3) Figure 2: TAG to TSG transformation algorithm. By removing adjunctions in the correct order we end up with a larger yet adjunction-free TSG.","Now imagine that we end up with a small grammar that consists of one initial tree α and two auxiliary trees β and γ, and the following adjunctions occurring between them α p ← β (n times) α p ← γ (m times) β q ← γ (k times) as shown in Figure 2. Assume that α itself occurs l > n + m times in total so that there is nonzero probability of no adjunction anywhere within α. Also assume that the node uniquely identified by α[p] has Goodman index i, which we denote as i = G(α[p]).","The general idea of this TAG-TSG approximation is that, for any auxiliary tree that adjoins at a node ν with Goodman index i, we create an initial tree out of it where the root and foot nodes of the auxiliary tree are both replaced by i. Further, we split the subtree rooted at ν from its parent and rename the substitution site that is newly created at ν as i as well. (See Figure 2.) We can separate the foot subtree from the rest of the initial tree since it is completely remembered by any adjoined auxiliary trees due to the nature of our refinement scheme. However this method fails for adjunctions that occur at spinal nodes of auxiliary trees that have foot nodes below them since we would not know in which order to do the initial tree creation. However when the spine-adjunction relation is amenable to a topological sort (as is the case in Figure 2), we can apply the method by go-ing in this order and doing some extra bookkeep-ing: updating the list of Goodman indices and redirecting adjunctions as we go along. When there is no such topological sort, we can approximate the TAG by heuristically dropping low-frequency adjunctions that introduce cycles.1","The algorithm is illustrated in Figure 2. In (1) we see the original TAG grammar and its adjunctions (n, m, k are adjunction counts). Note that the adjunction relation has a topological sort of α, β, γ. We process auxiliary trees in this order and iteratively remove their adjunctions by creating specialized initial tree duplicates. In (2) we first visit β, which has adjunctions into α at the node denoted α[p] where p is the unique path from the root to this node. We retrieve the Goodman index of this node i = G(α[p]), split the subtree rooted at this node as a new initial tree αi, relabel its root as i, and rename the newly-created substitution site at α[p] as i. Since β has only this adjunction, we replace it with initial tree version βi where root/foot labels of β are replaced with i, and update all adjunctions into β as being into βi. In (3) we visit γ which now has adjunctions into α and βi. For the α[p] adjunction we create γi the same way we created βi but this time we can-not remove γ as it still has an adjunction into βi. We retrieve the Goodman index of the node of adjunction j = G(βi[q]), split the subtree rooted at this node as new initial tree βij, relabel its root as j, and rename the newly-created substitution site at βi[q] as j. Since γ now has only this adjunction left, we remove it by also creating initial tree version γj where root/foot labels of γ are replaced with j. At this point we have an adjunction-free TSG with elementary trees (and counts) α(l), αi(l), βi(n), βij(n), γi(m), γj(k) where l is the count of initial tree α. These counts, when they are normalized, lead to the appropriate adjunc-","1","We found that, on average, about half of our grammars have a topological sort of their spine-adjunctions. (On average fewer than 100 spine adjunctions even exist.) When no such sort exists, only a few low-frequency adjunctions have to be removed to eliminate cycles. 600 0 5 10 15 20 25 30 35 40 0 10 20 30 40 50 60 Parsing time (seconds) Sentence length (#tokens) Figure 3: Nonparametric TAG (blue) parsing is efficient and incurs only a small increase in parsing time compared to nonparametric TSG (red). tion probability refinement scheme of μc × p(aj | a<j, c) where c is the Goodman index.","Although this algorithm increases grammar size, the sparsity of the nonparametric solution ensures that the increase is almost negligible: on average the final Goodman-transformed CFG has 173.9K rules for TSG, 189.2K for TAG. Figure 3 demonstrates the comparable Viterbi parsing times for TSG and TAG."]},{"title":"6 Evaluation","paragraphs":["We use the standard Penn treebank methodology of training on sections 2–21 and testing on section 23. All our data is head-binarized, all hyperparameters are resampled under appropriate vague gamma and beta priors. Samplers are run 1000 iterations each; all reported numbers are averages over 5 runs. For simplicity, parsing results are based on the maximum probability derivation (Viterbi algorithm).","In Table 4, we compare TAG inference schemes and TSG. TAGGibbs operates by locally adding/removing potential adjunctions, similar to Cohn et al. (2009). TAG ′ is the O(n2",") algorithm","that disallows spine adjunction. We see that TAG ′ has the best parsing performance, while TAG provides the most compact representation. model F measure # initial trees # auxiliary trees TSG 84.15 69.5K - TAGGibbs 82.47 69.9K 1.7K TAG ′","84.87 66.4K 1.5K TAG 84.82 66.4K 1.4K Figure 4: EVALB results. Note that the Gibbs sampler for TAG has poor performance and provides no grammar compaction due to its lack of convergence.","label #adj ave. #lex. #left #right #wrap (spine adj) depth trees trees trees trees VP 4532 (23) 1.06 45 22 65 0 NP 2891 (46) 1.71 68 94 13 1 NN 2160 (3) 1.08 85 16 110 0 NNP 1478 (2) 1.12 90 19 90 0 NNS 1217 (1) 1.10 43 9 60 0 VBN 1121 (1) 1.05 6 18 0 0 VBD 976 (0) 1.0 16 25 0 0 NP 937 (0) 3.0 1 5 0 0 VB 870 (0) 1.02 14 31 4 0 S 823 (11) 1.48 42 36 35 3 total 23320 (118) 1.25 824 743 683 9 Table 2: Grammar analysis for an estimated TAG, categorized by label. Only the most common top 10 are shown, binarization variables are denoted with overline. A total number of 98 wrapping adjunctions (9 unique wrapping trees) and 118 spine adjunctions occur. ADJP “ “ ADJP ADJP* ” ” NP -LRB- NP NP* -RRB-S -LRB- -LRB-S S* -RRB- -RRB-S “ “ S S* ” ” NP -LRB- -LRB-NP NP* -RRB- -RRB-NNP , , NNP NNP NNP* CC & NNP NP “ “ NP NP* ” ” NP NP NP : NP NP* PP Figure 5: Example wrapping trees from estimated TAGs."]},{"title":"7 Conclusion","paragraphs":["We described a nonparametric Bayesian inference scheme for estimating TAG grammars and showed the power of TAG formalism over TSG for return-ing rich, generalizable, yet compact representations of data. The nonparametric inference scheme presents a principled way of addressing the difficult model selection problem with TAG. Our sampler has near quadratic-time efficiency, and our parsing approach remains context-free allowing for fast cubic-time parsing, so that our overall parsing framework is highly scalable.2","There are a number of extensions of this work: Experimenting with automatically in-duced adjunction refinements as well as in-corporating substitution refinements can benefit Bayesian TAG (Shindo et al., 2012; Petrov et al., 2006). We are also planning to investigate TAG for more context-sensitive languages, and synchronous TAG for machine translation.","2","An extensive report of our algorithms and experiments will be provided in the PhD thesis of the first author (Yamangil, 2013). Our code will be made publicly available at code.seas.harvard.edu/ẽlif. 601"]},{"title":"References","paragraphs":["Xavier Carreras, Michael Collins, and Terry Koo. 2008. TAG, dynamic programming, and the perceptron for efficient, feature-rich parsing. In Proceedings of the Twelfth Conference on Computational Natural Language Learning, CoNLL ’08, pages 9– 16, Stroudsburg, PA, USA. Association for Computational Linguistics.","David Chiang. 2000. Statistical parsing with an automatically-extracted tree adjoining grammar. In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics, ACL ’00, pages 456–463, Morristown, NJ, USA. Association for Computational Linguistics.","Trevor Cohn and Phil Blunsom. 2010. Blocked inference in Bayesian tree substitution grammars. In Proceedings of the ACL 2010 Conference Short Papers, ACLShort ’10, pages 225–230, Stroudsburg, PA, USA. Association for Computational Linguistics.","Trevor Cohn, Sharon Goldwater, and Phil Blunsom. 2009. Inducing compact but accurate tree-substitution grammars. In NAACL ’09: Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 548–556, Morristown, NJ, USA. Association for Computational Linguistics.","Christine Doran, Beth Hockey, Philip Hopely, Joseph Rosenzweig, Anoop Sarkar, B. Srinivas, Fei Xia, Alexis Nasr, and Owen Rambow. 1997. Maintain-ing the forest and burning out the underbrush in xtag. In Proceedings of the ENVGRAM Workshop.","Joshua Goodman. 2003. Efficient parsing of DOP with PCFG-reductions. In Rens Bod, Remko Scha, and Khalil Sima’an, editors, Data-Oriented Parsing. CSLI Publications, Stanford, CA.","Rebecca Hwa. 1998. An empirical evaluation of probabilistic lexicalized tree insertion grammars. In Proceedings of the 17th international conference on Computational linguistics - Volume 1, pages 557– 563, Morristown, NJ, USA. Association for Computational Linguistics.","Mark Johnson, Thomas Griffiths, and Sharon Goldwater. 2007. Bayesian inference for PCFGs via Markov chain Monte Carlo. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 139–146, Rochester, New York, April. Association for Computational Linguistics.","Aravind K. Joshi, Leon S. Levy, and Masako Takahashi. 1975. Tree adjunct grammars. Journal of Computer and System Sciences, 10(1):136–163.","Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 433–440, Sydney, Australia, July. Association for Computational Linguistics.","Matt Post and Daniel Gildea. 2009. Bayesian learning of a tree substitution grammar. In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 45–48, Suntec, Singapore, August. Association for Computational Linguistics.","Philip Resnik. 1992. Probabilistic tree-adjoining grammar as a framework for statistical natural language processing. In Proceedings of the 14th conference on Computational linguistics - Volume 2, COLING ’92, pages 418–424, Stroudsburg, PA, USA. Association for Computational Linguistics.","Yves Schabes and Stuart M. Shieber. 1994. An alternative conception of tree-adjoining derivation. Computational Linguistics, 20(1):91–124. Also available as cmp-lg/9404001.","Stuart M. Shieber, Yves Schabes, and Fernando C. N. Pereira. 1995. Principles and implementation of deductive parsing. J. Log. Program., 24(1&2):3–36.","Stuart M. Shieber. 2007. Probabilistic synchronous tree-adjoining grammars for machine translation: The argument from bilingual dictionaries. In Dekai Wu and David Chiang, editors, Proceedings of the Workshop on Syntax and Structure in Statistical Translation, Rochester, New York, 26 April.","Hiroyuki Shindo, Akinori Fujino, and Masaaki Nagata. 2011. Insertion operator for Bayesian tree substitution grammars. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers - Volume 2, HLT ’11, pages 206–211, Stroudsburg, PA, USA. Association for Computational Linguistics.","Hiroyuki Shindo, Yusuke Miyao, Akinori Fujino, and Masaaki Nagata. 2012. Bayesian symbol-refined tree substitution grammars for syntactic parsing. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 440–448, Jeju Island, Korea, July. Association for Computational Linguistics.","Fei Xia, Chung-hye Han, Martha Palmer, and Aravind Joshi. 2001. Automatically extracting and compar-ing lexicalized grammars for different languages. In Proceedings of the 17th international joint conference on Artificial intelligence - Volume 2, IJCAI’01, pages 1321–1326, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.","XTAG Research Group. 2001. A lexicalized tree adjoining grammar for English. Technical Report IRCS-01-03, IRCS, University of Pennsylvania. 602","Elif Yamangil and Stuart Shieber. 2012. Estimating compact yet rich tree insertion grammars. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 110–114, Jeju Island, Korea, July. Association for Computational Linguistics.","Elif Yamangil. 2013. Rich Linguistic Structure from Large-Scale Web Data. Ph.D. thesis, Harvard University. Forthcoming. 603"]}],"references":[{"authors":[{"first":"Xavier","last":"Carreras"},{"first":"Michael","last":"Collins"},{"first":"Terry","last":"Koo"}],"year":"2008","title":"TAG, dynamic programming, and the perceptron for efficient, feature-rich parsing","source":"Xavier Carreras, Michael Collins, and Terry Koo. 2008. TAG, dynamic programming, and the perceptron for efficient, feature-rich parsing. In Proceedings of the Twelfth Conference on Computational Natural Language Learning, CoNLL ’08, pages 9– 16, Stroudsburg, PA, USA. Association for Computational Linguistics."},{"authors":[{"first":"David","last":"Chiang"}],"year":"2000","title":"Statistical parsing with an automatically-extracted tree adjoining grammar","source":"David Chiang. 2000. Statistical parsing with an automatically-extracted tree adjoining grammar. In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics, ACL ’00, pages 456–463, Morristown, NJ, USA. Association for Computational Linguistics."},{"authors":[{"first":"Trevor","last":"Cohn"},{"first":"Phil","last":"Blunsom"}],"year":"2010","title":"Blocked inference in Bayesian tree substitution grammars","source":"Trevor Cohn and Phil Blunsom. 2010. Blocked inference in Bayesian tree substitution grammars. In Proceedings of the ACL 2010 Conference Short Papers, ACLShort ’10, pages 225–230, Stroudsburg, PA, USA. Association for Computational Linguistics."},{"authors":[{"first":"Trevor","last":"Cohn"},{"first":"Sharon","last":"Goldwater"},{"first":"Phil","last":"Blunsom"}],"year":"2009","title":"Inducing compact but accurate tree-substitution grammars","source":"Trevor Cohn, Sharon Goldwater, and Phil Blunsom. 2009. Inducing compact but accurate tree-substitution grammars. In NAACL ’09: Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 548–556, Morristown, NJ, USA. Association for Computational Linguistics."},{"authors":[{"first":"Christine","last":"Doran"},{"first":"Beth","last":"Hockey"},{"first":"Philip","last":"Hopely"},{"first":"Joseph","last":"Rosenzweig"},{"first":"Anoop","last":"Sarkar"},{"first":"B.","last":"Srinivas"},{"first":"Fei","last":"Xia"},{"first":"Alexis","last":"Nasr"},{"first":"Owen","last":"Rambow"}],"year":"1997","title":"Maintain-ing the forest and burning out the underbrush in xtag","source":"Christine Doran, Beth Hockey, Philip Hopely, Joseph Rosenzweig, Anoop Sarkar, B. Srinivas, Fei Xia, Alexis Nasr, and Owen Rambow. 1997. Maintain-ing the forest and burning out the underbrush in xtag. In Proceedings of the ENVGRAM Workshop."},{"authors":[{"first":"Joshua","last":"Goodman"}],"year":"2003","title":"Efficient parsing of DOP with PCFG-reductions","source":"Joshua Goodman. 2003. Efficient parsing of DOP with PCFG-reductions. In Rens Bod, Remko Scha, and Khalil Sima’an, editors, Data-Oriented Parsing. CSLI Publications, Stanford, CA."},{"authors":[{"first":"Rebecca","last":"Hwa"}],"year":"1998","title":"An empirical evaluation of probabilistic lexicalized tree insertion grammars","source":"Rebecca Hwa. 1998. An empirical evaluation of probabilistic lexicalized tree insertion grammars. In Proceedings of the 17th international conference on Computational linguistics - Volume 1, pages 557– 563, Morristown, NJ, USA. Association for Computational Linguistics."},{"authors":[{"first":"Mark","last":"Johnson"},{"first":"Thomas","last":"Griffiths"},{"first":"Sharon","last":"Goldwater"}],"year":"2007","title":"Bayesian inference for PCFGs via Markov chain Monte Carlo","source":"Mark Johnson, Thomas Griffiths, and Sharon Goldwater. 2007. Bayesian inference for PCFGs via Markov chain Monte Carlo. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 139–146, Rochester, New York, April. Association for Computational Linguistics."},{"authors":[{"first":"Aravind","middle":"K.","last":"Joshi"},{"first":"Leon","middle":"S.","last":"Levy"},{"first":"Masako","last":"Takahashi"}],"year":"1975","title":"Tree adjunct grammars","source":"Aravind K. Joshi, Leon S. Levy, and Masako Takahashi. 1975. Tree adjunct grammars. Journal of Computer and System Sciences, 10(1):136–163."},{"authors":[{"first":"Slav","last":"Petrov"},{"first":"Leon","last":"Barrett"},{"first":"Romain","last":"Thibaux"},{"first":"Dan","last":"Klein"}],"year":"2006","title":"Learning accurate, compact, and interpretable tree annotation","source":"Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 433–440, Sydney, Australia, July. Association for Computational Linguistics."},{"authors":[{"first":"Matt","last":"Post"},{"first":"Daniel","last":"Gildea"}],"year":"2009","title":"Bayesian learning of a tree substitution grammar","source":"Matt Post and Daniel Gildea. 2009. Bayesian learning of a tree substitution grammar. In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 45–48, Suntec, Singapore, August. Association for Computational Linguistics."},{"authors":[{"first":"Philip","last":"Resnik"}],"year":"1992","title":"Probabilistic tree-adjoining grammar as a framework for statistical natural language processing","source":"Philip Resnik. 1992. Probabilistic tree-adjoining grammar as a framework for statistical natural language processing. In Proceedings of the 14th conference on Computational linguistics - Volume 2, COLING ’92, pages 418–424, Stroudsburg, PA, USA. Association for Computational Linguistics."},{"authors":[{"first":"Yves","last":"Schabes"},{"first":"Stuart","middle":"M.","last":"Shieber"}],"year":"1994","title":"An alternative conception of tree-adjoining derivation","source":"Yves Schabes and Stuart M. Shieber. 1994. An alternative conception of tree-adjoining derivation. Computational Linguistics, 20(1):91–124. Also available as cmp-lg/9404001."},{"authors":[{"first":"Stuart","middle":"M.","last":"Shieber"},{"first":"Yves","last":"Schabes"},{"first":"Fernando","middle":"C. N.","last":"Pereira"}],"year":"1995","title":"Principles and implementation of deductive parsing","source":"Stuart M. Shieber, Yves Schabes, and Fernando C. N. Pereira. 1995. Principles and implementation of deductive parsing. J. Log. Program., 24(1&2):3–36."},{"authors":[{"first":"Stuart","middle":"M.","last":"Shieber"}],"year":"2007","title":"Probabilistic synchronous tree-adjoining grammars for machine translation: The argument from bilingual dictionaries","source":"Stuart M. Shieber. 2007. Probabilistic synchronous tree-adjoining grammars for machine translation: The argument from bilingual dictionaries. In Dekai Wu and David Chiang, editors, Proceedings of the Workshop on Syntax and Structure in Statistical Translation, Rochester, New York, 26 April."},{"authors":[{"first":"Hiroyuki","last":"Shindo"},{"first":"Akinori","last":"Fujino"},{"first":"Masaaki","last":"Nagata"}],"year":"2011","title":"Insertion operator for Bayesian tree substitution grammars","source":"Hiroyuki Shindo, Akinori Fujino, and Masaaki Nagata. 2011. Insertion operator for Bayesian tree substitution grammars. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers - Volume 2, HLT ’11, pages 206–211, Stroudsburg, PA, USA. Association for Computational Linguistics."},{"authors":[{"first":"Hiroyuki","last":"Shindo"},{"first":"Yusuke","last":"Miyao"},{"first":"Akinori","last":"Fujino"},{"first":"Masaaki","last":"Nagata"}],"year":"2012","title":"Bayesian symbol-refined tree substitution grammars for syntactic parsing","source":"Hiroyuki Shindo, Yusuke Miyao, Akinori Fujino, and Masaaki Nagata. 2012. Bayesian symbol-refined tree substitution grammars for syntactic parsing. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 440–448, Jeju Island, Korea, July. Association for Computational Linguistics."},{"authors":[{"first":"Fei","last":"Xia"},{"first":"Chung-hye","last":"Han"},{"first":"Martha","last":"Palmer"},{"first":"Aravind","last":"Joshi"}],"year":"2001","title":"Automatically extracting and compar-ing lexicalized grammars for different languages","source":"Fei Xia, Chung-hye Han, Martha Palmer, and Aravind Joshi. 2001. Automatically extracting and compar-ing lexicalized grammars for different languages. In Proceedings of the 17th international joint conference on Artificial intelligence - Volume 2, IJCAI’01, pages 1321–1326, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc."},{"authors":[{"first":"XTAG","middle":"Research","last":"Group"}],"year":"2001","title":"A lexicalized tree adjoining grammar for English","source":"XTAG Research Group. 2001. A lexicalized tree adjoining grammar for English. Technical Report IRCS-01-03, IRCS, University of Pennsylvania. 602"},{"authors":[{"first":"Elif","last":"Yamangil"},{"first":"Stuart","last":"Shieber"}],"year":"2012","title":"Estimating compact yet rich tree insertion grammars","source":"Elif Yamangil and Stuart Shieber. 2012. Estimating compact yet rich tree insertion grammars. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 110–114, Jeju Island, Korea, July. Association for Computational Linguistics."},{"authors":[{"first":"Elif","last":"Yamangil"}],"year":"2013","title":"Rich Linguistic Structure from Large-Scale Web Data","source":"Elif Yamangil. 2013. Rich Linguistic Structure from Large-Scale Web Data. Ph.D. thesis, Harvard University. Forthcoming. 603"}],"cites":[{"style":0,"text":"Goodman, 2003","origin":{"pointer":"/sections/2/paragraphs/0","offset":724,"length":13},"authors":[{"last":"Goodman"}],"year":"2003","references":["/references/5"]},{"style":0,"text":"Cohn et al., 2009","origin":{"pointer":"/sections/2/paragraphs/1","offset":252,"length":17},"authors":[{"last":"Cohn"},{"last":"al."}],"year":"2009","references":["/references/3"]},{"style":0,"text":"Post and Gildea, 2009","origin":{"pointer":"/sections/2/paragraphs/1","offset":271,"length":21},"authors":[{"last":"Post"},{"last":"Gildea"}],"year":"2009","references":["/references/10"]},{"style":0,"text":"Yamangil and Shieber, 2012","origin":{"pointer":"/sections/2/paragraphs/1","offset":547,"length":26},"authors":[{"last":"Yamangil"},{"last":"Shieber"}],"year":"2012","references":["/references/19"]},{"style":0,"text":"Chiang, 2000","origin":{"pointer":"/sections/2/paragraphs/1","offset":575,"length":12},"authors":[{"last":"Chiang"}],"year":"2000","references":["/references/1"]},{"style":0,"text":"Resnik, 1992","origin":{"pointer":"/sections/2/paragraphs/1","offset":589,"length":12},"authors":[{"last":"Resnik"}],"year":"1992","references":["/references/11"]},{"style":0,"text":"Joshi et al., 1975","origin":{"pointer":"/sections/2/paragraphs/2","offset":96,"length":18},"authors":[{"last":"Joshi"},{"last":"al."}],"year":"1975","references":["/references/8"]},{"style":0,"text":"Joshi et al., 1975","origin":{"pointer":"/sections/2/paragraphs/2","offset":449,"length":18},"authors":[{"last":"Joshi"},{"last":"al."}],"year":"1975","references":["/references/8"]},{"style":0,"text":"Group, 2001","origin":{"pointer":"/sections/2/paragraphs/2","offset":483,"length":11},"authors":[{"last":"Group"}],"year":"2001","references":["/references/18"]},{"style":0,"text":"Doran et al., 1997","origin":{"pointer":"/sections/2/paragraphs/4","offset":245,"length":18},"authors":[{"last":"Doran"},{"last":"al."}],"year":"1997","references":["/references/4"]},{"style":0,"text":"Shieber, 2007","origin":{"pointer":"/sections/2/paragraphs/4","offset":355,"length":13},"authors":[{"last":"Shieber"}],"year":"2007","references":["/references/14"]},{"style":0,"text":"Hwa, 1998","origin":{"pointer":"/sections/2/paragraphs/4","offset":437,"length":9},"authors":[{"last":"Hwa"}],"year":"1998","references":["/references/6"]},{"style":0,"text":"Xia et al., 2001","origin":{"pointer":"/sections/2/paragraphs/4","offset":531,"length":16},"authors":[{"last":"Xia"},{"last":"al."}],"year":"2001","references":["/references/17"]},{"style":0,"text":"Carreras et al., 2008","origin":{"pointer":"/sections/2/paragraphs/4","offset":567,"length":21},"authors":[{"last":"Carreras"},{"last":"al."}],"year":"2008","references":["/references/0"]},{"style":0,"text":"Cohn and Blunsom (2010)","origin":{"pointer":"/sections/2/paragraphs/5","offset":15,"length":23},"authors":[{"last":"Cohn"},{"last":"Blunsom"}],"year":"2010","references":["/references/2"]},{"style":0,"text":"Shindo et al. (2011)","origin":{"pointer":"/sections/2/paragraphs/5","offset":430,"length":20},"authors":[{"last":"Shindo"},{"last":"al."}],"year":"2011","references":["/references/15"]},{"style":0,"text":"Yamangil and Shieber (2012)","origin":{"pointer":"/sections/2/paragraphs/5","offset":455,"length":27},"authors":[{"last":"Yamangil"},{"last":"Shieber"}],"year":"2012","references":["/references/19"]},{"style":0,"text":"Cohn and Blunsom, 2010","origin":{"pointer":"/sections/2/paragraphs/6","offset":685,"length":22},"authors":[{"last":"Cohn"},{"last":"Blunsom"}],"year":"2010","references":["/references/2"]},{"style":0,"text":"Shindo et al., 2011","origin":{"pointer":"/sections/2/paragraphs/6","offset":709,"length":19},"authors":[{"last":"Shindo"},{"last":"al."}],"year":"2011","references":["/references/15"]},{"style":0,"text":"Yamangil and Shieber, 2012","origin":{"pointer":"/sections/2/paragraphs/6","offset":730,"length":26},"authors":[{"last":"Yamangil"},{"last":"Shieber"}],"year":"2012","references":["/references/19"]},{"style":0,"text":"Cohn et al., 2009","origin":{"pointer":"/sections/3/paragraphs/7","offset":35,"length":17},"authors":[{"last":"Cohn"},{"last":"al."}],"year":"2009","references":["/references/3"]},{"style":0,"text":"Cohn et al., 2009","origin":{"pointer":"/sections/3/paragraphs/8","offset":285,"length":17},"authors":[{"last":"Cohn"},{"last":"al."}],"year":"2009","references":["/references/3"]},{"style":0,"text":"Schabes and Shieber, 1994","origin":{"pointer":"/sections/3/paragraphs/11","offset":262,"length":25},"authors":[{"last":"Schabes"},{"last":"Shieber"}],"year":"1994","references":["/references/12"]},{"style":0,"text":"Cohn and Blunsom, 2010","origin":{"pointer":"/sections/4/paragraphs/0","offset":275,"length":22},"authors":[{"last":"Cohn"},{"last":"Blunsom"}],"year":"2010","references":["/references/2"]},{"style":0,"text":"Shindo et al., 2011","origin":{"pointer":"/sections/4/paragraphs/0","offset":299,"length":19},"authors":[{"last":"Shindo"},{"last":"al."}],"year":"2011","references":["/references/15"]},{"style":0,"text":"Yamangil and Shieber, 2012","origin":{"pointer":"/sections/4/paragraphs/0","offset":320,"length":26},"authors":[{"last":"Yamangil"},{"last":"Shieber"}],"year":"2012","references":["/references/19"]},{"style":0,"text":"Goodman, 2003","origin":{"pointer":"/sections/4/paragraphs/0","offset":433,"length":13},"authors":[{"last":"Goodman"}],"year":"2003","references":["/references/5"]},{"style":0,"text":"Shieber et al. (1995)","origin":{"pointer":"/sections/4/paragraphs/0","offset":620,"length":21},"authors":[{"last":"Shieber"},{"last":"al."}],"year":"1995","references":["/references/13"]},{"style":0,"text":"Johnson et al., 2007","origin":{"pointer":"/sections/4/paragraphs/1","offset":160,"length":20},"authors":[{"last":"Johnson"},{"last":"al."}],"year":"2007","references":["/references/7"]},{"style":0,"text":"Shieber et al., 1995","origin":{"pointer":"/sections/5/paragraphs/1","offset":21,"length":20},"authors":[{"last":"Shieber"},{"last":"al."}],"year":"1995","references":["/references/13"]},{"style":0,"text":"Hwa, 1998","origin":{"pointer":"/sections/5/paragraphs/16","offset":93,"length":9},"authors":[{"last":"Hwa"}],"year":"1998","references":["/references/6"]},{"style":0,"text":"Cohn et al. (2009)","origin":{"pointer":"/sections/7/paragraphs/1","offset":133,"length":18},"authors":[{"last":"Cohn"},{"last":"al."}],"year":"2009","references":["/references/3"]},{"style":0,"text":"Shindo et al., 2012","origin":{"pointer":"/sections/8/paragraphs/1","offset":189,"length":19},"authors":[{"last":"Shindo"},{"last":"al."}],"year":"2012","references":["/references/16"]},{"style":0,"text":"Petrov et al., 2006","origin":{"pointer":"/sections/8/paragraphs/1","offset":210,"length":19},"authors":[{"last":"Petrov"},{"last":"al."}],"year":"2006","references":["/references/9"]},{"style":0,"text":"Yamangil, 2013","origin":{"pointer":"/sections/8/paragraphs/3","offset":110,"length":14},"authors":[{"last":"Yamangil"}],"year":"2013","references":["/references/20"]}]}
