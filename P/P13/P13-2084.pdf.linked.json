{"sections":[{"title":"","paragraphs":["Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 474–478, Sofia, Bulgaria, August 4-9 2013. c⃝2013 Association for Computational Linguistics"]},{"title":"Text Classification from Positive and Unlabeled Data using Misclassified Data Correction Fumiyo Fukumoto and Yoshimi Suzuki and Suguru Matsuyoshi Interdisciplinary Graduate School of Medicine and Engineering University of Yamanashi, Kofu, 400-8511, JAPAN {fukumoto,ysuzuki,sugurum}@yamanashi.ac.jp Abstract","paragraphs":["This paper addresses the problem of deal-ing with a collection of labeled training documents, especially annotating negative training documents and presents a method of text classification from positive and unlabeled data. We applied an error detection and correction technique to the results of positive and negative documents classified by the Support Vector Machines (SVM). The results using Reuters documents showed that the method was comparable to the current state-of-the-art biased-SVM method as the F-score obtained by our method was 0.627 and biased-SVM was 0.614."]},{"title":"1 Introduction","paragraphs":["Text classification using machine learning (ML) techniques with a small number of labeled data has become more important with the rapid increase in volume of online documents. Quite a lot of learning techniques e.g., semi-supervised learning, self-training, and active learning have been proposed. Blum et al. proposed a semi-supervised learning approach called the Graph Mincut algorithm which uses a small number of positive and negative examples and assigns values to unlabeled examples in a way that optimizes consistency in a nearest-neighbor sense (Blum et al., 2001). Cabrera et al. described a method for self-training text categorization using the Web as the corpus (Cabrera et al., 2009). The method extracts unlabeled documents automatically from the Web and applies an enriched self-training for constructing the classifier.","Several authors have attempted to improve classification accuracy using only positive and unlabeled data (Yu et al., 2002; Ho et al., 2011). Liu et al. proposed a method called biased-SVM that uses soft-margin SVM as the underlying classifiers (Liu et al., 2003). Elkan and Noto proposed a theoretically justified method (Elkan and Noto, 2008). They showed that under the assumption that the labeled documents are selected randomly from the positive documents, a classifier trained on positive and unlabeled documents predicts probabilities that differ by only a constant factor from the true conditional probabilities of being positive. They reported that the results were comparable to the current state-of-the-art biased SVM method. The methods of Liu et al. and Elkan et al. model a region containing most of the available positive data. However, these methods are sensitive to the parameter values, especially the small size of labeled data presents special difficulties in tuning the parameters to produce optimal results.","In this paper, we propose a method for eliminating the need for manually collecting training documents, especially annotating negative training documents based on supervised ML techniques. Our goal is to eliminate the need for manually collecting training documents, and hopefully achieve classification accuracy from positive and unlabeled data as high as that from labeled positive and labeled negative data. Like much previous work on semi-supervised ML, we apply SVM to the positive and unlabeled data, and add the classification results to the training data. The difference is that before adding the classification results, we applied the MisClassified data Detection and Correction (MCDC) technique to the results of SVM learning in order to improve classification accuracy obtained by the final classifiers."]},{"title":"2 Framework of the System","paragraphs":["The MCDC method involves category error correction, i.e., correction of misclassified candidates, while there are several strategies for automatically detecting lexical/syntactic errors in corpora (Abney et al., 1999; Eskin, 2000; Dickinson and 474 training","UPP1 N1 N1 training SVM MCDC N1 RC1 UN1 CP1","CN1SVM","MCDC","... Final results SVM training selection classification","PCPN1 RC1 N1 RC2 N1 CN MCDC Figure 1: Overview of the system Meurers., 2005; Boyd et al., 2008) or categorical data errors (Akoglu et al., 2013). The method first detects error candidates. As error candidates, we focus on support vectors (SVs) extracted from the training documents by SVM. Training by SVM is performed to find the optimal hyperplane consisting of SVs, and only the SVs affect the performance. Thus, if some training document reduces the overall performance of text classification because of an outlier, we can assume that the document is a SV.","Figure 1 illustrates our system. First, we randomly select documents from unlabeled data (U ) where the number of documents is equal to that of the initial positive training documents (P1). We set these selected documents to negative training documents (N1), and apply SVM to learn classifiers. Next, we apply the MCDC technique to the results of SVM learning. For the result of correction (RC1)1",", we train SVM classifiers, and classify the remaining unlabeled data (U \\ N1). For the result of classification, we randomly select positive (CP1) and negative (CN1) documents classified by SVM and add to the SVM training data (RC1). We re-train SVM classifiers with the training documents, and apply the MCDC. The procedure is repeated until there are no unlabeled documents judged to be either positive or negative. Finally, the test data are classified using the final classifiers. In the following subsections, we present the MCDC procedure shown in Figure 2. It consists of three steps: extraction of misclassified candidates, estimation of error reduction, and correction of misclassified candidates.","1","The manually annotated positive examples are not corrected. Extraction of miss-classified candidates Training data D test learning D SV (Support vectors) Estimation of error reduction classification SV label  NB label D Error candidates","Correction of misclassified candidates D1 D2 Final results Error candidates SVM NB Loss function Judgment using loss values Figure 2: The MCDC procedure 2.1 Extraction of misclassified candidates Let D be a set of training documents and xk ∈ {x1, x2, ···, xm} be a SV of negative or positive documents obtained by SVM. We remove ∪m","k=1xk from the training documents D. The resulting D \\∪m","k=1xk is used for training Naive Bayes (NB) (McCallum, 2001), leading to a classification model. This classification model is tested on each xk, and assigns a positive or negative label. If the label is different from that assigned to xk, we declare xk an error candidate. 2.2 Estimation of error reduction We detect misclassified data from the extracted candidates by estimating error reduction. The estimation of error reduction is often used in active learning. The earliest work is the method of Roy and McCallum (Roy and McCallum, 2001). They proposed a method that directly optimizes expected future error by log-loss or 0-1 loss, using the entropy of the posterior class distribution on a sample of unlabeled documents. We used their method to detect misclassified data. Specifically, we estimated future error rate by log-loss function. It uses the entropy of the posterior class distribution on a sample of the unlabeled documents. A loss function is defined by Eq (1).","E P̂D","2∪(x","k,y","k) = −","1 | X | ∑ x∈X ∑ y∈Y P (y|x) × log( P̂D 2∪(xk,yk)(y|x)). (1) Eq (1) denotes the expected error of the learner. P (y | x) denotes the true distribution of output classes y ∈ Y given inputs x. X denotes a 475 set of test documents. P̂D","2∪(xk,yk)(y | x) shows the learner’s prediction, and D2 denotes the training documents D except for the error candidates ∪ l k=1xk. If the value of Eq (1) is sufficiently small, the learner’s prediction is close to the true output distribution.","We used bagging to reduce variance of P (y | x) as it is unknown for each test document x. More precisely, from the training documents D, a different training set consisting of positive and negative documents is created2",". The learner then creates a new classifier from the training documents. The procedure is repeated m times3",", and the final class posterior for an instance is taken to be the unweighted average of the class posteriori for each of the classifiers. 2.3 Correction of misclassified candidates For each error candidate xk, we calculated the expected error of the learner, EP̂D","2∪(xk,yk old) and E P̂D","2∪(xk,yk new) by using Eq (1). Here, yk old refers to the original label assigned to xk, and yk new is the resulting category label estimated by NB classifiers. If the value of the latter is smaller than that of the former, we declare the document xk to be misclassified, i.e., the label yk old is an error, and its true label is yk new. Otherwise, the label of xk is yk old."]},{"title":"3 Experiments 3.1 Experimental setup","paragraphs":["We chose the 1996 Reuters data (Reuters, 2000) for evaluation. After eliminating unlabeled documents, we divided these into three. The data (20,000 documents) extracted from 20 Aug to 19 Sept is used as training data indicating positive and unlabeled documents. We set the range of δ from 0.1 to 0.9 to create a wide range of scenarios, where δ refers to the ratio of documents from the positive class first selected from a fold as the positive set. The rest of the positive and negative documents are used as unlabeled data. We used categories assigned to more than 100 documents in the training data as it is necessary to examine a wide range of δ values. These categories are 88 in all. The data from 20 Sept to 19 Nov is used 2 We set the number of negative documents extracted ran-","domly from the unlabeled documents to the same number of","positive training documents. 3 We set the number of m to 100 in the experiments. as a test set X, to estimate true output distribution. The remaining data consisting 607,259 from 20 Nov 1996 to 19 Aug 1997 is used as a test data for text classification. We obtained a vocabulary of 320,935 unique words after eliminating words which occur only once, stemming by a part-of-speech tagger (Schmid, 1995), and stop word removal. The number of categories per documents is 3.21 on average. We used the SVM-Light package (Joachims, 1998)4",". We used a linear kernel and set all parameters to their default values.","We compared our method, MCDC with three baselines: (1) SVM, (2) Positive Example-Based Learning (PEBL) proposed by (Yu et al., 2002), and (3) biased-SVM (Liu et al., 2003). We chose PEBL because the convergence procedure is very similar to our framework. Biased-SVM is the state-of-the-art SVM method, and often used for comparison (Elkan and Noto, 2008). To make comparisons fair, all methods were based on a linear kernel. We randomly selected 1,000 positive and 1,000 negative documents classified by SVM and added to the SVM training data in each iteration5",". For biased-SVM, we used training data and classified test documents directly. We empirically selected values of two parameters, “c” (trade-off between training error and margin) and “j”, i.e., cost (cost-factor, by which training errors on positive examples) that optimized the F-score obtained by classification of test documents.","The positive training data in SVM are assigned to the target category. The negative training data are the remaining data except for the documents that were assigned to the target category, i.e., this is the ideal method as we used all the training data with positive/negative labeled documents. The number of positive training data in other three methods depends on the value of δ, and the rest of the positive and negative documents were used as unlabeled data. 3.2 Text classification Classification results for 88 categories are shown in Figure 3. Figure 3 shows micro-averaged F-score against the δ value. As expected, the results obtained by SVM were the best among all δ values. However, this is the ideal method that requires 20,000 documents labeled positive/negative, while other methods including our 4 http://svmlight.joachims.org 5 We set the number of documents up to 1,000. 476","SVM PEBL Biased-SVM MCDC","Level (# of Cat) Cat F Cat F (Iter) Cat F (Iter) Cat F (Iter) Best GSPO .955 GSPO .802 (26) CCAT .939 GSPO .946 (9)","Top (22) Worst GODD .099 GODD .079 (6) GODD .038 GODD .104 (4) Avg .800 .475 (19) .593 .619 (8) Best M14 .870 E71 .848 (7) M14 .869 M14 .875 (9)","Second (32) Worst C16 .297 E14 .161 (14) C16 .148 C16 .150 (3) Avg .667 .383 (22) .588 .593 (7) Best M141 .878 C174 .792 (27) M141 .887 M141 .885 (8)","Third (33) Worst G152 .102 C331 .179 (16) G155 .130 C331 .142 (6) Avg .717 .313 (18) .518 .557 (8)","Fourth (1) – C1511 .738 C1511 .481 (16) C1511 .737 C1511 .719 (4)","Micro Avg F-score .718 .428 (19) .614 .627 (8) Table 1: Classification performance (δ = 0.7) 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 F-score Delta Value SVM PEBL","Biased-SVM MCDC Figure 3: F-score against the value of δ method used only positive and unlabeled documents. Overall performance obtained by MCDC was better for those obtained by PEBL and biased-SVM methods in all δ values, especially when the positive set was small, e.g., δ = 0.3, the improvement of MCDC over biased-SVM and PEBL was significant.","Table 1 shows the results obtained by each method with a δ value of 0.7. “Level” indicates each level of the hierarchy and the numbers in parentheses refer to the number of categories. “Best” and “Worst” refer to the best and the lowest F-scores in each level of a hierarchy, respectively. “Iter” in PEBL indicates the number of iterations until the number of negative documents is zero in the convergence procedure. Similarly, “Iter” in the MCDC indicates the number of iterations until no unlabeled documents are judged to be either positive or negative. As can be seen clearly from Table 1, the results with MCDC were better than those obtained by PEBL in each level of the hierarchy. Similarly, the results were bet- δ SV Ec Err","Correct","Prec Rec F","0.3 227,547 54,943 79,329 .693 .649 .670","0.7 141,087 34,944 42,385 .712 .673 .692 Table 2: Miss-classified data correction results ter than those of biased-SVM except for the fourth level, “C1511”(Annual results). The average numbers of iterations with MCDC and PEBL were 8 and 19 times, respectively. In biased-SVM, it is necessary to run SVM many times, as we searched “c” and “j”. In contrast, MCDC does not require such parameter tuning. 3.3 Correction of misclassified candidates Our goal is to achieve classification accuracy from only positive documents and unlabeled data as high as that from labeled positive and negative data. We thus applied a miss-classified data detection and correction technique for the classification results obtained by SVM. Therefore, it is important to examine the accuracy of miss-classified correction. Table 2 shows detection and correction performance against all categories. “SV” shows the total number of SVs in 88 categories in all iterations. “Ec” refers to the total number of extracted error candidates. “Err” denotes the number of documents classified incorrectly by SVM and added to the training data, i.e., the number of documents that should be assigned correctly by the correction procedure. “Prec” and “Rec” show the precision and recall of correction, respectively.","Table 2 shows that precision was better than recall with both δ values, as the precision obtained by γ value = 0.3 and 0.7 were 4.4% and 3.9% improvement against recall values, respectively. These observations indicated that the error candidates extracted by our method were appropriately 477 corrected. In contrast, there were still other documents that were miss-classified but not extracted as error candidates. We extracted error candidates using the results of SVM and NB classifiers. Ensemble of other techniques such as boosting and kNN for further efficacy gains seems promising to try with our method."]},{"title":"4 Conclusion","paragraphs":["The research described in this paper involved text classification using positive and unlabeled data. Miss-classified data detection and correction technique was incorporated in the existing classification technique. The results using the 1996 Reuters corpora showed that the method was comparable to the current state-of-the-art biased-SVM method as the F-score obtained by our method was 0.627 and biased-SVM was 0.614. Future work will in-clude feature reduction and investigation of other classification algorithms to obtain further advantages in efficiency and efficacy in manipulating real-world large corpora."]},{"title":"References","paragraphs":["S. Abney, R. E. Schapire, and Y. Singer. 1999. Boost-ing Applied to Tagging and PP Attachment. In Proc. of the Joint SIGDAT Conference on EMNLP and Very Large Corpora, pages 38–45.","L. Akoglu, H. Tong, J. Vreeken, and C. Faloutsos. 2013. Fast and Reliable Anomaly Detection in Categorical Data. In Proc. of the CIKM, pages 415–424.","A. Blum, J. Lafferty, M. Rwebangira, and R. Reddy. 2001. Learning from Labeled and Unlabeled Data using Graph Mincuts. In Proc. of the 18th ICML, pages 19–26.","A. Boyd, M. Dickinson, and D. Meurers. 2008. On Detecting Errors in Dependency Treebanks. Research on Language and Computation, 6(2):113– 137.","R. G. Cabrera, M. M. Gomez, P. Rosso, and L. V. Pineda. 2009. Using the Web as Corpus for Self-Training Text Categorization. Information Retrieval, 12(3):400–415.","M. Dickinson and W. D. Meurers. 2005. Detecting Errors in Discontinuous Structural Annotation. In Proc. of the ACL’05, pages 322–329.","C. Elkan and K. Noto. 2008. Learning Classifiers from Only Positive and Unlabeled Data. In Proc. of the KDD’08, pages 213–220.","E. Eskin. 2000. Detectiong Errors within a Corpus using Anomaly Detection. In Proc. of the 6th ANLP Conference and the 1st Meeting of the NAACL, pages 148–153.","C. H. Ho, M. H. Tsai, and C. J. Lin. 2011. Active Learning and Experimental Design with SVMs. In Proc. of the JMLR Workshop on Active Learning and Experimental Design, pages 71–84.","T. Joachims. 1998. SVM Light Support Vector Machine. In Dept. of Computer Science Cornell University.","B. Liu, Y. Dai, X. Li, W. S. Lee, and P. S. Yu. 2003. Building Text Classifiers using Positive and Unlabeled Examples. In Proc. of the ICDM’03, pages 179–188.","A. K. McCallum. 2001. Multi-label Text Classification with a Mixture Model Trained by EM. In Revised Version of Paper Appearing in AAAI’99 Workshop on Text Learning, pages 135–168.","Reuters. 2000. Reuters Corpus Volume1 English Language. 1996-08-20 to 1997-08-19 Release Date 2000-11-03 Format Version 1.","N. Roy and A. K. McCallum. 2001. Toward Optimal Active Learning through Sampling Estimation of Error Reduction. In Proc. of the 18th ICML, pages 441–448.","H. Schmid. 1995. Improvements in Part-of-Speech Tagging with an Application to German. In Proc. of the EACL SIGDAT Workshop, pages 47–50.","H. Yu, H. Han, and K. C-C. Chang. 2002. PEBL: Positive Example based Learning for Web Page Classification using SVM. In Proc. of the ACM Special Interest Group on Knowledge Discovery and Data Mining, pages 239–248. 478"]}],"references":[{"authors":[{"first":"S.","last":"Abney"},{"first":"R.","middle":"E.","last":"Schapire"},{"first":"Y.","last":"Singer"}],"year":"1999","title":"Boost-ing Applied to Tagging and PP Attachment","source":"S. Abney, R. E. Schapire, and Y. Singer. 1999. Boost-ing Applied to Tagging and PP Attachment. In Proc. of the Joint SIGDAT Conference on EMNLP and Very Large Corpora, pages 38–45."},{"authors":[{"first":"L.","last":"Akoglu"},{"first":"H.","last":"Tong"},{"first":"J.","last":"Vreeken"},{"first":"C.","last":"Faloutsos"}],"year":"2013","title":"Fast and Reliable Anomaly Detection in Categorical Data","source":"L. Akoglu, H. Tong, J. Vreeken, and C. Faloutsos. 2013. Fast and Reliable Anomaly Detection in Categorical Data. In Proc. of the CIKM, pages 415–424."},{"authors":[{"first":"A.","last":"Blum"},{"first":"J.","last":"Lafferty"},{"first":"M.","last":"Rwebangira"},{"first":"R.","last":"Reddy"}],"year":"2001","title":"Learning from Labeled and Unlabeled Data using Graph Mincuts","source":"A. Blum, J. Lafferty, M. Rwebangira, and R. Reddy. 2001. Learning from Labeled and Unlabeled Data using Graph Mincuts. In Proc. of the 18th ICML, pages 19–26."},{"authors":[{"first":"A.","last":"Boyd"},{"first":"M.","last":"Dickinson"},{"first":"D.","last":"Meurers"}],"year":"2008","title":"On Detecting Errors in Dependency Treebanks","source":"A. Boyd, M. Dickinson, and D. Meurers. 2008. On Detecting Errors in Dependency Treebanks. Research on Language and Computation, 6(2):113– 137."},{"authors":[{"first":"R.","middle":"G.","last":"Cabrera"},{"first":"M.","middle":"M.","last":"Gomez"},{"first":"P.","last":"Rosso"},{"first":"L.","middle":"V.","last":"Pineda"}],"year":"2009","title":"Using the Web as Corpus for Self-Training Text Categorization","source":"R. G. Cabrera, M. M. Gomez, P. Rosso, and L. V. Pineda. 2009. Using the Web as Corpus for Self-Training Text Categorization. Information Retrieval, 12(3):400–415."},{"authors":[{"first":"M.","last":"Dickinson"},{"first":"W.","middle":"D.","last":"Meurers"}],"year":"2005","title":"Detecting Errors in Discontinuous Structural Annotation","source":"M. Dickinson and W. D. Meurers. 2005. Detecting Errors in Discontinuous Structural Annotation. In Proc. of the ACL’05, pages 322–329."},{"authors":[{"first":"C.","last":"Elkan"},{"first":"K.","last":"Noto"}],"year":"2008","title":"Learning Classifiers from Only Positive and Unlabeled Data","source":"C. Elkan and K. Noto. 2008. Learning Classifiers from Only Positive and Unlabeled Data. In Proc. of the KDD’08, pages 213–220."},{"authors":[{"first":"E.","last":"Eskin"}],"year":"2000","title":"Detectiong Errors within a Corpus using Anomaly Detection","source":"E. Eskin. 2000. Detectiong Errors within a Corpus using Anomaly Detection. In Proc. of the 6th ANLP Conference and the 1st Meeting of the NAACL, pages 148–153."},{"authors":[{"first":"C.","middle":"H.","last":"Ho"},{"first":"M.","middle":"H.","last":"Tsai"},{"first":"C.","middle":"J.","last":"Lin"}],"year":"2011","title":"Active Learning and Experimental Design with SVMs","source":"C. H. Ho, M. H. Tsai, and C. J. Lin. 2011. Active Learning and Experimental Design with SVMs. In Proc. of the JMLR Workshop on Active Learning and Experimental Design, pages 71–84."},{"authors":[{"first":"T.","last":"Joachims"}],"year":"1998","title":"SVM Light Support Vector Machine","source":"T. Joachims. 1998. SVM Light Support Vector Machine. In Dept. of Computer Science Cornell University."},{"authors":[{"first":"B.","last":"Liu"},{"first":"Y.","last":"Dai"},{"first":"X.","last":"Li"},{"first":"W.","middle":"S.","last":"Lee"},{"first":"P.","middle":"S.","last":"Yu"}],"year":"2003","title":"Building Text Classifiers using Positive and Unlabeled Examples","source":"B. Liu, Y. Dai, X. Li, W. S. Lee, and P. S. Yu. 2003. Building Text Classifiers using Positive and Unlabeled Examples. In Proc. of the ICDM’03, pages 179–188."},{"authors":[{"first":"A.","middle":"K.","last":"McCallum"}],"year":"2001","title":"Multi-label Text Classification with a Mixture Model Trained by EM","source":"A. K. McCallum. 2001. Multi-label Text Classification with a Mixture Model Trained by EM. In Revised Version of Paper Appearing in AAAI’99 Workshop on Text Learning, pages 135–168."},{"authors":[{"last":"Reuters"}],"year":"2000","title":"Reuters Corpus Volume1 English Language","source":"Reuters. 2000. Reuters Corpus Volume1 English Language. 1996-08-20 to 1997-08-19 Release Date 2000-11-03 Format Version 1."},{"authors":[{"first":"N.","last":"Roy"},{"first":"A.","middle":"K.","last":"McCallum"}],"year":"2001","title":"Toward Optimal Active Learning through Sampling Estimation of Error Reduction","source":"N. Roy and A. K. McCallum. 2001. Toward Optimal Active Learning through Sampling Estimation of Error Reduction. In Proc. of the 18th ICML, pages 441–448."},{"authors":[{"first":"H.","last":"Schmid"}],"year":"1995","title":"Improvements in Part-of-Speech Tagging with an Application to German","source":"H. Schmid. 1995. Improvements in Part-of-Speech Tagging with an Application to German. In Proc. of the EACL SIGDAT Workshop, pages 47–50."},{"authors":[{"first":"H.","last":"Yu"},{"first":"H.","last":"Han"},{"first":"K.","middle":"C-C.","last":"Chang"}],"year":"2002","title":"PEBL: Positive Example based Learning for Web Page Classification using SVM","source":"H. Yu, H. Han, and K. C-C. Chang. 2002. PEBL: Positive Example based Learning for Web Page Classification using SVM. In Proc. of the ACM Special Interest Group on Knowledge Discovery and Data Mining, pages 239–248. 478"}],"cites":[{"style":0,"text":"Blum et al., 2001","origin":{"pointer":"/sections/2/paragraphs/0","offset":555,"length":17},"authors":[{"last":"Blum"},{"last":"al."}],"year":"2001","references":["/references/2"]},{"style":0,"text":"Cabrera et al., 2009","origin":{"pointer":"/sections/2/paragraphs/0","offset":676,"length":20},"authors":[{"last":"Cabrera"},{"last":"al."}],"year":"2009","references":["/references/4"]},{"style":0,"text":"Yu et al., 2002","origin":{"pointer":"/sections/2/paragraphs/1","offset":106,"length":15},"authors":[{"last":"Yu"},{"last":"al."}],"year":"2002","references":["/references/15"]},{"style":0,"text":"Ho et al., 2011","origin":{"pointer":"/sections/2/paragraphs/1","offset":123,"length":15},"authors":[{"last":"Ho"},{"last":"al."}],"year":"2011","references":["/references/8"]},{"style":0,"text":"Liu et al., 2003","origin":{"pointer":"/sections/2/paragraphs/1","offset":245,"length":16},"authors":[{"last":"Liu"},{"last":"al."}],"year":"2003","references":["/references/10"]},{"style":0,"text":"Elkan and Noto, 2008","origin":{"pointer":"/sections/2/paragraphs/1","offset":322,"length":20},"authors":[{"last":"Elkan"},{"last":"Noto"}],"year":"2008","references":["/references/6"]},{"style":0,"text":"Abney et al., 1999","origin":{"pointer":"/sections/3/paragraphs/0","offset":198,"length":18},"authors":[{"last":"Abney"},{"last":"al."}],"year":"1999","references":["/references/0"]},{"style":0,"text":"Eskin, 2000","origin":{"pointer":"/sections/3/paragraphs/0","offset":218,"length":11},"authors":[{"last":"Eskin"}],"year":"2000","references":["/references/7"]},{"style":0,"text":"Meurers., 2005","origin":{"pointer":"/sections/3/paragraphs/5","offset":61,"length":14},"authors":[{"last":"Meurers."}],"year":"2005","references":[]},{"style":0,"text":"Boyd et al., 2008","origin":{"pointer":"/sections/3/paragraphs/5","offset":77,"length":17},"authors":[{"last":"Boyd"},{"last":"al."}],"year":"2008","references":["/references/3"]},{"style":0,"text":"Akoglu et al., 2013","origin":{"pointer":"/sections/3/paragraphs/5","offset":124,"length":19},"authors":[{"last":"Akoglu"},{"last":"al."}],"year":"2013","references":["/references/1"]},{"style":0,"text":"McCallum, 2001","origin":{"pointer":"/sections/3/paragraphs/12","offset":45,"length":14},"authors":[{"last":"McCallum"}],"year":"2001","references":["/references/11"]},{"style":0,"text":"Roy and McCallum, 2001","origin":{"pointer":"/sections/3/paragraphs/12","offset":518,"length":22},"authors":[{"last":"Roy"},{"last":"McCallum"}],"year":"2001","references":["/references/13"]},{"style":0,"text":"Reuters, 2000","origin":{"pointer":"/sections/4/paragraphs/0","offset":32,"length":13},"authors":[{"last":"Reuters"}],"year":"2000","references":["/references/12"]},{"style":0,"text":"Schmid, 1995","origin":{"pointer":"/sections/4/paragraphs/2","offset":387,"length":12},"authors":[{"last":"Schmid"}],"year":"1995","references":["/references/14"]},{"style":0,"text":"Joachims, 1998","origin":{"pointer":"/sections/4/paragraphs/2","offset":515,"length":14},"authors":[{"last":"Joachims"}],"year":"1998","references":["/references/9"]},{"style":0,"text":"Yu et al., 2002","origin":{"pointer":"/sections/4/paragraphs/4","offset":116,"length":15},"authors":[{"last":"Yu"},{"last":"al."}],"year":"2002","references":["/references/15"]},{"style":0,"text":"Liu et al., 2003","origin":{"pointer":"/sections/4/paragraphs/4","offset":154,"length":16},"authors":[{"last":"Liu"},{"last":"al."}],"year":"2003","references":["/references/10"]},{"style":0,"text":"Elkan and Noto, 2008","origin":{"pointer":"/sections/4/paragraphs/4","offset":333,"length":20},"authors":[{"last":"Elkan"},{"last":"Noto"}],"year":"2008","references":["/references/6"]}]}
