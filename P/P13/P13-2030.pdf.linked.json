{"sections":[{"title":"","paragraphs":["Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 166–170, Sofia, Bulgaria, August 4-9 2013. c⃝2013 Association for Computational Linguistics"]},{"title":"An Improved MDL-Based Compression Algorithm for Unsupervised Word Segmentation Ruey-Cheng Chen National Taiwan University 1 Roosevelt Rd. Sec. 4 Taipei 106, Taiwan rueycheng@turing.csie.ntu.edu.tw Abstract","paragraphs":["We study the mathematical properties of a recently proposed MDL-based unsupervised word segmentation algorithm, called regularized compression. Our analysis shows that its objective function can be efficiently approximated using the negative empirical pointwise mutual information. The proposed extension improves the baseline performance in both efficiency and accuracy on a standard benchmark."]},{"title":"1 Introduction","paragraphs":["Hierarchical Bayes methods have been mainstream in unsupervised word segmentation since the dawn of hierarchical Dirichlet process (Goldwater et al., 2009) and adaptors grammar (Johnson and Goldwater, 2009). Despite this wide recognition, they are also notoriously computational prohibitive and have limited adoption on larger corpora. While much effort has been directed to mitigating this issue within the Bayes framework (Borschinger and Johnson, 2011), many have found minimum description length (MDL) based methods more promising in addressing the scalability problem.","MDL-based methods (Rissanen, 1978) rely on underlying search algorithms to segment the text in as many possible ways and use description length to decide which to output. As different algorithms explore different trajectories in the search space, segmentation accuracy depends largely on the search coverage. Early work in this line focused more on existing segmentation algorithm, such as branching entropy (Tanaka-Ishii, 2005; Zhikov et al., 2010) and bootstrap voting experts (Hewlett and Cohen, 2009; Hewlett and Cohen, 2011). A recent study (Chen et al., 2012) on a compression-based algorithm, regularized compression, has achieved comparable performance result to hierarchical Bayes methods.","Along this line, in this paper we present a novel extension to the regularized compressor algorithm. We propose a lower-bound approximate to the original objective and show that, through analysis and experimentation, this amendment improves segmentation performance and runtime efficiency."]},{"title":"2 Regularized Compression","paragraphs":["The dynamics behind regularized compression is similar to digram coding (Witten et al., 1999). One first breaks the text down to a sequence of characters (W0) and then works from that representation up in an agglomerative fashion, iteratively removing word boundaries between the two selected word types. Hence, a new sequence Wi is created in the i-th iteration by merging all the occurrences of some selected bigram (x, y) in the original sequence Wi−1. Unlike in digram coding, where the most frequent pair of word types is always selected, in regularized compression a specialized decision criterion is used to balance compression rate and vocabulary complexity: min. −αf (x, y) + |Wi−1|∆ H̃(Wi−1, Wi) s.t. either x or y is a character","f (x, y) > nms. Here, the criterion is written slightly differently. Note that f (x, y) is the bigram frequency, |Wi−1| the sequence length of Wi−1, and ∆ H̃(Wi−1, Wi) = H̃(Wi) − H̃(Wi−1) is the difference between the empirical Shannon entropy measured on Wi and Wi−1, using maximum likelihood estimates. Specifically, this empirical estimate H̃(W ) for a sequence W corresponds to: log |W | − 1 |W | ∑ x:types f (x) log f (x). For this equation to work, one needs to estimate other model parameters. See Chen et al. (2012) for a comprehensive treatment. 166","f (x) f (y) f (z) |W | Wi−1 k l 0 N Wi k − m l − m m N − m Table 1: The change between iterations in word frequency and sequence length in regularized compression. In the new sequence Wi, each occurrence of the x-y bigram is replaced with a new (conceptually unseen) word z. This has an effect of reducing the number of words in the sequence."]},{"title":"3 Change in Description Length","paragraphs":["The second term of the aforementioned objective is in fact an approximate to the change in description length. This is made obvious by coding up a sequence W using the Shannon code, with which the description length of W is equal to |W | H̃(W ). Here, the change in description length between sequences Wi−1 and Wi is written as: ∆L = |Wi| H̃(W ) − |Wi−1| H̃(Wi−1). (1)","Let us focus on this equation. Suppose that the original sequence Wi−1 is N -word long, the selected word type pair x and y each occurs k and l times, respectively, and altogether x-y bigram occurs m times in Wi−1. In the new sequence Wi, each of the m bigrams is replaced with an unseen word z = xy. These altogether have reduced the sequence length by m. The end result is that compression moves probability masses from one place to the other, causing a change in description length. See Table 1 for a summary to this exchange.","Now, as we expand Equation (1) and reorganize the remaining, we find that: ∆L = (N − m) log(N − m) − N log N + k log k − (k − m) log(k − m) + l log l − (l − m) log(l − m) + 0 log 0 − m log m (2) Note that each line in Equation (2) is of the form x1 log x1 − x2 log x2 for some x1, x2 ≥ 0. We exploit this pattern and derive a bound for ∆L through analysis. Consider g(x) = x log x. Since g′′","(x) > 0 for x ≥ 0, by the Taylor series we have the following relations for any x1, x2 ≥ 0:","g(x1) − g(x2) ≤ (x1 − x2)g′ (x 1),","g(x1) − g(x2) ≥ (x1 − x2)g′ (x 2). Plugging these into Equation (2), we have: m log","(k − m)(l − m) N m ≤ ∆L ≤ ∞. (3) The lower bound1","at the left-hand side is a best-case estimate. As our aim is to minimize ∆L, we use this quantity to serve as an approximate."]},{"title":"4 Proposed Method","paragraphs":["Based on this finding, we propose the following two variations (see Figure 1) for the regularized compression framework:","• G1: Replacing the second term in the original objective with the lower bound in Equation (3). The new objective function is written out as Equation (4).","• G2: Same as G1 except that the lower bound is divided by f (x, y) beforehand. The normalized lower bound approximates the per-word change in description length, as shown in Equation (5). With this variation, the function remains in a scalarized form as the original does.","We use the following procedure to compute description length. Given a word sequence W , we write out all the induced word types (say, M types in total) entry by entry as a character sequence, denoted as C. Then the overall description length is:","|W | H̃(W ) + |C| H̃(C) + M − 1 2 log |W |. (6)","Three free parameters, α, ρ, and nms remain to be estimated. A detailed treatment on parameter estimation is given in the following paragraphs. Trade-off α This parameter controls the balance between compression rate and vocabulary complexity. Throughout this experiment, we estimated this parameter using MDL-based grid search. Multiple search runs at different granularity levels were employed as necessary. Compression rate ρ This is the minimum threshold value for compression rate. The compressor algorithm would go on as many iteration as possible until the overall compression rate (i.e.,","1","Sharp-eyed readers may have noticed the similarity between the lower bound and the negative (empirical) pointwise mutual information. In fact, when f(z) > 0 in Wi−1, it can be shown that limm→0 ∆L/m converges to the empirical pointwise mutual information (proof omitted here). 167 G1 ≡ f (x, y) ( log","(f (x) − f (x, y))(f (y) − f (x, y)) |Wi−1|f (x, y) − α ) (4) G2 ≡ −αf (x, y) + log","(f (x) − f (x, y))(f (y) − f (x, y)) |Wi−1|f (x, y) (5) Figure 1: The two newly-proposed objective functions. word/character ratio) is lower than ρ. Setting this value to 0 forces the compressor to go on until no more can be done. In this paper, we experimented with predetermined ρ values as well as those learned from MDL-based grid search. Minimum support nms We simply followed the suggested setting nms = 3 (Chen et al., 2012)."]},{"title":"5 Evaluation 5.1 Setup","paragraphs":["In the experiment, we tested our methods on Brent’s derivation of the Bernstein-Ratner corpus (Brent and Cartwright, 1996; Bernstein-Ratner, 1987). This dataset is distributed via the CHILDES project (MacWhinney and Snow, 1990) and has been commonly used as a standard benchmark for phonetic segmentation. Our baseline method is the original regularized compressor algorithm (Chen et al., 2012). In our experiment, we considered the following three search settings for finding the model parameters:","(a) Fix ρ to 0 and vary α to find the best value (in the sense of description length);","(b) Fix α to the best value found in setting (a) and vary ρ;","(c) Set ρ to a heuristic value 0.37 (Chen et al., 2012) and vary α.","Settings (a) and (b) can be seen as running a stochastic grid searcher one round for each parameter2",". Note that we tested (c) here only to compare with the best baseline setting. 5.2 Result Table 2 summarizes the result for each objective and each search setting. The best (α, ρ) pair for","2","A more formal way to estimate both α and ρ is to run a stochastic searcher that varies between settings (a) and (b), fixing the best value found in the previous run. Here, for simplicity, we leave this to future work. Run P R F Baseline 76.9 81.6 79.2 G1 (a) α : 0.030 76.4 79.9 78.1 G1 (b) ρ : 0.38 73.4 80.2 76.8 G1 (c) α : 0.010 75.7 80.4 78.0 G2 (a) α : 0.002 82.1 80.0 81.0 G2 (b) ρ : 0.36 79.1 81.7 80.4 G2 (c) α : 0.004 79.3 84.2 81.7 Table 2: The performance result on the Bernstein-Ratner corpus. Segmentation performance is measured using word-level precision (P), recall (R), and F-measure (F). G1 is (0.03, 0.38) and the best for G2 is (0.002, 0.36). On one hand, the performance of G1 is consistently inferior to the baseline across all settings. Although approximation error was one possible cause, we noticed that the compression process was no longer properly regularized, since f (x, y) and the ∆L estimate in the objective are intermingled. In this case, adjusting α has little effect in balancing compression rate and complexity.","The second objective G2, on the other hand, did not suffer as much from the aforementioned lack of regularization. We found that, in all three settings, G2 outperforms the baseline by 1 to 2 percentage points in F-measure. The best performance result achieved by G2 in our experiment is 81.7 in word-level F-measure, although this was obtained from search setting (c), using a heuristic ρ value 0.37. It is interesting to note that G1 (b) and G2 (b) also gave very close estimates to this heuristic value. Nevertheless, it remains an open issue whether there is a connection between the optimal ρ value and the true word/token ratio (≈ 0.35 for Bernstein-Ratner corpus).","The result has led us to conclude that MDL-based grid search is efficient in optimizing segmentation accuracy. Minimization of description length is in general aligned with performance improvement, although under finer granularity MDL-based search may not be as effec-168 Method P R F Adaptors grammar, colloc3-syllable Johnson and Goldwater (2009) 86.1 88.4 87.2 Regularized compression + MDL, G2 (b) — 79.1 81.7 80.4 Regularized compression + MDL Chen et al. (2012) 76.9 81.6 79.2 Adaptors grammar, colloc Johnson and Goldwater (2009) 78.4 75.7 77.1 Particle filter, unigram Börschinger and Johnson (2012) – – 77.1 Regularized compression + MDL, G1 (b) — 73.4 80.2 76.8 Bootstrap voting experts + MDL Hewlett and Cohen (2011) 79.3 73.4 76.2 Nested Pitman-Yor process, bigram Mochihashi et al. (2009) 74.8 76.7 75.7 Branching entropy + MDL Zhikov et al. (2010) 76.3 74.5 75.4 Particle filter, bigram Börschinger and Johnson (2012) – – 74.5 Hierarchical Dirichlet process Goldwater et al. (2009) 75.2 69.6 72.3 Table 3: The performance chart on the Bernstein-Ratner corpus, in descending order of word-level F-measure. We deliberately reproduced the results for adaptors grammar and regularized compression. The other measurements came directly from the literature. tive. In our experiment, search setting (b) won out on description length for both objectives, while the best performance was in fact achieved by the others. It would be interesting to confirm this by studying the correlation between description length and word-level F-measure.","In Table 3, we summarize many published results for segmentation methods ever tested on the Bernstein-Ratner corpus. Of the proposed methods, we include only setting (b) since it is more general than the others. From Table 3, we find that the performance of G2 (b) is competitive to other state-of-the-art hierarchical Bayesian models and MDL methods, though it still lags 7 percentage points behind the best result achieved by adaptors grammar with colloc3-syllable. We also compare adaptors grammar to regularized compressor on average running time, which is shown in Table 4. On our test machine, it took roughly 15 hours for one instance of adaptors grammar with colloc3-syllable to run to the finish. Yet an improved regularized compressor could deliver the result in merely 1.25 second. In other words, even in an 100 × 100 grid search, the regularized compressor algorithm can still finish 4 to 5 times earlier than one single adaptors grammar instance."]},{"title":"6 Concluding Remarks","paragraphs":["In this paper, we derive a new lower-bound approximate to the objective function used in the regularized compression algorithm. As computing the approximate no longer relies on the change in lexicon entropy, the new compressor algorithm is made more efficient than the original. Besides run-Method Time (s) Adaptors grammar, colloc3-syllable 53826 Adaptors grammar, colloc 10498 Regularized compressor 1.51 Regularized compressor, G1 (b) 0.60 Regularized compressor, G2 (b) 1.25 Table 4: The average running time in seconds on the Bernstein-Ratner corpus for adaptors grammar (per fold, based on trace output) and regularized compressors, tested on an Intel Xeon 2.5GHz 8core machine with 8GB RAM. time efficiency, our experiment result also shows improved performance. Using MDL alone, one proposed method outperforms the original regularized compressor (Chen et al., 2012) in precision by 2 percentage points and in F-measure by 1. Its performance is only second to the state of the art, achieved by adaptors grammar with colloc3-syllable (Johnson and Goldwater, 2009).","A natural extension of this work is to reproduce this result on some other word segmentation benchmarks, specifically those in other Asian languages (Emerson, 2005; Zhikov et al., 2010). Furthermore, it would be interesting to investigate stochastic optimization techniques for regularized compression that simultaneously fit both α and ρ. We believe this would be the key to adapt the algorithm to larger datasets."]},{"title":"Acknowledgments","paragraphs":["We thank the anonymous reviewers for their valuable feedback. 169"]},{"title":"References","paragraphs":["Nan Bernstein-Ratner. 1987. The phonology of parent child speech. Children’s language, 6:159–174.","Benjamin Borschinger and Mark Johnson. 2011. A particle filter algorithm for bayesian word segmentation. In Proceedings of the Australasian Language Technology Association Workshop 2011, pages 10– 18, Canberra, Australia, December.","Benjamin Börschinger and Mark Johnson. 2012. Using rejuvenation to improve particle filtering for bayesian word segmentation. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 85–89, Jeju Island, Korea, July. Association for Computational Linguistics.","Michael R. Brent and Timothy A. Cartwright. 1996. Distributional regularity and phonotactic constraints are useful for segmentation. In Cognition, pages 93– 125.","Ruey-Cheng Chen, Chiung-Min Tsai, and Jieh Hsiang. 2012. A regularized compression method to unsupervised word segmentation. In Proceedings of the Twelfth Meeting of the Special Interest Group on Computational Morphology and Phonology, SIG-MORPHON ’12, pages 26–34, Montreal, Canada. Association for Computational Linguistics.","Thomas Emerson. 2005. The second international chinese word segmentation bakeoff. In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing, volume 133. Jeju Island, Korea.","Sharon Goldwater, Thomas L. Griffiths, and Mark Johnson. 2009. A bayesian framework for word segmentation: Exploring the effects of context. Cognition, 112(1):21–54, July.","Daniel Hewlett and Paul Cohen. 2009. Bootstrap voting experts. In Proceedings of the 21st international jont conference on Artifical intelligence, IJCAI’09, pages 1071–1076, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.","Daniel Hewlett and Paul Cohen. 2011. Fully unsupervised word segmentation with BVE and MDL. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers - Volume 2, HLT ’11, pages 540–545, Portland, Oregon. Association for Computational Linguistics.","Mark Johnson and Sharon Goldwater. 2009. Improving nonparameteric bayesian inference: experiments on unsupervised word segmentation with adaptor grammars. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL ’09, pages 317–325, Boulder, Colorado. Association for Computational Linguistics.","Brian MacWhinney and Catherine Snow. 1990. The child language data exchange system: an update. Journal of child language, 17(2):457–472, June.","Daichi Mochihashi, Takeshi Yamada, and Naonori Ueda. 2009. Bayesian unsupervised word segmentation with nested Pitman-Yor language modeling. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1 - Volume 1, ACL ’09, pages 100–108, Suntec, Singapore. Association for Computational Linguistics.","Jorma Rissanen. 1978. Modeling by shortest data description. Automatica, 14(5):465–471, September.","Kumiko Tanaka-Ishii. 2005. Entropy as an indicator of context boundaries: An experiment using a web search engine. In Robert Dale, Kam-Fai Wong, Jian Su, and Oi Kwong, editors, Natural Language Processing IJCNLP 2005, volume 3651 of Lecture Notes in Computer Science, chapter 9, pages 93– 105. Springer Berlin / Heidelberg, Berlin, Heidelberg.","Ian H. Witten, Alistair Moffat, and Timothy C. Bell. 1999. Managing gigabytes (2nd ed.): compressing and indexing documents and images. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA.","Valentin Zhikov, Hiroya Takamura, and Manabu Okumura. 2010. An efficient algorithm for unsupervised word segmentation with branching entropy and MDL. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10, pages 832–842, Cambridge, Massachusetts. Association for Computational Linguistics. 170"]}],"references":[{"authors":[{"first":"Nan","last":"Bernstein-Ratner"}],"year":"1987","title":"The phonology of parent child speech","source":"Nan Bernstein-Ratner. 1987. The phonology of parent child speech. Children’s language, 6:159–174."},{"authors":[{"first":"Benjamin","last":"Borschinger"},{"first":"Mark","last":"Johnson"}],"year":"2011","title":"A particle filter algorithm for bayesian word segmentation","source":"Benjamin Borschinger and Mark Johnson. 2011. A particle filter algorithm for bayesian word segmentation. In Proceedings of the Australasian Language Technology Association Workshop 2011, pages 10– 18, Canberra, Australia, December."},{"authors":[{"first":"Benjamin","last":"Börschinger"},{"first":"Mark","last":"Johnson"}],"year":"2012","title":"Using rejuvenation to improve particle filtering for bayesian word segmentation","source":"Benjamin Börschinger and Mark Johnson. 2012. Using rejuvenation to improve particle filtering for bayesian word segmentation. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 85–89, Jeju Island, Korea, July. Association for Computational Linguistics."},{"authors":[{"first":"Michael","middle":"R.","last":"Brent"},{"first":"Timothy","middle":"A.","last":"Cartwright"}],"year":"1996","title":"Distributional regularity and phonotactic constraints are useful for segmentation","source":"Michael R. Brent and Timothy A. Cartwright. 1996. Distributional regularity and phonotactic constraints are useful for segmentation. In Cognition, pages 93– 125."},{"authors":[{"first":"Ruey-Cheng","last":"Chen"},{"first":"Chiung-Min","last":"Tsai"},{"first":"Jieh","last":"Hsiang"}],"year":"2012","title":"A regularized compression method to unsupervised word segmentation","source":"Ruey-Cheng Chen, Chiung-Min Tsai, and Jieh Hsiang. 2012. A regularized compression method to unsupervised word segmentation. In Proceedings of the Twelfth Meeting of the Special Interest Group on Computational Morphology and Phonology, SIG-MORPHON ’12, pages 26–34, Montreal, Canada. Association for Computational Linguistics."},{"authors":[{"first":"Thomas","last":"Emerson"}],"year":"2005","title":"The second international chinese word segmentation bakeoff","source":"Thomas Emerson. 2005. The second international chinese word segmentation bakeoff. In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing, volume 133. Jeju Island, Korea."},{"authors":[{"first":"Sharon","last":"Goldwater"},{"first":"Thomas","middle":"L.","last":"Griffiths"},{"first":"Mark","last":"Johnson"}],"year":"2009","title":"A bayesian framework for word segmentation: Exploring the effects of context","source":"Sharon Goldwater, Thomas L. Griffiths, and Mark Johnson. 2009. A bayesian framework for word segmentation: Exploring the effects of context. Cognition, 112(1):21–54, July."},{"authors":[{"first":"Daniel","last":"Hewlett"},{"first":"Paul","last":"Cohen"}],"year":"2009","title":"Bootstrap voting experts","source":"Daniel Hewlett and Paul Cohen. 2009. Bootstrap voting experts. In Proceedings of the 21st international jont conference on Artifical intelligence, IJCAI’09, pages 1071–1076, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc."},{"authors":[{"first":"Daniel","last":"Hewlett"},{"first":"Paul","last":"Cohen"}],"year":"2011","title":"Fully unsupervised word segmentation with BVE and MDL","source":"Daniel Hewlett and Paul Cohen. 2011. Fully unsupervised word segmentation with BVE and MDL. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers - Volume 2, HLT ’11, pages 540–545, Portland, Oregon. Association for Computational Linguistics."},{"authors":[{"first":"Mark","last":"Johnson"},{"first":"Sharon","last":"Goldwater"}],"year":"2009","title":"Improving nonparameteric bayesian inference: experiments on unsupervised word segmentation with adaptor grammars","source":"Mark Johnson and Sharon Goldwater. 2009. Improving nonparameteric bayesian inference: experiments on unsupervised word segmentation with adaptor grammars. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL ’09, pages 317–325, Boulder, Colorado. Association for Computational Linguistics."},{"authors":[{"first":"Brian","last":"MacWhinney"},{"first":"Catherine","last":"Snow"}],"year":"1990","title":"The child language data exchange system: an update","source":"Brian MacWhinney and Catherine Snow. 1990. The child language data exchange system: an update. Journal of child language, 17(2):457–472, June."},{"authors":[{"first":"Daichi","last":"Mochihashi"},{"first":"Takeshi","last":"Yamada"},{"first":"Naonori","last":"Ueda"}],"year":"2009","title":"Bayesian unsupervised word segmentation with nested Pitman-Yor language modeling","source":"Daichi Mochihashi, Takeshi Yamada, and Naonori Ueda. 2009. Bayesian unsupervised word segmentation with nested Pitman-Yor language modeling. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1 - Volume 1, ACL ’09, pages 100–108, Suntec, Singapore. Association for Computational Linguistics."},{"authors":[{"first":"Jorma","last":"Rissanen"}],"year":"1978","title":"Modeling by shortest data description","source":"Jorma Rissanen. 1978. Modeling by shortest data description. Automatica, 14(5):465–471, September."},{"authors":[{"first":"Kumiko","last":"Tanaka-Ishii"}],"year":"2005","title":"Entropy as an indicator of context boundaries: An experiment using a web search engine","source":"Kumiko Tanaka-Ishii. 2005. Entropy as an indicator of context boundaries: An experiment using a web search engine. In Robert Dale, Kam-Fai Wong, Jian Su, and Oi Kwong, editors, Natural Language Processing IJCNLP 2005, volume 3651 of Lecture Notes in Computer Science, chapter 9, pages 93– 105. Springer Berlin / Heidelberg, Berlin, Heidelberg."},{"authors":[{"first":"Ian","middle":"H.","last":"Witten"},{"first":"Alistair","last":"Moffat"},{"first":"Timothy","middle":"C.","last":"Bell"}],"year":"1999","title":"Managing gigabytes (2nd ed","source":"Ian H. Witten, Alistair Moffat, and Timothy C. Bell. 1999. Managing gigabytes (2nd ed.): compressing and indexing documents and images. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA."},{"authors":[{"first":"Valentin","last":"Zhikov"},{"first":"Hiroya","last":"Takamura"},{"first":"Manabu","last":"Okumura"}],"year":"2010","title":"An efficient algorithm for unsupervised word segmentation with branching entropy and MDL","source":"Valentin Zhikov, Hiroya Takamura, and Manabu Okumura. 2010. An efficient algorithm for unsupervised word segmentation with branching entropy and MDL. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10, pages 832–842, Cambridge, Massachusetts. Association for Computational Linguistics. 170"}],"cites":[{"style":0,"text":"Goldwater et al., 2009","origin":{"pointer":"/sections/2/paragraphs/0","offset":132,"length":22},"authors":[{"last":"Goldwater"},{"last":"al."}],"year":"2009","references":["/references/6"]},{"style":0,"text":"Johnson and Goldwater, 2009","origin":{"pointer":"/sections/2/paragraphs/0","offset":178,"length":27},"authors":[{"last":"Johnson"},{"last":"Goldwater"}],"year":"2009","references":["/references/9"]},{"style":0,"text":"Borschinger and Johnson, 2011","origin":{"pointer":"/sections/2/paragraphs/0","offset":425,"length":29},"authors":[{"last":"Borschinger"},{"last":"Johnson"}],"year":"2011","references":["/references/1"]},{"style":0,"text":"Rissanen, 1978","origin":{"pointer":"/sections/2/paragraphs/1","offset":19,"length":14},"authors":[{"last":"Rissanen"}],"year":"1978","references":["/references/12"]},{"style":0,"text":"Tanaka-Ishii, 2005","origin":{"pointer":"/sections/2/paragraphs/1","offset":409,"length":18},"authors":[{"last":"Tanaka-Ishii"}],"year":"2005","references":["/references/13"]},{"style":0,"text":"Zhikov et al., 2010","origin":{"pointer":"/sections/2/paragraphs/1","offset":429,"length":19},"authors":[{"last":"Zhikov"},{"last":"al."}],"year":"2010","references":["/references/15"]},{"style":0,"text":"Hewlett and Cohen, 2009","origin":{"pointer":"/sections/2/paragraphs/1","offset":480,"length":23},"authors":[{"last":"Hewlett"},{"last":"Cohen"}],"year":"2009","references":["/references/7"]},{"style":0,"text":"Hewlett and Cohen, 2011","origin":{"pointer":"/sections/2/paragraphs/1","offset":505,"length":23},"authors":[{"last":"Hewlett"},{"last":"Cohen"}],"year":"2011","references":["/references/8"]},{"style":0,"text":"Chen et al., 2012","origin":{"pointer":"/sections/2/paragraphs/1","offset":547,"length":17},"authors":[{"last":"Chen"},{"last":"al."}],"year":"2012","references":["/references/4"]},{"style":0,"text":"Witten et al., 1999","origin":{"pointer":"/sections/3/paragraphs/0","offset":73,"length":19},"authors":[{"last":"Witten"},{"last":"al."}],"year":"1999","references":["/references/14"]},{"style":0,"text":"Chen et al. (2012)","origin":{"pointer":"/sections/3/paragraphs/1","offset":505,"length":18},"authors":[{"last":"Chen"},{"last":"al."}],"year":"2012","references":["/references/4"]},{"style":0,"text":"Chen et al., 2012","origin":{"pointer":"/sections/5/paragraphs/9","offset":413,"length":17},"authors":[{"last":"Chen"},{"last":"al."}],"year":"2012","references":["/references/4"]},{"style":0,"text":"Brent and Cartwright, 1996","origin":{"pointer":"/sections/6/paragraphs/0","offset":95,"length":26},"authors":[{"last":"Brent"},{"last":"Cartwright"}],"year":"1996","references":["/references/3"]},{"style":0,"text":"Bernstein-Ratner, 1987","origin":{"pointer":"/sections/6/paragraphs/0","offset":123,"length":22},"authors":[{"last":"Bernstein-Ratner"}],"year":"1987","references":["/references/0"]},{"style":0,"text":"MacWhinney and Snow, 1990","origin":{"pointer":"/sections/6/paragraphs/0","offset":201,"length":25},"authors":[{"last":"MacWhinney"},{"last":"Snow"}],"year":"1990","references":["/references/10"]},{"style":0,"text":"Chen et al., 2012","origin":{"pointer":"/sections/6/paragraphs/0","offset":376,"length":17},"authors":[{"last":"Chen"},{"last":"al."}],"year":"2012","references":["/references/4"]},{"style":0,"text":"Chen et al., 2012","origin":{"pointer":"/sections/6/paragraphs/3","offset":37,"length":17},"authors":[{"last":"Chen"},{"last":"al."}],"year":"2012","references":["/references/4"]},{"style":0,"text":"Johnson and Goldwater (2009)","origin":{"pointer":"/sections/6/paragraphs/9","offset":320,"length":28},"authors":[{"last":"Johnson"},{"last":"Goldwater"}],"year":"2009","references":["/references/9"]},{"style":0,"text":"Chen et al. (2012)","origin":{"pointer":"/sections/6/paragraphs/9","offset":449,"length":18},"authors":[{"last":"Chen"},{"last":"al."}],"year":"2012","references":["/references/4"]},{"style":0,"text":"Johnson and Goldwater (2009)","origin":{"pointer":"/sections/6/paragraphs/9","offset":508,"length":28},"authors":[{"last":"Johnson"},{"last":"Goldwater"}],"year":"2009","references":["/references/9"]},{"style":0,"text":"Börschinger and Johnson (2012)","origin":{"pointer":"/sections/6/paragraphs/9","offset":577,"length":30},"authors":[{"last":"Börschinger"},{"last":"Johnson"}],"year":"2012","references":["/references/2"]},{"style":0,"text":"Hewlett and Cohen (2011)","origin":{"pointer":"/sections/6/paragraphs/9","offset":703,"length":24},"authors":[{"last":"Hewlett"},{"last":"Cohen"}],"year":"2011","references":["/references/8"]},{"style":0,"text":"Mochihashi et al. (2009)","origin":{"pointer":"/sections/6/paragraphs/9","offset":777,"length":24},"authors":[{"last":"Mochihashi"},{"last":"al."}],"year":"2009","references":["/references/11"]},{"style":0,"text":"Zhikov et al. (2010)","origin":{"pointer":"/sections/6/paragraphs/9","offset":841,"length":20},"authors":[{"last":"Zhikov"},{"last":"al."}],"year":"2010","references":["/references/15"]},{"style":0,"text":"Börschinger and Johnson (2012)","origin":{"pointer":"/sections/6/paragraphs/9","offset":901,"length":30},"authors":[{"last":"Börschinger"},{"last":"Johnson"}],"year":"2012","references":["/references/2"]},{"style":0,"text":"Goldwater et al. (2009)","origin":{"pointer":"/sections/6/paragraphs/9","offset":972,"length":23},"authors":[{"last":"Goldwater"},{"last":"al."}],"year":"2009","references":["/references/6"]},{"style":0,"text":"Chen et al., 2012","origin":{"pointer":"/sections/7/paragraphs/0","offset":856,"length":17},"authors":[{"last":"Chen"},{"last":"al."}],"year":"2012","references":["/references/4"]},{"style":0,"text":"Johnson and Goldwater, 2009","origin":{"pointer":"/sections/7/paragraphs/0","offset":1042,"length":27},"authors":[{"last":"Johnson"},{"last":"Goldwater"}],"year":"2009","references":["/references/9"]},{"style":0,"text":"Emerson, 2005","origin":{"pointer":"/sections/7/paragraphs/1","offset":150,"length":13},"authors":[{"last":"Emerson"}],"year":"2005","references":["/references/5"]},{"style":0,"text":"Zhikov et al., 2010","origin":{"pointer":"/sections/7/paragraphs/1","offset":165,"length":19},"authors":[{"last":"Zhikov"},{"last":"al."}],"year":"2010","references":["/references/15"]}]}
