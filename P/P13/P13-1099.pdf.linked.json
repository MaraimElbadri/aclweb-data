{"sections":[{"title":"","paragraphs":["Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1004–1013, Sofia, Bulgaria, August 4-9 2013. c⃝2013 Association for Computational Linguistics"]},{"title":"Using Supervised Bigram-based ILP for Extractive Summarization Chen Li, Xian Qian, and Yang Liu The University of Texas at Dallas Computer Science Department chenli,qx,yangl@hlt.utdallas.edu Abstract","paragraphs":["In this paper, we propose a bigram based supervised method for extractive document summarization in the integer linear programming (ILP) framework. For each bigram, a regression model is used to estimate its frequency in the reference summary. The regression model uses a variety of indicative features and is trained discriminatively to minimize the distance between the estimated and the ground truth bigram frequency in the reference summary. During testing, the sentence selection problem is formulated as an ILP problem to maximize the bigram gains. We demonstrate that our system consistently outperforms the previous ILP method on different TAC data sets, and performs competitively compared to the best results in the TAC evaluations. We also conducted various analysis to show the impact of bigram selection, weight estimation, and ILP setup."]},{"title":"1 Introduction","paragraphs":["Extractive summarization is a sentence selection problem: identifying important summary sentences from one or multiple documents. Many methods have been developed for this problem, including supervised approaches that use classifiers to predict summary sentences, graph based approaches to rank the sentences, and recent global optimization methods such as integer linear programming (ILP) and submodular methods. These global optimization methods have been shown to be quite powerful for extractive summarization, because they try to select important sentences and remove redundancy at the same time under the length constraint.","Gillick and Favre (Gillick and Favre, 2009) introduced the concept-based ILP for summarization. Their system achieved the best result in the TAC 09 summarization task based on the ROUGE evaluation metric. In this approach the goal is to maximize the sum of the weights of the language concepts that appear in the summary. They used bigrams as such language concepts. The as-sociation between the language concepts and sentences serves as the constraints. This ILP method is formally represented as below (see (Gillick and Favre, 2009) for more details): max","∑ i wici (1)","s.t. sjOccij ≤ ci (2)","∑ j sjOccij ≥ ci (3) ∑","j ljsj ≤ L (4) ci ∈ {0, 1} ∀i (5) sj ∈ {0, 1} ∀j (6) ci and sj are binary variables (shown in (5) and (6)) that indicate the presence of a concept and a sentence respectively. wi is a concept’s weight and Occij means the occurrence of concept i in sentence j. Inequalities (2)(3) associate the sentences and concepts. They ensure that selecting a sentence leads to the selection of all the concepts it contains, and selecting a concept only happens when it is present in at least one of the selected sentences.","There are two important components in this concept-based ILP: one is how to select the concepts (ci); the second is how to set up their weights (wi). Gillick and Favre (Gillick and Favre, 2009) used bigrams as concepts, which are selected from a subset of the sentences, and their document frequency as the weight in the objective function.","In this paper, we propose to find a candidate summary such that the language concepts (e.g., bigrams) in this candidate summary and the reference summary can have the same frequency. We expect this restriction is more consistent with the 1004 ROUGE evaluation metric used for summarization (Lin, 2004). In addition, in the previous concept-based ILP method, the constraints are with respect to the appearance of language concepts, hence it cannot distinguish the importance of different language concepts in the reference summary. Our method can decide not only which language concepts to use in ILP, but also the frequency of these language concepts in the candidate summary. To estimate the bigram frequency in the summary, we propose to use a supervised regression model that is discriminatively trained using a variety of features. Our experiments on several TAC summarization data sets demonstrate this proposed method outperforms the previous ILP system and often the best performing TAC system."]},{"title":"2 Proposed Method 2.1 Bigram Gain Maximization by ILP","paragraphs":["We choose bigrams as the language concepts in our proposed method since they have been successfully used in previous work. In addition, we expect that the bigram oriented ILP is consistent with the ROUGE-2 measure widely used for summarization evaluation.","We start the description of our approach for the scenario where a human abstractive summary is provided, and the task is to select sentences to form an extractive summary. Then Our goal is to make the bigram frequency in this system summary as close as possible to that in the reference. For each bigram b, we define its gain: Gain(b, sum) = min{nb,ref , nb,sum} (7) where nb,ref is the frequency of b in the reference summary, and nb,sum is the frequency of b in the automatic summary. The gain of a bigram is no more than its frequency in the reference summary, hence adding redundant bigrams will not increase the gain.","The total gain of an extractive summary is defined as the sum of every bigram gain in the summary: Gain(sum) = ∑ b Gain(b, sum) = ∑ b min{nb,ref , ∑ s z(s) ∗ nb,s} (8) where s is a sentence in the document, nb,s is the frequency of b in sentence s, z(s) is a binary variable, indicating whether s is selected in the summary. The goal is to find z that maximizes Gain(sum) (formula (8)) under the length constraint L.","This problem can be casted as an ILP problem. First, using the fact that min{a, x} = 0.5(−|x − a| + x + a), x, a ≥ 0","we have ∑ b min{nb,ref , ∑ s z(s) ∗ nb,s} = ∑ b 0.5 ∗ (−|nb,ref − ∑ s z(s) ∗ nb,s| + nb,ref + ∑ s z(s) ∗ nb,s) Now the problem is equivalent to:","max z ∑ b (−|nb,ref − ∑ s z(s) ∗ nb,s| + nb,ref + ∑ s z(s) ∗ nb,s) s.t. ∑ s z(s) ∗ |S| ≤ L; z(s) ∈ {0, 1} This is equivalent to the ILP: max ∑ b (∑ s z(s) ∗ nb,s − Cb) (9) s.t. ∑ s z(s) ∗ |S| ≤ L (10) z(s) ∈ {0, 1} (11) −Cb ≤ nb,ref − ∑ s z(s) ∗ nb,s ≤ Cb (12) where Cb is an auxiliary variable we introduce that is equal to |nb,ref −","∑","s z(s) ∗ nb,s|, and nb,ref is","a constant that can be dropped from the objective","function.","2.2 Regression Model for Bigram Frequency Estimation In the previous section, we assume that nb,ref is at hand (reference abstractive summary is given) and propose a bigram-based optimization frame-work for extractive summarization. However, for the summarization task, the bigram frequency is unknown, and thus our first goal is to estimate such frequency. We propose to use a regression model for this.","Since a bigram’s frequency depends on the summary length (L), we use a normalized frequency 1005 in our method. Let nb,ref = Nb,ref ∗ L, where Nb,ref =","n(b,ref)","∑","b n(b,ref) is the normalized frequency in the summary. Now the problem is to automatically estimate Nb,ref . Since the normalized frequency Nb,ref is a real number, we choose to use a logistic regression model to predict it: Nb,ref = exp{w′","f (b)}","∑ j exp{w′","f (bj)} (13) where f (bj) is the feature vector of bigram bj and w′","is the corresponding feature weight. Since even for identical bigrams bi = bj , their feature vectors may be different (f (bi) ̸= f (bj)) due to their different contexts, we sum up frequencies for identical bigrams {bi|bi = b}: Nb,ref = ∑ i,bi=b Nbi,ref =","∑ i,bi=b exp{w′","f (b i)}","∑","j exp{w′","f (bj)} (14)","To train this regression model using the given reference abstractive summaries, rather than trying to minimize the squared error as typically done, we propose a new objective function. Since the normalized frequency satisfies the probability constraint","∑","b Nb,ref = 1, we propose to use KL divergence to measure the distance between the estimated frequencies and the ground truth values. The objective function for training is thus to minimize the KL distance: min ∑ b Ñb,ref log Ñb,ref Nb,ref (15) where Ñb,ref is the true normalized frequency of bigram b in reference summaries.","Finally, we replace Nb,ref in Formula (15) with Eq (14) and get the objective function below: max ∑ b Ñb,ref log","∑ i,bi=b exp{w′","f (b i)}","∑ j exp{w′","f (bj)} (16)","This shares the same form as the contrastive estimation proposed by (Smith and Eisner, 2005). We use gradient decent method for parameter estimation, initial w is set with zero. 2.3 Features Each bigram is represented using a set of features in the above regression model. We use two types of features: word level and sentence level features. Some of these features have been used in previous work (Aker and Gaizauskas, 2009; Brandow et al., 1995; Edmundson, 1969; Radev, 2001):","• Word Level: – 1. Term frequency1: The frequency of","this bigram in the given topic.","– 2. Term frequency2: The frequency of this bigram in the selected sentences1",".","– 3. Stop word ratio: Ratio of stop words in this bigram. The value can be {0, 0.5, 1}.","– 4. Similarity with topic title: The number of common tokens in these two strings, divided by the length of the longer string.","– 5. Similarity with description of the topic: Similarity of the bigram with topic description (see next data section about the given topics in the summarization task).","• Sentence Level: (information of sentence","containing the bigram) – 6. Sentence ratio: Number of sentences","that include this bigram, divided by the","total number of the selected sentences.","– 7. Sentence similarity: Sentence similarity with topic’s query, which is the concatenation of topic title and description.","– 8. Sentence position: Sentence posi-tion in the document.","– 9. Sentence length: The number of words in the sentence.","– 10. Paragraph starter: Binary feature indicating whether this sentence is the beginning of a paragraph."]},{"title":"3 Experiments 3.1 Data","paragraphs":["We evaluate our method using several recent TAC data sets, from 2008 to 2011. The TAC summarization task is to generate at most 100 words summaries from 10 documents for a given topic query (with a title and more detailed description). For model training, we also included two years’ DUC data (2006 and 2007). When evaluating on one TAC data set, we use the other years of the TAC data plus the two DUC data sets as the training data. 1 See next section about the sentence selection step 1006 3.2 Summarization System We use the same system pipeline described in (Gillick et al., 2008; McDonald, 2007). The key modules in the ICSI ILP system (Gillick et al., 2008) are briefly described below.","• Step 1: Clean documents, split text into sentences.","• Step 2: Extract bigrams from all the sentences, then select those bigrams with document frequency equal to more than 3. We call this subset as initial bigram set in the following.","• Step 3: Select relevant sentences that contain at least one bigram from the initial bigram set.","• Step 4: Feed the ILP with sentences and the bigram set to get the result.","• Step 5: Order sentences identified by ILP as the final result of summary.","The difference between the ICSI and our system is in the 4th step. In our method, we first extract all the bigrams from the selected sentences and then estimate each bigram’s Nb,ref using the regression model. Then we use the top-n bigrams with their Nb,ref and all the selected sentences in our proposed ILP module for summary sentence selection. When training our bigram regression model, we use each of the 4 reference summaries separately, i.e., the bigram frequency is obtained from one reference summary. The same pre-selection of sentences described above is also applied in training, that is, the bigram instances used in training are from these selected sentences and the reference summary."]},{"title":"4 Experiment and Analysis 4.1 Experimental Results","paragraphs":["Table 1 shows the ROUGE-2 results of our proposed system, the ICSI system, and also the best performing system in the NIST TAC evaluation. We can see that our proposed system consistently outperforms ICSI ILP system (the gain is statistically significant based on ROUGE’s 95% confidence internal results). Compared to the best reported TAC result, our method has better performance on three data sets, except 2011 data. Note that the best performing system for the 2009 data is the ICSI ILP system, with an additional compression step. Our ILP method is purely extractive. Even without using compression, our approach performs better than the full ICSI system. The best performing system for the 2011 data also has some compression module. We expect that after applying sentence compression and merging, we will have even better performance, however, our focus in this paper is on the bigram-based extractive summarization.","ICSI Proposed TAC Rank1","ILP System System 2008 0.1023 0.1076 0.1038 2009 0.1160 0.1246 0.1216 2010 0.1003 0.1067 0.0957 2011 0.1271 0.1327 0.1344 Table 1: ROUGE-2 summarization results.","There are several differences between the ICSI system and our proposed method. First is the bigrams (concepts) used. We use the top 100 bigrams from our bigram estimation module; whereas the ICSI system just used the initial bigram set described in Section 3.2. Second, the weights for those bigrams differ. We used the estimated value from the regression model; the ICSI system just uses the bigram’s document frequency in the original text as weight. Finally, two systems use different ILP setups. To analyze which factors (or all of them) explain the performance difference, we conducted various controlled experiments for these three factors (bigrams, weights, ILP). All of the following experiments use the TAC 2009 data as the test set. 4.2 Effect of Bigram Weights In this experiment, we vary the weighting methods for the two systems: our proposed method and the ICSI system. We use three weighting setups: the estimated bigram frequency value in our method, document frequency, or term frequency from the original text. Table 2 and 3 show the results using the top 100 bigrams from our system and the initial bigram set from the ICSI system respectively. We also evaluate using the two different ILP configurations in these experiments.","First of all, we can see that for both ILP systems, our estimated bigram weights outperform the other frequency-based weights. For the ICSI ILP system, using bigram document frequency achieves better performance than term frequency (which verified why document frequency is used in their system). In contrast, for our ILP method, 1007 # Weight ILP ROUGE-2 1 Estimated value","Proposed 0.1246 2 ICSI 0.1178 3 Document freq","Proposed 0.1109 4 ICSI 0.1132 5 Term freq","Proposed 0.1116 6 ICSI 0.1080 Table 2: Results using different weighting methods on the top 100 bigrams generated from our proposed system. # Weight ILP ROUGE-2 1 Estimated value","Proposed 0.1157 2 ICSI 0.1161 3 Document freq","Proposed 0.1101 4 ICSI 0.1160 5 Term freq","Proposed 0.1109 6 ICSI 0.1072 Table 3: Results using different weighting methods based on the initial bigram sets. The average number of bigrams is around 80 for each topic. the bigram’s term frequency is slightly more use-ful than its document frequency. This indicates that our estimated value is more related to bigram’s term frequency in the original text. When the weight is document frequency, the ICSI’s result is better than our proposed ILP; whereas when using term frequency as the weights, our ILP has better results, again suggesting term frequency fits our ILP system better. When the weight is estimated value, the results depend on the bigram set used. The ICSI’s ILP performs slightly better than ours when it is equipped with the initial bigram, but our proposed ILP has much better results using our selected top100 bigrams. This shows that the size and quality of the bigrams has an impact on the ILP modules. 4.3 The Effect of Bigram Set’s size In our proposed system, we use 100 top bigrams. There are about 80 bigrams used in the ICSI ILP system. A natural question to ask is the impact of the number of bigrams and their quality on the summarization system. Table 4 shows some statistics of the bigrams. We can see that about one third of bigrams in the reference summary are in the original text (127.3 out of 321.93), verifying that people do use different words/bigram when writing abstractive summaries. We mentioned that we only use the top-N (n is 100 in previous experiments) bigrams in our summarization system. On one hand, this is to save computational cost for the ILP module. On the other hand, we see from the table that only 127 of these more than 2K bigrams are in the reference summary and are thus expected to help the summary responsiveness. In-cluding all the bigrams would lead to huge noise.","# bigrams in ref summary 321.93","# bigrams in text and ref summary 127.3","# bigrams used in our regression model 2140.7","(i.e., in selected sentences) Table 4: Bigram statistics. The numbers are the average ones for each topic.","Fig 1 shows the bigram coverage (number of bigrams used in the system that are also in reference summaries) when we vary N selected bigrams. As expected, we can see that as n increases, there are more reference summary bigrams included in the system. There are 25 summary bigrams in the top-50 bigrams and about 38 in top-100 bigrams. Compared with the ICSI system that has around 80 bigrams in the initial bigram set and 29 in the reference summary, our estimation module has better coverage. 0102030405060708090100110120130","50 500 950 1400 1850 2300 2750 3200 Number of Selected Bigram N u m b e r o f B i g r a m b o t h i n S e l e c t e d a n d R e f e r e n c e Figure 1: Coverage of bigrams (number of bigrams in reference summary) when varying the number of bigrams used in the ILP systems.","Increasing the number of bigrams used in the system will lead to better coverage, however, the incorrect bigrams also increase and have a negative impact on the system performance. To examine the best tradeoff, we conduct the experiments by choosing the different top-N bigram set for the two ILP systems, as shown in Fig 2. For both the ILP systems, we used the estimated weight value for the bigrams. 1008","We can see that the ICSI ILP system performs better when the input bigrams have less noise (those bigrams that are not in summary). However, our proposed method is slightly more robust to this kind of noise, possibly because of the weights we use in our system – the noisy bigrams have lower weights and thus less impact on the final system performance. Overall the two systems have similar trends: performance increases at the beginning when using more bigrams, and after certain points starts degrading with too many bigrams. The optimal number of bigrams differs for the two systems, with a larger number of bigrams in our method. We also notice that the ICSI ILP system achieved a ROUGE-2 of 0.1218 when using top 60 bigrams, which is better than using the initial bigram set in their method (0.1160). 0.1090.1110.1130.1150.1170.1190.1210.1230.125","40 50 60 70 80 90 100 110 120 130 Number of selected bigram R o u g e - 2 Proposed ILP ICSI Figure 2: Summarization performance when varying the number of bigrams for two systems. 4.4 Oracle Experiments Based on the above analysis, we can see the impact of the bigram set and their weights. The following experiments are designed to demonstrate the best system performance we can achieve if we have access to good quality bigrams and weights. Here we use the information from the reference summary.","The first is an oracle experiment, where we use all the bigrams from the reference summaries that are also in the original text. In the ICSI ILP system, the weights are the document frequency from the multiple reference summaries. In our ILP module, we use the term frequency of the bigram. The oracle results are shown in Table 5. We can see these are significantly better than the automatic systems.","From Table 5, we notice that ICSI’s ILP performs marginally better than our proposed ILP. We hypothesize that one reason may be that many bigrams in the summary reference only appear once. Table 6 shows the frequency of the bigrams in the summary. Indeed 85% of bigram only appear once ILP System ROUGE-2 Our ILP 0.2124 ICSI ILP 0.2128 Table 5: Oracle experiment: using bigrams and their frequencies in the reference summary as weights. and no bigrams appear more than 9 times. For the majority of the bigrams, our method and the ICSI ILP are the same. For the others, our system has slight disadvantage when using the reference term frequency. We expect the high term frequency may need to be properly smoothed/normalized. Freq 1 2 3 4 5 6 7 8 9 Ave# 277 32 7.5 3.2 1.1 0.3 0.1 0.1 0.04 Table 6: Average number of bigrams for each term frequency in one topic’s reference summary.","We also treat the oracle results as the gold standard for extractive summarization and compared how the two automatic summarization systems differ at the sentence level. This is different from the results in Table 1, which are the ROUGE results comparing to human written abstractive summaries at the n-gram level. We found that among the 188 sentences in this gold standard, our system hits 31 and ICSI only has 23. This again shows that our system has better performance, not just at the word level based on ROUGE measures, but also at the sentence level. There are on average 3 different sentences per topic between these two results.","In the second experiment, after we obtain the estimated Nb,ref for every bigram in the selected sentences from our regression model, we only keep those bigrams that are in the reference summary, and use the estimated weights for both ILP modules. Table 7 shows the results. We can consider these as the upper bound the system can achieve if we use the automatically estimated weights for the correct bigrams. In this experiment ICSI ILP’s performance still performs better than ours. This might be attributed to the fact there is less noise (all the bigrams are the correct ones) and thus the ICSI ILP system performs well. We can see that these results are worse than the previous oracle experiments, but are better than using the automatically generated bigrams, again show-ing the bigram and weight estimation is critical for 1009 summarization. # Weight ILP ROUGE-2 1 Estimated value","Proposed 0.1888 2 ICSI 0.1942 Table 7: Summarization results when using the estimated weights and only keeping the bigrams that are in the reference summary. 4.5 Effect of Training Set Since our method uses supervised learning, we conduct the experiment to show the impact of training size. In TAC’s data, each topic has two sets of documents. For set A, the task is a standard summarization, and there are 4 reference summaries, each 100 words long; for set B, it is an up-date summarization task – the summary includes information not mentioned in the summary from set A. There are also 4 reference summaries, with 400 words in total. Table 8 shows the results on 2009 data when using the data from different years and different sets for training. We notice that when the training data only contains set A, the performance is always better than using set B or the combined set A and B. This is not surprising because of the different task definition. Therefore, for the rest of the study on data size impact, we only use data set A from the TAC data and the DUC data as the training set. In total there are about 233 topics from the two years’ DUC data (06, 07) and three years’ TAC data (08, 10, 11). We incrementally add 20 topics every time (from DUC06 to TAC11) and plot the learning curve, as shown in Fig 3. As expected, more training data results in better performance. Training Set # Topics ROUGE-2 08 Corpus (A) 48 0.1192 08 Corpus( B) 48 0.1178 08 Corpus (A+B) 96 0.1188 10 Corpus (A) 46 0.1174 10 Corpus (B) 46 0.1167 10 Corpus (A+B) 92 0.1170 11 Corpus (A) 44 0.1157 11 Corpus (B) 44 0.1130 11 Corpus (A+B) 88 0.1140 Table 8: Summarization performance when using different training corpora. 0.1120.1130.1140.1150.1160.1170.1180.1190.120.1210.1220.1230.1240.125","20 40 60 80 100 120 140 160 180 200 220 240 Number of trainning topics R o u g e - 2 Figure 3: Learning curve 4.6 Summary of Analysis The previous experiments have shown the impact of the three factors: the quality of the bigrams themselves, the weights used for these bigrams, and the ILP module. We found that the bigrams and their weights are critical for both the ILP setups. However, there is negligible difference between the two ILP methods.","An important part of our system is the supervised method for bigram and weight estimation. We have already seen for the previous ILP method, when using our bigrams together with the weights, better performance can be achieved. Therefore we ask the question whether this is simply because we use supervised learning, or whether our proposed regression model is the key. To answer this, we trained a simple supervised binary classifier for bigram prediction (positive means that a bigram appears in the summary) using the same set of features as used in our bigram weight estimation module, and then used their document frequency in the ICSI ILP system. The result for this method is 0.1128 on the TAC 2009 data. This is much lower than our result. We originally expected that using the supervised method may outperform the unsupervised bigram selection which only uses term frequency information. Further experiments are needed to investigate this. From this we can see that it is not just the supervised methods or using annotated data that yields the over-all improved system performance, but rather our proposed regression setup for bigrams is the main reason."]},{"title":"5 Related Work","paragraphs":["We briefly describe some prior work on summarization in this section. Unsupervised methods have been widely used. In particular, recently several optimization approaches have demonstrated 1010 competitive performance for extractive summarization task. Maximum marginal relevance (MMR) (Carbonell and Goldstein, 1998) uses a greedy algorithm to find summary sentences. (Mc-Donald, 2007) improved the MMR algorithm to dynamic programming. They used a modified objective function in order to consider whether the selected sentence is globally optimal. Sentence-level ILP was also first introduced in (McDonald, 2007), but (Gillick and Favre, 2009) revised it to concept-based ILP. (Woodsend and Lapata, 2012) utilized ILP to jointly optimize different as-pects including content selection, surface realiza-tion, and rewrite rules in summarization. (Galanis et al., 2012) uses ILP to jointly maximize the importance of the sentences and their diversity in the summary. (Berg-Kirkpatrick et al., 2011) applied a similar idea to conduct the sentence compression and extraction for multiple document summarization. (Jin et al., 2010) made a comparative study on sentence/concept selection and pairwise and list ranking algorithms, and concluded ILP performed better than MMR and the diversity penalty strategy in sentence/concept selection. Other global optimization methods include submodularity (Lin and Bilmes, 2010) and graph-based approaches (Erkan and Radev, 2004; Leskovec et al., 2005; Mihalcea and Tarau, 2004). Various unsupervised probabilistic topic models have also been investigated for summarization and shown promising. For example, (Celikyilmaz and Hakkani-T ür, 2011) used it to model the hidden abstract concepts across documents as well as the correlation between these concepts to generate topically coherent and non-redundant summaries. (Darling and Song, 2011) applied it to separate the semantically important words from the lowcontent function words.","In contrast to these unsupervised approaches, there are also various efforts on supervised learning for summarization where a model is trained to predict whether a sentence is in the summary or not. Different features and classifiers have been explored for this task, such as Bayesian method (Kupiec et al., 1995), maximum entropy (Osborne, 2002), CRF (Galley, 2006), and recently reinforce-ment learning (Ryang and Abekawa, 2012). (Aker et al., 2010) used discriminative reranking on multiple candidates generated by A* search. Recently, research has also been performed to address some issues in the supervised setup, such as the class data imbalance problem (Xie and Liu, 2010).","In this paper, we propose to incorporate the supervised method into the concept-based ILP framework. Unlike previous work using sentence-based supervised learning, we use a regression model to estimate the bigrams and their weights, and use these to guide sentence selection. Compared to the direct sentence-based classification or regression methods mentioned above, our method has an advantage. When abstractive summaries are given, one needs to use that information to automatically generate reference labels (a sentence is in the summary or not) for extractive summarization. Most researchers have used the similarity between a sentence in the document and the abstractive summary for labeling. This is not a perfect process. In our method, we do not need to generate this extra label for model training since ours is based on bigrams – it is straightforward to obtain the reference frequency for bigrams by simply looking at the reference summary. We expect our approach also paves an easy way for future automatic abstractive summarization. One previous study that is most related to ours is (Conroy et al., 2011), which utilized a Naive Bayes classifier to predict the probability of a bigram, and applied ILP for the final sentence selection. They used more features than ours, whereas we use a discriminatively trained regression model and a modified ILP framework. Our proposed method performs better than their reported results in TAC 2011 data. Another study closely related to ours is (Davis et al., 2012), which leveraged Latent Semantic Analysis (LSA) to produce term weights and selected summary sentences by computing an approximate solution to the Budgeted Maximal Coverage problem."]},{"title":"6 Conclusion and Future Work","paragraphs":["In this paper, we leverage the ILP method as a core component in our summarization system. Different from the previous ILP summarization approach, we propose a supervised learning method (a discriminatively trained regression model) to determine the importance of the bigrams fed to the ILP module. In addition, we revise the ILP to maximize the bigram gain (which is expected to be highly correlated with ROUGE-2 scores) rather than the concept/bigram coverage. Our proposed method yielded better results than the previous state-of-the-art ILP system on different TAC data 1011 sets. From a series of experiments, we found that there is little difference between the two ILP modules, and that the improved system performance is attributed to the fact that our proposed supervised bigram estimation module can successfully gather the important bigram and assign them appropriate weights. There are several directions that warrant further research. We plan to consider the context of bigrams to better predict whether a bigram is in the reference summary. We will also investigate the relationship between concepts and sentences, which may help move towards abstractive summarization."]},{"title":"Acknowledgments","paragraphs":["This work is partly supported by DARPA under Contract No. HR0011-12-C-0016 and FA8750-13-2-0041, and NSF IIS-0845484. Any opinions expressed in this material are those of the authors and do not necessarily reflect the views of DARPA or NSF."]},{"title":"References","paragraphs":["Ahmet Aker and Robert Gaizauskas. 2009. Summary generation for toponym-referenced images using object type language models. In Proceedings of the International Conference RANLP.","Ahmet Aker, Trevor Cohn, and Robert Gaizauskas. 2010. Multi-document summarization using a* search and discriminative training. In Proceedings of the EMNLP.","Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein. 2011. Jointly learning to extract and compress. In Proceedings of the ACL.","Ronald Brandow, Karl Mitze, and Lisa F. Rau. 1995. Automatic condensation of electronic publications by sentence selection. Inf. Process. Manage.","Jaime Carbonell and Jade Goldstein. 1998. The use of mmr, diversity-based reranking for reordering documents and producing summaries. In Proceedings of the SIGIR.","Asli Celikyilmaz and Dilek Hakkani-Tür. 2011. Discovery of topically coherent sentences for extractive summarization. In Proceedings of the ACL.","John M. Conroy, Judith D. Schlesinger, Jeff Kubina, Peter A. Rankel, and Dianne P. O’Leary. 2011. Classy 2011 at tac: Guided and multi-lingual summaries and evaluation metrics. In Proceedings of the TAC.","William M. Darling and Fei Song. 2011. Probabilistic document modeling for syntax removal in text summarization. In Proceedings of the ACL.","Sashka T. Davis, John M. Conroy, and Judith D. Schlesinger. 2012. Occams - an optimal combinatorial covering algorithm for multi-document summarization. In Proceedings of the ICDM.","H. P. Edmundson. 1969. New methods in automatic extracting. J. ACM.","Günes Erkan and Dragomir R. Radev. 2004. Lexrank: graph-based lexical centrality as salience in text summarization. J. Artif. Int. Res.","Dimitrios Galanis, Gerasimos Lampouras, and Ion Androutsopoulos. 2012. Extractive multi-document summarization with integer linear programming and support vector regression. In Proceedings of the COLING.","Michel Galley. 2006. A skip-chain conditional random field for ranking meeting utterances by importance. In Proceedings of the EMNLP.","Dan Gillick and Benoit Favre. 2009. A scalable global model for summarization. In Proceedings of the Workshop on Integer Linear Programming for Natural Langauge Processing on NAACL.","Dan Gillick, Benoit Favre, and Dilek Hakkani-Tür. 2008. In The ICSI Summarization System at TAC 2008.","Feng Jin, Minlie Huang, and Xiaoyan Zhu. 2010. A comparative study on ranking and selection strategies for multi-document summarization. In Proceedings of the COLING.","Julian Kupiec, Jan Pedersen, and Francine Chen. 1995. A trainable document summarizer. In Proceedings of the SIGIR.","Jure Leskovec, Natasa Milic-Frayling, and Marko Grobelnik. 2005. Impact of linguistic analysis on the semantic graph coverage and learning of document extracts. In Proceedings of the AAAI.","Hui Lin and Jeff Bilmes. 2010. Multi-document summarization via budgeted maximization of submodular functions. In Proceedings of the NAACL.","Chin-Yew Lin. 2004. Rouge: a package for automatic evaluation of summaries. In Proceedings of the ACL.","Ryan McDonald. 2007. A study of global inference algorithms in multi-document summarization. In Proceedings of the European conference on IR research.","Rada Mihalcea and Paul Tarau. 2004. Textrank: Bringing order into text. In Proceedings of the EMNLP.","Miles Osborne. 2002. Using maximum entropy for sentence extraction. In Proceedings of the ACL-02 Workshop on Automatic Summarization. 1012","Dragomir R. Radev. 2001. Experiments in single and multidocument summarization using mead. In In First Document Understanding Conference.","Seonggi Ryang and Takeshi Abekawa. 2012. Frame-work of automatic text summarization using reinforcement learning. In Proceedings of the EMNLP.","Noah A. Smith and Jason Eisner. 2005. Contrastive estimation: training log-linear models on unlabeled data. In Proceedings of the ACL.","Kristian Woodsend and Mirella Lapata. 2012. Multiple aspect summarization using integer linear programming. In Proceedings of the EMNLP.","Shasha Xie and Yang Liu. 2010. Improving supervised learning for meeting summarization using sampling and regression. Comput. Speech Lang. 1013"]}],"references":[{"authors":[{"first":"Ahmet","last":"Aker"},{"first":"Robert","last":"Gaizauskas"}],"year":"2009","title":"Summary generation for toponym-referenced images using object type language models","source":"Ahmet Aker and Robert Gaizauskas. 2009. Summary generation for toponym-referenced images using object type language models. In Proceedings of the International Conference RANLP."},{"authors":[{"first":"Ahmet","last":"Aker"},{"first":"Trevor","last":"Cohn"},{"first":"Robert","last":"Gaizauskas"}],"year":"2010","title":"Multi-document summarization using a* search and discriminative training","source":"Ahmet Aker, Trevor Cohn, and Robert Gaizauskas. 2010. Multi-document summarization using a* search and discriminative training. In Proceedings of the EMNLP."},{"authors":[{"first":"Taylor","last":"Berg-Kirkpatrick"},{"first":"Dan","last":"Gillick"},{"first":"Dan","last":"Klein"}],"year":"2011","title":"Jointly learning to extract and compress","source":"Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein. 2011. Jointly learning to extract and compress. In Proceedings of the ACL."},{"authors":[{"first":"Ronald","last":"Brandow"},{"first":"Karl","last":"Mitze"},{"first":"Lisa","middle":"F.","last":"Rau"}],"year":"1995","title":"Automatic condensation of electronic publications by sentence selection","source":"Ronald Brandow, Karl Mitze, and Lisa F. Rau. 1995. Automatic condensation of electronic publications by sentence selection. Inf. Process. Manage."},{"authors":[{"first":"Jaime","last":"Carbonell"},{"first":"Jade","last":"Goldstein"}],"year":"1998","title":"The use of mmr, diversity-based reranking for reordering documents and producing summaries","source":"Jaime Carbonell and Jade Goldstein. 1998. The use of mmr, diversity-based reranking for reordering documents and producing summaries. In Proceedings of the SIGIR."},{"authors":[{"first":"Asli","last":"Celikyilmaz"},{"first":"Dilek","last":"Hakkani-Tür"}],"year":"2011","title":"Discovery of topically coherent sentences for extractive summarization","source":"Asli Celikyilmaz and Dilek Hakkani-Tür. 2011. Discovery of topically coherent sentences for extractive summarization. In Proceedings of the ACL."},{"authors":[{"first":"John","middle":"M.","last":"Conroy"},{"first":"Judith","middle":"D.","last":"Schlesinger"},{"first":"Jeff","last":"Kubina"},{"first":"Peter","middle":"A.","last":"Rankel"},{"first":"Dianne","middle":"P.","last":"O’Leary"}],"year":"2011","title":"Classy 2011 at tac: Guided and multi-lingual summaries and evaluation metrics","source":"John M. Conroy, Judith D. Schlesinger, Jeff Kubina, Peter A. Rankel, and Dianne P. O’Leary. 2011. Classy 2011 at tac: Guided and multi-lingual summaries and evaluation metrics. In Proceedings of the TAC."},{"authors":[{"first":"William","middle":"M.","last":"Darling"},{"first":"Fei","last":"Song"}],"year":"2011","title":"Probabilistic document modeling for syntax removal in text summarization","source":"William M. Darling and Fei Song. 2011. Probabilistic document modeling for syntax removal in text summarization. In Proceedings of the ACL."},{"authors":[{"first":"Sashka","middle":"T.","last":"Davis"},{"first":"John","middle":"M.","last":"Conroy"},{"first":"Judith","middle":"D.","last":"Schlesinger"}],"year":"2012","title":"Occams - an optimal combinatorial covering algorithm for multi-document summarization","source":"Sashka T. Davis, John M. Conroy, and Judith D. Schlesinger. 2012. Occams - an optimal combinatorial covering algorithm for multi-document summarization. In Proceedings of the ICDM."},{"authors":[{"first":"H.","middle":"P.","last":"Edmundson"}],"year":"1969","title":"New methods in automatic extracting","source":"H. P. Edmundson. 1969. New methods in automatic extracting. J. ACM."},{"authors":[{"first":"Günes","last":"Erkan"},{"first":"Dragomir","middle":"R.","last":"Radev"}],"year":"2004","title":"Lexrank: graph-based lexical centrality as salience in text summarization","source":"Günes Erkan and Dragomir R. Radev. 2004. Lexrank: graph-based lexical centrality as salience in text summarization. J. Artif. Int. Res."},{"authors":[{"first":"Dimitrios","last":"Galanis"},{"first":"Gerasimos","last":"Lampouras"},{"first":"Ion","last":"Androutsopoulos"}],"year":"2012","title":"Extractive multi-document summarization with integer linear programming and support vector regression","source":"Dimitrios Galanis, Gerasimos Lampouras, and Ion Androutsopoulos. 2012. Extractive multi-document summarization with integer linear programming and support vector regression. In Proceedings of the COLING."},{"authors":[{"first":"Michel","last":"Galley"}],"year":"2006","title":"A skip-chain conditional random field for ranking meeting utterances by importance","source":"Michel Galley. 2006. A skip-chain conditional random field for ranking meeting utterances by importance. In Proceedings of the EMNLP."},{"authors":[{"first":"Dan","last":"Gillick"},{"first":"Benoit","last":"Favre"}],"year":"2009","title":"A scalable global model for summarization","source":"Dan Gillick and Benoit Favre. 2009. A scalable global model for summarization. In Proceedings of the Workshop on Integer Linear Programming for Natural Langauge Processing on NAACL."},{"authors":[{"first":"Dan","last":"Gillick"},{"first":"Benoit","last":"Favre"},{"first":"Dilek","last":"Hakkani-Tür"}],"year":"2008","title":"In The ICSI Summarization System at TAC 2008","source":"Dan Gillick, Benoit Favre, and Dilek Hakkani-Tür. 2008. In The ICSI Summarization System at TAC 2008."},{"authors":[{"first":"Feng","last":"Jin"},{"first":"Minlie","last":"Huang"},{"first":"Xiaoyan","last":"Zhu"}],"year":"2010","title":"A comparative study on ranking and selection strategies for multi-document summarization","source":"Feng Jin, Minlie Huang, and Xiaoyan Zhu. 2010. A comparative study on ranking and selection strategies for multi-document summarization. In Proceedings of the COLING."},{"authors":[{"first":"Julian","last":"Kupiec"},{"first":"Jan","last":"Pedersen"},{"first":"Francine","last":"Chen"}],"year":"1995","title":"A trainable document summarizer","source":"Julian Kupiec, Jan Pedersen, and Francine Chen. 1995. A trainable document summarizer. In Proceedings of the SIGIR."},{"authors":[{"first":"Jure","last":"Leskovec"},{"first":"Natasa","last":"Milic-Frayling"},{"first":"Marko","last":"Grobelnik"}],"year":"2005","title":"Impact of linguistic analysis on the semantic graph coverage and learning of document extracts","source":"Jure Leskovec, Natasa Milic-Frayling, and Marko Grobelnik. 2005. Impact of linguistic analysis on the semantic graph coverage and learning of document extracts. In Proceedings of the AAAI."},{"authors":[{"first":"Hui","last":"Lin"},{"first":"Jeff","last":"Bilmes"}],"year":"2010","title":"Multi-document summarization via budgeted maximization of submodular functions","source":"Hui Lin and Jeff Bilmes. 2010. Multi-document summarization via budgeted maximization of submodular functions. In Proceedings of the NAACL."},{"authors":[{"first":"Chin-Yew","last":"Lin"}],"year":"2004","title":"Rouge: a package for automatic evaluation of summaries","source":"Chin-Yew Lin. 2004. Rouge: a package for automatic evaluation of summaries. In Proceedings of the ACL."},{"authors":[{"first":"Ryan","last":"McDonald"}],"year":"2007","title":"A study of global inference algorithms in multi-document summarization","source":"Ryan McDonald. 2007. A study of global inference algorithms in multi-document summarization. In Proceedings of the European conference on IR research."},{"authors":[{"first":"Rada","last":"Mihalcea"},{"first":"Paul","last":"Tarau"}],"year":"2004","title":"Textrank: Bringing order into text","source":"Rada Mihalcea and Paul Tarau. 2004. Textrank: Bringing order into text. In Proceedings of the EMNLP."},{"authors":[{"first":"Miles","last":"Osborne"}],"year":"2002","title":"Using maximum entropy for sentence extraction","source":"Miles Osborne. 2002. Using maximum entropy for sentence extraction. In Proceedings of the ACL-02 Workshop on Automatic Summarization. 1012"},{"authors":[{"first":"Dragomir","middle":"R.","last":"Radev"}],"year":"2001","title":"Experiments in single and multidocument summarization using mead","source":"Dragomir R. Radev. 2001. Experiments in single and multidocument summarization using mead. In In First Document Understanding Conference."},{"authors":[{"first":"Seonggi","last":"Ryang"},{"first":"Takeshi","last":"Abekawa"}],"year":"2012","title":"Frame-work of automatic text summarization using reinforcement learning","source":"Seonggi Ryang and Takeshi Abekawa. 2012. Frame-work of automatic text summarization using reinforcement learning. In Proceedings of the EMNLP."},{"authors":[{"first":"Noah","middle":"A.","last":"Smith"},{"first":"Jason","last":"Eisner"}],"year":"2005","title":"Contrastive estimation: training log-linear models on unlabeled data","source":"Noah A. Smith and Jason Eisner. 2005. Contrastive estimation: training log-linear models on unlabeled data. In Proceedings of the ACL."},{"authors":[{"first":"Kristian","last":"Woodsend"},{"first":"Mirella","last":"Lapata"}],"year":"2012","title":"Multiple aspect summarization using integer linear programming","source":"Kristian Woodsend and Mirella Lapata. 2012. Multiple aspect summarization using integer linear programming. In Proceedings of the EMNLP."},{"authors":[{"first":"Shasha","last":"Xie"},{"first":"Yang","last":"Liu"}],"year":"2010","title":"Improving supervised learning for meeting summarization using sampling and regression","source":"Shasha Xie and Yang Liu. 2010. Improving supervised learning for meeting summarization using sampling and regression. Comput. Speech Lang. 1013"}],"cites":[{"style":0,"text":"Gillick and Favre, 2009","origin":{"pointer":"/sections/2/paragraphs/1","offset":19,"length":23},"authors":[{"last":"Gillick"},{"last":"Favre"}],"year":"2009","references":["/references/13"]},{"style":0,"text":"Gillick and Favre, 2009","origin":{"pointer":"/sections/2/paragraphs/1","offset":510,"length":23},"authors":[{"last":"Gillick"},{"last":"Favre"}],"year":"2009","references":["/references/13"]},{"style":0,"text":"Gillick and Favre, 2009","origin":{"pointer":"/sections/2/paragraphs/6","offset":169,"length":23},"authors":[{"last":"Gillick"},{"last":"Favre"}],"year":"2009","references":["/references/13"]},{"style":0,"text":"Lin, 2004","origin":{"pointer":"/sections/2/paragraphs/7","offset":291,"length":9},"authors":[{"last":"Lin"}],"year":"2004","references":["/references/19"]},{"style":0,"text":"Smith and Eisner, 2005","origin":{"pointer":"/sections/3/paragraphs/32","offset":69,"length":22},"authors":[{"last":"Smith"},{"last":"Eisner"}],"year":"2005","references":["/references/25"]},{"style":0,"text":"Aker and Gaizauskas, 2009","origin":{"pointer":"/sections/3/paragraphs/32","offset":399,"length":25},"authors":[{"last":"Aker"},{"last":"Gaizauskas"}],"year":"2009","references":["/references/0"]},{"style":0,"text":"Brandow et al., 1995","origin":{"pointer":"/sections/3/paragraphs/32","offset":426,"length":20},"authors":[{"last":"Brandow"},{"last":"al."}],"year":"1995","references":["/references/3"]},{"style":0,"text":"Edmundson, 1969","origin":{"pointer":"/sections/3/paragraphs/32","offset":448,"length":15},"authors":[{"last":"Edmundson"}],"year":"1969","references":["/references/9"]},{"style":0,"text":"Radev, 2001","origin":{"pointer":"/sections/3/paragraphs/32","offset":465,"length":11},"authors":[{"last":"Radev"}],"year":"2001","references":["/references/23"]},{"style":0,"text":"Gillick et al., 2008","origin":{"pointer":"/sections/4/paragraphs/0","offset":564,"length":20},"authors":[{"last":"Gillick"},{"last":"al."}],"year":"2008","references":["/references/14"]},{"style":0,"text":"McDonald, 2007","origin":{"pointer":"/sections/4/paragraphs/0","offset":586,"length":14},"authors":[{"last":"McDonald"}],"year":"2007","references":["/references/20"]},{"style":0,"text":"Gillick et al., 2008","origin":{"pointer":"/sections/4/paragraphs/0","offset":643,"length":20},"authors":[{"last":"Gillick"},{"last":"al."}],"year":"2008","references":["/references/14"]},{"style":0,"text":"Carbonell and Goldstein, 1998","origin":{"pointer":"/sections/6/paragraphs/0","offset":286,"length":29},"authors":[{"last":"Carbonell"},{"last":"Goldstein"}],"year":"1998","references":["/references/4"]},{"style":0,"text":"Mc-Donald, 2007","origin":{"pointer":"/sections/6/paragraphs/0","offset":369,"length":15},"authors":[{"last":"Mc-Donald"}],"year":"2007","references":[]},{"style":0,"text":"McDonald, 2007","origin":{"pointer":"/sections/6/paragraphs/0","offset":598,"length":14},"authors":[{"last":"McDonald"}],"year":"2007","references":["/references/20"]},{"style":0,"text":"Gillick and Favre, 2009","origin":{"pointer":"/sections/6/paragraphs/0","offset":620,"length":23},"authors":[{"last":"Gillick"},{"last":"Favre"}],"year":"2009","references":["/references/13"]},{"style":0,"text":"Woodsend and Lapata, 2012","origin":{"pointer":"/sections/6/paragraphs/0","offset":679,"length":25},"authors":[{"last":"Woodsend"},{"last":"Lapata"}],"year":"2012","references":["/references/26"]},{"style":0,"text":"Galanis et al., 2012","origin":{"pointer":"/sections/6/paragraphs/0","offset":846,"length":20},"authors":[{"last":"Galanis"},{"last":"al."}],"year":"2012","references":["/references/11"]},{"style":0,"text":"Berg-Kirkpatrick et al., 2011","origin":{"pointer":"/sections/6/paragraphs/0","offset":966,"length":29},"authors":[{"last":"Berg-Kirkpatrick"},{"last":"al."}],"year":"2011","references":["/references/2"]},{"style":0,"text":"Jin et al., 2010","origin":{"pointer":"/sections/6/paragraphs/0","offset":1109,"length":16},"authors":[{"last":"Jin"},{"last":"al."}],"year":"2010","references":["/references/15"]},{"style":0,"text":"Lin and Bilmes, 2010","origin":{"pointer":"/sections/6/paragraphs/0","offset":1391,"length":20},"authors":[{"last":"Lin"},{"last":"Bilmes"}],"year":"2010","references":["/references/18"]},{"style":0,"text":"Erkan and Radev, 2004","origin":{"pointer":"/sections/6/paragraphs/0","offset":1441,"length":21},"authors":[{"last":"Erkan"},{"last":"Radev"}],"year":"2004","references":["/references/10"]},{"style":0,"text":"Leskovec et al., 2005","origin":{"pointer":"/sections/6/paragraphs/0","offset":1464,"length":21},"authors":[{"last":"Leskovec"},{"last":"al."}],"year":"2005","references":["/references/17"]},{"style":0,"text":"Mihalcea and Tarau, 2004","origin":{"pointer":"/sections/6/paragraphs/0","offset":1487,"length":24},"authors":[{"last":"Mihalcea"},{"last":"Tarau"}],"year":"2004","references":["/references/21"]},{"style":0,"text":"Darling and Song, 2011","origin":{"pointer":"/sections/6/paragraphs/0","offset":1853,"length":22},"authors":[{"last":"Darling"},{"last":"Song"}],"year":"2011","references":["/references/7"]},{"style":0,"text":"Kupiec et al., 1995","origin":{"pointer":"/sections/6/paragraphs/1","offset":293,"length":19},"authors":[{"last":"Kupiec"},{"last":"al."}],"year":"1995","references":["/references/16"]},{"style":0,"text":"Osborne, 2002","origin":{"pointer":"/sections/6/paragraphs/1","offset":332,"length":13},"authors":[{"last":"Osborne"}],"year":"2002","references":["/references/22"]},{"style":0,"text":"Galley, 2006","origin":{"pointer":"/sections/6/paragraphs/1","offset":353,"length":12},"authors":[{"last":"Galley"}],"year":"2006","references":["/references/12"]},{"style":0,"text":"Ryang and Abekawa, 2012","origin":{"pointer":"/sections/6/paragraphs/1","offset":406,"length":23},"authors":[{"last":"Ryang"},{"last":"Abekawa"}],"year":"2012","references":["/references/24"]},{"style":0,"text":"Aker et al., 2010","origin":{"pointer":"/sections/6/paragraphs/1","offset":433,"length":17},"authors":[{"last":"Aker"},{"last":"al."}],"year":"2010","references":["/references/1"]},{"style":0,"text":"Xie and Liu, 2010","origin":{"pointer":"/sections/6/paragraphs/1","offset":662,"length":17},"authors":[{"last":"Xie"},{"last":"Liu"}],"year":"2010","references":["/references/27"]},{"style":0,"text":"Conroy et al., 2011","origin":{"pointer":"/sections/6/paragraphs/2","offset":1099,"length":19},"authors":[{"last":"Conroy"},{"last":"al."}],"year":"2011","references":["/references/6"]},{"style":0,"text":"Davis et al., 2012","origin":{"pointer":"/sections/6/paragraphs/2","offset":1499,"length":18},"authors":[{"last":"Davis"},{"last":"al."}],"year":"2012","references":["/references/8"]}]}
