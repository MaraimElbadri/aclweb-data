{"sections":[{"title":"","paragraphs":["Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 311–321, Sofia, Bulgaria, August 4-9 2013. c⃝2013 Association for Computational Linguistics"]},{"title":"Fast and Adaptive Online Training of Feature-Rich Translation Models Spence Green, Sida Wang, Daniel Cer, and Christopher D. Manning Computer Science Department, Stanford University {spenceg,sidaw,danielcer,manning}@stanford.edu Abstract","paragraphs":["We present a fast and scalable online method for tuning statistical machine translation models with large feature sets. The standard tuning algorithm—MERT—only scales to tens of features. Recent discriminative algorithms that accommodate sparse features have produced smaller than expected translation quality gains in large systems. Our method, which is based on stochastic gradient descent with an adaptive learning rate, scales to millions of features and tuning sets with tens of thousands of sentences, while still converging after only a few epochs. Large-scale experiments on Arabic-English and Chinese-English show that our method produces significant translation quality gains by exploiting sparse features. Equally important is our analysis, which suggests techniques for mitigating overfitting and domain mismatch, and applies to other recent discriminative methods for machine translation."]},{"title":"1 Introduction","paragraphs":["Sparse, overlapping features such as words and ngram contexts improve many NLP systems such as parsers and taggers. Adaptation of discriminative learning methods for these types of features to statistical machine translation (MT) systems, which have historically used idiosyncratic learning techniques for a few dense features, has been an active research area for the past half-decade. However, despite some research successes, feature-rich models are rarely used in annual MT evaluations. For example, among all submissions to the WMT and IWSLT 2012 shared tasks, just one participant tuned more than 30 features (Hasler et al., 2012a). Slow uptake of these methods may be due to implementation complexities, or to practical difficulties of configur-ing them for specific translation tasks (Gimpel and Smith, 2012; Simianer et al., 2012, inter alia).","We introduce a new method for training feature-rich MT systems that is effective yet comparatively easy to implement. The algorithm scales to millions of features and large tuning sets. It optimizes a logistic objective identical to that of PRO (Hopkins and May, 2011) with stochastic gradient descent, although other objectives are possible. The learning rate is set adaptively using AdaGrad (Duchi et al., 2011), which is particularly effective for the mixture of dense and sparse features present in MT models. Finally, feature selection is implemented as efficient L1 regularization in the forward-backward splitting (FOBOS) framework (Duchi and Singer, 2009). Experiments show that our algorithm converges faster than batch alternatives.","To learn good weights for the sparse features, most algorithms—including ours—benefit from more tuning data, and the natural source is the training bitext. However, the bitext presents two problems. First, it has a single reference, sometimes of lower quality than the multiple references in tuning sets from MT competitions. Second, large bitexts often comprise many text genres (Haddow and Koehn, 2012), a virtue for classical dense MT models but a curse for high dimensional models: bitext tuning can lead to a significant domain adaptation problem when evaluating on standard test sets. Our analysis separates and quantifies these two issues.","We conduct large-scale translation quality experiments on Arabic-English and Chinese-English. As baselines we use MERT (Och, 2003), PRO, and the Moses (Koehn et al., 2007) implementation of k-best MIRA, which Cherry and Foster (2012) recently showed to work as well as online MIRA (Chiang, 2012) for feature-rich models. The first experiment uses standard tuning and test sets from the NIST OpenMT competitions. The second experiment uses tuning and test sets sampled from the large bitexts. The new method yields significant improvements in both experiments. Our code is included in the Phrasal (Cer et al., 2010) toolkit, which is freely available. 311"]},{"title":"2 Adaptive Online Algorithms","paragraphs":["Machine translation is an unusual machine learning setting because multiple correct translations exist and decoding is comparatively expensive. When we have a large feature set and therefore want to tune on a large data set, batch methods are infeasible. Online methods can converge faster, and in practice they often find better solutions (Liang and Klein, 2009; Bottou and Bousquet, 2011, inter alia).","Recall that stochastic gradient descent (SGD), a fundamental online method, updates weights w according to wt = wt−1 − η∇lt(wt−1) (1)","with loss function1","l","t(w) of the tth","example, (sub)gradient of the loss with respect to the parameters ∇lt(wt−1), and learning rate η.","SGD is sensitive to the learning rate η, which is difficult to set in an MT system that mixes frequent “dense” features (like the language model) with sparse features (e.g., for translation rules). Further-more, η applies to each coordinate in the gradient, an undesirable property in MT where good sparse features may fire very infrequently. We would instead like to take larger steps for sparse features and smaller steps for dense features. 2.1 AdaGrad AdaGrad is a method for setting an adaptive learning rate that comes with good theoretical guarantees. The theoretical improvement over SGD is most significant for high-dimensional, sparse features. AdaGrad makes the following update: wt = wt−1 − ηΣ 1/2 t ∇lt(wt−1) (2) Σ−1 t = Σ−1","t−1 + ∇lt(wt−1)∇lt(wt−1)⊤ = t ∑ i=1","∇li(wi−1)∇li(wi−1)⊤ (3)","A diagonal approximation to Σ can be used for a high-dimensional vector wt. In this case, AdaGrad is simple to implement and computationally cheap. Consider a single dimension j, and let scalars vt = wt,j, gt = ∇jlt(wt−1), Gt =","∑t i=1 g2","i , then the update rule is","vt = vt−1 − η G−1/2 t gt (4)","Gt = Gt−1 + g2","t (5)","Compared to SGD, we just need to store Gt = Σ−1","t,jj","for each dimension j. 1 We specify the loss function for MT in section 3.1. 2.2 Prior Online Algorithms in MT AdaGrad is related to two previous online learning methods for MT. MIRA Chiang et al. (2008) described an adaption of MIRA (Crammer et al., 2006) to MT. MIRA makes the following update:","wt = arg min w 1 2η","∥w − wt−1∥2 2 + lt(w) (6) The first term expressesconservativity: the weight should change as little as possible based on a single example, ensuring that it is never beneficial to overshoot the minimum.","The relationship to SGD can be seen by lineariz-ing the loss function lt(w) ≈ lt(wt−1) + (w − wt−1)⊤","∇lt(wt−1) and taking the derivative of (6). The result is exactly (1). AROW Chiang (2012) adapted AROW (Crammer et al., 2009) to MT. AROW models the current weight as a Gaussian centered at wt−1 with covariance Σt−1, and does the following update upon seeing training example xt: wt, Σt =","arg min w,Σ 1 η DKL(N (w, Σ)||N (wt−1, Σt−1)) + lt(w) + 1 2η x⊤ t Σxt (7) The KL-divergence term expresses a more general, directionally sensitive conservativity. Ignoring the third term, the Σ that minimizes the KL is actually Σt−1. As a result, the first two terms of (7) generalize MIRA so that we may be more conservative in some directions specified by Σ. To see this, we can write out the KL-divergence between two Gaussians in closed form, and observe that the terms involving w do not interact with the terms involving Σ:","wt = arg min w 1 2η","(w − wt−1)⊤ Σ−1 t−1(w − wt−1) + lt(w) (8)","Σt = arg min Σ 1 2η log ( |Σt−1| |Σ| ) + 1 2η Tr ( Σ−1 t−1Σ ) + 1 2η x⊤ t Σxt (9) The third term in (7), called the confidence term, gives us adaptivity, the notion that we should have smaller variance in the direction v as more data xt 312 is seen in direction v. For example, if Σ is diagonal and xt are indicator features, the confidence term then says that the weight for a rarer feature should have more variance and vice-versa. Recall that for generalized linear models ∇lt(w) ∝ xt; if we substitute xt = αt∇lt(w) into (9), differentiate and solve, we get:","Σ−1","t = Σ−1","t−1 + xtx⊤","t","= Σ−1 0 + t ∑ i=1 α2 i ∇li(wi−1)∇li(wi−1)⊤ (10)","The precision Σ−1","t generally grows as more data is seen. Frequently updated features receive an especially high precision, whereas the model maintains large variance for rarely seen features.","If we substitute (10) into (8), linearize the loss lt(w) as before, and solve, then we have the linearized AROW update wt = wt−1 − ηΣt∇lt(wt−1) (11) which is also an adaptive update with per-coordinate learning rates specified byΣt (as opposed to Σ 1/2 t in AdaGrad). 2.3 Comparing AdaGrad, MIRA, AROW Compare (3) to (10) and observe that if we set Σ−1 0 = 0 and αt = 1, then the only difference between the AROW update (11) and the AdaGrad update (2) is a square root. Under a constant gradient, AROW decays the step size more aggressively (1/t) compared to AdaGrad (1/√ t), and it is sensitive to the specification ofΣ−1","0 .","Informally, SGD can be improved in the conservativity direction using MIRA so the updates do not overshoot. Second, SGD can be improved in the adaptivity direction using AdaGrad where the decaying stepsize is more robust and the adaptive stepsize allows better weight updates to features differing in sparsity and scale. Finally, AROW combines both adaptivity and conservativity. For MT, adaptivity allows us to deal with mixed dense/sparse features effectively without specific normalization.","Why do we choose AdaGrad over AROW? MIRA/AROW requires selecting the loss function l(w) so that wt can be solved in closed-form, by a quadratic program (QP), or in some other way that is better than linearizing. This usually means choosing a hinge loss. On the other hand, AdaGrad/linearized AROW only requires that the gradient of the loss function can be computed efficiently. Algorithm 1 Adaptive online tuning for MT. Require: Tuning set {fi, e1:k","i }i=1:M 1: Set w0 = 0 2: Set t = 1 3: repeat 4: for i in 1 . . . M in random order do 5: Decode n-best list Ni for fi 6: Sample pairs {dj,+, dj,−}j=1:s from Ni 7: Compute Dt = {φ(dj,+) − φ(dj,−)}j=1:s 8: Set gt = ∇l(Dt; wt−1)} 9: Set Σ−1","t = Σ−1","t−1 + gtg⊤","t ▷ Eq. (3) 10: Update wt = wt−1 − ηΣ1/2","t gt ▷ Eq. (2) 11: Regularize wt ▷ Eq. (15) 12: Set t = t + 1 13: end for 14: until convergence Linearized AROW, however, is less robust than AdaGrad empirically2","and lacks known theoretical guarantees. Finally, by using AdaGrad, we separate adaptivity from conservativity. Our experiments suggest that adaptivity is actually more important."]},{"title":"3 Adaptive Online MT","paragraphs":["Algorithm 1 shows the full algorithm introduced in this paper. AdaGrad (lines 9–10) is a crucial piece, but the loss function, regularization technique, and parallelization strategy described in this section are equally important in the MT setting. 3.1 Pairwise Logistic Loss Function Algorithm 1 lines 5–8 describe the gradient computation. We cast MT tuning as pairwise ranking (Herbrich et al., 1999, inter alia), which Hopkins and May (2011) applied to MT. The pairwise approach results in simple, convex loss functions suitable for online learning. The idea is that for any two derivations, the ranking predicted by the model should be consistent with the ranking predicted by a gold sentence-level metric G like BLEU+1 (Lin and Och, 2004).","Consider a single source sentence f with associated references e1:k",". Let d be a derivation in an n-best list of f that has the target e = e(d) and the feature map φ(d). Let M (d) = w · φ(d) be the model score. For any derivation d+ that is better than d− under G, we desire pairwise agreement such that G ( e(d+), e1:k ) > G ( e(d−), e1:k ) ⇐⇒ M (d+) > M (d−) 2 According to experiments not reported in this paper. 313 Ensuring pairwise agreement is the same as ensuring w · [φ(d+) − φ(d−)] > 0.","For learning, we need to select derivation pairs (d+, d−) to compute difference vectors x+ = φ(d+) − φ(d−). Then we have a 1-class separation problem trying to ensure w · x+ > 0. The derivation pairs are sampled with the algorithm of Hopkins and May (2011).","We compute difference vectors Dt = {x1:s","+ } (Algorithm 1 line 7) from s pairs (d+, d−) for source sentence ft. We use the familiar logistic loss: lt(w) = l(Dt, w) = − ∑ x+∈Dt log","1 1 + e−w·x+","(12)","Choosing the hinge loss instead of the logistic loss results in the 1-class SVM problem. The 1-class separation problem is equivalent to the binary classification problem withx+ = φ(d+) − φ(d−) as positive data and x− = −x+ as negative data, which may be plugged into an existing logistic regression solver.","We find that Algorithm 1 works best with minibatches instead of single examples. In line 4 we simply partition the tuning set so that i becomes a mini-batch of examples. 3.2 Updating and Regularization Algorithm 1 lines 9–11 compute the adaptive learning rate, update the weights, and apply regularization. Section 2.1 explained the AdaGrad learning rate computation. To update and regularize the weights we apply the Forward-Backward Splitting (FOBOS) (Duchi and Singer, 2009) framework, which separates the two operations. The two-step FOBOS update is","wt− 1 2 = wt−1 − ηt−1∇lt−1(wt−1) (13)","wt = arg min w 1 2","∥w − wt− 1 2 ∥2 2 + ηt−1r(w) (14) where (13) is just an unregularized gradient descent step and (14) balances the regularization term r(w) with staying close to the gradient step.","Equation (14) permits efficient L1 regularization, which is well-suited for selecting good features from exponentially many irrelevant features (Ng, 2004). It is well-known that feature selection is very important for feature-rich MT. For example, simple indicator features like lexicalized re-ordering classes are potentially useful yet bloat the the feature set and, in the worst case, can negatively impact Algorithm 2 “Stale gradient” parallelization method for Algorithm 1. Require: Tuning set {fi, e1:k","i }i=1:M 1: Initialize threadpool p1, . . . , pj 2: Set t = 1 3: repeat 4: for i in 1 . . . M in random order do 5: Wait until any thread p is idle 6: Send (fi, e1:k","i , t) to p ▷ Alg. 1 lines 5–8 7: while ∃ p′","done with gradient gt′","do ▷ t′","≤ t 8: Update wt = wt−1 − ηgt′","▷ Alg. 1 lines 9–11 9: Set t = t + 1 10: end while 11: end for 12: until convergence search. Some of the features generalize, but many do not. This was well understood in previous work, so heuristic filtering was usually applied (Chiang et al., 2009, inter alia). In contrast, we need only select an appropriate regularization strength λ.","Specifically, when r(w) = λ∥w∥1, the closedform solution to (14) is","wt = sign(wt− 1 2 ) [ |wt− 1","2 | − ηt−1λ ] + (15) where [x]+ = max(x, 0) is the clipping function that in this case sets a weight to 0 when it falls below the threshold ηt−1λ. It is straightforward to adapt this to AdaGrad with diagonal Σ by setting each dimension of ηt−1,j = ηΣ 1 2 t,jj and by taking element-wise products.","We find that∇lt−1(wt−1) only involves several hundred active features for the current example (or mini-batch). However, naively following the FOBOS framework requires updating millions of weights. But a practical benefit of FOBOS is that we can do lazy updates on just the active dimensions without any approximations. 3.3 Parallelization Algorithm 1 is inherently sequential like standard online learning. This is undesirable in MT where decoding is costly. We therefore parallelize the algorithm with the “stale gradient” method of Langford et al. (2009) (Algorithm 2). A fixed threadpool of workers computes gradients in parallel and sends them to a master thread, which updates a central weight vector. Crucially, the weight updates need not be applied in order, so synchronization is unnecessary; the workers only idle at the end of an epoch. The consequence is that the update in line 8 of Algorithm 2 is with respect to gradient gt′ with t′","≤ t. Langford et al. (2009) gave convergence results for 314 stale updating, but the bounds do not apply to our setting since we use L1 regularization. Nevertheless, Gimpel et al. (2010) applied this framework to other non-convex objectives and obtained good empirical results.","Our asynchronous, stochastic method has practical appeal for MT. During a tuning run, the online method decodes the tuning set under many more weight vectors than a MERT-style batch method. This characteristic may result in broader exploration of the search space, and make the learner more robust to local optima local optima (Liang and Klein, 2009; Bottou and Bousquet, 2011, inter alia). The adaptive algorithm identifies appropriate learning rates for the mixture of dense and sparse features. Finally, large data structures such as the language model (LM) and phrase table exist in shared memory, obviating the need for remote queries."]},{"title":"4 Experiments","paragraphs":["We built Arabic-English and Chinese-English MT systems with Phrasal (Cer et al., 2010), a phrase-based system based on alignment templates (Och and Ney, 2004). The corpora3","in our experiments (Table 1) derive from several LDC sources from 2012 and earlier. We de-duplicated each bitext according to exact string match, and ensured that no overlap existed with the test sets. We produced alignments with the Berkeley aligner (Liang et al., 2006b) with standard settings and symmetrized via the grow-diag heuristic.","For each language we used SRILM (Stolcke, 2002) to estimate 5-gram LMs with modified Kneser-Ney smoothing. We included the monolingual English data and the respective target bitexts. 4.1 Feature Templates The baseline “dense” model contains 19 features: the nine Moses baseline features, the hierarchical lexicalized re-ordering model of Galley and Manning (2008), the (log) count of each rule, and an indicator for unique rules.","To the dense features we add three high dimensional “sparse” feature sets. Discrimina-","3","We tokenized the English with packages from the Stanford Parser (Klein and Manning, 2003) according to the Penn Treebank standard (Marcus et al., 1993), the Arabic with the Stanford Arabic segmenter (Green and DeNero, 2012) according to the Penn Arabic Treebank standard (Maamouri et al., 2008), and the Chinese with the Stanford Chinese segmenter (Chang et al., 2008) according to the Penn Chinese Treebank standard (Xue et al., 2005).","Bilingual Monolingual Sentences Tokens Tokens Ar-En 6.6M 375M 990M Zh-En 9.3M 538M Table 1: Bilingual and monolingual corpora used in these experiments. The monolingual English data comes from the AFP and Xinhua sections of English Gigaword 4 (LDC2009T13). tive phrase table (PT): indicators for each rule in the phrase table. Alignments (AL): indicators for phrase-internal alignments and deleted (unaligned) source words. Discriminative re-ordering (LO): indicators for eight lexicalized re-ordering classes, including the six standard monotone/swap/discontinuous classes plus the two simpler Moses monotone/non-monotone classes. 4.2 Tuning Algorithms The primary baseline is the dense feature set tuned with MERT (Och, 2003). The Phrasal implementation uses the line search algorithm of Cer et al. (2008), uniform initialization, and 20 random starting points.4","We tuned according to BLEU-4 (Papineni et al., 2002).","We built high dimensional baselines with two different algorithms. First, we tuned with batch PRO using the default settings in Phrasal (L2 regularization with σ=0.1). Second, we ran the k-best batch MIRA (kb-MIRA) (Cherry and Foster, 2012) implementation in Moses. We did implement an online version of MIRA, and in small-scale experiments found that the batch variant worked just as well. Cherry and Foster (2012) reported the same result, and their implementation is available in Moses. We ran their code with standard settings.","Moses5","also contains the discriminative phrase table implementation of (Hasler et al., 2012b), which is identical to our implementation using Phrasal. Moses and Phrasal accept the same phrase table and LM formats, so we kept those data structures in common. The two decoders also use the same multi-stack beam search (Och and Ney, 2004).","For our method, we used uniform initialization, 16 threads, and a mini-batch size of 20. We found that η=0.02 and λ=0.1 worked well on development sets for both languages. To compute the gradients","4","Other system settings for all experiments: distortion limit of 5, a maximum phrase length of 7, and an n-best size of 200.","5","v1.0 (28 January 2013) 315 Model #features Algorithm Tuning Set MT02 MT03 MT04 MT09 Dense 19 MERT MT06 45.08 51.32 52.26 51.42 48.44 Dense 19 This paper MT06 44.19 51.42 52.52 50.16 48.13 +PT 151k kb-MIRA MT06 42.08 47.25 48.98 47.08 45.64 +PT 23k PRO MT06 44.31 51.06 52.18 50.23 47.52 +PT 50k This paper MT06 50.61 51.71 52.89 50.42 48.74 +PT+AL+LO 109k PRO MT06 44.87 51.25 52.43 50.05 47.76 +PT+AL+LO 242k This paper MT06 57.84 52.45 53.18 51.38 49.37 Dense 19 MERT MT05/6/8 49.63 51.60 52.29 51.73 48.68 +PT+AL+LO 390k This paper MT05/6/8 58.20 53.61 54.99 52.79 49.94 (Chiang, 2012)*","10-20k MIRA MT04/6 – – – – 45.90 (Chiang, 2012)*","10-20k AROW MT04/6 – – – – 47.60","#sentences 728 663 1,075 1,313 Table 2: Ar-En results [BLEU-4 % uncased] for the NIST tuning experiment. The tuning and test sets each have four references. MT06 has 1,717 sentences, while the concatenated MT05/6/8 set has 4,213 sentences. Bold indicates statistical significance relative to thebest baseline in each block at p < 0.001; bold-italic at p < 0.05. We assessed significance with the permutation test of Riezler and Maxwell (2005). (*) Chiang (2012) used a similar-sized bitext, but two LMs trained on twice as much monolingual data. Model #features Algorithm Tuning Set MT02 MT03 MT04 Dense 19 MERT MT06 33.90 35.72 33.71 34.26 Dense 19 This paper MT06 32.60 36.23 35.14 34.78 +PT 105k kb-MIRA MT06 29.46 30.67 28.96 30.05 +PT 26k PRO MT06 33.70 36.87 34.62 34.80 +PT 66k This paper MT06 33.90 36.09 34.86 34.73 +PT+AL+LO 148k PRO MT06 34.81 36.31 33.81 34.41 +PT+AL+LO 344k This paper MT06 38.99 36.40 35.07 34.84 Dense 19 MERT MT05/6/8 32.36 35.69 33.83 34.33 +PT+AL+LO 487k This paper MT05/6/8 37.64 37.81 36.26 36.15","#sentences 878 919 1,597 Table 3: Zh-En results [BLEU-4 % uncased] for the NIST tuning experiment. MT05/6/8 has 4,103 sentences. OpenMT 2009 did not include Zh-En, hence the asymmetry with Table 2. we sampled 15 derivation pairs for each tuning example and scored them with BLEU+1. 4.3 NIST OpenMT Experiment The first experiment evaluates our algorithm when tuning and testing on standard test sets, each with four references. When we add features, our algorithm tends to overfit to a standard-sized tuning set like MT06. We thus concatenated MT05, MT06, and MT08 to create a larger tuning set.","Table 2 shows the Ar-En results. Our algorithm is competitive with MERT in the low dimensional “dense” setting, and compares favorably to PRO with the PT feature set. PRO does not benefit from additional features, whereas our algorithm improves with both additional features and data. The underperformance of kb-MIRA may result from a difference between Moses and Phrasal: Moses MERT achieves only 45.62 on MT09. Moses PRO with the PT feature set is slightly worse, e.g., 44.52 on MT09. Nevertheless, kb-MIRA does not improve significantly over MERT, and also selects an unnecessarily large model.","The full feature set PT+AL+LO does help. With the PT feature set alone, our algorithm tuned on MT05/6/8 scores well below the best model, e.g. 316 Model #features Algorithm Tuning Set #refs bitext5k-test MT04 Dense 19 MERT MT06 45.08 4 39.28 51.42 +PT 72k This paper MT05/6/8 51.29 4 39.50 50.60 +PT 79k This paper bitext5k 44.79 1 43.85 45.73 +PT+AL+LO 647k This paper bitext15k 45.68 1 43.93 45.24 Table 4: Ar-En results [BLEU-4 % uncased] for the bitext tuning experiment. Statistical significance is relative to the Dense baseline. We include MT04 for comparison to the NIST genre. Model #features Algorithm Tuning Set #refs bitext5k-test MT04 Dense 19 MERT MT06 33.90 4 33.44 34.26 +PT 97k This paper MT05/6/8 34.45 4 35.08 35.19 +PT 67k This paper bitext5k 36.26 1 36.01 33.76 +PT+AL+LO 536k This paper bitext15k 37.57 1 36.30 34.05 Table 5: Zh-En results [BLEU-4 % uncased] for the bitext tuning experiment. 48.56 BLEU on MT09. For Ar-En, our algorithm thus has the desirable property of benefiting from more and better features, and more data.","Table 3 shows Zh-En results. Somewhat surprisingly our algorithm improves over MERT in the dense setting. When we add the discriminative phrase table, our algorithm improves over kb-MIRA, and over batch PRO on two evaluation sets. With all features and the MT05/6/8 tuning set, we improve significantly over all other models. PRO learns a smaller model with the PT+AL+LO feature set which is surprising given that it applies L2 regularization (AdaGrad uses L1). We speculate that this may be an consequence of stochastic learning. Our algorithm decodes each example with a new weight vector, thus exploring more of the search space for the same tuning set. 4.4 Bitext Tuning Experiment Tables 2 and 3 show that adding tuning examples improves translation quality. Nevertheless, even the larger tuning set is small relative to the bitext from which rules were extracted. He and Deng (2012) and Simianer et al. (2012) showed significant translation quality gains by tuning on the bitext. However, their bitexts matched the genre of their test sets. Our bitexts, like those of most large-scale systems, do not. Domain mismatch matters for the dense feature set (Haddow and Koehn, 2012). We show that it also matters for feature-rich MT.","Before aligning each bitext, we randomly sampled and sequestered 5k and 15k sentence tuning sets, and a 5k test set. We prevented overlap be-DA DB |A| |B| |A ∩ B| MT04 MT06 70k 72k 5.9k MT04 MT568 70k 96k 7.6k MT04 bitext5k 70k 67k 4.4k MT04 bitext15k 70k 310k 10.5k 5ktest bitext5k 82k 67k 5.6k 5ktest bitext15k 82k 310k 14k Table 6: Number of overlapping phrase table (+PT) features on various Zh-En dataset pairs. tween the tuning sets and the test set. We then tuned a dense model with MERT on MT06, and feature-rich models on both MT05/6/8 and the bitext tuning set. Table 4 shows the Ar-En results. When tuned on bitext5k the translation quality gains are significant for bitext5k-test relative to tuning on MT05/6/8, which has multiple references. However, the bitext5k models do not generalize as well to the NIST evaluation sets as represented by the MT04 result. Table 5 shows similar trends for Zh-En."]},{"title":"5 Analysis 5.1 Feature Overlap Analysis","paragraphs":["How many sparse features appear in both the tuning and test sets? In Table 6, A is the set of phrase table features that received a non-zero weight when tuned on dataset DA (same for B). Column DA lists several Zh-En test sets used and column DB lists tuning sets. Our experiments showed that tuning on MT06 generalizes better to MT04 than tuning 317 on bitext5k, whereas tuning on bitext5k generalizes better to bitext5k-test than tuning on MT06. These trends are consistent with the level of feature overlap. Phrase table features in A ∩ B are overwhelmingly short, simple, and correct phrases, suggesting L1 regularization is effective for feature selection. It is also important to balance the number of features with how well weights can be learned for those features, as tuning on bitext15k produced higher coverage for MT04 but worse generalization than tuning on MT06. 5.2 Domain Adaptation Analysis To understand the domain adaptation issue we compared the non-zero weights in the discriminative phrase table (PT) for Ar-En models tuned on bitext5k and MT05/6/8. Table 7 illustrates a statistical idiosyncrasy in the data for the American and British spellings of program/programme. The mass is concentrated along the diagonal, probably because MT05/6/8 was prepared by NIST, an American agency, while the bitext was collected from many sources including Agence France Presse.","Of course, this discrepancy is consequential for both dense and feature-rich models. However, we observe that the feature-rich models fit the tuning data more closely. For example, the MT05/6/8 model learns rules like"]},{"title":"l .AKQK. Æ JK","paragraphs":["→ program includes,"]},{"title":"l .AKQK.","paragraphs":["→ program of, and"]},{"title":"l .AKQ.@̧ Ł Y flAK","paragraphs":["→ program window. Crucially, it does not learn the basic rule"]},{"title":"l .AKQK.","paragraphs":["→ program. In contrast, the bitext5k model contains ba-","sic rules such"]},{"title":"l .AKQK.","paragraphs":["→ programme,"]},{"title":"l .AKQ.@̧ @ Yo","paragraphs":["→ this programme, and"]},{"title":"l .AKQ.@̧ ‰ ̧ X","paragraphs":["→ that programme. It also contains more elaborate rules such as"]},{"title":"l .AKQ.@̧ HAfi fiK IKA¿","paragraphs":["→ programme expenses were and "]},{"title":"Øæ̧o A@̌ ØJKA fi@̧ HCgQ@̧ l .@QK.","paragraphs":["→ manned space flight programmes. We observed similar trends for ‘defense/defence’, ‘analyze/analyse’, etc. This particular genre problem could be addressed with language-specific pre-processing, but our system solves it in a data-driven manner. 5.3 Re-ordering Analysis We also analyzed re-ordering differences. Arabic matrix clauses tend to be verb-initial, meaning that the subject and verb must be swapped when translat-ing to English. To assess re-ordering differences— if any—between the dense and feature-rich models, we selected all MT09 segments that began with one # bitext5k # MT05/6/8 programme 185 0 program 19 449 PT rules w/ programme 353 79 PT rules w/ program 9 31 Table 7: Top: comparison of token counts in two Ar-En tuning sets for programme and program. Bottom: rule counts in the discriminative phrase table (PT) for models tuned on the two tuning sets. Both spellings correspond to the Arabic"]},{"title":"l .AKQK.","paragraphs":[". of seven common verbs:"]},{"title":"Ä fl","paragraphs":["qaal ‘said’,"]},{"title":"hQ","paragraphs":["SrH ‘declared’,"]},{"title":"PA @","paragraphs":["ashaar ‘indicated’,"]},{"title":"A¿","paragraphs":["kaan ‘was’,"]},{"title":"Q» X","paragraphs":["dhkr ‘commented’, "]},{"title":"‹A @","paragraphs":["aDaaf ‘added’,"]},{"title":"Æ«̊ @","paragraphs":["ac","ln ‘announced’. We compared the output of the MERT Dense model to our method with the full feature set, both tuned on MT06. Of the 208 source segments, 32 of the translation pairs contained different word order in the matrix clause. Our feature-rich model was correct 18 times (56.3%), Dense was correct 4 times (12.5%), and neither method was correct 10 times (31.3%).","(1) ref: lebanese prime minister , fuad siniora , announced","a. and lebanese prime minister fuad siniora","that","b. the lebanese prime minister fouad siniora","announced (2) ref: the newspaper and television reported a. she said the newspaper and television b. television and newspaper said In (1) the dense model (1a) drops the verb while the feature-rich model correctly re-orders and inserts it after the subject (1b). The coordinated subject in (2) becomes an embedded subject in the dense output (2a). The feature-rich model (2b) performs the correct re-ordering. 5.4 Runtime Comparison Table 8 compares our method to standard implementations of the other algorithms. MERT parallelizes easily but runtime increases quadratically with n-best list size. PRO runs (single-threaded) L-BFGS to convergence on every epoch, a potentially slow procedure for the larger feature set. Moreover, both 318 epochs min. MERT Dense 22 180 PRO +PT 25 35 kb-MIRA*","+PT 26 25 This paper +PT 10 10 PRO +PT+AL+LO 13 150 This paper +PT+AL+LO 5 15 Table 8: Epochs to convergence (“epochs”) and approximate runtime per epoch in minutes (“min.”) for selected Zh-En experiments tuned on MT06. All runs executed on the same dedicated system with the same number of threads. (*) Moses and kb-MIRA are written in C++, while all other rows refer to Java implementations in Phrasal. the Phrasal and Moses PRO implementations use L2 regularization, which regularizes every weight on every update. kb-MIRA makes multiple passes through the n-best lists during each epoch. The Moses implementation parallelizes decoding but weight updating is sequential.","The core of our method is an inner product between the adaptive learning rate vector and the gradient. This is easy to implement and is very fast even for large feature sets. Since we applied lazy regularization, this inner product usually involves hundred-dimensional vectors. Finally, our method does not need to accumulate n-best lists, a practice that slows down the other algorithms."]},{"title":"6 Related Work","paragraphs":["Our work relates most closely to that of Hasler et al. (2012b), who tuned models containing both sparse and dense features with Moses. A discriminative phrase table helped them improve slightly over a dense, online MIRA baseline, but their best results required initialization with MERT-tuned weights and re-tuning a single, shared weight for the discriminative phrase table with MERT. In contrast, our algorithm learned good high dimensional models from a uniform starting point.","Chiang (2012) adapted AROW to MT and extended previous work on online MIRA (Chiang et al., 2008; Watanabe et al., 2007). It was not clear if his improvements came from the novel Hope/Fear search, the conservativity gain from MIRA/AROW by solving the QP exactly, adaptivity, or sophisticated parallelization. In contrast, we show that AdaGrad, which ignores conservativity and only capturing adaptivity, is sufficient.","Simianer et al. (2012) investigated SGD with a pairwise perceptron objective. Their best algorithm used iterative parameter mixing (McDonald et al., 2010), which we found to be slower than the stale gradient method in section 3.3. They regularized once at the end of each epoch, whereas we regularized each weight update. An empirical comparison of these two strategies would be an interesting fu-ture contribution.","Watanabe (2012) investigated SGD and even randomly selected pairwise samples as we did. He considered both softmax and hinge losses, observ-ing better results with the latter, which solves a QP. Their parallelization strategy required a line search at the end of each epoch.","Many other discriminative techniques have been proposed based on: ramp loss (Gimpel, 2012); hinge loss (Cherry and Foster, 2012; Haddow et al., 2011; Arun and Koehn, 2007); maximum entropy (Xiang and Ittycheriah, 2011; Ittycheriah and Roukos, 2007; Och and Ney, 2002); perceptron (Liang et al., 2006a); and structured SVM (Tillmann and Zhang, 2006). These works use radically different experimental setups, and to our knowledge only (Cherry and Foster, 2012) and this work compare to at least two high dimensional baselines. Broader comparisons, though time-intensive, could help differentiate these methods."]},{"title":"7 Conclusion and Outlook","paragraphs":["We introduced a new online method for tuning feature-rich translation models. The method is faster per epoch than MERT, scales to millions of features, and converges quickly. We used efficient L1 regularization for feature selection, obviating the need for the feature scaling and heuristic filtering common in prior work. Those comfortable with implementing vanilla SGD should find our method easy to implement. Even basic discriminative features were effective, so we believe that our work enables fresh approaches to more sophisticated MT feature engineering. Acknowledgments We thank John DeNero for helpful comments on an earlier draft. The first author is supported by a National Science Foundation Graduate Research Fellowship. We also acknowledge the support of the Defense Advanced Research Projects Agency (DARPA) Broad Operational Language Translation (BOLT) program through IBM. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the view of the DARPA or the US government. 319"]},{"title":"References","paragraphs":["A. Arun and P. Koehn. 2007. Online learning methods for discriminative training of phrase based statistical machine translation. In MT Summit XI.","L. Bottou and O. Bousquet. 2011. The tradeoffs of large scale learning. In Optimization for Machine Learning, pages 351–368. MIT Press.","D. Cer, D. Jurafsky, and C. D. Manning. 2008. Regularization and search for minimum error rate training. In WMT.","D. Cer, M. Galley, D. Jurafsky, and C. D. Manning. 2010. Phrasal: A statistical machine translation toolkit for exploring new model features. In HLT-NAACL, Demonstration Session.","P-C. Chang, M. Galley, and C. D. Manning. 2008. Optimizing Chinese word segmentation for machine translation performance. In WMT.","C. Cherry and G. Foster. 2012. Batch tuning strategies for statistical machine translation. In HLT-NAACL.","D. Chiang, Y. Marton, and P. Resnik. 2008. Online large-margin training of syntactic and structural translation features. In EMNLP.","D. Chiang, K. Knight, and W. Wang. 2009. 11,001 new features for statistical machine translation. In HLT-NAACL.","D. Chiang. 2012. Hope and fear for discriminative training of statistical translation models. JMLR, 13:1159–1187.","K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz, and Y. Singer. 2006. Online passive-aggressive algorithms. JMLR, 7:551–585.","K. Crammer, A. Kulesza, and M. Dredze. 2009. Adaptive regularization of weight vectors. In NIPS.","J. Duchi and Y. Singer. 2009. Efficient online and batch learning using forward backward splitting. JMLR, 10:2899–2934.","J. Duchi, E. Hazan, and Y. Singer. 2011. Adaptive sub-gradient methods for online learning and stochastic optimization. JMLR, 12:2121–2159.","M. Galley and C. D. Manning. 2008. A simple and effective hierarchical phrase reordering model. In EMNLP.","K. Gimpel and N. A. Smith. 2012. Structured ramp loss minimization for machine translation. In HLT-NAACL.","K. Gimpel, D. Das, and N. A. Smith. 2010. Distributed asynchronous online learning for natural language processing. In CoNLL.","K. Gimpel. 2012. Discriminative Feature-Rich Modeling for Syntax-Based Machine Translation. Ph.D. thesis, Language Technologies Institute, Carnegie Mellon University.","S. Green and J. DeNero. 2012. A class-based agreement model for generating accurately inflected translations. In ACL.","B. Haddow and P. Koehn. 2012. Analysing the effect of out-of-domain data on SMT systems. In WMT.","B. Haddow, A. Arun, and P. Koehn. 2011. SampleRank training for phrase-based machine translation. In WMT.","E. Hasler, P. Bell, A. Ghoshal, B. Haddow, P. Koehn, F. McInnes, et al. 2012a. The UEDIN systems for the IWSLT 2012 evaluation. In IWSLT.","E. Hasler, B. Haddow, and P. Koehn. 2012b. Sparse lexicalised features and topic adaptation for SMT. In IWSLT.","X. He and L. Deng. 2012. Maximum expected BLEU training of phrase and lexicon translation models. In ACL.","R. Herbrich, T. Graepel, and K. Obermayer. 1999. Support vector learning for ordinal regression. In ICANN.","M. Hopkins and J. May. 2011. Tuning as ranking. In EMNLP.","A. Ittycheriah and S. Roukos. 2007. Direct translation model 2. In HLT-NAACL.","D. Klein and C. D. Manning. 2003. Accurate unlexicalized parsing. In ACL.","P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, et al. 2007. Moses: Open source toolkit for statistical machine translation. In ACL, Demonstration Session.","J. Langford, A. J. Smola, and M. Zinkevich. 2009. Slow learners are fast. In NIPS.","P. Liang and D. Klein. 2009. Online EM for unsupervised models. In HLT-NAACL.","P. Liang, A. Bouchard-Côté, D. Klein, and B. Taskar. 2006a. An end-to-end discriminative approach to machine translation. In ACL.","P. Liang, B. Taskar, and D. Klein. 2006b. Alignment by agreement. In NAACL.","C.-Y. Lin and F. J. Och. 2004. ORANGE: a method for evaluating automatic evaluation metrics for machine translation. In COLING.","M. Maamouri, A. Bies, and S. Kulick. 2008. Enhanc-ing the Arabic Treebank: A collaborative effort to-ward new annotation guidelines. In LREC. 320","M. Marcus, M. A. Marcinkiewicz, and B. Santorini. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19:313–330.","R. McDonald, K. Hall, and G. Mann. 2010. Distributed training strategies for the structured perceptron. In NAACL-HLT.","A. Y. Ng. 2004. Feature selection, L1 vs. L2 regularization, and rotational invariance. In ICML.","F. J. Och and H. Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In ACL.","F. J. Och and H. Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30(4):417–449.","F. J. Och. 2003. Minimum error rate training for statistical machine translation. In ACL.","K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In ACL.","S. Riezler and J. T. Maxwell. 2005. On some pitfalls in automatic evaluation and significance testing in MT. In ACL Workshop on Intrinsic and Extrinsic Evalua-tion Measures for Machine Translation and/or Summarization (MTSE).","P. Simianer, S. Riezler, and C. Dyer. 2012. Joint feature selection in distributed stochastic learning for large-scale discriminative training in SMT. In ACL.","A Stolcke. 2002. SRILM—an extensible language modeling toolkit. In ICSLP.","C. Tillmann and T. Zhang. 2006. A discriminative global training algorithm for statistical MT. In ACL-COLING.","T. Watanabe, J. Suzuki, H. Tsukada, and H. Isozaki. 2007. Online large-margin training for statistical machine translation. In EMNLP-CoNLL.","T. Watanabe. 2012. Optimized online rank learning for machine translation. In HLT-NAACL. Association for Computational Linguistics.","B. Xiang and A. Ittycheriah. 2011. Discriminative feature-tied mixture modeling for statistical machine translation. In ACL-HLT.","N. Xue, F. Xia, F. Chiou, and M. Palmer. 2005. The Penn Chinese Treebank: Phrase structure annotation of a large corpus. Natural Language Engineering, 11(2):207–238. 321"]}],"references":[{"authors":[{"first":"A.","last":"Arun"},{"first":"P.","last":"Koehn"}],"year":"2007","title":"Online learning methods for discriminative training of phrase based statistical machine translation","source":"A. Arun and P. Koehn. 2007. Online learning methods for discriminative training of phrase based statistical machine translation. In MT Summit XI."},{"authors":[{"first":"L.","last":"Bottou"},{"first":"O.","last":"Bousquet"}],"year":"2011","title":"The tradeoffs of large scale learning","source":"L. Bottou and O. Bousquet. 2011. The tradeoffs of large scale learning. In Optimization for Machine Learning, pages 351–368. MIT Press."},{"authors":[{"first":"D.","last":"Cer"},{"first":"D.","last":"Jurafsky"},{"first":"C.","middle":"D.","last":"Manning"}],"year":"2008","title":"Regularization and search for minimum error rate training","source":"D. Cer, D. Jurafsky, and C. D. Manning. 2008. Regularization and search for minimum error rate training. In WMT."},{"authors":[{"first":"D.","last":"Cer"},{"first":"M.","last":"Galley"},{"first":"D.","last":"Jurafsky"},{"first":"C.","middle":"D.","last":"Manning"}],"year":"2010","title":"Phrasal: A statistical machine translation toolkit for exploring new model features","source":"D. Cer, M. Galley, D. Jurafsky, and C. D. Manning. 2010. Phrasal: A statistical machine translation toolkit for exploring new model features. In HLT-NAACL, Demonstration Session."},{"authors":[{"first":"P-C.","last":"Chang"},{"first":"M.","last":"Galley"},{"first":"C.","middle":"D.","last":"Manning"}],"year":"2008","title":"Optimizing Chinese word segmentation for machine translation performance","source":"P-C. Chang, M. Galley, and C. D. Manning. 2008. Optimizing Chinese word segmentation for machine translation performance. In WMT."},{"authors":[{"first":"C.","last":"Cherry"},{"first":"G.","last":"Foster"}],"year":"2012","title":"Batch tuning strategies for statistical machine translation","source":"C. Cherry and G. Foster. 2012. Batch tuning strategies for statistical machine translation. In HLT-NAACL."},{"authors":[{"first":"D.","last":"Chiang"},{"first":"Y.","last":"Marton"},{"first":"P.","last":"Resnik"}],"year":"2008","title":"Online large-margin training of syntactic and structural translation features","source":"D. Chiang, Y. Marton, and P. Resnik. 2008. Online large-margin training of syntactic and structural translation features. In EMNLP."},{"authors":[{"first":"D.","last":"Chiang"},{"first":"K.","last":"Knight"},{"first":"W.","last":"Wang"}],"year":"2009","title":"11,001 new features for statistical machine translation","source":"D. Chiang, K. Knight, and W. Wang. 2009. 11,001 new features for statistical machine translation. In HLT-NAACL."},{"authors":[{"first":"D.","last":"Chiang"}],"year":"2012","title":"Hope and fear for discriminative training of statistical translation models","source":"D. Chiang. 2012. Hope and fear for discriminative training of statistical translation models. JMLR, 13:1159–1187."},{"authors":[{"first":"K.","last":"Crammer"},{"first":"O.","last":"Dekel"},{"first":"J.","last":"Keshet"},{"first":"S.","last":"Shalev-Shwartz"},{"first":"Y.","last":"Singer"}],"year":"2006","title":"Online passive-aggressive algorithms","source":"K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz, and Y. Singer. 2006. Online passive-aggressive algorithms. JMLR, 7:551–585."},{"authors":[{"first":"K.","last":"Crammer"},{"first":"A.","last":"Kulesza"},{"first":"M.","last":"Dredze"}],"year":"2009","title":"Adaptive regularization of weight vectors","source":"K. Crammer, A. Kulesza, and M. Dredze. 2009. Adaptive regularization of weight vectors. In NIPS."},{"authors":[{"first":"J.","last":"Duchi"},{"first":"Y.","last":"Singer"}],"year":"2009","title":"Efficient online and batch learning using forward backward splitting","source":"J. Duchi and Y. Singer. 2009. Efficient online and batch learning using forward backward splitting. JMLR, 10:2899–2934."},{"authors":[{"first":"J.","last":"Duchi"},{"first":"E.","last":"Hazan"},{"first":"Y.","last":"Singer"}],"year":"2011","title":"Adaptive sub-gradient methods for online learning and stochastic optimization","source":"J. Duchi, E. Hazan, and Y. Singer. 2011. Adaptive sub-gradient methods for online learning and stochastic optimization. JMLR, 12:2121–2159."},{"authors":[{"first":"M.","last":"Galley"},{"first":"C.","middle":"D.","last":"Manning"}],"year":"2008","title":"A simple and effective hierarchical phrase reordering model","source":"M. Galley and C. D. Manning. 2008. A simple and effective hierarchical phrase reordering model. In EMNLP."},{"authors":[{"first":"K.","last":"Gimpel"},{"first":"N.","middle":"A.","last":"Smith"}],"year":"2012","title":"Structured ramp loss minimization for machine translation","source":"K. Gimpel and N. A. Smith. 2012. Structured ramp loss minimization for machine translation. In HLT-NAACL."},{"authors":[{"first":"K.","last":"Gimpel"},{"first":"D.","last":"Das"},{"first":"N.","middle":"A.","last":"Smith"}],"year":"2010","title":"Distributed asynchronous online learning for natural language processing","source":"K. Gimpel, D. Das, and N. A. Smith. 2010. Distributed asynchronous online learning for natural language processing. In CoNLL."},{"authors":[{"first":"K.","last":"Gimpel"}],"year":"2012","title":"Discriminative Feature-Rich Modeling for Syntax-Based Machine Translation","source":"K. Gimpel. 2012. Discriminative Feature-Rich Modeling for Syntax-Based Machine Translation. Ph.D. thesis, Language Technologies Institute, Carnegie Mellon University."},{"authors":[{"first":"S.","last":"Green"},{"first":"J.","last":"DeNero"}],"year":"2012","title":"A class-based agreement model for generating accurately inflected translations","source":"S. Green and J. DeNero. 2012. A class-based agreement model for generating accurately inflected translations. In ACL."},{"authors":[{"first":"B.","last":"Haddow"},{"first":"P.","last":"Koehn"}],"year":"2012","title":"Analysing the effect of out-of-domain data on SMT systems","source":"B. Haddow and P. Koehn. 2012. Analysing the effect of out-of-domain data on SMT systems. In WMT."},{"authors":[{"first":"B.","last":"Haddow"},{"first":"A.","last":"Arun"},{"first":"P.","last":"Koehn"}],"year":"2011","title":"SampleRank training for phrase-based machine translation","source":"B. Haddow, A. Arun, and P. Koehn. 2011. SampleRank training for phrase-based machine translation. In WMT."},{"authors":[{"first":"E.","last":"Hasler"},{"first":"P.","last":"Bell"},{"first":"A.","last":"Ghoshal"},{"first":"B.","last":"Haddow"},{"first":"P.","last":"Koehn"},{"first":"F.","last":"McInnes"},{"last":"al"}],"year":"2012a","title":"The UEDIN systems for the IWSLT 2012 evaluation","source":"E. Hasler, P. Bell, A. Ghoshal, B. Haddow, P. Koehn, F. McInnes, et al. 2012a. The UEDIN systems for the IWSLT 2012 evaluation. In IWSLT."},{"authors":[{"first":"E.","last":"Hasler"},{"first":"B.","last":"Haddow"},{"first":"P.","last":"Koehn"}],"year":"2012b","title":"Sparse lexicalised features and topic adaptation for SMT","source":"E. Hasler, B. Haddow, and P. Koehn. 2012b. Sparse lexicalised features and topic adaptation for SMT. In IWSLT."},{"authors":[{"first":"X.","last":"He"},{"first":"L.","last":"Deng"}],"year":"2012","title":"Maximum expected BLEU training of phrase and lexicon translation models","source":"X. He and L. Deng. 2012. Maximum expected BLEU training of phrase and lexicon translation models. In ACL."},{"authors":[{"first":"R.","last":"Herbrich"},{"first":"T.","last":"Graepel"},{"first":"K.","last":"Obermayer"}],"year":"1999","title":"Support vector learning for ordinal regression","source":"R. Herbrich, T. Graepel, and K. Obermayer. 1999. Support vector learning for ordinal regression. In ICANN."},{"authors":[{"first":"M.","last":"Hopkins"},{"first":"J.","last":"May"}],"year":"2011","title":"Tuning as ranking","source":"M. Hopkins and J. May. 2011. Tuning as ranking. In EMNLP."},{"authors":[{"first":"A.","last":"Ittycheriah"},{"first":"S.","last":"Roukos"}],"year":"2007","title":"Direct translation model 2","source":"A. Ittycheriah and S. Roukos. 2007. Direct translation model 2. In HLT-NAACL."},{"authors":[{"first":"D.","last":"Klein"},{"first":"C.","middle":"D.","last":"Manning"}],"year":"2003","title":"Accurate unlexicalized parsing","source":"D. Klein and C. D. Manning. 2003. Accurate unlexicalized parsing. In ACL."},{"authors":[{"first":"P.","last":"Koehn"},{"first":"H.","last":"Hoang"},{"first":"A.","last":"Birch"},{"first":"C.","last":"Callison-Burch"},{"first":"M.","last":"Federico"},{"first":"N.","last":"Bertoldi"},{"last":"al"}],"year":"2007","title":"Moses: Open source toolkit for statistical machine translation","source":"P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, et al. 2007. Moses: Open source toolkit for statistical machine translation. In ACL, Demonstration Session."},{"authors":[{"first":"J.","last":"Langford"},{"first":"A.","middle":"J.","last":"Smola"},{"first":"M.","last":"Zinkevich"}],"year":"2009","title":"Slow learners are fast","source":"J. Langford, A. J. Smola, and M. Zinkevich. 2009. Slow learners are fast. In NIPS."},{"authors":[{"first":"P.","last":"Liang"},{"first":"D.","last":"Klein"}],"year":"2009","title":"Online EM for unsupervised models","source":"P. Liang and D. Klein. 2009. Online EM for unsupervised models. In HLT-NAACL."},{"authors":[{"first":"P.","last":"Liang"},{"first":"A.","last":"Bouchard-Côté"},{"first":"D.","last":"Klein"},{"first":"B.","last":"Taskar"}],"year":"2006a","title":"An end-to-end discriminative approach to machine translation","source":"P. Liang, A. Bouchard-Côté, D. Klein, and B. Taskar. 2006a. An end-to-end discriminative approach to machine translation. In ACL."},{"authors":[{"first":"P.","last":"Liang"},{"first":"B.","last":"Taskar"},{"first":"D.","last":"Klein"}],"year":"2006b","title":"Alignment by agreement","source":"P. Liang, B. Taskar, and D. Klein. 2006b. Alignment by agreement. In NAACL."},{"authors":[{"first":"C.","middle":"-Y.","last":"Lin"},{"first":"F.","middle":"J.","last":"Och"}],"year":"2004","title":"ORANGE: a method for evaluating automatic evaluation metrics for machine translation","source":"C.-Y. Lin and F. J. Och. 2004. ORANGE: a method for evaluating automatic evaluation metrics for machine translation. In COLING."},{"authors":[{"first":"M.","last":"Maamouri"},{"first":"A.","last":"Bies"},{"first":"S.","last":"Kulick"}],"year":"2008","title":"Enhanc-ing the Arabic Treebank: A collaborative effort to-ward new annotation guidelines","source":"M. Maamouri, A. Bies, and S. Kulick. 2008. Enhanc-ing the Arabic Treebank: A collaborative effort to-ward new annotation guidelines. In LREC. 320"},{"authors":[{"first":"M.","last":"Marcus"},{"first":"M.","middle":"A.","last":"Marcinkiewicz"},{"first":"B.","last":"Santorini"}],"year":"1993","title":"Building a large annotated corpus of English: The Penn Treebank","source":"M. Marcus, M. A. Marcinkiewicz, and B. Santorini. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19:313–330."},{"authors":[{"first":"R.","last":"McDonald"},{"first":"K.","last":"Hall"},{"first":"G.","last":"Mann"}],"year":"2010","title":"Distributed training strategies for the structured perceptron","source":"R. McDonald, K. Hall, and G. Mann. 2010. Distributed training strategies for the structured perceptron. In NAACL-HLT."},{"authors":[{"first":"A.","middle":"Y.","last":"Ng"}],"year":"2004","title":"Feature selection, L1 vs","source":"A. Y. Ng. 2004. Feature selection, L1 vs. L2 regularization, and rotational invariance. In ICML."},{"authors":[{"first":"F.","middle":"J.","last":"Och"},{"first":"H.","last":"Ney"}],"year":"2002","title":"Discriminative training and maximum entropy models for statistical machine translation","source":"F. J. Och and H. Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In ACL."},{"authors":[{"first":"F.","middle":"J.","last":"Och"},{"first":"H.","last":"Ney"}],"year":"2004","title":"The alignment template approach to statistical machine translation","source":"F. J. Och and H. Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30(4):417–449."},{"authors":[{"first":"F.","middle":"J.","last":"Och"}],"year":"2003","title":"Minimum error rate training for statistical machine translation","source":"F. J. Och. 2003. Minimum error rate training for statistical machine translation. In ACL."},{"authors":[{"first":"K.","last":"Papineni"},{"first":"S.","last":"Roukos"},{"first":"T.","last":"Ward"},{"first":"W.","last":"Zhu"}],"year":"2002","title":"BLEU: a method for automatic evaluation of machine translation","source":"K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In ACL."},{"authors":[{"first":"S.","last":"Riezler"},{"first":"J.","middle":"T.","last":"Maxwell"}],"year":"2005","title":"On some pitfalls in automatic evaluation and significance testing in MT","source":"S. Riezler and J. T. Maxwell. 2005. On some pitfalls in automatic evaluation and significance testing in MT. In ACL Workshop on Intrinsic and Extrinsic Evalua-tion Measures for Machine Translation and/or Summarization (MTSE)."},{"authors":[{"first":"P.","last":"Simianer"},{"first":"S.","last":"Riezler"},{"first":"C.","last":"Dyer"}],"year":"2012","title":"Joint feature selection in distributed stochastic learning for large-scale discriminative training in SMT","source":"P. Simianer, S. Riezler, and C. Dyer. 2012. Joint feature selection in distributed stochastic learning for large-scale discriminative training in SMT. In ACL."},{"authors":[{"first":"A","last":"Stolcke"}],"year":"2002","title":"SRILM—an extensible language modeling toolkit","source":"A Stolcke. 2002. SRILM—an extensible language modeling toolkit. In ICSLP."},{"authors":[{"first":"C.","last":"Tillmann"},{"first":"T.","last":"Zhang"}],"year":"2006","title":"A discriminative global training algorithm for statistical MT","source":"C. Tillmann and T. Zhang. 2006. A discriminative global training algorithm for statistical MT. In ACL-COLING."},{"authors":[{"first":"T.","last":"Watanabe"},{"first":"J.","last":"Suzuki"},{"first":"H.","last":"Tsukada"},{"first":"H.","last":"Isozaki"}],"year":"2007","title":"Online large-margin training for statistical machine translation","source":"T. Watanabe, J. Suzuki, H. Tsukada, and H. Isozaki. 2007. Online large-margin training for statistical machine translation. In EMNLP-CoNLL."},{"authors":[{"first":"T.","last":"Watanabe"}],"year":"2012","title":"Optimized online rank learning for machine translation","source":"T. Watanabe. 2012. Optimized online rank learning for machine translation. In HLT-NAACL. Association for Computational Linguistics."},{"authors":[{"first":"B.","last":"Xiang"},{"first":"A.","last":"Ittycheriah"}],"year":"2011","title":"Discriminative feature-tied mixture modeling for statistical machine translation","source":"B. Xiang and A. Ittycheriah. 2011. Discriminative feature-tied mixture modeling for statistical machine translation. In ACL-HLT."},{"authors":[{"first":"N.","last":"Xue"},{"first":"F.","last":"Xia"},{"first":"F.","last":"Chiou"},{"first":"M.","last":"Palmer"}],"year":"2005","title":"The Penn Chinese Treebank: Phrase structure annotation of a large corpus","source":"N. Xue, F. Xia, F. Chiou, and M. Palmer. 2005. The Penn Chinese Treebank: Phrase structure annotation of a large corpus. Natural Language Engineering, 11(2):207–238. 321"}],"cites":[{"style":0,"text":"Hasler et al., 2012a","origin":{"pointer":"/sections/2/paragraphs/0","offset":616,"length":20},"authors":[{"last":"Hasler"},{"last":"al."}],"year":"2012a","references":["/references/20"]},{"style":0,"text":"Gimpel and Smith, 2012","origin":{"pointer":"/sections/2/paragraphs/0","offset":793,"length":22},"authors":[{"last":"Gimpel"},{"last":"Smith"}],"year":"2012","references":["/references/14"]},{"style":0,"text":"Simianer et al., 2012","origin":{"pointer":"/sections/2/paragraphs/0","offset":817,"length":21},"authors":[{"last":"Simianer"},{"last":"al."}],"year":"2012","references":["/references/42"]},{"style":0,"text":"Hopkins and May, 2011","origin":{"pointer":"/sections/2/paragraphs/1","offset":246,"length":21},"authors":[{"last":"Hopkins"},{"last":"May"}],"year":"2011","references":["/references/24"]},{"style":0,"text":"Duchi et al., 2011","origin":{"pointer":"/sections/2/paragraphs/1","offset":394,"length":18},"authors":[{"last":"Duchi"},{"last":"al."}],"year":"2011","references":["/references/12"]},{"style":0,"text":"Duchi and Singer, 2009","origin":{"pointer":"/sections/2/paragraphs/1","offset":640,"length":22},"authors":[{"last":"Duchi"},{"last":"Singer"}],"year":"2009","references":["/references/11"]},{"style":0,"text":"Haddow and Koehn, 2012","origin":{"pointer":"/sections/2/paragraphs/2","offset":381,"length":22},"authors":[{"last":"Haddow"},{"last":"Koehn"}],"year":"2012","references":["/references/18"]},{"style":0,"text":"Och, 2003","origin":{"pointer":"/sections/2/paragraphs/3","offset":120,"length":9},"authors":[{"last":"Och"}],"year":"2003","references":["/references/39"]},{"style":0,"text":"Koehn et al., 2007","origin":{"pointer":"/sections/2/paragraphs/3","offset":152,"length":18},"authors":[{"last":"Koehn"},{"last":"al."}],"year":"2007","references":["/references/27"]},{"style":0,"text":"Cherry and Foster (2012)","origin":{"pointer":"/sections/2/paragraphs/3","offset":209,"length":24},"authors":[{"last":"Cherry"},{"last":"Foster"}],"year":"2012","references":["/references/5"]},{"style":0,"text":"Chiang, 2012","origin":{"pointer":"/sections/2/paragraphs/3","offset":282,"length":12},"authors":[{"last":"Chiang"}],"year":"2012","references":["/references/8"]},{"style":0,"text":"Cer et al., 2010","origin":{"pointer":"/sections/2/paragraphs/3","offset":597,"length":16},"authors":[{"last":"Cer"},{"last":"al."}],"year":"2010","references":["/references/3"]},{"style":0,"text":"Liang and Klein, 2009","origin":{"pointer":"/sections/3/paragraphs/0","offset":341,"length":21},"authors":[{"last":"Liang"},{"last":"Klein"}],"year":"2009","references":["/references/29"]},{"style":0,"text":"Bottou and Bousquet, 2011","origin":{"pointer":"/sections/3/paragraphs/0","offset":364,"length":25},"authors":[{"last":"Bottou"},{"last":"Bousquet"}],"year":"2011","references":["/references/1"]},{"style":0,"text":"Chiang et al. (2008)","origin":{"pointer":"/sections/3/paragraphs/17","offset":182,"length":20},"authors":[{"last":"Chiang"},{"last":"al."}],"year":"2008","references":["/references/6"]},{"style":0,"text":"Crammer et al., 2006","origin":{"pointer":"/sections/3/paragraphs/17","offset":234,"length":20},"authors":[{"last":"Crammer"},{"last":"al."}],"year":"2006","references":["/references/9"]},{"style":0,"text":"Chiang (2012)","origin":{"pointer":"/sections/3/paragraphs/21","offset":76,"length":13},"authors":[{"last":"Chiang"}],"year":"2012","references":["/references/8"]},{"style":0,"text":"Crammer et al., 2009","origin":{"pointer":"/sections/3/paragraphs/21","offset":104,"length":20},"authors":[{"last":"Crammer"},{"last":"al."}],"year":"2009","references":["/references/10"]},{"style":0,"text":"Herbrich et al., 1999","origin":{"pointer":"/sections/4/paragraphs/0","offset":381,"length":21},"authors":[{"last":"Herbrich"},{"last":"al."}],"year":"1999","references":["/references/23"]},{"style":0,"text":"Hopkins and May (2011)","origin":{"pointer":"/sections/4/paragraphs/0","offset":423,"length":22},"authors":[{"last":"Hopkins"},{"last":"May"}],"year":"2011","references":["/references/24"]},{"style":0,"text":"Lin and Och, 2004","origin":{"pointer":"/sections/4/paragraphs/0","offset":726,"length":17},"authors":[{"last":"Lin"},{"last":"Och"}],"year":"2004","references":["/references/32"]},{"style":0,"text":"Hopkins and May (2011)","origin":{"pointer":"/sections/4/paragraphs/3","offset":234,"length":22},"authors":[{"last":"Hopkins"},{"last":"May"}],"year":"2011","references":["/references/24"]},{"style":0,"text":"Duchi and Singer, 2009","origin":{"pointer":"/sections/4/paragraphs/9","offset":454,"length":22},"authors":[{"last":"Duchi"},{"last":"Singer"}],"year":"2009","references":["/references/11"]},{"style":0,"text":"Ng, 2004","origin":{"pointer":"/sections/4/paragraphs/13","offset":145,"length":8},"authors":[{"last":"Ng"}],"year":"2004","references":["/references/36"]},{"style":0,"text":"Chiang et al., 2009","origin":{"pointer":"/sections/4/paragraphs/19","offset":230,"length":19},"authors":[{"last":"Chiang"},{"last":"al."}],"year":"2009","references":["/references/7"]},{"style":0,"text":"Langford et al. (2009)","origin":{"pointer":"/sections/4/paragraphs/23","offset":534,"length":22},"authors":[{"last":"Langford"},{"last":"al."}],"year":"2009","references":["/references/28"]},{"style":0,"text":"Langford et al. (2009)","origin":{"pointer":"/sections/4/paragraphs/24","offset":5,"length":22},"authors":[{"last":"Langford"},{"last":"al."}],"year":"2009","references":["/references/28"]},{"style":0,"text":"Gimpel et al. (2010)","origin":{"pointer":"/sections/4/paragraphs/24","offset":166,"length":20},"authors":[{"last":"Gimpel"},{"last":"al."}],"year":"2010","references":["/references/15"]},{"style":0,"text":"Liang and Klein, 2009","origin":{"pointer":"/sections/4/paragraphs/25","offset":328,"length":21},"authors":[{"last":"Liang"},{"last":"Klein"}],"year":"2009","references":["/references/29"]},{"style":0,"text":"Bottou and Bousquet, 2011","origin":{"pointer":"/sections/4/paragraphs/25","offset":351,"length":25},"authors":[{"last":"Bottou"},{"last":"Bousquet"}],"year":"2011","references":["/references/1"]},{"style":0,"text":"Cer et al., 2010","origin":{"pointer":"/sections/5/paragraphs/0","offset":69,"length":16},"authors":[{"last":"Cer"},{"last":"al."}],"year":"2010","references":["/references/3"]},{"style":0,"text":"Och and Ney, 2004","origin":{"pointer":"/sections/5/paragraphs/0","offset":140,"length":17},"authors":[{"last":"Och"},{"last":"Ney"}],"year":"2004","references":["/references/38"]},{"style":0,"text":"Liang et al., 2006b","origin":{"pointer":"/sections/5/paragraphs/1","offset":252,"length":19},"authors":[{"last":"Liang"},{"last":"al."}],"year":"2006b","references":["/references/31"]},{"style":0,"text":"Stolcke, 2002","origin":{"pointer":"/sections/5/paragraphs/2","offset":33,"length":13},"authors":[{"last":"Stolcke"}],"year":"2002","references":["/references/43"]},{"style":0,"text":"Galley and Manning (2008)","origin":{"pointer":"/sections/5/paragraphs/2","offset":338,"length":25},"authors":[{"last":"Galley"},{"last":"Manning"}],"year":"2008","references":["/references/13"]},{"style":0,"text":"Klein and Manning, 2003","origin":{"pointer":"/sections/5/paragraphs/5","offset":65,"length":23},"authors":[{"last":"Klein"},{"last":"Manning"}],"year":"2003","references":["/references/26"]},{"style":0,"text":"Marcus et al., 1993","origin":{"pointer":"/sections/5/paragraphs/5","offset":131,"length":19},"authors":[{"last":"Marcus"},{"last":"al."}],"year":"1993","references":["/references/34"]},{"style":0,"text":"Green and DeNero, 2012","origin":{"pointer":"/sections/5/paragraphs/5","offset":200,"length":22},"authors":[{"last":"Green"},{"last":"DeNero"}],"year":"2012","references":["/references/17"]},{"style":0,"text":"Maamouri et al., 2008","origin":{"pointer":"/sections/5/paragraphs/5","offset":272,"length":21},"authors":[{"last":"Maamouri"},{"last":"al."}],"year":"2008","references":["/references/33"]},{"style":0,"text":"Chang et al., 2008","origin":{"pointer":"/sections/5/paragraphs/5","offset":349,"length":18},"authors":[{"last":"Chang"},{"last":"al."}],"year":"2008","references":["/references/4"]},{"style":0,"text":"Xue et al., 2005","origin":{"pointer":"/sections/5/paragraphs/5","offset":418,"length":16},"authors":[{"last":"Xue"},{"last":"al."}],"year":"2005","references":["/references/48"]},{"style":0,"text":"Och, 2003","origin":{"pointer":"/sections/5/paragraphs/6","offset":717,"length":9},"authors":[{"last":"Och"}],"year":"2003","references":["/references/39"]},{"style":0,"text":"Cer et al. (2008)","origin":{"pointer":"/sections/5/paragraphs/6","offset":790,"length":17},"authors":[{"last":"Cer"},{"last":"al."}],"year":"2008","references":["/references/2"]},{"style":0,"text":"Papineni et al., 2002","origin":{"pointer":"/sections/5/paragraphs/7","offset":30,"length":21},"authors":[{"last":"Papineni"},{"last":"al."}],"year":"2002","references":["/references/40"]},{"style":0,"text":"Cherry and Foster, 2012","origin":{"pointer":"/sections/5/paragraphs/8","offset":216,"length":23},"authors":[{"last":"Cherry"},{"last":"Foster"}],"year":"2012","references":["/references/5"]},{"style":0,"text":"Cherry and Foster (2012)","origin":{"pointer":"/sections/5/paragraphs/8","offset":391,"length":24},"authors":[{"last":"Cherry"},{"last":"Foster"}],"year":"2012","references":["/references/5"]},{"style":0,"text":"Hasler et al., 2012b","origin":{"pointer":"/sections/5/paragraphs/10","offset":65,"length":20},"authors":[{"last":"Hasler"},{"last":"al."}],"year":"2012b","references":["/references/21"]},{"style":0,"text":"Och and Ney, 2004","origin":{"pointer":"/sections/5/paragraphs/10","offset":311,"length":17},"authors":[{"last":"Och"},{"last":"Ney"}],"year":"2004","references":["/references/38"]},{"style":0,"text":"Chiang, 2012","origin":{"pointer":"/sections/5/paragraphs/15","offset":575,"length":12},"authors":[{"last":"Chiang"}],"year":"2012","references":["/references/8"]},{"style":0,"text":"Chiang, 2012","origin":{"pointer":"/sections/5/paragraphs/16","offset":34,"length":12},"authors":[{"last":"Chiang"}],"year":"2012","references":["/references/8"]},{"style":0,"text":"Riezler and Maxwell (2005)","origin":{"pointer":"/sections/5/paragraphs/18","offset":416,"length":26},"authors":[{"last":"Riezler"},{"last":"Maxwell"}],"year":"2005","references":["/references/41"]},{"style":0,"text":"Chiang (2012)","origin":{"pointer":"/sections/5/paragraphs/18","offset":448,"length":13},"authors":[{"last":"Chiang"}],"year":"2012","references":["/references/8"]},{"style":0,"text":"He and Deng (2012)","origin":{"pointer":"/sections/5/paragraphs/22","offset":870,"length":18},"authors":[{"last":"He"},{"last":"Deng"}],"year":"2012","references":["/references/22"]},{"style":0,"text":"Simianer et al. (2012)","origin":{"pointer":"/sections/5/paragraphs/22","offset":893,"length":22},"authors":[{"last":"Simianer"},{"last":"al."}],"year":"2012","references":["/references/42"]},{"style":0,"text":"Haddow and Koehn, 2012","origin":{"pointer":"/sections/5/paragraphs/22","offset":1159,"length":22},"authors":[{"last":"Haddow"},{"last":"Koehn"}],"year":"2012","references":["/references/18"]},{"style":0,"text":"Hasler et al. (2012b)","origin":{"pointer":"/sections/24/paragraphs/0","offset":41,"length":21},"authors":[{"last":"Hasler"},{"last":"al."}],"year":"2012b","references":["/references/21"]},{"style":0,"text":"Chiang (2012)","origin":{"pointer":"/sections/24/paragraphs/1","offset":0,"length":13},"authors":[{"last":"Chiang"}],"year":"2012","references":["/references/8"]},{"style":0,"text":"Chiang et al., 2008","origin":{"pointer":"/sections/24/paragraphs/1","offset":76,"length":19},"authors":[{"last":"Chiang"},{"last":"al."}],"year":"2008","references":["/references/6"]},{"style":0,"text":"Watanabe et al., 2007","origin":{"pointer":"/sections/24/paragraphs/1","offset":97,"length":21},"authors":[{"last":"Watanabe"},{"last":"al."}],"year":"2007","references":["/references/45"]},{"style":0,"text":"Simianer et al. (2012)","origin":{"pointer":"/sections/24/paragraphs/2","offset":0,"length":22},"authors":[{"last":"Simianer"},{"last":"al."}],"year":"2012","references":["/references/42"]},{"style":0,"text":"McDonald et al., 2010","origin":{"pointer":"/sections/24/paragraphs/2","offset":132,"length":21},"authors":[{"last":"McDonald"},{"last":"al."}],"year":"2010","references":["/references/35"]},{"style":0,"text":"Watanabe (2012)","origin":{"pointer":"/sections/24/paragraphs/3","offset":0,"length":15},"authors":[{"last":"Watanabe"}],"year":"2012","references":["/references/46"]},{"style":0,"text":"Gimpel, 2012","origin":{"pointer":"/sections/24/paragraphs/4","offset":77,"length":12},"authors":[{"last":"Gimpel"}],"year":"2012","references":["/references/16"]},{"style":0,"text":"Cherry and Foster, 2012","origin":{"pointer":"/sections/24/paragraphs/4","offset":104,"length":23},"authors":[{"last":"Cherry"},{"last":"Foster"}],"year":"2012","references":["/references/5"]},{"style":0,"text":"Haddow et al., 2011","origin":{"pointer":"/sections/24/paragraphs/4","offset":129,"length":19},"authors":[{"last":"Haddow"},{"last":"al."}],"year":"2011","references":["/references/19"]},{"style":0,"text":"Arun and Koehn, 2007","origin":{"pointer":"/sections/24/paragraphs/4","offset":150,"length":20},"authors":[{"last":"Arun"},{"last":"Koehn"}],"year":"2007","references":["/references/0"]},{"style":0,"text":"Xiang and Ittycheriah, 2011","origin":{"pointer":"/sections/24/paragraphs/4","offset":190,"length":27},"authors":[{"last":"Xiang"},{"last":"Ittycheriah"}],"year":"2011","references":["/references/47"]},{"style":0,"text":"Ittycheriah and Roukos, 2007","origin":{"pointer":"/sections/24/paragraphs/4","offset":219,"length":28},"authors":[{"last":"Ittycheriah"},{"last":"Roukos"}],"year":"2007","references":["/references/25"]},{"style":0,"text":"Och and Ney, 2002","origin":{"pointer":"/sections/24/paragraphs/4","offset":249,"length":17},"authors":[{"last":"Och"},{"last":"Ney"}],"year":"2002","references":["/references/37"]},{"style":0,"text":"Liang et al., 2006a","origin":{"pointer":"/sections/24/paragraphs/4","offset":281,"length":19},"authors":[{"last":"Liang"},{"last":"al."}],"year":"2006a","references":["/references/30"]},{"style":0,"text":"Tillmann and Zhang, 2006","origin":{"pointer":"/sections/24/paragraphs/4","offset":323,"length":24},"authors":[{"last":"Tillmann"},{"last":"Zhang"}],"year":"2006","references":["/references/44"]},{"style":0,"text":"Cherry and Foster, 2012","origin":{"pointer":"/sections/24/paragraphs/4","offset":434,"length":23},"authors":[{"last":"Cherry"},{"last":"Foster"}],"year":"2012","references":["/references/5"]}]}
