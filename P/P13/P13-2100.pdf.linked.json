{"sections":[{"title":"","paragraphs":["Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 561–566, Sofia, Bulgaria, August 4-9 2013. c⃝2013 Association for Computational Linguistics"]},{"title":"Using Integer Linear Programming in Concept-to-Text Generation to Produce More Compact Texts Gerasimos Lampouras and Ion Androutsopoulos Department of Informatics Athens University of Economics and Business Patission 76, GR-104 34 Athens, Greece http://nlp.cs.aueb.gr/ Abstract","paragraphs":["We present an ILP model of concept-to-text generation. Unlike pipeline architectures, our model jointly considers the choices in content selection, lexicalization, and aggregation to avoid greedy decisions and produce more compact texts."]},{"title":"1 Introduction","paragraphs":["Concept-to-text natural language generation (NLG) generates texts from formal knowledge representations (Reiter and Dale, 2000). With the emergence of the Semantic Web (Antoniou and van Harmelen, 2008), interest in concept-to-text NLG has been revived and several methods have been proposed to express axioms of OWL ontologies (Grau et al., 2008) in natural language (Bontcheva, 2005; Mellish and Sun, 2006; Galanis and Androutsopoulos, 2007; Mellish and Pan, 2008; Schwitter et al., 2008; Schwitter, 2010; Liang et al., 2011; Williams et al., 2011).","NLG systems typically employ a pipeline architecture. They usually start by selecting the logical facts to express. The next stage, text planning, ranges from simply ordering the selected facts to complex decisions about the rhetorical structure of the text. Lexicalization then selects the words and syntactic structures that will realize each fact, specifying how each fact can be expressed as a single sentence. Sentence aggregation then combines sentences into longer ones. Another component generates appropriate referring expressions, and surface realization produces the final text.","Each stage of the pipeline is treated as a local optimization problem, where the decisions of the previous stages cannot be modified. This arrangement produces texts that may not be optimal, since the decisions of the stages have been shown to be co-dependent (Danlos, 1984; Marciniak and Strube, 2005; Belz, 2008). For example, content selection and lexicalization may lead to more or fewer sentence aggregation opportunities.","We present an Integer Linear Programming (ILP) model that combines content selection, lexicalization, and sentence aggregation. Our model does not consider text planning, nor referring expression generation, which we hope to include in future work, but it is combined with an external simple text planner and a referring expression generation component; we also do not discuss surface realization. Unlike pipeline architectures, our model jointly examines the possible choices in the three NLG stages it considers, to avoid greedy local decisions. Given an individual (entity) or class of an OWL ontology and a set of facts (OWL axioms) about the individual or class, we aim to produce a text that expresses as many of the facts in as few words as possible. This is important when space is limited or expensive (e.g., product descriptions on smartphones, advertisements in search engines).","Although the search space of our model is very large and ILP problems are in general NP-hard, ILP solvers can be used, they are very fast in practice, and they guarantee finding a global optimum. Experiments show that our ILP model outperforms, in terms of compression, an NLG system that uses the same components, but connected in a pipeline, with no deterioration in fluency and clarity."]},{"title":"2 Related work","paragraphs":["Marciniak and Strube (2005) propose a general ILP approach for language processing applications where the decisions of classifiers that consider particular, but co-dependent, subtasks need to be combined. They also show how their approach can be used to generate multi-sentence route directions, in a setting with very different inputs and processing stages than the ones we consider.","Barzilay and Lapata (2005) treat content selection as an optimization problem. Given a pool of facts and scores indicating the importance of each 561 fact or pair of facts, they select the facts to express by formulating an optimization problem similar to energy minimization. In other work, Barzilay and Lapata (2006) consider sentence aggregation. Given a set of facts that a content selection stage has produced, aggregation is viewed as the problem of partitioning the facts into optimal subsets. Sentences expressing facts that are placed in the same subset are aggregated to form a longer sentence. An ILP model is used to find the partitioning that maximizes the pairwise similarity of the facts in each subset, subject to constraints limiting the number of subsets and the facts in each subset.","Althaus et al. (2004) show that ordering a set of sentences to maximize sentence-to-sentence coherence is equivalent to the traveling salesman problem and, hence, NP-complete. They also show how an ILP solver can be used in practice.","Joint optimization ILP models have also been used in multi-document text summarization and sentence compression (McDonald, 2007; Clarke and Lapata, 2008; Berg-Kirkpatrick et al., 2011; Galanis et al., 2012; Woodsend and Lapata, 2012), where the input is text, not formal knowledge represetations. Statistical methods to jointly perform content selection, lexicalization, and surface realization have also been proposed in NLG (Liang et al., 2009; Konstas and Lapata, 2012a; Konstas and Lapata, 2012b), but they are currently limited to generating single sentences from flat records.","To the best of our knowledge, this article is the first one to consider content selection, lexicalization, and sentence aggregation as an ILP joint optimization problem in the context of multi-sentence concept-to-text generation. It is also the first article to consider ILP in NLG from OWL ontologies."]},{"title":"3 Our ILP model of NLG","paragraphs":["Let F = {f1, . . . , fn} be the set of all the facts fi (OWL axioms) about the individual or class to be described. OWL axioms can be represented as sets of RDF triples of the form ⟨S, R, O⟩, where S is an individual or class, O is another individual, class, or datatype value, and R is a relation (property) that connects S to O. Hence, we can assume that each fact fi is a triple ⟨Si, Ri, Oi⟩.1","For each fact fi, a set Pi = {pi1, pi2, . . . } of alternative sentence plans is available. Each 1","We actually convert the RDF triples to simpler message triples, so that each message triple can be easily expressed by a simple sentence, but we do not discuss this conversion here. sentence plan pik specifies how to express fi = ⟨Si, Ri, Oi⟩ as an alternative single sentence. In our work, a sentence plan is a sequence of slots, along with instructions specifying how to fill the slots in; and each sentence plan is associated with the relations it can express. For example, ⟨exhibit12, foundIn, athens⟩ could be expressed using a sentence plan like “[ ref (S)] [findpast] [in] [ref (O)]”, where square brackets denote slots, ref (S) and ref (O) are instructions requiring referring expressions for S and O in the corresponding slots, and “findpast” requires the simple past form of “find”. In our example, the sentence plan would lead to a sentence like “Exhibit 12 was found in Athens”. We call elements the slots with their instructions, but with “ S” and “ O” accompanied by the individuals, classes, or datatype values they refer to; in our example, the elements are “[ ref (S: exhibit12)]”, “[findpast]”, “[in]”, “[ ref (O: athens)]”. Different sentence plans may lead to more or fewer aggregation opportunities; for example, sentences with the same verb are easier to aggregate. We use aggregation rules (Dalianis, 1999) that operate on sentence plans and usually lead to shorter texts.","Let s1, . . . , sm be disjoint subsets of F , each containing 0 to n facts, with m < n. A single sentence is generated for each subset sj by aggregating the sentences (more precisely, the sentence plans) expressing the facts of sj.2","An empty sj generates no sentence, i.e., the resulting text can be at most m sentences long. Let us also define: ai = { 1, if fact fi is selected 0, otherwise (1) likj =    1, if sentence plan pik is used to express","fact fi, and fi is in subset sj 0, otherwise (2) btj = { 1, if element et is used in subset sj 0, otherwise (3) and let B be the set of all the distinct elements (no duplicates) from all the available sentence plans that can express the facts of F . The length of an aggregated sentence resulting from a subset sj can be roughly estimated by counting the distinct elements of the sentence plans that have been chosen to express the facts of sj; elements that occur more than once in the chosen sentence plans of sj","2","All the sentences of every possible subset sj can be aggregated, because all the sentences share the same subject, the class or individual being described. If multiple aggregation rules apply, we use the one that leads to a shorter text. 562 are counted only once, because they will probably be expressed only once, due to aggregation.","Our objective function (4) maximizes the number of selected facts fi and minimizes the number of distinct elements in each subset sj, i.e., the approximate length of the corresponding aggregated sentence; an alternative explanation is that by minimizing the number of distinct elements in each sj, we favor subsets that aggregate well. By a and b we jointly denote all the ai and btj variables. The two parts (sums) of the objective function are normalized to [0, 1] by dividing by the total number of available facts |F | and the number of subsets m times the total number of distinct elements |B|. In the first part of the objective, we treat all the facts as equally important; if importance scores are also available for the facts, they can be added as multipliers of αi. The parameters λ1 and λ2 are used to tune the priority given to expressing many facts vs. generating shorter texts; we set λ1 + λ2 = 1. max a,b λ1 · |F| ∑ i=1 ai |F | − λ2 · m ∑ j=1 |B| ∑ t=1","btj m · |B| (4) subject to: ai = m ∑ j=1 |P i| ∑ k=1 likj, for i = 1, . . . , n (5) ∑ e t∈B","ik btj ≥ |Bik| · likj, for i = 1, . . . , n j = 1, . . . , m k = 1, . . . , |Pi| (6) ∑ p ik∈P(e","t) likj ≥ btj, for t = 1, . . . , |B| j = 1, . . . , m (7) |B| ∑ t=1 btj ≤ Bmax, for j = 1, . . . , m (8) |P i| ∑ k=1 likj + |P i′| ∑ k′ =1","li′ k′ j ≤ 1, for j = 1, . . . , m, i = 2, . . . , n i′","= 1, . . . , n − 1; i ̸= i′ section(fi) ̸= section(f′","i)","(9)","Constraint 5 ensures that for each selected fact, only one sentence plan in only one subset is selected; if a fact is not selected, no sentence plan for the fact is selected either. |σ| denotes the cardinality of a set σ. In constraint 6, Bik is the set of distinct elements et of the sentence plan pik. This constraint ensures that if pik is selected in a subset sj, then all the elements of pik are also present in sj. If pik is not selected in sj, then some of its elements may still be present in sj, if they appear in another selected sentence plan of sj.","In constraint 7, P (et) is the set of sentence plans that contain element et. If et is used in a subset sj, then at least one of the sentence plans of P (et) must also be selected in sj. If et is not used in sj, then no sentence plan of P (et) may be selected in sj. Lastly, constraint 8 limits the number of elements that a subset sj can contain to a maximum allowed number Bmax, in effect limiting the maximum length of an aggregated sentence.","We assume that each relation R has been manually mapped to a single topical section; e.g., relations expressing the color, body, and flavor of a wine may be grouped in one section, and relations about the wine’s producer in another. The section of a fact fi = ⟨Si, Ri, Oi⟩ is the section of its relation Ri. Constraint 9 ensures that facts from different sections will not be placed in the same subset sj, to avoid unnatural aggregations."]},{"title":"4 Experiments","paragraphs":["We used NaturalOWL (Galanis and Androutsopoulos, 2007; Galanis et al., 2009; Androutsopoulos et al., 2013), an NLG system for OWL ontologies that relies on a pipeline of content selection, text planning, lexicalization, aggregation, referring expression generation, and surface realization.3","We modified content selection, lexicalization, and aggregation to use our ILP model, maintaining the aggregation rules of the original system.4","For referring expression generation and surface realization, the new system, called ILPNLG, invokes the corresponding components of NaturalOWL.","The original system, called PIPELINE, assumes that each relation has been mapped to a topical section, as in ILPNLG. It also assumes that a manually specified order of the sections and the relations of each section is available, which is used by the text planner to order the selected facts (by their relations). The subsequent components of the pipeline are not allowed to change the order of the facts, and aggregation operates only on sentence plans of adjacent facts from the same section. In ILPNLG, the manually specified order of sections and relations is used to order the sentences of each subset sj (before aggregating them), the aggregated sentences in each section (each aggregated sentence inherits the minimum order of its constituents), and the sections (with their sentences).","We used the Wine Ontology, which had been 3 All the software and data we used are freely available","from http://nlp.cs.aueb.gr/software.html.","We use version 2 of NaturalOWL. 4 We use the Branch and Cut implementation of GLPK; see","sourceforge.net/projects/winglpk/. 563 used in previous experiments with PIPELINE.5","We kept the 2 topical sections, the ordering of sections and relations, and the sentence plans that had been used in the previous experiments, but we added more sentence plans to ensure that 3 sentence plans were available per fact. We generated texts for the 52 wine individuals of the ontology; we did not experiment with texts describ-ing classes of wines, because we could not think of multiple alternative sentence plans for many of their axioms. For each individual, there were 5 facts on average and a maximum of 6 facts.","PIPELINE has a parameter M specifying the maximum number of facts it is allowed to report per text. When M is smaller than the number of available facts |F | and all the facts are treated as equally important, as in our experiments, it selects randomly M of the available facts. We repeated the generation of PIPELINE’s texts for the 52 individuals for M = 2, 3, 4, 5, 6. For each M , the texts of PIPELINE for the 52 individuals were generated three times, each time using one of the different alternative sentence plans of each relation. We also generated the texts using a variant of PIPELINE, dubbed PIPELINESHORT, which always selects the shortest (in elements) sentence plan among the available ones. In all cases, PIPELINE and PIPELINESHORT were allowed to form aggregated sentences containing up to Bmax = 22 distinct elements, which was the number of distinct elements of the longest aggregated sentence in the previous experiments, where PIPELINE was allowed to aggregate up to 3 original sentences.","With ILPNLG, we repeated the generation of the texts of the 52 individuals using different values of λ1 (λ2 = 1 − λ1), which led to texts expressing from zero to all of the available facts. We set the maximum number of fact subsets to m = 3, which was the maximum number of aggregated sentences observed in the texts of PIPELINE and PIPELINESHORT. Again, we set Bmax = 22.","We compared ILPNLG to PIPELINE and PIPELINESHORT by measuring the average number of facts they reported divided by the average text length (in words). Figure 1 shows this ratio as a function of the average number of reported facts, along with 95% confidence intervals (of sample means). PIPELINESHORT achieved better results than PIPELINE, but the differences were small.","For λ1 < 0.2, ILPNLG produces empty texts, 5 See www.w3.org/TR/owl-guide/wine.rdf. Figure 1: Facts/words ratio of the generated texts. since it focuses on minimizing the number of distinct elements of each text. For λ1 ≥ 0.225, it per-forms better than the other systems. For λ1 ≈ 0.3, it obtains the highest fact/words ratio by selecting the facts and sentence plans that lead to the most compressive aggregations. For greater values of λ1, it selects additional facts whose sentence plans do not aggregate that well, which is why the ratio declines. For small numbers of facts, the two pipeline systems select facts and sentence plans that offer very few aggregation opportunities; as the number of selected facts increases, some more aggregation opportunities arise, which is why the facts/words ratio of the two systems improves. In all the experiments, the ILP solver was very fast (average: 0.08 sec, worst: 0.14 sec). Experiments with human judges also showed that the texts of ILPNLG cannot be distinguished from those of PIPELINESHORT in terms of fluency and text clarity. Hence, the highest compactness of the texts of ILPNLG does not come at the expense of lower text quality. Space does not permit a more detailed description of these experiments.","We show below texts produced by PIPELINE (M = 4) and ILPNLG (λ1 = 0.3). PIPELINE: This is a strong Sauternes. It is made from Semillon grapes and it is produced by Chateau D’ychem. ILPNLG: This is a strong Sauternes. It is made from Semillon grapes by Chateau D’ychem. PIPELINE: This is a full Riesling and it has moderate flavor. It is produced by Volrad. ILPNLG: This is a full sweet moderate Riesling. In the first pair, PIPELINE uses different verbs for the grapes and producer, whereas ILPNLG uses the same verb, which leads to a more compressive aggregation; both texts describe the same wine and report 4 facts. In the second pair, ILPNLG has chosen to express the sweetness instead of the producer, and uses the same verb (“be”) for all the facts, leading to a shorter sentence; again both texts describe the same wine and report 4 facts. 564 In both examples, some facts are not aggregated because they belong in different sections."]},{"title":"5 Conclusions","paragraphs":["We presented an ILP model for NLG that jointly considers the choices in content selection, lexicalization, and aggregation to avoid greedy local decisions and produce more compact texts. Experiments verified that our model can express more facts per word, compared to a pipeline, which is important when space is scarce. An off-the-shelf ILP solver took approximately 0.1 sec for each text. We plan to extend our model to include text planning and referring expressions generation."]},{"title":"Acknowledgments","paragraphs":["This research has been co-financed by the European Union (European Social Fund – ESF) and Greek national funds through the Operational Program “Education and Lifelong Learning” of the National Strategic Reference Framework (NSRF) – Research Funding Program: Heracleitus II. In-vesting in knowledge society through the European Social Fund."]},{"title":"References","paragraphs":["E. Althaus, N. Karamanis, and A. Koller. 2004. Computing locally coherent discourses. In 42nd Annual Meeting of ACL, pages 399–406, Barcelona, Spain.","I. Androutsopoulos, G. Lampouras, and D. Galanis. 2013. Generating natural language descriptions from OWL ontologies: the NaturalOWL system. Technical report, Natural Language Processing Group, Department of Informatics, Athens University of Economics and Business.","G. Antoniou and F. van Harmelen. 2008. A Semantic Web primer. MIT Press, 2nd edition.","R. Barzilay and M. Lapata. 2005. Collective content selection for concept-to-text generation. In HLT-EMNLP, pages 331–338, Vancouver, BC, Canada.","R. Barzilay and M. Lapata. 2006. Aggregation via set partitioning for natural language generation. In HLT-NAACL, pages 359–366, New York, NY.","A. Belz. 2008. Automatic generation of weather forecast texts using comprehensive probabilistic generation-space models. Natural Language Engineering, 14(4):431–455.","T. Berg-Kirkpatrick, D. Gillick, and D. Klein. 2011. Jointly learning to extract and compress. In 49th Annual Meeting of ACL, pages 481–490, Portland, OR.","K. Bontcheva. 2005. Generating tailored textual summaries from ontologies. In 2nd European Semantic Web Conf., pages 531–545, Heraklion, Greece.","J. Clarke and M. Lapata. 2008. Global inference for sentence compression: An integer linear programming approach. Journal of Artificial Intelligence Research, 1(31):399–429.","H. Dalianis. 1999. Aggregation in natural language generation. Comput. Intelligence, 15(4):384–414.","L. Danlos. 1984. Conceptual and linguistic decisions in generation. In 10th COLING, pages 501–504, Stanford, CA.","D. Galanis and I. Androutsopoulos. 2007. Generating multilingual descriptions from linguistically annotated OWL ontologies: the NaturalOWL system. In 11th European Workshop on Natural Lang. Generation, pages 143–146, Schloss Dagstuhl, Germany.","D. Galanis, G. Karakatsiotis, G. Lampouras, and I. Androutsopoulos. 2009. An open-source natural language generator for OWL ontologies and its use in Protégé and Second Life. In 12th Conf. of the European Chapter of ACL (demos), Athens, Greece.","D. Galanis, G. Lampouras, and I. Androutsopoulos. 2012. Extractive multi-document summarization with Integer Linear Programming and Support Vector Regression. In COLING, pages 911–926, Mumbai, India.","B.C. Grau, I. Horrocks, B. Motik, B. Parsia, P. Patel-Schneider, and U. Sattler. 2008. OWL 2: The next step for OWL. Web Semantics, 6:309–322.","I. Konstas and M. Lapata. 2012a. Concept-to-text generation via discriminative reranking. In 50th Annual Meeting of ACL, pages 369–378, Jeju Island, Korea.","I. Konstas and M. Lapata. 2012b. Unsupervised concept-to-text generation with hypergraphs. In HLT-NAACL, pages 752–761, Montréal, Canada.","P. Liang, M. Jordan, and D. Klein. 2009. Learning semantic correspondences with less supervision. In 47th Meeting of ACL and 4th AFNLP, pages 91–99, Suntec, Singapore.","S.F. Liang, R. Stevens, D. Scott, and A. Rector. 2011. Automatic verbalisation of SNOMED classes using OntoVerbal. In 13th Conf. AI in Medicine, pages 338–342, Bled, Slovenia.","T. Marciniak and M. Strube. 2005. Beyond the pipeline: Discrete optimization in NLP. In 9th Conference on Computational Natural Language Learning, pages 136–143, Ann Arbor, MI.","R. McDonald. 2007. A study of global inference algorithms in multi-document summarization. In European Conference on Information Retrieval, pages 557–564, Rome, Italy. 565","C. Mellish and J.Z. Pan. 2008. Natural language directed inference from ontologies. Artificial Intelligence, 172:1285–1315.","C. Mellish and X. Sun. 2006. The Semantic Web as a linguistic resource: opportunities for nat. lang. generation. Knowledge Based Systems, 19:298–303.","E. Reiter and R. Dale. 2000. Building Natural Language Generation Systems. Cambridge Univ. Press.","R. Schwitter, K. Kaljurand, A. Cregan, C. Dolbear, and G. Hart. 2008. A comparison of three controlled nat. languages for OWL 1.1. In 4th OWL Experiences and Directions Workshop, Washington DC.","R. Schwitter. 2010. Controlled natural languages for knowledge representation. In 23rd COLING, pages 1113–1121, Beijing, China.","S. Williams, A. Third, and R. Power. 2011. Levels of organization in ontology verbalization. In 13th European Workshop on Natural Lang. Generation, pages 158–163, Nancy, France.","K. Woodsend and M. Lapata. 2012. Multiple aspect summarization using integer linear programming. In EMNLP-CoNLL, pages 233–243, Jesu Island, Korea. 566"]}],"references":[{"authors":[{"first":"E.","last":"Althaus"},{"first":"N.","last":"Karamanis"},{"first":"A.","last":"Koller"}],"year":"2004","title":"Computing locally coherent discourses","source":"E. Althaus, N. Karamanis, and A. Koller. 2004. Computing locally coherent discourses. In 42nd Annual Meeting of ACL, pages 399–406, Barcelona, Spain."},{"authors":[{"first":"I.","last":"Androutsopoulos"},{"first":"G.","last":"Lampouras"},{"first":"D.","last":"Galanis"}],"year":"2013","title":"Generating natural language descriptions from OWL ontologies: the NaturalOWL system","source":"I. Androutsopoulos, G. Lampouras, and D. Galanis. 2013. Generating natural language descriptions from OWL ontologies: the NaturalOWL system. Technical report, Natural Language Processing Group, Department of Informatics, Athens University of Economics and Business."},{"authors":[{"first":"G.","last":"Antoniou"},{"first":"F.","last":"van Harmelen"}],"year":"2008","title":"A Semantic Web primer","source":"G. Antoniou and F. van Harmelen. 2008. A Semantic Web primer. MIT Press, 2nd edition."},{"authors":[{"first":"R.","last":"Barzilay"},{"first":"M.","last":"Lapata"}],"year":"2005","title":"Collective content selection for concept-to-text generation","source":"R. Barzilay and M. Lapata. 2005. Collective content selection for concept-to-text generation. In HLT-EMNLP, pages 331–338, Vancouver, BC, Canada."},{"authors":[{"first":"R.","last":"Barzilay"},{"first":"M.","last":"Lapata"}],"year":"2006","title":"Aggregation via set partitioning for natural language generation","source":"R. Barzilay and M. Lapata. 2006. Aggregation via set partitioning for natural language generation. In HLT-NAACL, pages 359–366, New York, NY."},{"authors":[{"first":"A.","last":"Belz"}],"year":"2008","title":"Automatic generation of weather forecast texts using comprehensive probabilistic generation-space models","source":"A. Belz. 2008. Automatic generation of weather forecast texts using comprehensive probabilistic generation-space models. Natural Language Engineering, 14(4):431–455."},{"authors":[{"first":"T.","last":"Berg-Kirkpatrick"},{"first":"D.","last":"Gillick"},{"first":"D.","last":"Klein"}],"year":"2011","title":"Jointly learning to extract and compress","source":"T. Berg-Kirkpatrick, D. Gillick, and D. Klein. 2011. Jointly learning to extract and compress. In 49th Annual Meeting of ACL, pages 481–490, Portland, OR."},{"authors":[{"first":"K.","last":"Bontcheva"}],"year":"2005","title":"Generating tailored textual summaries from ontologies","source":"K. Bontcheva. 2005. Generating tailored textual summaries from ontologies. In 2nd European Semantic Web Conf., pages 531–545, Heraklion, Greece."},{"authors":[{"first":"J.","last":"Clarke"},{"first":"M.","last":"Lapata"}],"year":"2008","title":"Global inference for sentence compression: An integer linear programming approach","source":"J. Clarke and M. Lapata. 2008. Global inference for sentence compression: An integer linear programming approach. Journal of Artificial Intelligence Research, 1(31):399–429."},{"authors":[{"first":"H.","last":"Dalianis"}],"year":"1999","title":"Aggregation in natural language generation","source":"H. Dalianis. 1999. Aggregation in natural language generation. Comput. Intelligence, 15(4):384–414."},{"authors":[{"first":"L.","last":"Danlos"}],"year":"1984","title":"Conceptual and linguistic decisions in generation","source":"L. Danlos. 1984. Conceptual and linguistic decisions in generation. In 10th COLING, pages 501–504, Stanford, CA."},{"authors":[{"first":"D.","last":"Galanis"},{"first":"I.","last":"Androutsopoulos"}],"year":"2007","title":"Generating multilingual descriptions from linguistically annotated OWL ontologies: the NaturalOWL system","source":"D. Galanis and I. Androutsopoulos. 2007. Generating multilingual descriptions from linguistically annotated OWL ontologies: the NaturalOWL system. In 11th European Workshop on Natural Lang. Generation, pages 143–146, Schloss Dagstuhl, Germany."},{"authors":[{"first":"D.","last":"Galanis"},{"first":"G.","last":"Karakatsiotis"},{"first":"G.","last":"Lampouras"},{"first":"I.","last":"Androutsopoulos"}],"year":"2009","title":"An open-source natural language generator for OWL ontologies and its use in Protégé and Second Life","source":"D. Galanis, G. Karakatsiotis, G. Lampouras, and I. Androutsopoulos. 2009. An open-source natural language generator for OWL ontologies and its use in Protégé and Second Life. In 12th Conf. of the European Chapter of ACL (demos), Athens, Greece."},{"authors":[{"first":"D.","last":"Galanis"},{"first":"G.","last":"Lampouras"},{"first":"I.","last":"Androutsopoulos"}],"year":"2012","title":"Extractive multi-document summarization with Integer Linear Programming and Support Vector Regression","source":"D. Galanis, G. Lampouras, and I. Androutsopoulos. 2012. Extractive multi-document summarization with Integer Linear Programming and Support Vector Regression. In COLING, pages 911–926, Mumbai, India."},{"authors":[{"first":"B.","middle":"C.","last":"Grau"},{"first":"I.","last":"Horrocks"},{"first":"B.","last":"Motik"},{"first":"B.","last":"Parsia"},{"first":"P.","last":"Patel-Schneider"},{"first":"U.","last":"Sattler"}],"year":"2008","title":"OWL 2: The next step for OWL","source":"B.C. Grau, I. Horrocks, B. Motik, B. Parsia, P. Patel-Schneider, and U. Sattler. 2008. OWL 2: The next step for OWL. Web Semantics, 6:309–322."},{"authors":[{"first":"I.","last":"Konstas"},{"first":"M.","last":"Lapata"}],"year":"2012a","title":"Concept-to-text generation via discriminative reranking","source":"I. Konstas and M. Lapata. 2012a. Concept-to-text generation via discriminative reranking. In 50th Annual Meeting of ACL, pages 369–378, Jeju Island, Korea."},{"authors":[{"first":"I.","last":"Konstas"},{"first":"M.","last":"Lapata"}],"year":"2012b","title":"Unsupervised concept-to-text generation with hypergraphs","source":"I. Konstas and M. Lapata. 2012b. Unsupervised concept-to-text generation with hypergraphs. In HLT-NAACL, pages 752–761, Montréal, Canada."},{"authors":[{"first":"P.","last":"Liang"},{"first":"M.","last":"Jordan"},{"first":"D.","last":"Klein"}],"year":"2009","title":"Learning semantic correspondences with less supervision","source":"P. Liang, M. Jordan, and D. Klein. 2009. Learning semantic correspondences with less supervision. In 47th Meeting of ACL and 4th AFNLP, pages 91–99, Suntec, Singapore."},{"authors":[{"first":"S.","middle":"F.","last":"Liang"},{"first":"R.","last":"Stevens"},{"first":"D.","last":"Scott"},{"first":"A.","last":"Rector"}],"year":"2011","title":"Automatic verbalisation of SNOMED classes using OntoVerbal","source":"S.F. Liang, R. Stevens, D. Scott, and A. Rector. 2011. Automatic verbalisation of SNOMED classes using OntoVerbal. In 13th Conf. AI in Medicine, pages 338–342, Bled, Slovenia."},{"authors":[{"first":"T.","last":"Marciniak"},{"first":"M.","last":"Strube"}],"year":"2005","title":"Beyond the pipeline: Discrete optimization in NLP","source":"T. Marciniak and M. Strube. 2005. Beyond the pipeline: Discrete optimization in NLP. In 9th Conference on Computational Natural Language Learning, pages 136–143, Ann Arbor, MI."},{"authors":[{"first":"R.","last":"McDonald"}],"year":"2007","title":"A study of global inference algorithms in multi-document summarization","source":"R. McDonald. 2007. A study of global inference algorithms in multi-document summarization. In European Conference on Information Retrieval, pages 557–564, Rome, Italy. 565"},{"authors":[{"first":"C.","last":"Mellish"},{"first":"J.","middle":"Z.","last":"Pan"}],"year":"2008","title":"Natural language directed inference from ontologies","source":"C. Mellish and J.Z. Pan. 2008. Natural language directed inference from ontologies. Artificial Intelligence, 172:1285–1315."},{"authors":[{"first":"C.","last":"Mellish"},{"first":"X.","last":"Sun"}],"year":"2006","title":"The Semantic Web as a linguistic resource: opportunities for nat","source":"C. Mellish and X. Sun. 2006. The Semantic Web as a linguistic resource: opportunities for nat. lang. generation. Knowledge Based Systems, 19:298–303."},{"authors":[{"first":"E.","last":"Reiter"},{"first":"R.","last":"Dale"}],"year":"2000","title":"Building Natural Language Generation Systems","source":"E. Reiter and R. Dale. 2000. Building Natural Language Generation Systems. Cambridge Univ. Press."},{"authors":[{"first":"R.","last":"Schwitter"},{"first":"K.","last":"Kaljurand"},{"first":"A.","last":"Cregan"},{"first":"C.","last":"Dolbear"},{"first":"G.","last":"Hart"}],"year":"2008","title":"A comparison of three controlled nat","source":"R. Schwitter, K. Kaljurand, A. Cregan, C. Dolbear, and G. Hart. 2008. A comparison of three controlled nat. languages for OWL 1.1. In 4th OWL Experiences and Directions Workshop, Washington DC."},{"authors":[{"first":"R.","last":"Schwitter"}],"year":"2010","title":"Controlled natural languages for knowledge representation","source":"R. Schwitter. 2010. Controlled natural languages for knowledge representation. In 23rd COLING, pages 1113–1121, Beijing, China."},{"authors":[{"first":"S.","last":"Williams"},{"first":"A.","last":"Third"},{"first":"R.","last":"Power"}],"year":"2011","title":"Levels of organization in ontology verbalization","source":"S. Williams, A. Third, and R. Power. 2011. Levels of organization in ontology verbalization. In 13th European Workshop on Natural Lang. Generation, pages 158–163, Nancy, France."},{"authors":[{"first":"K.","last":"Woodsend"},{"first":"M.","last":"Lapata"}],"year":"2012","title":"Multiple aspect summarization using integer linear programming","source":"K. Woodsend and M. Lapata. 2012. Multiple aspect summarization using integer linear programming. In EMNLP-CoNLL, pages 233–243, Jesu Island, Korea. 566"}],"cites":[{"style":0,"text":"Reiter and Dale, 2000","origin":{"pointer":"/sections/2/paragraphs/0","offset":105,"length":21},"authors":[{"last":"Reiter"},{"last":"Dale"}],"year":"2000","references":["/references/23"]},{"style":0,"text":"Harmelen, 2008","origin":{"pointer":"/sections/2/paragraphs/0","offset":186,"length":14},"authors":[{"last":"Harmelen"}],"year":"2008","references":[]},{"style":0,"text":"Grau et al., 2008","origin":{"pointer":"/sections/2/paragraphs/0","offset":328,"length":17},"authors":[{"last":"Grau"},{"last":"al."}],"year":"2008","references":["/references/14"]},{"style":0,"text":"Bontcheva, 2005","origin":{"pointer":"/sections/2/paragraphs/0","offset":368,"length":15},"authors":[{"last":"Bontcheva"}],"year":"2005","references":["/references/7"]},{"style":0,"text":"Mellish and Sun, 2006","origin":{"pointer":"/sections/2/paragraphs/0","offset":385,"length":21},"authors":[{"last":"Mellish"},{"last":"Sun"}],"year":"2006","references":["/references/22"]},{"style":0,"text":"Galanis and Androutsopoulos, 2007","origin":{"pointer":"/sections/2/paragraphs/0","offset":408,"length":33},"authors":[{"last":"Galanis"},{"last":"Androutsopoulos"}],"year":"2007","references":["/references/11"]},{"style":0,"text":"Mellish and Pan, 2008","origin":{"pointer":"/sections/2/paragraphs/0","offset":443,"length":21},"authors":[{"last":"Mellish"},{"last":"Pan"}],"year":"2008","references":["/references/21"]},{"style":0,"text":"Schwitter et al., 2008","origin":{"pointer":"/sections/2/paragraphs/0","offset":466,"length":22},"authors":[{"last":"Schwitter"},{"last":"al."}],"year":"2008","references":["/references/24"]},{"style":0,"text":"Schwitter, 2010","origin":{"pointer":"/sections/2/paragraphs/0","offset":490,"length":15},"authors":[{"last":"Schwitter"}],"year":"2010","references":["/references/25"]},{"style":0,"text":"Liang et al., 2011","origin":{"pointer":"/sections/2/paragraphs/0","offset":507,"length":18},"authors":[{"last":"Liang"},{"last":"al."}],"year":"2011","references":["/references/18"]},{"style":0,"text":"Williams et al., 2011","origin":{"pointer":"/sections/2/paragraphs/0","offset":527,"length":21},"authors":[{"last":"Williams"},{"last":"al."}],"year":"2011","references":["/references/26"]},{"style":0,"text":"Danlos, 1984","origin":{"pointer":"/sections/2/paragraphs/2","offset":261,"length":12},"authors":[{"last":"Danlos"}],"year":"1984","references":["/references/10"]},{"style":0,"text":"Marciniak and Strube, 2005","origin":{"pointer":"/sections/2/paragraphs/2","offset":275,"length":26},"authors":[{"last":"Marciniak"},{"last":"Strube"}],"year":"2005","references":["/references/19"]},{"style":0,"text":"Belz, 2008","origin":{"pointer":"/sections/2/paragraphs/2","offset":303,"length":10},"authors":[{"last":"Belz"}],"year":"2008","references":["/references/5"]},{"style":0,"text":"Marciniak and Strube (2005)","origin":{"pointer":"/sections/3/paragraphs/0","offset":0,"length":27},"authors":[{"last":"Marciniak"},{"last":"Strube"}],"year":"2005","references":["/references/19"]},{"style":0,"text":"Barzilay and Lapata (2005)","origin":{"pointer":"/sections/3/paragraphs/1","offset":0,"length":26},"authors":[{"last":"Barzilay"},{"last":"Lapata"}],"year":"2005","references":["/references/3"]},{"style":0,"text":"Barzilay and Lapata (2006)","origin":{"pointer":"/sections/3/paragraphs/1","offset":292,"length":26},"authors":[{"last":"Barzilay"},{"last":"Lapata"}],"year":"2006","references":["/references/4"]},{"style":0,"text":"Althaus et al. (2004)","origin":{"pointer":"/sections/3/paragraphs/2","offset":0,"length":21},"authors":[{"last":"Althaus"},{"last":"al."}],"year":"2004","references":["/references/0"]},{"style":0,"text":"McDonald, 2007","origin":{"pointer":"/sections/3/paragraphs/3","offset":113,"length":14},"authors":[{"last":"McDonald"}],"year":"2007","references":["/references/20"]},{"style":0,"text":"Clarke and Lapata, 2008","origin":{"pointer":"/sections/3/paragraphs/3","offset":129,"length":23},"authors":[{"last":"Clarke"},{"last":"Lapata"}],"year":"2008","references":["/references/8"]},{"style":0,"text":"Berg-Kirkpatrick et al., 2011","origin":{"pointer":"/sections/3/paragraphs/3","offset":154,"length":29},"authors":[{"last":"Berg-Kirkpatrick"},{"last":"al."}],"year":"2011","references":["/references/6"]},{"style":0,"text":"Galanis et al., 2012","origin":{"pointer":"/sections/3/paragraphs/3","offset":185,"length":20},"authors":[{"last":"Galanis"},{"last":"al."}],"year":"2012","references":["/references/13"]},{"style":0,"text":"Woodsend and Lapata, 2012","origin":{"pointer":"/sections/3/paragraphs/3","offset":207,"length":25},"authors":[{"last":"Woodsend"},{"last":"Lapata"}],"year":"2012","references":["/references/27"]},{"style":0,"text":"Liang et al., 2009","origin":{"pointer":"/sections/3/paragraphs/3","offset":427,"length":18},"authors":[{"last":"Liang"},{"last":"al."}],"year":"2009","references":["/references/17"]},{"style":0,"text":"Konstas and Lapata, 2012a","origin":{"pointer":"/sections/3/paragraphs/3","offset":447,"length":25},"authors":[{"last":"Konstas"},{"last":"Lapata"}],"year":"2012a","references":["/references/15"]},{"style":0,"text":"Konstas and Lapata, 2012b","origin":{"pointer":"/sections/3/paragraphs/3","offset":474,"length":25},"authors":[{"last":"Konstas"},{"last":"Lapata"}],"year":"2012b","references":["/references/16"]},{"style":0,"text":"Dalianis, 1999","origin":{"pointer":"/sections/4/paragraphs/2","offset":1314,"length":14},"authors":[{"last":"Dalianis"}],"year":"1999","references":["/references/9"]},{"style":0,"text":"Galanis and Androutsopoulos, 2007","origin":{"pointer":"/sections/5/paragraphs/0","offset":20,"length":33},"authors":[{"last":"Galanis"},{"last":"Androutsopoulos"}],"year":"2007","references":["/references/11"]},{"style":0,"text":"Galanis et al., 2009","origin":{"pointer":"/sections/5/paragraphs/0","offset":55,"length":20},"authors":[{"last":"Galanis"},{"last":"al."}],"year":"2009","references":["/references/12"]},{"style":0,"text":"Androutsopoulos et al., 2013","origin":{"pointer":"/sections/5/paragraphs/0","offset":77,"length":28},"authors":[{"last":"Androutsopoulos"},{"last":"al."}],"year":"2013","references":["/references/1"]}]}
