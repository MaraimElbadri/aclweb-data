{"sections":[{"title":"","paragraphs":["Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 393–398, Sofia, Bulgaria, August 4-9 2013. c⃝2013 Association for Computational Linguistics"]},{"title":"A Tightly-coupled Unsupervised Clustering and Bilingual Alignment Model for Transliteration Tingting Li","paragraphs":["1"]},{"title":", Tiejun Zhao","paragraphs":["1"]},{"title":", Andrew Finch","paragraphs":["2"]},{"title":", Chunyue Zhang","paragraphs":["1 1"]},{"title":"Harbin Institute of Technology, Harbin, China","paragraphs":["2"]},{"title":"NICT, Japan","paragraphs":["1"]},{"title":"{ttli, tjzhao, cyzhang}@mtlab.hit.edu.cn","paragraphs":["2"]},{"title":"andrew.finch@nict.go.jp Abstract","paragraphs":["Machine Transliteration is an essential task for many NLP applications. However, names and loan words typically originate from various languages, obey different transliteration rules, and therefore may benefit from being modeled independently. Recently, transliteration models based on Bayesian learning have overcome issues with over-fitting allowing for many-to-many alignment in the training of transliteration models. We propose a nov-el coupled Dirichlet process mixture model (cDPMM) that simultaneously clusters and bilingually aligns transliteration data within a single unified model. The unified model decomposes into two classes of non-parametric Bayesian component models: a Dirichlet process mixture model for clustering, and a set of multinomial Dirichlet process models that perform bilingual alignment independently for each cluster. The experimental results show that our method considerably outperforms conventional alignment models."]},{"title":"1 Introduction","paragraphs":["Machine transliteration methods can be categorized into phonetic-based models (Knight et al., 1998), spelling-based models (Brill et al., 2000), and hybrid models which utilize both phonetic and spelling information (Oh et al., 2005; Oh et al., 2006). Among them, statistical spelling-based models which directly align characters in the training corpus have become popular because they are language-independent, do not require phonetic knowledge, and are capable of achieving state-of-the-art performance (Zhang et al., 2012b). A major problem with real-word transliteration corpora is that they are usually not clean, may contain name pairs with various linguistic origins and this can hinder the performance of spelling-based models because names from different origins obey different pronunciation rules, for example: “Kim Jong-il/金正恩” (Korea), “Kana Gaski/金崎” (Japan), “Haw King/霍金” (England),","“Jin yong/金庸’ (China). The same Chinese character “金” should be aligned to different romanized character sequences: “Kim”, “Kana”, “King”, “Jin”. To address this issue, many name classification methods have been proposed, such as the supervised language model-based approach of (Li et al., 2007), and the unsupervised approach of (Huang et al., 2005) that used a bottom-up clustering algorithm. (Li et al., 2007) proposed a supervised transliteration model which classifies names based on their origins and genders using a language model; it switches between transliteration models based on the input. (Hagiwara et al., 2011) tackled the is-sue by using an unsupervised method based on the EM algorithm to perform a soft classification.","Recently, non-parametric Bayesian models (Finch et al., 2010; Huang et al., 2011; Hagiwara et al., 2012) have attracted much attention in the transliteration field. In comparison to many of the previous alignment models (Li et al., 2004; Jiampojamarn et al., 2007; Berg-Kirkpatrick et al., 2011), the non-parametric Bayesian models allow unconstrained monotonic many-to-many alignment and are able to overcome the inherent over-fitting problem.","Until now most of the previous work (Li et al., 2007; Hagiwara et al., 2011) is either affected by the multi-origins factor, or has issues with over-fitting. (Hagiwara et al., 2012) took these two factors into consideration, but their approach still operates within an EM framework and model order selection by hand is necessary prior to training. 393","We propose a simple, elegant, fully-unsupervised solution based on a single generative model able to both cluster and align simultaneously. The coupled Dirichlet Process Mixture Model (cDPMM) integrates a Dirichlet process mixture model (DPMM) (Antoniak, 1974) and a Bayesian Bilingual Alignment Model (BBAM) (Finch et al., 2010). The two component models work synergistically to support one another: the clustering model sorts the data into classes so that self-consistent alignment models can be built using data of the same type, and at the same time the alignment probabilities from the alignment models drive the clustering process.","In summary, the key advantages of our model are as follows:","• it is based on a single, unified generative model; • it is fully unsupervised;","• it is an infinite mixture model, and does not require model order selection – it is effectively capable of discovering an appropriate number of clusters from the data; • it is able to handle data from multiple origins;","• it can perform many-to-many alignment without over-fitting."]},{"title":"2 Model Description","paragraphs":["In this section we describe the methodology and realization of the proposed cDPMM in detail. 2.1 Terminology In this paper, we concentrate on the alignment process for transliteration. The proposed cDPMM segments a bilingual corpus of transliteration pairs into bilingual character sequence-pairs. We will call these sequence-pairs Transliteration Units (TUs). We denote the source and target of a TU as sm","1 = ⟨s1, ..., sm⟩ and tn","1 = ⟨t1, ..., tn⟩ respectively, where si (ti) is a single character in source (target) language. We use the same no-tation (s, t) = (⟨s1, ..., sm⟩, ⟨t1, ..., tn⟩) to denote a transliteration pair, which we can write as x = (sm","1 , tn","1 ) for simplicity. Finally, we express the training set itself as a set of sequence pairs: D = {xi}I","i=1. Our aim is to obtain a bilingual alignment ⟨(s1, t1), ..., (sl, tl)⟩ for each transliteration pair xi, where each (sj, tj) is a segment of the whole pair (a TU) and l is the number of segments used to segment xi. 2.2 Methodology Our cDPMM integrates two Dirichlet process models: the DPMM clustering model, and the BBAM alignment model which is a multinomial Dirichlet process.","A Dirichlet process mixture model, models the data as a mixture of distributions – one for each cluster. It is an infinite mixture model, and the number of components is not fixed prior to training. Equation 1 expresses the DPMM hierarchically. Gc|αc, G0c ∼ DP (αc, G0c) θk|Gc ∼ Gc xi|θk ∼ f (xi|θk) (1) where G0c is the base measure and αc > 0 is the concentration parameter for the distribution Gc. xi is a name pair in training data, and θk represents the parameters of a candidate cluster k for xi. Specifically θk contains the probabilities of all the T U s in cluster k. f (xi|θk) (defined in Equation 7) is the probability that mixture component k parameterized by θk will generate xi.","The alignment component of our cDPMM is a multinomial Dirichlet process and is defined as follows: Ga|αa, G0a ∼ DP (αa, G0a) (sj, tj)|Ga ∼ Ga (2)","The subscripts ‘ c’ and ‘ a’ in Equations 1 and 2 indicate whether the terms belong to the clustering or alignment model respectively.","The generative story for the cDPMM is simple: first generate an infinite number of clusters, choose one, then generate a transliteration pair using the parameters that describe the cluster. The basic sampling unit of the cDPMM for the clustering process is a transliteration pair, but the basic sampling unit for BBAM is a TU. In order to integrate the two processes in a single model we treat a transliteration pair as a sequence of TUs generated by a BBAM model. The BBAM generates a sequence (a transliteration pair) based on the joint source-channel model (Li et al., 2004). We use a blocked version of a Gibbs sampler to train each BBAM (see (Mochihashi et al., 2009) for details of this process). 2.3 The Alignment Model This model is a multinomial DP model. Under the Chinese restaurant process (CRP) (Aldous, 1985) 394 interpretation, each unique TU corresponds to a dish served at a table, and the number of customers in each table represents the count of a particular TU in the model.","The probability of generating the jth","TU (sj, tj) is, P ( (sj, tj)|(s−j, t−j)) = N ( (sj, tj)) + αaG0a ( (sj, tj)) N + αa (3) where N is the total number of TUs generated so far, and N ( (sj, tj)) is the count of (sj, tj). (s−j, t−j) are all the TUs generated so far except (sj, tj). The base measure G0a is a joint spelling model: G0a ( (s, t)) = P (|s|)P (s||s|)P (|t|)P (t||t|) = λ|s| s |s|!","e−λs v−|s| s × λ |t| t |t|!","e−λt v−|t| t (4) where |s| (|t|) is the length of the source (target) sequence, vs (vt) is the vocabulary (alphabet) size of the source (target) language, and λs (λt) is the expected length of source (target) side. 2.4 The Clustering Model This model is a DPMM. Under the CRP interpretation, a transliteration pair corresponds to a customer, the dish served on each table corresponds to an origin of names.","We use z = (z1, ..., zI ), zi ∈ {1, ..., K} to indicate the cluster of each transliteration pair xi in the training set and θ = (θ1, ..., θK ) to represent the parameters of the component associated with each cluster.","In our model, each mixture component is a multinomial DP model, and since θk contains the probabilities of all the TUs in cluster k, the number of parameters in each θk is uncertain and changes with the transliteration pairs that belong to the cluster. For a new cluster (the K + 1th","cluster), we use Equation 4 to calculate the probability of each TU. The cluster membership probability of a transliteration pair xi is calculated as follows, P (zi = k|D, θ, z−i) ∝","nk n − 1 + αc P (xi|z, θk) (5) P (zi = K + 1|D, θ, z−i) ∝","αc n − 1 + αc P (xi|z, θK+1) (6) where nk is the number of transliteration pairs in the existing cluster k ∈ {1, ..., K} (cluster K + 1 is a newly created cluster), zi is the cluster indicator for xi, and z−i is the sequence of observed clusters up to xi. As mentioned earlier, basic sampling units are inconsistent for the clustering and alignment model, therefore to couple the models the BBAM generates transliteration pairs as a sequence of TUs, these pairs are then used directly in the DPMM.","Let γ = ⟨(s1, t1), ..., (sl, tl)⟩ be a derivation of a transliteration pair xi. To make the model integration process explicit, we use function f to calculate the probability P (xi|z, θk), where f is defined as follows, f(xi|θk) = { ∑ γ∈R","∏","(s,t)∈γ P (s, t|θk) k ∈ {1, ..., K}","∑","γ∈R ∏ (s,t)∈γ G0c(s, t) k = K + 1 (7) where R denotes the set of all derivations of xi, G0c is the same as Equation 4.","The cluster membership zi is sampled together with the derivation γ in a single step according to P (zi = k|D, θ, z−i) and f (xi|θk). Following the method of (Mochihashi et al., 2009), first f (xi|θk) is calculated by forward filtering, and then a sample γ is taken by backward sampling."]},{"title":"3 Experiments 3.1 Corpora","paragraphs":["To empirically validate our approach, we investigate the effectiveness of our model by conduct-ing English-Chinese name transliteration genera-tion on three corpora containing name pairs of varying degrees of mixed origin. The first two corpora were drawn from the “Names of The World’s Peoples” dictionary published by Xin Hua Publishing House. The first corpus was constructed with names only originating from English language (EO), and the second with names originating from English, Chinese, Japanese evenly (ECJ-O). The third corpus was created by extracting name pairs from LDC (Linguistic Data Consortium) Named Entity List, which contains names from all over the world (Multi-O). We divided the datasets into training, development and test sets for each corpus with a ratio of 10:1:1. The details of the division are displayed in Table 2. 395","cDPMM Alignment BBAM Alignment mun|蒙 din|丁 ger|格(0, English) mun|蒙 din|丁 ger|格 ding|丁 guo|果(2, Chinese) din|丁 g| guo|果","tei|丁 be|部(3, Japanese) t| |丁 e| ibe|部 fan|范 chun|纯 yi|一(2, Chinese) fan|范 chun|纯 y| i|一 hong|洪 il|一 sik|植(5, Korea) hong|洪 i|一 l| si|植 k| sei|静 ichi|一 ro|郎(4, Japanese) seii|静 ch| i|一 ro|郎","dom|东 b|布 ro|罗 w|夫 s|斯 ki|基(0, Russian) do|东 mb|布 ro|罗 w|夫 s|斯 ki|基","he|何 dong|东 chang|昌(2, Chinese) he|何 don|东 gchang|昌 b|布 ran|兰 don|东(0, English) b|布 ran|兰 don|东 Table 1: Typical alignments from the BBAM and cDPMM. 3.2 Baselines We compare our alignment model with GIZA++ (Och et al., 2003) and the Bayesian bilingual alignment model (BBAM). We employ two decoding models: a phrase-based machine translation decoder (specifically Moses (Koehn et al., 2007)), and the DirecTL decoder (Jiampojamarn et al., 2009). They are based on different decoding strategies and optimization targets, and therefore make the comparison more comprehensive. For the Moses decoder, we applied the grow-diag-final-and heuristic algorithm to extract the phrase table, and tuned the parameters using the BLEU metric. Corpora","Corpus Scale Training Development Testing EO 32,681 3,267 3,267 ECJ-O 32,500 3,250 3,250 Multi-O 33,291 3,328 3,328 Table 2: Statistics of the experimental corpora.","To evaluate the experimental results, we utilized 3 metrics from the Named Entities Workshop (NEWS) (Zhang et al., 2012a): word accuracy in top-1 (ACC), fuzziness in top-1 (Mean F-score) and mean reciprocal rank (MRR). 3.3 Parameter Setting In our model, there are several important parameters: 1) max s, the maximum length of the source sequences of the alignment tokens; 2) max t, the maximum length of the target sequences of the alignment tokens; and 3) nc, the initial number of classes for the training data. We set max s = 6, max t = 1 and nc = 5 empirically based on a small pilot experiment. The Moses decoder was used with default settings except for the distortionlimit which was set to 0 to ensure monotonic decoding. For the DirecTL decoder the following settings were used: cs = 4, ng = 9 and nBest = 5. cs denotes the size of context window for features, ng indicates the size of n-gram features and nBest is the size of transliteration candidate list for updating the model in each iteration. The concentration parameter αc, αa of the clustering model and the BBAM was learned by sampling its value. Following (Blunsom et al., 2009) we used a vague gamma prior Γ(10−4",", 104","), and sampled new values from a log-normal distribution whose mean was the value of the parameter, and variance was 0.3. We used the Metropolis-Hastings algorithm to determine whether this new sample would be accepted. The parameters λs and λt in Equation 4 were set to λs = 4 and λt = 1.","Model EO ECJ-O Multi-O #(Clusters) cDPMM 5.8 9.5 14.3 #(Targets) GIZA++ 14.43 5.35 6.62 BBAM 6.06 2.45 2.91 cDPMM 9.32 3.45 4.28 Table 3: Alignment statistics. 3.4 Experimental Results Table 3 shows some details of the alignment results. The #(Clusters) represents the average number of clusters from the cDPMM. It is averaged over the final 50 iterations, and the classes which contain less than 10 name pairs are excluded. The #(Targets) represents the average number of English character sequences that are aligned to each Chinese sequence. From the results we can see that in terms of the number of alignment targets: GIZA++ > cDPMM > BBAM. GIZA++ has considerably more targets than the other approach-es, and this is likely to be a symptom of it over-fitting the data. cDPMM can alleviate the over-fitting through its BBAM component, and at the same time effectively model the diversity in Chinese character sequences caused by multi-origin. Table 1 shows some typical TUs from the alignments produced by BBAM and cDPMM on corpus Multi-O. The information in brackets in Table 1, represents the ID of the class and origin of 396","Corpora Model Evaluation ACC M-Fscore MRR EO GIZA 0.7241 0.8881 0.8061 BBAM 0.7286 0.8920 0.8043 cDPMM 0.7398 0.8983 0.8126 ECJ-O GIZA 0.5471 0.7278 0.6268 BBAM 0.5522 0.7370 0.6344 cDPMM 0.5643 0.7420 0.6446 Multi-O GIZA 0.4993 0.7587 0.5986 BBAM 0.5163 0.7769 0.6123 cDPMM 0.5237 0.7796 0.6188 Table 4: Comparison of different methods using the Moses phrase-based decoder. the name pair; the symbol ‘ ’ indicates a “NUL-L” alignment. We can see the Chinese characters “丁(ding) 一(yi) 东(dong)” have different alignments in different origins, and that the cDPMM has provided the correct alignments for them.","We used the sampled alignment from running the BBAM and cDPMM models for 100 iterations, and combined the alignment tables of each class together. The experiments are therefore investigat-ing whether the alignment has been meaningfully improved by the clustering process. We would expect further gains from exploiting the class information in the decoding process (as in (Li et al., 2007)), but this remains future research. The top-10 transliteration candidates were used for testing. The detailed experimental results are shown in Tables 4 and 5.","Our proposed model obtained the highest performance on all three datasets for all evaluation metrics by a considerable margin. Surprisingly, for dataset EO although there is no multi-origin factor, we still observed a respectable improvement in every metric. This shows that although names may have monolingual origin, there are hidden factors which can allow our model to succeed, possibly related to gender or convention. Other models based on supervised classification or clustering with fixed classes may fail to capture these characteristics.","To guarantee the reliability of the comparative results, we performed significance testing based on paired bootstrap resampling (Efron et al., 1993). We found all differences to be significant (p < 0.05)."]},{"title":"4 Conclusion","paragraphs":["In this paper we propose an elegant unsupervised technique for monotonic sequence alignment based on a single generative model. The key ben-","Corpora Model Evaluation ACC M-Fscore MRR EO GIZA 0.6950 0.8812 0.7632 BBAM 0.7152 0.8899 0.7839 cDPMM 0.7231 0.8933 0.7941 ECJ-O GIZA 0.3325 0.6208 0.4064 BBAM 0.3427 0.6259 0.4192 cDPMM 0.3521 0.6302 0.4316 Multi-O GIZA 0.3815 0.7053 0.4592 BBAM 0.3934 0.7146 0.4799 cDPMM 0.3970 0.7179 0.4833 Table 5: Comparison of different methods using the DirecTL decoder. efits of our model are that it can handle data from multiple origins, and model using many-to-many alignment without over-fitting. The model operates by clustering the data into classes while simultaneously aligning it, and is able to discover an appropriate number of classes from the data. Our results show that our alignment model can improve the performance of a transliteration generation system relative to two other state-of-the-art aligners. Furthermore, the system produced gains even on data of monolingual origin, where no obvious clusters in the data were expected."]},{"title":"Acknowledgments","paragraphs":["We thank the anonymous reviewers for their valu-able comments and helpful suggestions.We also thank Chonghui Zhu, Mo Yu, and Wenwen Zhang for insightful discussions. This work was support-ed by National Natural Science Foundation of China (61173073), and the Key Project of the National High Technology Research and Development Program of China (2011AA01A207)."]},{"title":"References","paragraphs":["D.J. Aldous. 1985. Exchangeability and Related Topics. École d’ Été St Flour 1983. Springer, 1985, 1117:1–198.","C.E. Antoniak. 1974. Mixtures of Dirichlet processes with applications to Bayesian nonparametric problems. Annals of Statistics. 2:1152, 174.","Taylor Berg-Kirkpatrick and Dan Klein. 2011. Simple effective decipherment via combinatorial optimization. In Proc. of EMNLP, pages 313–321.","P. Blunsom, T. Cohn, C. Dyer, and Osborne, M. 2009. A Gibbs sampler for phrasal synchronous grammar induction. In Proc. of ACL, pages 782–790.","Eric Brill and Robert C. Moore. 2000. An Improved Error Model for Noisy Channel Spelling Correction. In Proc. of ACL, pages 286–293. 397","B. Efron and R. J. Tibshirani 1993. An Introduction to the Bootstrap. Chapman & Hall, New York, NY.","Andrew Finch and Eiichiro Sumita. 2010. A Bayesian Model of Bilingual Segmentation for Transliteration. In Proc. of the 7th International Workshop on Spoken Language Translation, pages 259–266.","Masato Hagiwara and Satoshi Sekine. 2011. Latent Class Transliteration based on Source Language Origin. In Proc. of ACL (Short Papers), pages 53-57.","Masato Hagiwara and Satoshi Sekine. 2012. Latent semantic transliteration using dirichlet mixture. In Proc. of the 4th Named Entity Workshop, pages 30– 37.","Fei Huang, Stephan Vogel, and Alex Waibel. 2005. Clustering and Classifying Person Names by Origin. In Proc. of AAAI, pages 1056–1061.","Yun Huang, Min Zhang and Chew Lim Tan. 2011. Nonparametric Bayesian Machine Transliteration with Synchronous Adaptor Grammars. In Proc. of ACL, pages 534–539.","Sittichai Jiampojamarn, Grzegorz Kondrak and Tarek Sherif. 2007. Applying Many-to-Many Alignments and Hidden Markov Models to Letter-to-Phoneme Conversion. In Proc. of NAACL, pages 372–379.","Sittichai Jiampojamarn, Aditya Bhargava, Qing Dou, Kenneth Dwyer and Grzegorz Kondrak. 2009. DirecTL: a Language Independent Approach to Transliteration. In Proc. of the 2009 Named Entities Workshop: Shared Task on Transliteration (NEWS 2009), pages 1056–1061.","Kevin Knight and Jonathan Graehl. 1998. Machine transliteration. Journal of Computational Linguistics, pages 28–31.","Philipp Koehn and Hieu Hoang and Alexandra Birch and Chris Callison-Burch and Marcello Federico and Nicola Bertoldi and Brooke Cowan and Wade Shen and Christine Moran and Richard Zens and Chris Dyer and Ondrej Bojar and Alexandra Constantin and Evan Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In Proc. of ACL.","Haizou Li, Min Zhang, and Jian Su 2004. A joint source-channel model for machine transliteration. In ACL ’04: Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics. Association for Computational Linguistics, Morristown, NJ, USA, 159.","Haizhou Li, Khe Chai Sim, Jin-Shea Kuo, and Minghui Dong. 2007. Semantic Transliteration of Personal Names. In Proc. of ACL, pages 120–127.","Daichi Mochihashi, Takeshi Yamada, and Naonori Ueda. 2009. Bayesian Unsupervised Word Segmentation with Nested Pitman-Yor Language Modeling. In Proc. of ACL/IJCNLP, pages 100–108.","Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Journal of Comput. Linguist., 29(1):19-51.","Jong-Hoon Oh, and Key-Sun Choi. 2005. Machine Learning Based English-to-Korean Transliteration Using Grapheme and Phoneme Information. Journal of IEICE Transactions, 88-D(7):1737-1748.","Jong-Hoon Oh, Key-Sun Choi, and Hitoshi Isahara. 2006. A machine transliteration model based on correspondence between graphemes and phonemes. Journal of ACM Trans. Asian Lang. Inf. Process., 5(3):185-208.","Min Zhang, Haizhou Li, Ming Liu and A Kumaran. 2012a. Whitepaper of NEWS 2012 shared task on machine transliteration. In Proc. of the 4th Named Entity Workshop (NEWS 2012), pages 1–9.","Min Zhang, Haizhou Li, A Kumaran and Ming Liu. 2012b. Report of NEWS 2012 Machine Transliteration Shared Task. In Proc. of the 4th Named Entity Workshop (NEWS 2012), pages 10–20. 398"]}],"references":[{"authors":[{"first":"D.","middle":"J.","last":"Aldous"}],"year":"1985","title":"Exchangeability and Related Topics","source":"D.J. Aldous. 1985. Exchangeability and Related Topics. École d’ Été St Flour 1983. Springer, 1985, 1117:1–198."},{"authors":[{"first":"C.","middle":"E.","last":"Antoniak"}],"year":"1974","title":"Mixtures of Dirichlet processes with applications to Bayesian nonparametric problems","source":"C.E. Antoniak. 1974. Mixtures of Dirichlet processes with applications to Bayesian nonparametric problems. Annals of Statistics. 2:1152, 174."},{"authors":[{"first":"Taylor","last":"Berg-Kirkpatrick"},{"first":"Dan","last":"Klein"}],"year":"2011","title":"Simple effective decipherment via combinatorial optimization","source":"Taylor Berg-Kirkpatrick and Dan Klein. 2011. Simple effective decipherment via combinatorial optimization. In Proc. of EMNLP, pages 313–321."},{"authors":[{"first":"P.","last":"Blunsom"},{"first":"T.","last":"Cohn"},{"first":"C.","last":"Dyer"},{"last":"Osborne"},{"last":"M"}],"year":"2009","title":"A Gibbs sampler for phrasal synchronous grammar induction","source":"P. Blunsom, T. Cohn, C. Dyer, and Osborne, M. 2009. A Gibbs sampler for phrasal synchronous grammar induction. In Proc. of ACL, pages 782–790."},{"authors":[{"first":"Eric","last":"Brill"},{"first":"Robert","middle":"C.","last":"Moore"}],"year":"2000","title":"An Improved Error Model for Noisy Channel Spelling Correction","source":"Eric Brill and Robert C. Moore. 2000. An Improved Error Model for Noisy Channel Spelling Correction. In Proc. of ACL, pages 286–293. 397"},{"authors":[{"first":"B.","last":"Efron"},{"first":"R.","middle":"J.","last":"Tibshirani"}],"year":"1993","title":"An Introduction to the Bootstrap","source":"B. Efron and R. J. Tibshirani 1993. An Introduction to the Bootstrap. Chapman & Hall, New York, NY."},{"authors":[{"first":"Andrew","last":"Finch"},{"first":"Eiichiro","last":"Sumita"}],"year":"2010","title":"A Bayesian Model of Bilingual Segmentation for Transliteration","source":"Andrew Finch and Eiichiro Sumita. 2010. A Bayesian Model of Bilingual Segmentation for Transliteration. In Proc. of the 7th International Workshop on Spoken Language Translation, pages 259–266."},{"authors":[{"first":"Masato","last":"Hagiwara"},{"first":"Satoshi","last":"Sekine"}],"year":"2011","title":"Latent Class Transliteration based on Source Language Origin","source":"Masato Hagiwara and Satoshi Sekine. 2011. Latent Class Transliteration based on Source Language Origin. In Proc. of ACL (Short Papers), pages 53-57."},{"authors":[{"first":"Masato","last":"Hagiwara"},{"first":"Satoshi","last":"Sekine"}],"year":"2012","title":"Latent semantic transliteration using dirichlet mixture","source":"Masato Hagiwara and Satoshi Sekine. 2012. Latent semantic transliteration using dirichlet mixture. In Proc. of the 4th Named Entity Workshop, pages 30– 37."},{"authors":[{"first":"Fei","last":"Huang"},{"first":"Stephan","last":"Vogel"},{"first":"Alex","last":"Waibel"}],"year":"2005","title":"Clustering and Classifying Person Names by Origin","source":"Fei Huang, Stephan Vogel, and Alex Waibel. 2005. Clustering and Classifying Person Names by Origin. In Proc. of AAAI, pages 1056–1061."},{"authors":[{"first":"Yun","last":"Huang"},{"first":"Min","last":"Zhang"},{"first":"Chew","middle":"Lim","last":"Tan"}],"year":"2011","title":"Nonparametric Bayesian Machine Transliteration with Synchronous Adaptor Grammars","source":"Yun Huang, Min Zhang and Chew Lim Tan. 2011. Nonparametric Bayesian Machine Transliteration with Synchronous Adaptor Grammars. In Proc. of ACL, pages 534–539."},{"authors":[{"first":"Sittichai","last":"Jiampojamarn"},{"first":"Grzegorz","last":"Kondrak"},{"first":"Tarek","last":"Sherif"}],"year":"2007","title":"Applying Many-to-Many Alignments and Hidden Markov Models to Letter-to-Phoneme Conversion","source":"Sittichai Jiampojamarn, Grzegorz Kondrak and Tarek Sherif. 2007. Applying Many-to-Many Alignments and Hidden Markov Models to Letter-to-Phoneme Conversion. In Proc. of NAACL, pages 372–379."},{"authors":[{"first":"Sittichai","last":"Jiampojamarn"},{"first":"Aditya","last":"Bhargava"},{"first":"Qing","last":"Dou"},{"first":"Kenneth","last":"Dwyer"},{"first":"Grzegorz","last":"Kondrak"}],"year":"2009","title":"DirecTL: a Language Independent Approach to Transliteration","source":"Sittichai Jiampojamarn, Aditya Bhargava, Qing Dou, Kenneth Dwyer and Grzegorz Kondrak. 2009. DirecTL: a Language Independent Approach to Transliteration. In Proc. of the 2009 Named Entities Workshop: Shared Task on Transliteration (NEWS 2009), pages 1056–1061."},{"authors":[{"first":"Kevin","last":"Knight"},{"first":"Jonathan","last":"Graehl"}],"year":"1998","title":"Machine transliteration","source":"Kevin Knight and Jonathan Graehl. 1998. Machine transliteration. Journal of Computational Linguistics, pages 28–31."},{"authors":[{"first":"Philipp","last":"Koehn"},{"first":"Hieu","last":"Hoang"},{"first":"Alexandra","last":"Birch"},{"first":"Chris","last":"Callison-Burch"},{"first":"Marcello","last":"Federico"},{"first":"Nicola","last":"Bertoldi"},{"first":"Brooke","last":"Cowan"},{"first":"Wade","last":"Shen"},{"first":"Christine","last":"Moran"},{"first":"Richard","last":"Zens"},{"first":"Chris","last":"Dyer"},{"first":"Ondrej","last":"Bojar"},{"first":"Alexandra","last":"Constantin"},{"first":"Evan","last":"Herbst"}],"year":"2007","title":"Moses: Open Source Toolkit for Statistical Machine Translation","source":"Philipp Koehn and Hieu Hoang and Alexandra Birch and Chris Callison-Burch and Marcello Federico and Nicola Bertoldi and Brooke Cowan and Wade Shen and Christine Moran and Richard Zens and Chris Dyer and Ondrej Bojar and Alexandra Constantin and Evan Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In Proc. of ACL."},{"authors":[{"first":"Haizou","last":"Li"},{"first":"Min","last":"Zhang"},{"first":"Jian","last":"Su"}],"year":"2004","title":"A joint source-channel model for machine transliteration","source":"Haizou Li, Min Zhang, and Jian Su 2004. A joint source-channel model for machine transliteration. In ACL ’04: Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics. Association for Computational Linguistics, Morristown, NJ, USA, 159."},{"authors":[{"first":"Haizhou","last":"Li"},{"first":"Khe","middle":"Chai","last":"Sim"},{"first":"Jin-Shea","last":"Kuo"},{"first":"Minghui","last":"Dong"}],"year":"2007","title":"Semantic Transliteration of Personal Names","source":"Haizhou Li, Khe Chai Sim, Jin-Shea Kuo, and Minghui Dong. 2007. Semantic Transliteration of Personal Names. In Proc. of ACL, pages 120–127."},{"authors":[{"first":"Daichi","last":"Mochihashi"},{"first":"Takeshi","last":"Yamada"},{"first":"Naonori","last":"Ueda"}],"year":"2009","title":"Bayesian Unsupervised Word Segmentation with Nested Pitman-Yor Language Modeling","source":"Daichi Mochihashi, Takeshi Yamada, and Naonori Ueda. 2009. Bayesian Unsupervised Word Segmentation with Nested Pitman-Yor Language Modeling. In Proc. of ACL/IJCNLP, pages 100–108."},{"authors":[{"first":"Franz","middle":"Josef","last":"Och"},{"first":"Hermann","last":"Ney"}],"year":"2003","title":"A systematic comparison of various statistical alignment models","source":"Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Journal of Comput. Linguist., 29(1):19-51."},{"authors":[{"first":"Jong-Hoon","last":"Oh"},{"first":"Key-Sun","last":"Choi"}],"year":"2005","title":"Machine Learning Based English-to-Korean Transliteration Using Grapheme and Phoneme Information","source":"Jong-Hoon Oh, and Key-Sun Choi. 2005. Machine Learning Based English-to-Korean Transliteration Using Grapheme and Phoneme Information. Journal of IEICE Transactions, 88-D(7):1737-1748."},{"authors":[{"first":"Jong-Hoon","last":"Oh"},{"first":"Key-Sun","last":"Choi"},{"first":"Hitoshi","last":"Isahara"}],"year":"2006","title":"A machine transliteration model based on correspondence between graphemes and phonemes","source":"Jong-Hoon Oh, Key-Sun Choi, and Hitoshi Isahara. 2006. A machine transliteration model based on correspondence between graphemes and phonemes. Journal of ACM Trans. Asian Lang. Inf. Process., 5(3):185-208."},{"authors":[{"first":"Min","last":"Zhang"},{"first":"Haizhou","last":"Li"},{"first":"Ming","last":"Liu"},{"first":"A","last":"Kumaran"}],"year":"2012a","title":"Whitepaper of NEWS 2012 shared task on machine transliteration","source":"Min Zhang, Haizhou Li, Ming Liu and A Kumaran. 2012a. Whitepaper of NEWS 2012 shared task on machine transliteration. In Proc. of the 4th Named Entity Workshop (NEWS 2012), pages 1–9."},{"authors":[{"first":"Min","last":"Zhang"},{"first":"Haizhou","last":"Li"},{"first":"A","last":"Kumaran"},{"first":"Ming","last":"Liu"}],"year":"2012b","title":"Report of NEWS 2012 Machine Transliteration Shared Task","source":"Min Zhang, Haizhou Li, A Kumaran and Ming Liu. 2012b. Report of NEWS 2012 Machine Transliteration Shared Task. In Proc. of the 4th Named Entity Workshop (NEWS 2012), pages 10–20. 398"}],"cites":[{"style":0,"text":"Knight et al., 1998","origin":{"pointer":"/sections/9/paragraphs/0","offset":79,"length":19},"authors":[{"last":"Knight"},{"last":"al."}],"year":"1998","references":[]},{"style":0,"text":"Brill et al., 2000","origin":{"pointer":"/sections/9/paragraphs/0","offset":124,"length":18},"authors":[{"last":"Brill"},{"last":"al."}],"year":"2000","references":[]},{"style":0,"text":"Oh et al., 2005","origin":{"pointer":"/sections/9/paragraphs/0","offset":217,"length":15},"authors":[{"last":"Oh"},{"last":"al."}],"year":"2005","references":[]},{"style":0,"text":"Oh et al., 2006","origin":{"pointer":"/sections/9/paragraphs/0","offset":234,"length":15},"authors":[{"last":"Oh"},{"last":"al."}],"year":"2006","references":["/references/20"]},{"style":0,"text":"Zhang et al., 2012b","origin":{"pointer":"/sections/9/paragraphs/0","offset":506,"length":19},"authors":[{"last":"Zhang"},{"last":"al."}],"year":"2012b","references":["/references/22"]},{"style":0,"text":"Li et al., 2007","origin":{"pointer":"/sections/9/paragraphs/1","offset":279,"length":15},"authors":[{"last":"Li"},{"last":"al."}],"year":"2007","references":["/references/16"]},{"style":0,"text":"Huang et al., 2005","origin":{"pointer":"/sections/9/paragraphs/1","offset":331,"length":18},"authors":[{"last":"Huang"},{"last":"al."}],"year":"2005","references":["/references/9"]},{"style":0,"text":"Li et al., 2007","origin":{"pointer":"/sections/9/paragraphs/1","offset":396,"length":15},"authors":[{"last":"Li"},{"last":"al."}],"year":"2007","references":["/references/16"]},{"style":0,"text":"Hagiwara et al., 2011","origin":{"pointer":"/sections/9/paragraphs/1","offset":603,"length":21},"authors":[{"last":"Hagiwara"},{"last":"al."}],"year":"2011","references":[]},{"style":0,"text":"Finch et al., 2010","origin":{"pointer":"/sections/9/paragraphs/2","offset":42,"length":18},"authors":[{"last":"Finch"},{"last":"al."}],"year":"2010","references":[]},{"style":0,"text":"Huang et al., 2011","origin":{"pointer":"/sections/9/paragraphs/2","offset":62,"length":18},"authors":[{"last":"Huang"},{"last":"al."}],"year":"2011","references":["/references/10"]},{"style":0,"text":"Hagiwara et al., 2012","origin":{"pointer":"/sections/9/paragraphs/2","offset":82,"length":21},"authors":[{"last":"Hagiwara"},{"last":"al."}],"year":"2012","references":[]},{"style":0,"text":"Li et al., 2004","origin":{"pointer":"/sections/9/paragraphs/2","offset":221,"length":15},"authors":[{"last":"Li"},{"last":"al."}],"year":"2004","references":["/references/15"]},{"style":0,"text":"Jiampojamarn et al., 2007","origin":{"pointer":"/sections/9/paragraphs/2","offset":238,"length":25},"authors":[{"last":"Jiampojamarn"},{"last":"al."}],"year":"2007","references":["/references/11"]},{"style":0,"text":"Berg-Kirkpatrick et al., 2011","origin":{"pointer":"/sections/9/paragraphs/2","offset":265,"length":29},"authors":[{"last":"Berg-Kirkpatrick"},{"last":"al."}],"year":"2011","references":[]},{"style":0,"text":"Li et al., 2007","origin":{"pointer":"/sections/9/paragraphs/3","offset":37,"length":15},"authors":[{"last":"Li"},{"last":"al."}],"year":"2007","references":["/references/16"]},{"style":0,"text":"Hagiwara et al., 2011","origin":{"pointer":"/sections/9/paragraphs/3","offset":54,"length":21},"authors":[{"last":"Hagiwara"},{"last":"al."}],"year":"2011","references":[]},{"style":0,"text":"Hagiwara et al., 2012","origin":{"pointer":"/sections/9/paragraphs/3","offset":159,"length":21},"authors":[{"last":"Hagiwara"},{"last":"al."}],"year":"2012","references":[]},{"style":0,"text":"Antoniak, 1974","origin":{"pointer":"/sections/9/paragraphs/4","offset":245,"length":14},"authors":[{"last":"Antoniak"}],"year":"1974","references":["/references/1"]},{"style":0,"text":"Finch et al., 2010","origin":{"pointer":"/sections/9/paragraphs/4","offset":310,"length":18},"authors":[{"last":"Finch"},{"last":"al."}],"year":"2010","references":[]},{"style":0,"text":"Li et al., 2004","origin":{"pointer":"/sections/10/paragraphs/9","offset":561,"length":15},"authors":[{"last":"Li"},{"last":"al."}],"year":"2004","references":["/references/15"]},{"style":0,"text":"Mochihashi et al., 2009","origin":{"pointer":"/sections/10/paragraphs/9","offset":648,"length":23},"authors":[{"last":"Mochihashi"},{"last":"al."}],"year":"2009","references":["/references/17"]},{"style":0,"text":"Aldous, 1985","origin":{"pointer":"/sections/10/paragraphs/9","offset":809,"length":12},"authors":[{"last":"Aldous"}],"year":"1985","references":["/references/0"]},{"style":0,"text":"Mochihashi et al., 2009","origin":{"pointer":"/sections/10/paragraphs/24","offset":159,"length":23},"authors":[{"last":"Mochihashi"},{"last":"al."}],"year":"2009","references":["/references/17"]},{"style":0,"text":"Och et al., 2003","origin":{"pointer":"/sections/11/paragraphs/4","offset":207,"length":16},"authors":[{"last":"Och"},{"last":"al."}],"year":"2003","references":[]},{"style":0,"text":"Koehn et al., 2007","origin":{"pointer":"/sections/11/paragraphs/4","offset":371,"length":18},"authors":[{"last":"Koehn"},{"last":"al."}],"year":"2007","references":["/references/14"]},{"style":0,"text":"Jiampojamarn et al., 2009","origin":{"pointer":"/sections/11/paragraphs/4","offset":418,"length":25},"authors":[{"last":"Jiampojamarn"},{"last":"al."}],"year":"2009","references":["/references/12"]},{"style":0,"text":"Zhang et al., 2012a","origin":{"pointer":"/sections/11/paragraphs/6","offset":101,"length":19},"authors":[{"last":"Zhang"},{"last":"al."}],"year":"2012a","references":["/references/21"]},{"style":0,"text":"Blunsom et al., 2009","origin":{"pointer":"/sections/11/paragraphs/6","offset":1127,"length":20},"authors":[{"last":"Blunsom"},{"last":"al."}],"year":"2009","references":["/references/3"]},{"style":0,"text":"Li et al., 2007","origin":{"pointer":"/sections/11/paragraphs/11","offset":372,"length":15},"authors":[{"last":"Li"},{"last":"al."}],"year":"2007","references":["/references/16"]},{"style":0,"text":"Efron et al., 1993","origin":{"pointer":"/sections/11/paragraphs/13","offset":129,"length":18},"authors":[{"last":"Efron"},{"last":"al."}],"year":"1993","references":[]}]}
