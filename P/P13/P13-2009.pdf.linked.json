{"sections":[{"title":"","paragraphs":["Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 47–52, Sofia, Bulgaria, August 4-9 2013. c⃝2013 Association for Computational Linguistics"]},{"title":"Semantic Parsing as Machine Translation Jacob Andreas Computer Laboratory University of Cambridge jda33@cam.ac.uk Andreas Vlachos Computer Laboratory University of Cambridge av308@cam.ac.uk Stephen Clark Computer Laboratory University of Cambridge sc609@cam.ac.uk Abstract","paragraphs":["Semantic parsing is the problem of deriving a structured meaning representation from a natural language utterance. Here we approach it as a straightforward machine translation task, and demonstrate that standard machine translation components can be adapted into a semantic parser. In experiments on the multilingual GeoQuery corpus we find that our parser is competitive with the state of the art, and in some cases achieves higher accuracy than recently proposed purpose-built systems. These results support the use of machine translation methods as an informative baseline in semantic parsing evaluations, and suggest that research in semantic parsing could benefit from advances in machine translation."]},{"title":"1 Introduction","paragraphs":["Semantic parsing (SP) is the problem of transforming a natural language (NL) utterance into a machine-interpretable meaning representation (MR). It is well-studied in NLP, and a wide variety of methods have been proposed to tackle it, e.g. rule-based (Popescu et al., 2003), supervised (Zelle, 1995), unsupervised (Goldwasser et al., 2011), and response-based (Liang et al., 2011).","At least superficially, SP is simply a machine translation (MT) task: we transform an NL utterance in one language into a statement of an-other (un-natural) meaning representation language (MRL). Indeed, successful semantic parsers often resemble MT systems in several important respects, including the use of word alignment models as a starting point for rule extraction (Wong and Mooney, 2006; Kwiatkowski et al., 2010) and the use of automata such as tree transducers (Jones et al., 2012) to encode the relationship between NL and MRL.","The key difference between the two tasks is that in SP, the target language (the MRL) has very different properties to an NL. In particular, MRs must conform strictly to a particular structure so that they are machine-interpretable. Contrast this with ordinary MT, where varying degrees of wrongness are tolerated by human readers (and evaluation metrics). To avoid producing malformed MRs, almost all of the existing research on SP has focused on developing models with richer structure than those commonly used for MT.","In this work we attempt to determine how accurate a semantic parser we can build by treating SP as a pure MT task, and describe pre- and postprocessing steps which allow structure to be preserved in the MT process.","Our contributions are as follows: We develop a semantic parser using off-the-shelf MT components, exploring phrase-based as well as hierarchical models. Experiments with four languages on the popular GeoQuery corpus (Zelle, 1995) show that our parser is competitve with the state-of-the-art, in some cases achieving higher accuracy than recently introduced purpose-built semantic parsers. Our approach also appears to require substantially less time to train than the two best-performing semantic parsers. These results support the use of MT methods as an informative baseline in SP evaluations and show that research in SP could benefit from research advances in MT."]},{"title":"2 MT-based semantic parsing","paragraphs":["The input is a corpus of NL utterances paired with MRs. In order to learn a semantic parser using MT we linearize the MRs, learn alignments between the MRL and the NL, extract translation rules, and learn a language model for the MRL. We also specify a decoding procedure that will return structured MRs for an utterance during prediction. 47","states bordering Texas state(next to(state(stateid(texas)))) ⇓ STEM & LINEARIZE","state border texa state1 next to1 state1 stateid1 texas0 ⇓ ALIGN state border texa state1 next to1 state1 stateid1 texas0 ⇓ EXTRACT (PHRASE)","⟨ state , state1 ⟩","⟨ state border , state1 border1 ⟩","⟨ texa , state1 stateid1 texas0 ⟩","... ⇓ EXTRACT (HIER) [X] → ⟨state , state1⟩ [X] → ⟨state [X] texa ,","state1 [X] state1 stateid1 texas0⟩ ... Figure 1: Illustration of preprocessing and rule extraction. Linearization We assume that the MRL is variable-free (that is, the meaning representation for each utterance is tree-shaped), noting that for-malisms with variables, like the λ-calculus, can be mapped onto variable-free logical forms with combinatory logics (Curry et al., 1980).","In order to learn a semantic parser using MT we begin by converting these MRs to a form more similar to NL. To do so, we simply take a preorder traversal of every functional form, and label every function with the number of arguments it takes. After translation, recovery of the function is easy: if the arity of every function in the MRL is known, then every traversal uniquely specifies its corresponding tree. Using an example from GeoQuery, given an input function of the form answer(population(city(cityid(‘seattle’, ‘wa’)))) we produce a “decorated” translation input of the form answer1 population1 city1 cityid2 seattle0 wa0 where each subscript indicates the symbol’s arity (constants, including strings, are treated as zero-argument functions). Explicit argument number labeling serves two functions. Most importantly, it eliminates any possible ambiguity from the tree reconstruction which takes place during decoding: given any sequence of decorated MRL tokens, we can always reconstruct the corresponding tree structure (if one exists). Arity labeling additionally allows functions with variable numbers of arguments (e.g. cityid, which in some training examples is unary) to align with different natural language strings depending on context. Alignment Following the linearization of the MRs, we find alignments between the MR tokens and the NL tokens using the IBM Model 4 (Brown et al., 1993). Once the alignment algorithm is run in both directions (NL to MRL, MRL to NL), we symmetrize the resulting alignments to obtain a consensus many-to-many alignment (Och and Ney, 2000; Koehn et al., 2005). Rule extraction From the many-to-many alignment we need to extract a translation rule table, consisting of corresponding phrases in NL and MRL. We consider a phrase-based translation model (Koehn et al., 2003) and a hierarchical translation model (Chiang, 2005). Rules for the phrase-based model consist of pairs of aligned source and target sequences, while hierarchical rules are SCFG productions containing at most two instances of a single nonterminal symbol.","Note that both extraction algorithms can learn rules which a traditional tree-transducer-based approach cannot—for example the right hand side [X] river1 all0 traverse1 [X] corresponding to the pair of disconnected tree fragments: [X] ","traverse ","river  [X]","all (where each X indicates a gap in the rule). Language modeling In addition to translation rules learned from a parallel corpus, MT systems also rely on an n-gram language model for the target language, estimated from a (typically larger) monolingual corpus. In the case of SP, such a monolingual corpus is rarely available, and we instead use the MRs available in the training data to learn a language model of the MRL. This informa-tion helps guide the decoder towards well-formed 48 structures; it encodes, for example, the preferences of predicates of the MRL for certain arguments. Prediction Given a new NL utterance, we need to find the n best translations (i.e. sequences of decorated MRL tokens) that maximize the weighted sum of the translation score (the probabilities of the translations according to the rule translation table) and the language model score, a process usually referred to as decoding. Standard decoding procedures for MT produce an n-best list of all possible translations, but here we need to restrict ourselves to translations corresponding to well-formed MRs. In principle this could be done by re-writing the beam search algorithm used in decoding to immediately discard malformed MRs; for the experiments in this paper we simply filter the regular n-best list until we find a well-formed MR. This filtering can be done with time linear in the length of the example by exploiting the argument label numbers introduced during linearization. Finally, we insert the brackets according to the tree structure specified by the argument number labels."]},{"title":"3 Experimental setup Dataset","paragraphs":["We conduct experiments on the GeoQuery data set. The corpus consists of a set of 880 natural-language questions about U.S. geography in four languages (English, German, Greek and Thai), and their representations in a variable-free MRL that can be executed against a Prolog database interface. Initial experimentation was done using 10 fold cross-validation on the 600sentence development set and the final evaluation on a held-out test set of 280 sentences. All semantic parsers for GeoQuery we compare against also makes use of NP lists (Jones et al., 2012), which contain MRs for every noun phrase that appears in the NL utterances of each language. In our experiments, the NP list was included by appending all entries as extra training sentences to the end of the training corpus of each language with 50 times the weight of regular training examples, to ensure that they are learned as translation rules.","Evaluation for each utterance is performed by executing both the predicted and the gold standard MRs against the database and obtaining their respective answers. An MR is correct if it obtains the same answer as the gold standard MR, allow-ing for a fair comparison between systems using different learning paradigms. Following Jones et al. (2012) we report accuracy, i.e. the percentage of NL questions with correct answers, and F1, i.e. the harmonic mean of precision (percentage of correct answers obtained). Implementation In all experiments, we use the IBM Model 4 implementation from the GIZA++ toolkit (Och and Ney, 2000) for alignment, and the phrase-based and hierarchical models implemented in the Moses toolkit (Koehn et al., 2007) for rule extraction. The best symmetrization algorithm, translation and language model weights for each language are selected using cross-validation on the development set. In the case of English and German, we also found that stemming (Bird et al., 2009; Porter, 1980) was hepful in reducing data sparsity."]},{"title":"4 Results","paragraphs":["We first compare the results for the two translation rule extraction models, phrase-based and hierarchical (“MT-phrase” and “MT-hier” respectively in Table 1). We find that the hierarchical model performs better in all languages apart from Greek, indicating that the long-range reorderings learned by a hierarchical translation system are useful for this task. These benefits are most pronounced in the case of Thai, likely due to the the language’s comparatively different word order.","We also present results for both models without using the NP lists for training in Table 2. As expected, the performances are almost uniformly lower, but the parser still produces correct output for the majority of examples.","As discussed above, one important modifica-tion of the MT paradigm which allows us to produce structured output is the addition of structure-checking to the beam search. It is not evident, a priori, that this search procedure is guaranteed to find any well-formed outputs in reasonable time; to test the effect of this extra requirement on en de el th MT-phrase 75.3 68.8 70.4 53.0 MT-phrase (-NP) 63.4 65.8 64.0 39.8 MT-hier 80.5 68.9 69.1 70.4 MT-hier (-NP) 62.5 69.9 62.9 62.1 Table 2: GeoQuery accuracies with and without NPs. Rows with (-NP) did not use the NP list. 49 English [en] German [de] Greek [el] Thai [th] Acc. F1 Acc. F1 Acc. F1 Acc. F1 WASP 71.1 77.7 65.7 74.9 70.7 78.6 71.4 75.0 UBL 82.1 82.1 75.0 75.0 73.6 73.7 66.4 66.4 tsVB 79.3 79.3 74.6 74.6 75.4 75.4 78.2 78.2","hybrid-tree 76.8 81.0 62.1 68.5 69.3 74.6 73.6 76.7","MT-phrase 75.3 75.8 68.8 70.8 70.4 73.0 53.0 54.4 MT-hier 80.5 81.8 68.9 71.8 69.1 72.3 70.4 70.7 Table 1: Accuracy and F1 scores for the multilingual GeoQuery test set. Results for other systems as reported by Jones et al. (2012). the speed of SP, we investigate how many MRs the decoder needs to generate before producing one which is well-formed. In practice, increasing search depth in the n-best list from 1 to 50 results in a gain of no more than a percentage point or two, and we conclude that our filtering method is appropriate for the task.","We also compare the MT-based semantic parsers to several recently published ones: WASP (Wong and Mooney, 2006), which like the hierarchical model described here learns a SCFG to translate between NL and MRL; tsVB (Jones et al., 2012), which uses variational Bayesian inference to learn weights for a tree transducer; UBL (Kwiatkowski et al., 2010), which learns a CCG lexicon with semantic annotations; and hybrid-tree (Lu et al., 2008), which learns a synchronous generative model over variable-free MRs and NL strings.","In the results shown in Table 1 we observe that on English GeoQuery data, the hierarchical translation model achieves scores competitive with the state of the art, and in every language one of the MT systems achieves accuracy at least as good as a purpose-built semantic parser.","We conclude with an informal test of training speeds. While differences in implementation and factors like programming language choice make a direct comparison of times necessarily imprecise, we note that the MT system takes less than three minutes to train on the GeoQuery corpus, while the publicly-available implementations of tsVB and UBL require roughly twenty minutes and five hours respectively on a 2.1 GHz CPU. So in addition to competitive performance, the MT-based parser also appears to be considerably more efficient at training time than other parsers in the literature."]},{"title":"5 Related Work","paragraphs":["WASP, an early automatically-learned SP system, was strongly influenced by MT techniques. Like the present work, it uses GIZA++ alignments as a starting point for the rule extraction procedure, and algorithms reminiscent of those used in syntactic MT to extract rules.","tsVB also uses a piece of standard MT machinery, specifically tree transducers, which have been profitably employed for syntax-based machine translation (Maletti, 2010). In that work, however, the usual MT parameter-estimation technique of simply counting the number of rule occurrences does not improve scores, and the authors instead resort to a variational inference procedure to acquire rule weights. The present work is also the first we are aware of which uses phrase-based rather than tree-based machine translation techniques to learn a semantic parser. hybrid-tree (Lu et al., 2008) similarly describes a generative model over derivations of MRL trees.","The remaining system discussed in this paper, UBL (Kwiatkowski et al., 2010), leverages the fact that the MRL does not simply encode trees, but rather λ-calculus expressions. It employs resolu-tion procedures specific to the λ-calculus such as splitting and unification in order to generate rule templates. Like other systems described, it uses GIZA alignments for initialization. Other work which generalizes from variable-free meaning representations to λ-calculus expressions includes the natural language generation procedure described by Lu and Ng (2011).","UBL, like an MT system (and unlike most of the other systems discussed in this section), extracts rules at multiple levels of granularity by means of this splitting and unification procedure. hybrid-tree similarly benefits from the introduction of 50 multi-level rules composed from smaller rules, a process similar to the one used for creating phrase tables in a phrase-based MT system."]},{"title":"6 Discussion","paragraphs":["Our results validate the hypothesis that it is possible to adapt an ordinary MT system into a work-ing semantic parser. In spite of the comparative simplicity of the approach, it achieves scores comparable to (and sometimes better than) many state-of-the-art systems. For this reason, we argue for the use of a machine translation baseline as a point of comparison for new methods. The results also demonstrate the usefulness of two techniques which are crucial for successful MT, but which are not widely used in semantic parsing. The first is the incorporation of a language model (or comparable long-distance structure-scoring model) to assign scores to predicted parses independent of the transformation model. The second is the use of large, composed rules (rather than rules which trigger on only one lexical item, or on tree portions of limited depth (Lu et al., 2008)) in order to “memorize” frequently-occurring largescale structures."]},{"title":"7 Conclusions","paragraphs":["We have presented a semantic parser which uses techniques from machine translation to learn mappings from natural language to variable-free meaning representations. The parser performs comparably to several recent purpose-built semantic parsers on the GeoQuery dataset, while training considerably faster than state-of-the-art systems. Our experiments demonstrate the usefulness of several techniques which might be broadly applied to other semantic parsers, and provides an informative basis for future work."]},{"title":"Acknowledgments","paragraphs":["Jacob Andreas is supported by a Churchill Scholarship. Andreas Vlachos is funded by the European Community’s Seventh Framework Programme (FP7/2007-2013) under grant agree-ment no. 270019 (SPACEBOOK project www. spacebook-project.eu)."]},{"title":"References","paragraphs":["Steven Bird, Edward Loper, and Edward Klein. 2009. Natural Language Processing with Python. O’Reilly Media, Inc.","Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: parameter estimation. Computational Linguistics, 19(2):263–311.","David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 263–270, Ann Arbor, Michigan.","H.B. Curry, J.R. Hindley, and J.P. Seldin. 1980. To H.B. Curry: Essays on Combinatory Logic, Lambda Calculus, and Formalism. Academic Press.","Dan Goldwasser, Roi Reichart, James Clarke, and Dan Roth. 2011. Confidence driven unsupervised semantic parsing. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 1486–1495, Portland, Oregon.","Bevan K. Jones, Mark Johnson, and Sharon Goldwater. 2012. Semantic parsing with bayesian tree transducers. In Proceedings of the 50th Annual Meeting of the Association of Computational Linguistics, pages 488–496, Jeju, Korea.","Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, pages 48–54, Edmonton, Canada.","Philipp Koehn, Amittai Axelrod, Alexandra Birch-Mayne, Chris Callison-Burch, Miles Osborne, and David Talbot. 2005. Edinburgh System Descrip-tion for the 2005 IWSLT Speech Translation Evaluation. In Proceedings of the International Workshop on Spoken Language Translation.","Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondřej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pages 177–180, Prague, Czech Republic.","Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwater, and Mark Steedman. 2010. Inducing probabilistic ccg grammars from logical form with higher-order unification. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1223–1233, Cambridge, Massachusetts.","Percy Liang, Michael Jordan, and Dan Klein. 2011. Learning dependency-based compositional semantics. In Proceedings of the 49th Annual Meeting of 51 the Association for Computational Linguistics: Human Language Technologies, pages 590–599, Portland, Oregon.","Wei Lu and Hwee Tou Ng. 2011. A probabilistic forest-to-string model for language generation from typed lambda calculus expressions. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11, pages 1611– 1622. Association for Computational Linguistics.","Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke Zettlemoyer. 2008. A generative model for parsing natural language to meaning representations. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 783– 792, Edinburgh, UK.","Andreas Maletti. 2010. Survey: Tree transducers in machine translation. In Proceedings of the 2nd Workshop on Non-Classical Models for Automata and Applications, Jena, Germany.","Franz Josef Och and Hermann Ney. 2000. Improved statistical alignment models. In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics, pages 440–447, Hong Kong, China.","Ana-Maria Popescu, Oren Etzioni, and Henry Kautz. 2003. Towards a theory of natural language interfaces to databases. In Proceedings of the 8th International Conference on Intelligent User Interfaces, pages 149–157, Santa Monica, CA.","M. Porter. 1980. An algorithm for suffix stripping. Program, 14(3):130–137.","Yuk Wah Wong and Raymond Mooney. 2006. Learning for semantic parsing with statistical machine translation. In Proceedings of the 2006 Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, pages 439–446, New York.","John M. Zelle. 1995. Using Inductive Logic Programming to Automate the Construction of Natural Language Parsers. Ph.D. thesis, Department of Computer Sciences, The University of Texas at Austin. 52"]}],"references":[{"authors":[{"first":"Steven","last":"Bird"},{"first":"Edward","last":"Loper"},{"first":"Edward","last":"Klein"}],"year":"2009","title":"Natural Language Processing with Python","source":"Steven Bird, Edward Loper, and Edward Klein. 2009. Natural Language Processing with Python. O’Reilly Media, Inc."},{"authors":[{"first":"Peter","middle":"F.","last":"Brown"},{"first":"Vincent","middle":"J. Della","last":"Pietra"},{"first":"Stephen","middle":"A. Della","last":"Pietra"},{"first":"Robert","middle":"L.","last":"Mercer"}],"year":"1993","title":"The mathematics of statistical machine translation: parameter estimation","source":"Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: parameter estimation. Computational Linguistics, 19(2):263–311."},{"authors":[{"first":"David","last":"Chiang"}],"year":"2005","title":"A hierarchical phrase-based model for statistical machine translation","source":"David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 263–270, Ann Arbor, Michigan."},{"authors":[{"first":"H.","middle":"B.","last":"Curry"},{"first":"J.","middle":"R.","last":"Hindley"},{"first":"J.","middle":"P.","last":"Seldin"}],"year":"1980","title":"To H","source":"H.B. Curry, J.R. Hindley, and J.P. Seldin. 1980. To H.B. Curry: Essays on Combinatory Logic, Lambda Calculus, and Formalism. Academic Press."},{"authors":[{"first":"Dan","last":"Goldwasser"},{"first":"Roi","last":"Reichart"},{"first":"James","last":"Clarke"},{"first":"Dan","last":"Roth"}],"year":"2011","title":"Confidence driven unsupervised semantic parsing","source":"Dan Goldwasser, Roi Reichart, James Clarke, and Dan Roth. 2011. Confidence driven unsupervised semantic parsing. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 1486–1495, Portland, Oregon."},{"authors":[{"first":"Bevan","middle":"K.","last":"Jones"},{"first":"Mark","last":"Johnson"},{"first":"Sharon","last":"Goldwater"}],"year":"2012","title":"Semantic parsing with bayesian tree transducers","source":"Bevan K. Jones, Mark Johnson, and Sharon Goldwater. 2012. Semantic parsing with bayesian tree transducers. In Proceedings of the 50th Annual Meeting of the Association of Computational Linguistics, pages 488–496, Jeju, Korea."},{"authors":[{"first":"Philipp","last":"Koehn"},{"first":"Franz","middle":"Josef","last":"Och"},{"first":"Daniel","last":"Marcu"}],"year":"2003","title":"Statistical phrase-based translation","source":"Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, pages 48–54, Edmonton, Canada."},{"authors":[{"first":"Philipp","last":"Koehn"},{"first":"Amittai","last":"Axelrod"},{"first":"Alexandra","last":"Birch-Mayne"},{"first":"Chris","last":"Callison-Burch"},{"first":"Miles","last":"Osborne"},{"first":"David","last":"Talbot"}],"year":"2005","title":"Edinburgh System Descrip-tion for the 2005 IWSLT Speech Translation Evaluation","source":"Philipp Koehn, Amittai Axelrod, Alexandra Birch-Mayne, Chris Callison-Burch, Miles Osborne, and David Talbot. 2005. Edinburgh System Descrip-tion for the 2005 IWSLT Speech Translation Evaluation. In Proceedings of the International Workshop on Spoken Language Translation."},{"authors":[{"first":"Philipp","last":"Koehn"},{"first":"Hieu","last":"Hoang"},{"first":"Alexandra","last":"Birch"},{"first":"Chris","last":"Callison-Burch"},{"first":"Marcello","last":"Federico"},{"first":"Nicola","last":"Bertoldi"},{"first":"Brooke","last":"Cowan"},{"first":"Wade","last":"Shen"},{"first":"Christine","last":"Moran"},{"first":"Richard","last":"Zens"},{"first":"Chris","last":"Dyer"},{"first":"Ondřej","last":"Bojar"},{"first":"Alexandra","last":"Constantin"},{"first":"Evan","last":"Herbst"}],"year":"2007","title":"Moses: open source toolkit for statistical machine translation","source":"Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondřej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pages 177–180, Prague, Czech Republic."},{"authors":[{"first":"Tom","last":"Kwiatkowski"},{"first":"Luke","last":"Zettlemoyer"},{"first":"Sharon","last":"Goldwater"},{"first":"Mark","last":"Steedman"}],"year":"2010","title":"Inducing probabilistic ccg grammars from logical form with higher-order unification","source":"Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwater, and Mark Steedman. 2010. Inducing probabilistic ccg grammars from logical form with higher-order unification. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1223–1233, Cambridge, Massachusetts."},{"authors":[{"first":"Percy","last":"Liang"},{"first":"Michael","last":"Jordan"},{"first":"Dan","last":"Klein"}],"year":"2011","title":"Learning dependency-based compositional semantics","source":"Percy Liang, Michael Jordan, and Dan Klein. 2011. Learning dependency-based compositional semantics. In Proceedings of the 49th Annual Meeting of 51 the Association for Computational Linguistics: Human Language Technologies, pages 590–599, Portland, Oregon."},{"authors":[{"first":"Wei","last":"Lu"},{"first":"Hwee","middle":"Tou","last":"Ng"}],"year":"2011","title":"A probabilistic forest-to-string model for language generation from typed lambda calculus expressions","source":"Wei Lu and Hwee Tou Ng. 2011. A probabilistic forest-to-string model for language generation from typed lambda calculus expressions. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11, pages 1611– 1622. Association for Computational Linguistics."},{"authors":[{"first":"Wei","last":"Lu"},{"first":"Hwee","middle":"Tou","last":"Ng"},{"first":"Wee","middle":"Sun","last":"Lee"},{"first":"Luke","last":"Zettlemoyer"}],"year":"2008","title":"A generative model for parsing natural language to meaning representations","source":"Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke Zettlemoyer. 2008. A generative model for parsing natural language to meaning representations. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 783– 792, Edinburgh, UK."},{"authors":[{"first":"Andreas","last":"Maletti"}],"year":"2010","title":"Survey: Tree transducers in machine translation","source":"Andreas Maletti. 2010. Survey: Tree transducers in machine translation. In Proceedings of the 2nd Workshop on Non-Classical Models for Automata and Applications, Jena, Germany."},{"authors":[{"first":"Franz","middle":"Josef","last":"Och"},{"first":"Hermann","last":"Ney"}],"year":"2000","title":"Improved statistical alignment models","source":"Franz Josef Och and Hermann Ney. 2000. Improved statistical alignment models. In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics, pages 440–447, Hong Kong, China."},{"authors":[{"first":"Ana-Maria","last":"Popescu"},{"first":"Oren","last":"Etzioni"},{"first":"Henry","last":"Kautz"}],"year":"2003","title":"Towards a theory of natural language interfaces to databases","source":"Ana-Maria Popescu, Oren Etzioni, and Henry Kautz. 2003. Towards a theory of natural language interfaces to databases. In Proceedings of the 8th International Conference on Intelligent User Interfaces, pages 149–157, Santa Monica, CA."},{"authors":[{"first":"M.","last":"Porter"}],"year":"1980","title":"An algorithm for suffix stripping","source":"M. Porter. 1980. An algorithm for suffix stripping. Program, 14(3):130–137."},{"authors":[{"first":"Yuk","middle":"Wah","last":"Wong"},{"first":"Raymond","last":"Mooney"}],"year":"2006","title":"Learning for semantic parsing with statistical machine translation","source":"Yuk Wah Wong and Raymond Mooney. 2006. Learning for semantic parsing with statistical machine translation. In Proceedings of the 2006 Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, pages 439–446, New York."},{"authors":[{"first":"John","middle":"M.","last":"Zelle"}],"year":"1995","title":"Using Inductive Logic Programming to Automate the Construction of Natural Language Parsers","source":"John M. Zelle. 1995. Using Inductive Logic Programming to Automate the Construction of Natural Language Parsers. Ph.D. thesis, Department of Computer Sciences, The University of Texas at Austin. 52"}],"cites":[{"style":0,"text":"Popescu et al., 2003","origin":{"pointer":"/sections/2/paragraphs/0","offset":252,"length":20},"authors":[{"last":"Popescu"},{"last":"al."}],"year":"2003","references":["/references/15"]},{"style":0,"text":"Zelle, 1995","origin":{"pointer":"/sections/2/paragraphs/0","offset":287,"length":11},"authors":[{"last":"Zelle"}],"year":"1995","references":["/references/18"]},{"style":0,"text":"Goldwasser et al., 2011","origin":{"pointer":"/sections/2/paragraphs/0","offset":315,"length":23},"authors":[{"last":"Goldwasser"},{"last":"al."}],"year":"2011","references":["/references/4"]},{"style":0,"text":"Liang et al., 2011","origin":{"pointer":"/sections/2/paragraphs/0","offset":361,"length":18},"authors":[{"last":"Liang"},{"last":"al."}],"year":"2011","references":["/references/10"]},{"style":0,"text":"Wong and Mooney, 2006","origin":{"pointer":"/sections/2/paragraphs/1","offset":373,"length":21},"authors":[{"last":"Wong"},{"last":"Mooney"}],"year":"2006","references":["/references/17"]},{"style":0,"text":"Kwiatkowski et al., 2010","origin":{"pointer":"/sections/2/paragraphs/1","offset":396,"length":24},"authors":[{"last":"Kwiatkowski"},{"last":"al."}],"year":"2010","references":["/references/9"]},{"style":0,"text":"Jones et al., 2012","origin":{"pointer":"/sections/2/paragraphs/1","offset":472,"length":18},"authors":[{"last":"Jones"},{"last":"al."}],"year":"2012","references":["/references/5"]},{"style":0,"text":"Zelle, 1995","origin":{"pointer":"/sections/2/paragraphs/4","offset":217,"length":11},"authors":[{"last":"Zelle"}],"year":"1995","references":["/references/18"]},{"style":0,"text":"Curry et al., 1980","origin":{"pointer":"/sections/3/paragraphs/7","offset":360,"length":18},"authors":[{"last":"Curry"},{"last":"al."}],"year":"1980","references":["/references/3"]},{"style":0,"text":"Brown et al., 1993","origin":{"pointer":"/sections/3/paragraphs/8","offset":1389,"length":18},"authors":[{"last":"Brown"},{"last":"al."}],"year":"1993","references":["/references/1"]},{"style":0,"text":"Och and Ney, 2000","origin":{"pointer":"/sections/3/paragraphs/8","offset":1574,"length":17},"authors":[{"last":"Och"},{"last":"Ney"}],"year":"2000","references":["/references/14"]},{"style":0,"text":"Koehn et al., 2005","origin":{"pointer":"/sections/3/paragraphs/8","offset":1593,"length":18},"authors":[{"last":"Koehn"},{"last":"al."}],"year":"2005","references":["/references/7"]},{"style":0,"text":"Koehn et al., 2003","origin":{"pointer":"/sections/3/paragraphs/8","offset":1804,"length":18},"authors":[{"last":"Koehn"},{"last":"al."}],"year":"2003","references":["/references/6"]},{"style":0,"text":"Chiang, 2005","origin":{"pointer":"/sections/3/paragraphs/8","offset":1862,"length":12},"authors":[{"last":"Chiang"}],"year":"2005","references":["/references/2"]},{"style":0,"text":"Jones et al., 2012","origin":{"pointer":"/sections/4/paragraphs/0","offset":539,"length":18},"authors":[{"last":"Jones"},{"last":"al."}],"year":"2012","references":["/references/5"]},{"style":0,"text":"Jones et al. (2012)","origin":{"pointer":"/sections/4/paragraphs/1","offset":328,"length":19},"authors":[{"last":"Jones"},{"last":"al."}],"year":"2012","references":["/references/5"]},{"style":0,"text":"Och and Ney, 2000","origin":{"pointer":"/sections/4/paragraphs/1","offset":610,"length":17},"authors":[{"last":"Och"},{"last":"Ney"}],"year":"2000","references":["/references/14"]},{"style":0,"text":"Koehn et al., 2007","origin":{"pointer":"/sections/4/paragraphs/1","offset":723,"length":18},"authors":[{"last":"Koehn"},{"last":"al."}],"year":"2007","references":["/references/8"]},{"style":0,"text":"Bird et al., 2009","origin":{"pointer":"/sections/4/paragraphs/1","offset":980,"length":17},"authors":[{"last":"Bird"},{"last":"al."}],"year":"2009","references":["/references/0"]},{"style":0,"text":"Porter, 1980","origin":{"pointer":"/sections/4/paragraphs/1","offset":999,"length":12},"authors":[{"last":"Porter"}],"year":"1980","references":["/references/16"]},{"style":0,"text":"Jones et al. (2012)","origin":{"pointer":"/sections/5/paragraphs/4","offset":211,"length":19},"authors":[{"last":"Jones"},{"last":"al."}],"year":"2012","references":["/references/5"]},{"style":0,"text":"Wong and Mooney, 2006","origin":{"pointer":"/sections/5/paragraphs/5","offset":88,"length":21},"authors":[{"last":"Wong"},{"last":"Mooney"}],"year":"2006","references":["/references/17"]},{"style":0,"text":"Jones et al., 2012","origin":{"pointer":"/sections/5/paragraphs/5","offset":214,"length":18},"authors":[{"last":"Jones"},{"last":"al."}],"year":"2012","references":["/references/5"]},{"style":0,"text":"Kwiatkowski et al., 2010","origin":{"pointer":"/sections/5/paragraphs/5","offset":322,"length":24},"authors":[{"last":"Kwiatkowski"},{"last":"al."}],"year":"2010","references":["/references/9"]},{"style":0,"text":"Lu et al., 2008","origin":{"pointer":"/sections/5/paragraphs/5","offset":420,"length":15},"authors":[{"last":"Lu"},{"last":"al."}],"year":"2008","references":["/references/12"]},{"style":0,"text":"Maletti, 2010","origin":{"pointer":"/sections/6/paragraphs/1","offset":154,"length":13},"authors":[{"last":"Maletti"}],"year":"2010","references":["/references/13"]},{"style":0,"text":"Lu et al., 2008","origin":{"pointer":"/sections/6/paragraphs/1","offset":575,"length":15},"authors":[{"last":"Lu"},{"last":"al."}],"year":"2008","references":["/references/12"]},{"style":0,"text":"Kwiatkowski et al., 2010","origin":{"pointer":"/sections/6/paragraphs/2","offset":51,"length":24},"authors":[{"last":"Kwiatkowski"},{"last":"al."}],"year":"2010","references":["/references/9"]},{"style":0,"text":"Lu and Ng (2011)","origin":{"pointer":"/sections/6/paragraphs/2","offset":543,"length":16},"authors":[{"last":"Lu"},{"last":"Ng"}],"year":"2011","references":["/references/11"]},{"style":0,"text":"Lu et al., 2008","origin":{"pointer":"/sections/7/paragraphs/0","offset":859,"length":15},"authors":[{"last":"Lu"},{"last":"al."}],"year":"2008","references":["/references/12"]}]}
