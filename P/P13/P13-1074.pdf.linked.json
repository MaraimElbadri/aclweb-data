{"sections":[{"title":"","paragraphs":["Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 752–760, Sofia, Bulgaria, August 4-9 2013. c⃝2013 Association for Computational Linguistics"]},{"title":"Punctuation Prediction with Transition-based Parsing Dongdong Zhang1, Shuangzhi Wu2, Nan Yang3, Mu Li1 ","paragraphs":["1"]},{"title":"Microsoft Research Asia, Beijing, China","paragraphs":["2"]},{"title":"Harbin Institute of Technology, Harbin, China","paragraphs":["3"]},{"title":"University of Science and Technology of China, Hefei, China {dozhang,v-shuawu,v-nayang,muli}@microsoft.com  Abstract","paragraphs":["Punctuations are not available in automatic speech recognition outputs, which could create barriers to many subsequent text processing tasks. This paper proposes a novel method to predict punctuation symbols for the stream of words in transcribed speech texts. Our method jointly performs parsing and punctuation prediction by integrating a rich set of syntactic features when processing words from left to right. It can exploit a global view to capture long-range dependencies for punctuation prediction with linear complexity. The experimental results on the test data sets of IWSLT and TDT4 show that our method can achieve high-level performance in punctuation prediction over the stream of words in transcribed speech text."]},{"title":"1 Introduction","paragraphs":["Standard automatic speech recognizers output unstructured streams of words. They neither perform a proper segmentation of the output into sentences, nor predict punctuation symbols. The unavailable punctuations and sentence boundaries in transcribed speech texts create barriers to many subsequent processing tasks, such as summarization, information extraction, question answering and machine translation. Thus, the segmentation of long texts is necessary in many real applications. For example, in speech-to-speech translation, continuously transcribed speech texts need to be segmented before being fed into subsequent machine translation systems (Takezawa et al., 1998; Nakamura, 2009). This is because current machine translation (MT) systems perform the translation at the sentence level, where various models used in MT are trained over segmented sentences and many algorithms inside MT have an exponential complexity with regard to the length of inputs.","The punctuation prediction problem has attracted research interest in both the speech processing community and the natural language processing community. Most previous work primarily exploits local features in their statistical models such as lexicons, prosodic cues and hidden event language model (HELM) (Liu et al., 2005; Matusov et al., 2006; Huang and Zweig, 2002; Stolcke and Shriberg, 1996). The word-level models integrating local features have narrow views about the input and could not achieve satisfied performance due to the limited context information access (Favre et al., 2008). Naturally, global contexts are required to model the punctuation prediction, especially for long-range dependencies. For instance, in English question sentences, the ending question mark is long-range dependent on the initial phrases (Lu and Ng, 2010), such as “could you” in Figure 1. There has been some work trying to incorporate syntactic features to broaden the view of hypotheses in the punctuation prediction models (Roark et al., 2006; Favre et al., 2008). In their methods, the punctuation prediction is treated as a separated post-procedure of parsing, which may suffer from the problem of error propagation. In addition, these approaches are not able to incrementally process inputs and are not efficient for very long inputs, especially in the cases of long transcribed speech texts from presentations where the number of streaming words could be larger than hundreds or thousands.","In this paper, we propose jointly performing punctuation prediction and transition-based dependency parsing over transcribed speech text. When the transition-based parsing consumes the stream of words left to right with the shift-reduce decoding algorithm, punctuation symbols are predicted for each word based on the contexts of the parsing tree. Two models are proposed to cause the punctuation prediction to interact with the transition actions in parsing. One is to conduct transition actions of parsing followed by punctuation predictions in a cascaded way. The other is to associate the conventional transition actions of parsing with punctuation perditions, so that predicted punctuations are directly inferred from the 752  (a). The transcribed speech text without punctuations      ","(b). Transition-based parsing trees and predicted punctuations over transcribed text "," (c). Two segmentations are formed when inserting the predicted punctuation symbols into the transcribed text Figure 1. An example of punctuation prediction. parsing tree. Our models have linear complexity and are capable of handling streams of words with any length. In addition, the computation of models use a rich set of syntactic features, which can improve the complicated punctuation predictions from a global view, especially for the long range dependencies.","Figure 1 shows an example of how parsing helps punctuation prediction over the transcribed speech text. As illustrated in Figure 1(b), two commas are predicted when their preceding words act as the adverbial modifiers (advmod) during parsing. The period after the word “menu” is predicted when the parsing of an adverbial clause modifier (advcl) is completed. The question mark at the end of the input is determined when a direct object modifier (dobj) is identified, together with the long range clue that the auxiliary word occurs before the nominal subject (nsubj). Eventually, two segmentations are formed according to the punctuation prediction results, shown in Figure 1(c).","The training data used for our models is adapted from Treebank data by excluding all punctuations but keeping the punctuation contexts, so that it can simulate the unavailable annotated transcribed speech texts. In decoding, beam search is used to get optimal punctuation prediction results. We conduct experiments on both IWSLT data and TDT4 test data sets. The experimental results show that our method can achieve higher performance than the CRF-based baseline method.","The paper is structured as follows: Section 2 conducts a survey of related work. The transition-based dependency parsing is introduced in Section 3. We explain our approach to predicting punctuations for transcribed speech texts in Section 4. Section 5 gives the results of our experiment. The conclusion and future work are given in Section 6."]},{"title":"2 Related Work","paragraphs":["Sentence boundary detection and punctuation prediction have been extensively studied in the speech processing field and have attracted research interest in the natural language processing field as well. Most previous work exploits local features for the task. Kim and Woodland (2001), Huang and Zweig (2002), Christensen et al. (2001), and Liu et al. (2005) integrate both prosodic features (pitch, pause duration, etc.) and lexical features (words, n-grams, etc.) to predict punctuation symbols during speech recognition, where Huang and Zweig (2002) uses a maximum entropy model, Christensen et al. (2001) focus on finite state and multi-layer perceptron methods, and Liu et al. (2005) uses conditional random fields. However, in some scenarios the prosodic cues are not available due to inaccessible original raw speech waveforms. Matusov et al. (2006) integrate segmentation features into the log-linear model in the statistical machine translation (SMT) framework to improve the translation performance when translating transcribed speech texts. Lu and Ng (2010) uses dynamic conditional random fields to perform both sentence boundary and sentence type prediction. They achieved promis-ing results on both English and Chinese transcribed speech texts. The above work only exanyway you may find your favorite if you go through the menu so could you tell me your choice","anyway you may find your favorite if you go through the menu so could you tell me your choice , N N N N N N N N N N . , N N N N N ? anyway, you may find your favorite if you go through the menu. so, could you tell me your choice? nsubj nsubj poss aux mark pobj iobj advmod advcl","nsubj dobj det poss aux prep advmod dobj 753 ploits local features, so they were limited to capturing long range dependencies for punctuation prediction.","It is natural to incorporate global knowledge, such as syntactic information, to improve punctuation prediction performance. Roark et al. (2006) use a rich set of non-local features including parser scores to re-rank full segmentations. Favre et al. (2008) integrate syntactic information from a PCFG parser into a log-linear and combine it with local features for sentence segmentation. The punctuation prediction in these works is performed as a post-procedure step of parsing, where a parse tree needs to be built in advance. As their parsing over the stream of words in transcribed speech text is exponentially complex, their approaches are only feasible for short input processing. Unlike these works, we incorporate punctuation prediction into the parsing which process left to right input without length limitations.","Numerous dependency parsing algorithms have been proposed in the natural language processing community, including transition-based and graph-based dependency parsing. Compared to graph-based parsing, transition-based parsing can offer linear time complexity and easily leverage non-local features in the models (Yamada and Matsumoto, 2003; Nivre et al., 2006b; Zhang and Clark, 2008; Huang and Sagae, 2010). Starting with the work from (Zhang and Nivre, 2011), in this paper we extend transition-based dependency parsing from the sentence-level to the stream of words and integrate the parsing with punctuation prediction.","Joint POS tagging and transition-based dependency parsing are studied in (Hatori et al., 2011; Bohnet and Nivre, 2012). The improve-ments are reported with the joint model compared to the pipeline model for Chinese and other richly inflected languages, which shows that it also makes sense to jointly perform punctuation prediction and parsing, although these two tasks of POS tagging and punctuation prediction are different in two ways: 1). The former usually works on a well-formed single sentence while the latter needs to process multiple sentences that are very lengthy. 2). POS tags are must-have features to parsing while punctuations are not. The parsing quality in the former is more sensitive to the performance of the entire task than in the latter."]},{"title":"3 Transition-based dependency parsing","paragraphs":["In a typical transition-based dependency parsing process, the shift-reduce decoding algorithm is applied and a queue and stack are maintained (Zhang and Nivre, 2011). The queue stores the stream of transcribed speech words, the front of which is indexed as the current word. The stack stores the unfinished words which may be linked with the current word or a future word in the queue. When words in the queue are consumed from left to right, a set of transition actions is applied to build a parse tree. There are four kinds of transition actions conducted in the parsing process (Zhang and Nivre, 2011), as described in Table 1. ","Action Description","Shift Fetches the current word from the queue and pushes it to the stack","Reduce Pops the stack","LeftArc Adds a dependency link from the current word to the stack top, and pops the stack","RightArc Adds a dependency link from the stack top to the current word, takes away the current word from the queue and pushes it to the stack Table 1. Action types in transition-based parsing","The choice of each transition action during the parsing is scored by a linear model that can be trained over a rich set of non-local features extracted from the contexts of the stack, the queue and the set of dependency labels. As described in (Zhang and Nivre, 2011), the feature templates could be defined over the lexicons, POS-tags and the combinations with syntactic information.","In parsing, beam search is performed to search the optimal sequence of transition actions, from which a parse tree is formed (Zhang and Clark, 2008). As each word must be pushed to the stack once and popped off once, the number of actions needed to parse a sentence is always 2n, where n is the length of the sentence. Thus, transition-based parsing has a linear complexity with the length of input and naturally it can be extended to process the stream of words."]},{"title":"4 Our method 4.1 Model","paragraphs":["In the task of punctuation prediction, we are given a stream of words from an automatic transcription of speech text, denoted by w1n",": = w1, w2, ... , wn. We are asked to output a sequence of punctuation symbols S1n",": = s1, s2, ... , sn where si is attached to wi to form a sentence like Figure 1(c). If there are no ambiguities, S1n","is also abbreviated as S, 754 similarly for w1n","as w. We model the search of the best sequence of predicted punctuation symbols S∗","as:  S∗","= argmax SP(S1n|w1n) (1)","","We introduce the transition-based parsing tree T to guide the punctuation prediction in Model (2), where parsing trees are constructed over the transcribed text while containing no punctuations.  S∗","= argmax S ∑ P(T|w1n) × P(S1n|T, w1n)T (2) ","Rather than enumerate all possible parsing trees, we jointly optimize the punctuation prediction model and the transition-based parsing model with the form: ","(S∗ , T∗) = argmax","(S,T)P(T|w1n) × P(S1n|T, w1n) (3)","","Let T1i be the constructed partial tree when w1i","","is consumed from the queue. We decompose the","Model (3) into:","","(S∗",", T∗) =","argmax(S,T) ∏ P(T1i","|T1i−1",", w1i",") × P(si|T1i",", w1i",")n","i=1","(4)","","It is noted that a partial parsing tree uniquely corresponds to a sequence of transition actions, and vice versa. Suppose T1i","corresponds to the action sequence A1i","and let ai denote the last action in A1i",". As the current word wi can only be consumed from the queue by either Shift or RightArc according to Table 1, we have ai ∈ {Shift, RightArc} . Thus, we synchronize the punctuation prediction with the application of Shift and RightArc during the parsing, which is explained by Model (5)."," (S∗",", T∗",") = argmax","(S,T) ∏ P(T1i",", A1i |T1i−1",", w1i",") n","i=1","× P(si|ai, T1i",", w1i",") (5)  The model is further refined by reducing the","computation scope. When a full-stop punctuation","is determined (i.e., a segmentation is formed), we","discard the previous contexts and restart a new  1 Specially, bi is equal to 1 if there are no previous full-stop punctuations. procedure for both parsing and punctuation prediction over the rest of words in the stream. In this way we are theoretically able to handle the unlimited stream of words without needing to always keep the entire context history of streaming words. Let bi be the position index of last full-stop punctuation1","before i, Tbii","and Abii","the partial tree and corresponding action sequence over the words wbii",", Model (5) can be rewritten by:  (S∗",", T∗",") = argmax(S,T) ∏ P(Tbii",", A","bii |Tbii−1",", wbii",") ×n","i=1","P(si|ai, Tbii , wbii",") (6)","","With different computation of Model (6), we induce two joint models for punctuation prediction: the cascaded punctuation prediction model and the unified punctuation prediction model."]},{"title":"4.2 Cascaded punctuation prediction model (CPP)","paragraphs":["In Model (6), the computation of two sub-models is independent. The first sub-model is computed based on the context of words and partial trees without any punctuation knowledge, while the computation of the second sub-model is conditional on the context from the partially built parsing tree Tbii","and the transition action. As the words in the stream are consumed, each computation of transition actions is followed by a computation of punctuation prediction. Thus, the two sub-models are computed in a cascaded way, until the optimal parsing tree and optimal punctuation symbols are generated. We call this model the cascaded punctuation prediction model (CPP)."]},{"title":"4.3 Unified punctuation prediction model (UPP)","paragraphs":["In Model (6), if the punctuation symbols can be deterministically inferred from the partial tree, P(si|ai, Tbii",", wbii",") can be omitted because it is always 1. Similar to the idea of joint POS tagging and parsing (Hatori et al., 2011; Bohnet and Nivre, 2012), we propose attaching the punctuation prediction onto the parsing tree by embedding si into ai . Thus, we extend the conventional transition actions illustrated in Table 1 to a new set of transition actions for the parsing, denoted by Â:  755","Â = {LeftArc, Reduce} ∪ {Shift(s)|s ∈ Q} ∪ {RightArc(s)|s ∈ Q}"," where Q is the set of punctuation symbols to be predicted, s is a punctuation symbol belonging to Q, Shift(s) is an action that attaches s to the current word on the basis of original Shift action in parsing, RightArc(s) attaches s to the current word on the basis of original RightArc action.","With the redefined transition action set Â, the computation of Model (6) is reformulated as:  (S∗",", T∗",") = argmax(S,T) ∏ P (Tbii",", Â bii","|Tbii−1 , Âbii−1",", wbii",")n","i=1 (7) ","Here, the computation of parsing tree and punctuation prediction is unified into one model where the sequence of transition action outputs uniquely determines the punctuations attached to the words. We refer to it as the unified punctuation prediction model (UPP).        ","(a). Parsing tree and attached punctuation symbols ","Shift(,), Shift(N), Shift(N), LeftArc, LeftArc, LeftArc,","Shift(N), RightArc(?), Reduce, Reduce ","(b). The corresponding sequence of transition actions Figure 2. An example of punctuation prediction using the UPP model, where N is a null type punctuation symbol denoting no need to attach any punctuation to the word.","Figure 2 illustrates an example how the UPP model works. Given an input “so could you tell me”, the optimal sequence of transition actions in Figure 2(b) is calculated based on the UPP model to produce the parsing tree in Figure 2(a). Accord-ing to the sequence of actions, we can determine the sequence of predicted punctuation symbols like “,NNN?” that have been attached to the words shown in Figure 2(a). The final segmentation with the predicted punctuation insertion could be “so, could you tell me?”."]},{"title":"4.4 Model training and decoding","paragraphs":["In practice, the sub-models in Model (6) and (7) with the form of P(Y|X) is computed with a linear model Score(Y, X) as  Score(Y, X) = Φ(Y, X) ∙ λ⃗  where Φ(Y, X) is the feature vector extracted from the output Y and the context X, and λ⃗ is the weight vector. For the features of the models, we incorporate the bag of words and POS tags as well as tree-based features shown in Table 2, which are the same as those defined in (Zhang and Nivre, 2011). ","(a) ws; w0; w1; w2; ps; p0; p1; p2; wsps; w0p0; w1p1; w2p2; wspsw0p0; wspsw0; wspsp0; wsw0p0; psw0p0; wsw0; psp0; p0p1; psp0p1; p0p1p2;","(b) pshpsp0; pspslp0; pspsrp0; psp0p0l; wsd; psd; w0d; p0d; wsw0d; psp0d; wsvl; psvl; wsvr; psvr; w0vl; p0vl; wsh; psh; ts; w0l; p0l; t0l; w0r; p0r; t0r; w1l; p1l; t1l; wsh2; psh2; tsh; wsl2; psl2; tsl2; wsr2; psr2; tsr2; w0l2; p0l2; t0l2; pspslpsl2; pspsrpsr2; pspshpsh2; p0p0lp0l2; wsTl; psTl; wsTr; psTr; w0Tl; p0Tl; Table 2. (a) Features of the bag of words and POS tags. (b). Tree-based features. wword; pPOS tag; ddistance between ws and w0; vnumber of modifiers; tdependency label; Tset of dependency labels; s, 0, 1 and 2 index the stack top and three front items in the queue respectively; hhead; lleft/leftmost; rright/rightmost; h2head of a head; l2second leftmost; r2second rightmost.","The training data for both the CPP and UPP models need to contain parsing trees and punctuation information. Due to the absence of annotation over transcribed speech data, we adapt the Treebank data for the purpose of model training. To do this, we remove all types of syntactic information related to punctuation symbols from the raw Treebank data, but record what punctuation symbols are attached to the words. We normalize various punctuation symbols into two types: Middle-paused punctuation (M) and Full-stop punctuation (F). Plus null type (N), there are three kinds of punctuation symbols attached to the words. Table 3 illustrates the normalizations of punctuation symbols. In the experiments, we did not further distinguish the type among full-stop punctuation because the question mark and the exclamation mark have very low frequency in Treebank data. so could you tell me , N N N ? nsubj iobj aux advmod 756 But our CPP and UPP models are both independent regarding the number of punctuation types to be predicted. ","Punctuations Normalization Period, question mark, exclamation mark","Full-stop punctuation (F)","Comma, Colon, semicolon","Middle-paused punctuation (M)","Multiple Punctuations (e.g., !!!!?)","Full-stop punctuation (F)","Quotations, brackets, etc. Null (N) Table 3. Punctuation normalization in training data As the feature templates are the same for the","model training of both CPP and UPP, the training","instances of CPP and UPP have the same contexts","but with different outputs. Similar to work in","(Zhang and Clark, 2008; Zhang and Nivre, 2011),","we train CPP and UPP by generalized perceptron","(Collins, 2002). In decoding, beam search is performed to get","the optimal sequence of transition actions in CPP","and UPP, and the optimal punctuation symbols in","CPP. To ensure each segment decided by a full-","stop punctuation corresponds to a single parsing","tree, two constraints are applied in decoding for","the pruning of deficient search paths. (1) Proceeding-constraint: If the partial pars-","ing result is not a single tree, the full-stop","punctuation prediction in CPP cannot be","performed. In UPP, if Shift(F) or","RightArc(F) fail to result in a single parsing","tree, they cannot be performed as well. (2) Succeeding-constraint: If the full-stop","punctuation is predicted in CPP, or Shift(F)","and RightArc(F) are performed in UPP, the","following transition actions must be a se-","quence of Reduce actions until the stack","becomes empty."]},{"title":"5 Experiments 5.1 Experimental setup","paragraphs":["Our training data of transition-based dependency trees are converted from phrasal structure trees in English Web Treebank (LDC2012T13) and the English portion of OntoNotes 4.0 (LDC2011T03) by the Stanford Conversion toolkit (Marneffe et al., 2006). It contains around 1.5M words in total and consist of various genres including weblogs, web texts, newsgroups, email, reviews, question-answer sessions, newswires, broadcast news and broadcast conversations. To simulate the transcribed speech text, all words in dependency trees are lowercased and punctuations are excluded before model training. In addition, every ten dependency trees are concatenated sequentially to simulate a parsing result of a stream of words in the model training.","There are two test data sets used in our experiments. One is the English corpus of the IWSLT09 evaluation campaign (Paul, 2009) that is the conversional speech text. The other is a subset of the TDT4 English data (LDC2005T16) which consists of 200 hours of closed-captioned broadcast news.","In the decoding, the beam size of both the transition-based parsing and punctuation prediction is set to 5. The part-of-speech tagger is our re-implementation of the work in (Collins, 2002).","The evaluation metrics of our experiments are precision (prec.), recall (rec.) and F1-measure (F1).","For the comparison, we also implement a baseline method based on the CRF model. It incorporates the features of bag of words and POS tags shown in Table 2(a), which are commonly used in previous related work."]},{"title":"5.2 Experimental results","paragraphs":["We test the performance of our method on both the correctly recognized texts and automatically recognized texts. The former data is used to evaluate the capability of punctuation prediction of our algorithm regardless of the noises from speech data, as our model training data come from formal text instead of transcribed speech data. The usage of the latter test data set aims to evaluate the effectiveness of our method in real applications where lots of substantial recognition errors could be contained. In addition, we also evaluate the quality of our transition-based parsing, as its performance could have a big influence on the quality of punctuation prediction.","5.2.1 Performance on correctly recognized text The evaluation of our method on correctly recognized text uses 10% of IWSLT09 training set, which consists of 19,972 sentences from BTEC (Basic Travel Expression Corpus) and 10,061 sentences from CT (Challenge Task). The average input length is about 10 words and each input contains 1.3 sentences on average. The evaluation results are presented in Table 4. 757 ","Measure Middle-Paused Full-stop Mixed Baseline (CRF) prec. 33.2% 81.5% 78.8% rec. 25.9% 83.8% 80.7% F1 29.1% 82.6% 79.8%  CPP prec. 51% 89% 89.6% rec. 50.3% 93.1% 92.7% F1 50.6% 91% 91.1% ","UPP  prec. 52.6% 93.2% 92% rec. 59.7% 91.3% 92.3% F1 55.9% 92.2% 92.2% Table 4. Punctuation prediction performance on correctly recognized text We achieved good performance on full-stop punctuation compared to the baseline, which shows our method can efficiently process sentence segmentation because each segment is decided by the structure of a single parsing tree. In addition, the global syntactic knowledge used in our work help capture long range dependencies of punctuations. The performance of middle-paused punctuation prediction is fairly low between all methods, which shows predicting middle-paused punctuations is a difficult task. This is because the usage of middle-paused punctuations is very flexible, especially in conversional data. The last column in Table 4 presents the performance of the pure segmentation task where the middle-paused and full-stop punctuations are mixed and not distinguished. The performance of our method is much higher than that of the baseline, which shows our method is good at segmentation. We also note that UPP yields slightly better performance than CPP on full-stop and mixed punctuation prediction, and much better performance on middle-paused punctuation prediction. This could be because the interaction of parsing and punctuation prediction is closer together in UPP than in CPP.","5.2.2 Performance on automatically recognized text Table 5 shows the experimental results of punctuation prediction on automatically recognized text from TDT4 data that is recognized using SRI’s English broadcast news ASR system where the word error rate is estimated to be 18%. As the annotation of middle-paused punctuations in TDT4 is not available, we can only evaluate the performance of full-stop punctuation prediction (i.e., detecting sentence boundaries). Thus, we merge every three sentences into one single input before performing full-stop prediction. The average input length is about 43 words. ","Measure Full-stop Baseline (CRF) prec. 37.7% rec. 60.7% F1 46.5%  CPP prec. 63% rec. 58.6% F1 60.2% ","UPP  prec. 73.9% rec. 51.6% F1 60.7% Table 5. Punctuation prediction performance on automatically recognized text","Generally, the performance shown in Table 5 is not as high as that in Table 4. This is because the speech recognition error from ASR systems degrades the capability of model prediction. Another reason might be that the domain and style of our training data mismatch those of TDT4 data. The baseline gets a little higher recall than our method, which shows the baseline method tends to make aggressive segmentation decisions. However, both precision and F1 score of our method are much higher than the baseline. CPP has higher recall than UPP, but with lower precision and F1 score. This is in line with Table 4, which consistently illustrates CPP can get higher recall on full-stop punctuation prediction for both correctly recognized and automatically recognized texts.","5.2.3 Performance of transition-based parsing","Performance of parsing affects the quality of punctuation prediction in our work. In this section, we separately evaluate the performance of our transition-based parser over various domains including the Wall Street Journal (WSJ), weblogs, newsgroups, answers, email messages and reviews. We divided annotated Treebank data into three data sets: 90% for model training, 5% for the development set and 5% for the test set. The accuracy of our POS-tagger achieves 96.71%. The beam size in the decoding of both our POS-tagging and parsing is set to 5. Table 6 presents the results of our experiments on the measures of UAS and LAS, where the overall accuracy is obtained from a general model which is trained over the combination of the training data from all domains. 758","We first evaluate the performance of our transition-based parsing over texts containing punctuations (TCP). The evaluation results show that our transition-based parser achieves state-of-the-art performance levels, referring to the best dependency parsing results reported in the shared task of SANCL 2012 workshop2",", although they cannot be compared directly due to the different training data and test data sets used in the experiments. Secondly, we evaluate our parsing model in CPP over the texts without punctuations (TOP). Surprisingly, the performance over TOP is better than that over TCP. The reason could be that we cleaned out data noises caused by punctuations when preparing TOP data. These results illustrate that the performance of transition-based parsing in our method does not degrade after being integrated with punctuation prediction. As a by-product of the punctuation prediction task, the outputs of parsing trees can benefit the subsequent text processing tasks. ","Data sets UAS LAS","","","Texts con-","taining punctuations (TCP) ","WSJ 92.6% 90.3% Weblogs 90.7% 88.2% Answers 89.4% 85.7%","Newsgroups 90.1% 87.6% Reviews 90.9% 88.4%","Email Messages 89.6% 87.1% Overall 90.5% 88%",""," Texts without punctuations (TOP)","WSJ 92.6% 91.1% Weblogs 92.5% 91.1% Answers 95% 94%","Newsgroups 92.6% 91.2% Reviews 92.6% 91.2%","Email Messages 92.9% 91.7% Overall 92.6% 91.2% Table 6. The performance of our transition-based parser on written texts. UAS=unlabeled attachment score; LAS=labeled attachment score"]},{"title":"6 Conclusion and Future Work","paragraphs":["In this paper, we proposed a novel method for punctuation prediction of transcribed speech texts. Our approach jointly performs parsing and punctuation prediction by integrating a rich set of syntactic features. It can not only yield parse trees, but also determine sentence boundaries and predict punctuation symbols from a global view of the in-  2 https://sites.google.com/site/sancl2012/home/shared-task/results puts. The proposed algorithm has linear complexity in the size of input, which can efficiently process the stream of words from a purely text processing perspective without the dependences on either the ASR systems or subsequent tasks. The experimental results show that our approach out-performs the CRF-based method on both the correctly recognized and automatically recognized texts. In addition, the performance of the parsing over the stream of transcribed words is state-of-the-art, which can benefit many subsequent text processing tasks. In future work, we will try our method on other languages such as Chinese and Japanese, where Treebank data is available. We would also like to test the MT performance over transcribed speech texts with punctuation symbols inserted based on our method proposed in this paper."]},{"title":"References","paragraphs":["B. Bohnet and J. Nivre. 2012. A transition-based system for joint part-of-speech tagging and labeled non-projective dependency parsing. In Proc. EMNLP-CoNLL 2012.","H. Christensen, Y. Gotoh, and S. Renals. 2001. Punctuation annotation using statistical prosody models. In Proc. of ISCA Workshop on Prosody in Speech Recognition and Understanding.","M. Collins. 2002. Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms. In Proc. EMNLP’02, pages 1-8.","B. Favre, R. Grishman, D. Hillard, H. Ji, D. Hakkani-Tur, and M. Ostendorf. 2008. Punctuating speech for information extraction. In Proc. of ICASSP’08.","B. Favre, D. HakkaniTur, S. Petrov and D. Klein. 2008. Efficient sentence segmentation using syntactic features. In Spoken Language Technologies (SLT).","A. Gravano, M. Jansche, and M. Bacchiani. 2009. Restoring punctuation and capitalization in transcribed speech. In Proc. of ICASSP’09.","J. Hatori, T. Matsuzaki, Y. Miyao and J. Tsujii. 2011. Incremental joint POS tagging and dependency parsing in Chinese. In Proc. Of IJCNLP’11.","J. Huang and G. Zweig. 2002. Maximum entropy model for punctuation annotation from speech. In Proc. Of ICSLP’02. 759","J.H. Kim and P.C. Woodland. 2001. The use of prosody in a combined system for punctuation genera-tion and speech recognition. In Proc. of EuroSpeech’01.","Y. Liu, A. Stolcke, E. Shriberg, and M. Harper. 2005. Using conditional random fields for sentence boundary detection in speech. In Proc. of ACL’05.","W. Lu and H.T. Ng. 2010. Better Punctuation Predic-tion with Dynamic Conditional Random Fields. In Proc. Of EMNLP’10. Pages 177-186.","M. Marneffe, B. MacCartney, C.D. Maning. 2006. Generating Typed Dependency Parses from Phrase Structure Parses. In Proc. LREC’06.","E. Matusov, A. Mauser, and H. Ney. 2006. Automatic sentence segmentation and punctuation prediction for spoken language translation. In Proc. of IWSLT’06.","S. Nakamura. 2009. Overcoming the language barrier with speech translation technology. In Science & Technology Trends - Quarterly Review. No. 31. April 2009.","J. Nivre. 2003. An efficient algorithm for projective dependency parsing. In Proceedings of IWPT, pages 149–160, Nancy, France.","J. Nivre and M. Scholz. 2004. Deterministic dependency parsing of English text. In Proc. COLING’04.","M. Paul. 2009. Overview of the IWSLT 2009 Evalua-tion Campaign. In Proceedings of IWSLT’09.","B. Roark, Y. Liu, M. Harper, R. Stewart, M. Lease, M. Snover, I. Shafran, B. Dorr, J. Hale, A. Krasnyanskaya, and L. Yung. 2006. Reranking for sentence boundary detection in conversational speech. In Proc. ICASSP, 2006.","A. Stolcke and E. Shriberg, “Automatic linguistic segmentation of conversational speech,” Proc. ICSLP, vol. 2, 1996.","A. Stolcke, E. Shriberg, R. Bates, M. Ostendorf, D. Hakkani, M. Plauche, G. Tur, and Y. Lu. 1998. Automatic detection of sentence boundaries and disfluencies based on recognized words. In Proc. of ICSLP’ 98.","Takezawa, T. Morimoto, T. Sagisaka, Y. Campbell, N. Iida, H. Sugaya, F. Yokoo, A. Yamamoto, Seiichi. 1998. A Japanese-to-English speech translation system: ATR-MATRIX. In Proc. ICSLP’98.","Y. Zhang and J. Nivre. 2011. Transition-based Dependency Parsing with Rich Non-local Features. In Proc. of ACL’11, pages 188-193.","Y. Zhang and S. Clark. A Tale of Two Parsers: investigating and combing graph-based and transition-based dependency parsing using beam-search. 2008. In Proc. of EMNLP’08, pages 562-571. 760"]}],"references":[{"authors":[{"first":"B.","last":"Bohnet"},{"first":"J.","last":"Nivre"}],"year":"2012","title":"A transition-based system for joint part-of-speech tagging and labeled non-projective dependency parsing","source":"B. Bohnet and J. Nivre. 2012. A transition-based system for joint part-of-speech tagging and labeled non-projective dependency parsing. In Proc. EMNLP-CoNLL 2012."},{"authors":[{"first":"H.","last":"Christensen"},{"first":"Y.","last":"Gotoh"},{"first":"S.","last":"Renals"}],"year":"2001","title":"Punctuation annotation using statistical prosody models","source":"H. Christensen, Y. Gotoh, and S. Renals. 2001. Punctuation annotation using statistical prosody models. In Proc. of ISCA Workshop on Prosody in Speech Recognition and Understanding."},{"authors":[{"first":"M.","last":"Collins"}],"year":"2002","title":"Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms","source":"M. Collins. 2002. Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms. In Proc. EMNLP’02, pages 1-8."},{"authors":[{"first":"B.","last":"Favre"},{"first":"R.","last":"Grishman"},{"first":"D.","last":"Hillard"},{"first":"H.","last":"Ji"},{"first":"D.","last":"Hakkani-Tur"},{"first":"M.","last":"Ostendorf"}],"year":"2008","title":"Punctuating speech for information extraction","source":"B. Favre, R. Grishman, D. Hillard, H. Ji, D. Hakkani-Tur, and M. Ostendorf. 2008. Punctuating speech for information extraction. In Proc. of ICASSP’08."},{"authors":[{"first":"B.","last":"Favre"},{"first":"D.","last":"HakkaniTur"},{"first":"S.","last":"Petrov"},{"first":"D.","last":"Klein"}],"year":"2008","title":"Efficient sentence segmentation using syntactic features","source":"B. Favre, D. HakkaniTur, S. Petrov and D. Klein. 2008. Efficient sentence segmentation using syntactic features. In Spoken Language Technologies (SLT)."},{"authors":[{"first":"A.","last":"Gravano"},{"first":"M.","last":"Jansche"},{"first":"M.","last":"Bacchiani"}],"year":"2009","title":"Restoring punctuation and capitalization in transcribed speech","source":"A. Gravano, M. Jansche, and M. Bacchiani. 2009. Restoring punctuation and capitalization in transcribed speech. In Proc. of ICASSP’09."},{"authors":[{"first":"J.","last":"Hatori"},{"first":"T.","last":"Matsuzaki"},{"first":"Y.","last":"Miyao"},{"first":"J.","last":"Tsujii"}],"year":"2011","title":"Incremental joint POS tagging and dependency parsing in Chinese","source":"J. Hatori, T. Matsuzaki, Y. Miyao and J. Tsujii. 2011. Incremental joint POS tagging and dependency parsing in Chinese. In Proc. Of IJCNLP’11."},{"authors":[{"first":"J.","last":"Huang"},{"first":"G.","last":"Zweig"}],"year":"2002","title":"Maximum entropy model for punctuation annotation from speech","source":"J. Huang and G. Zweig. 2002. Maximum entropy model for punctuation annotation from speech. In Proc. Of ICSLP’02. 759"},{"authors":[{"first":"J.","middle":"H.","last":"Kim"},{"first":"P.","middle":"C.","last":"Woodland"}],"year":"2001","title":"The use of prosody in a combined system for punctuation genera-tion and speech recognition","source":"J.H. Kim and P.C. Woodland. 2001. The use of prosody in a combined system for punctuation genera-tion and speech recognition. In Proc. of EuroSpeech’01."},{"authors":[{"first":"Y.","last":"Liu"},{"first":"A.","last":"Stolcke"},{"first":"E.","last":"Shriberg"},{"first":"M.","last":"Harper"}],"year":"2005","title":"Using conditional random fields for sentence boundary detection in speech","source":"Y. Liu, A. Stolcke, E. Shriberg, and M. Harper. 2005. Using conditional random fields for sentence boundary detection in speech. In Proc. of ACL’05."},{"authors":[{"first":"W.","last":"Lu"},{"first":"H.","middle":"T.","last":"Ng"}],"year":"2010","title":"Better Punctuation Predic-tion with Dynamic Conditional Random Fields","source":"W. Lu and H.T. Ng. 2010. Better Punctuation Predic-tion with Dynamic Conditional Random Fields. In Proc. Of EMNLP’10. Pages 177-186."},{"authors":[{"first":"M.","last":"Marneffe"},{"first":"B.","last":"MacCartney"},{"first":"C.","middle":"D.","last":"Maning"}],"year":"2006","title":"Generating Typed Dependency Parses from Phrase Structure Parses","source":"M. Marneffe, B. MacCartney, C.D. Maning. 2006. Generating Typed Dependency Parses from Phrase Structure Parses. In Proc. LREC’06."},{"authors":[{"first":"E.","last":"Matusov"},{"first":"A.","last":"Mauser"},{"first":"H.","last":"Ney"}],"year":"2006","title":"Automatic sentence segmentation and punctuation prediction for spoken language translation","source":"E. Matusov, A. Mauser, and H. Ney. 2006. Automatic sentence segmentation and punctuation prediction for spoken language translation. In Proc. of IWSLT’06."},{"authors":[{"first":"S.","last":"Nakamura"}],"year":"2009","title":"Overcoming the language barrier with speech translation technology","source":"S. Nakamura. 2009. Overcoming the language barrier with speech translation technology. In Science & Technology Trends - Quarterly Review. No. 31. April 2009."},{"authors":[{"first":"J.","last":"Nivre"}],"year":"2003","title":"An efficient algorithm for projective dependency parsing","source":"J. Nivre. 2003. An efficient algorithm for projective dependency parsing. In Proceedings of IWPT, pages 149–160, Nancy, France."},{"authors":[{"first":"J.","last":"Nivre"},{"first":"M.","last":"Scholz"}],"year":"2004","title":"Deterministic dependency parsing of English text","source":"J. Nivre and M. Scholz. 2004. Deterministic dependency parsing of English text. In Proc. COLING’04."},{"authors":[{"first":"M.","last":"Paul"}],"year":"2009","title":"Overview of the IWSLT 2009 Evalua-tion Campaign","source":"M. Paul. 2009. Overview of the IWSLT 2009 Evalua-tion Campaign. In Proceedings of IWSLT’09."},{"authors":[{"first":"B.","last":"Roark"},{"first":"Y.","last":"Liu"},{"first":"M.","last":"Harper"},{"first":"R.","last":"Stewart"},{"first":"M.","last":"Lease"},{"first":"M.","last":"Snover"},{"first":"I.","last":"Shafran"},{"first":"B.","last":"Dorr"},{"first":"J.","last":"Hale"},{"first":"A.","last":"Krasnyanskaya"},{"first":"L.","last":"Yung"}],"year":"2006","title":"Reranking for sentence boundary detection in conversational speech","source":"B. Roark, Y. Liu, M. Harper, R. Stewart, M. Lease, M. Snover, I. Shafran, B. Dorr, J. Hale, A. Krasnyanskaya, and L. Yung. 2006. Reranking for sentence boundary detection in conversational speech. In Proc. ICASSP, 2006."},{"authors":[],"source":"A. Stolcke and E. Shriberg, “Automatic linguistic segmentation of conversational speech,” Proc. ICSLP, vol. 2, 1996."},{"authors":[{"first":"A.","last":"Stolcke"},{"first":"E.","last":"Shriberg"},{"first":"R.","last":"Bates"},{"first":"M.","last":"Ostendorf"},{"first":"D.","last":"Hakkani"},{"first":"M.","last":"Plauche"},{"first":"G.","last":"Tur"},{"first":"Y.","last":"Lu"}],"year":"1998","title":"Automatic detection of sentence boundaries and disfluencies based on recognized words","source":"A. Stolcke, E. Shriberg, R. Bates, M. Ostendorf, D. Hakkani, M. Plauche, G. Tur, and Y. Lu. 1998. Automatic detection of sentence boundaries and disfluencies based on recognized words. In Proc. of ICSLP’ 98."},{"authors":[{"first":"T.","last":"Takezawa"},{"first":"T.","last":"Morimoto"},{"first":"Y.","last":"Sagisaka"},{"first":"N.","last":"Campbell"},{"first":"H.","last":"Iida"},{"first":"F.","last":"Sugaya"},{"first":"A.","last":"Yokoo"},{"last":"Yamamoto"},{"last":"Seiichi"}],"year":"1998","title":"A Japanese-to-English speech translation system: ATR-MATRIX","source":"Takezawa, T. Morimoto, T. Sagisaka, Y. Campbell, N. Iida, H. Sugaya, F. Yokoo, A. Yamamoto, Seiichi. 1998. A Japanese-to-English speech translation system: ATR-MATRIX. In Proc. ICSLP’98."},{"authors":[{"first":"Y.","last":"Zhang"},{"first":"J.","last":"Nivre"}],"year":"2011","title":"Transition-based Dependency Parsing with Rich Non-local Features","source":"Y. Zhang and J. Nivre. 2011. Transition-based Dependency Parsing with Rich Non-local Features. In Proc. of ACL’11, pages 188-193."},{"authors":[{"first":"Y.","last":"Zhang"},{"first":"S.","middle":"Clark. A Tale of Two Parsers:","last":"investigating"},{"first":"combing","last":"graph-based"},{"first":"transition-based","middle":"dependency parsing using","last":"beam-search"}],"year":"2008","title":"In Proc","source":"Y. Zhang and S. Clark. A Tale of Two Parsers: investigating and combing graph-based and transition-based dependency parsing using beam-search. 2008. In Proc. of EMNLP’08, pages 562-571. 760"}],"cites":[{"style":0,"text":"Takezawa et al., 1998","origin":{"pointer":"/sections/5/paragraphs/0","offset":651,"length":21},"authors":[{"last":"Takezawa"},{"last":"al."}],"year":"1998","references":["/references/20"]},{"style":0,"text":"Nakamura, 2009","origin":{"pointer":"/sections/5/paragraphs/0","offset":674,"length":14},"authors":[{"last":"Nakamura"}],"year":"2009","references":["/references/13"]},{"style":0,"text":"Liu et al., 2005","origin":{"pointer":"/sections/5/paragraphs/1","offset":307,"length":16},"authors":[{"last":"Liu"},{"last":"al."}],"year":"2005","references":["/references/9"]},{"style":0,"text":"Matusov et al., 2006","origin":{"pointer":"/sections/5/paragraphs/1","offset":325,"length":20},"authors":[{"last":"Matusov"},{"last":"al."}],"year":"2006","references":["/references/12"]},{"style":0,"text":"Huang and Zweig, 2002","origin":{"pointer":"/sections/5/paragraphs/1","offset":347,"length":21},"authors":[{"last":"Huang"},{"last":"Zweig"}],"year":"2002","references":["/references/7"]},{"style":0,"text":"Stolcke and Shriberg, 1996","origin":{"pointer":"/sections/5/paragraphs/1","offset":370,"length":26},"authors":[{"last":"Stolcke"},{"last":"Shriberg"}],"year":"1996","references":[]},{"style":0,"text":"Favre et al., 2008","origin":{"pointer":"/sections/5/paragraphs/1","offset":573,"length":18},"authors":[{"last":"Favre"},{"last":"al."}],"year":"2008","references":["/references/3","/references/4"]},{"style":0,"text":"Lu and Ng, 2010","origin":{"pointer":"/sections/5/paragraphs/1","offset":829,"length":15},"authors":[{"last":"Lu"},{"last":"Ng"}],"year":"2010","references":["/references/10"]},{"style":0,"text":"Roark et al., 2006","origin":{"pointer":"/sections/5/paragraphs/1","offset":1018,"length":18},"authors":[{"last":"Roark"},{"last":"al."}],"year":"2006","references":["/references/17"]},{"style":0,"text":"Favre et al., 2008","origin":{"pointer":"/sections/5/paragraphs/1","offset":1038,"length":18},"authors":[{"last":"Favre"},{"last":"al."}],"year":"2008","references":["/references/3","/references/4"]},{"style":0,"text":"Kim and Woodland (2001)","origin":{"pointer":"/sections/6/paragraphs/0","offset":260,"length":23},"authors":[{"last":"Kim"},{"last":"Woodland"}],"year":"2001","references":["/references/8"]},{"style":0,"text":"Huang and Zweig (2002)","origin":{"pointer":"/sections/6/paragraphs/0","offset":285,"length":22},"authors":[{"last":"Huang"},{"last":"Zweig"}],"year":"2002","references":["/references/7"]},{"style":0,"text":"Christensen et al. (2001)","origin":{"pointer":"/sections/6/paragraphs/0","offset":309,"length":25},"authors":[{"last":"Christensen"},{"last":"al."}],"year":"2001","references":["/references/1"]},{"style":0,"text":"Liu et al. (2005)","origin":{"pointer":"/sections/6/paragraphs/0","offset":340,"length":17},"authors":[{"last":"Liu"},{"last":"al."}],"year":"2005","references":["/references/9"]},{"style":0,"text":"Huang and Zweig (2002)","origin":{"pointer":"/sections/6/paragraphs/0","offset":529,"length":22},"authors":[{"last":"Huang"},{"last":"Zweig"}],"year":"2002","references":["/references/7"]},{"style":0,"text":"Christensen et al. (2001)","origin":{"pointer":"/sections/6/paragraphs/0","offset":582,"length":25},"authors":[{"last":"Christensen"},{"last":"al."}],"year":"2001","references":["/references/1"]},{"style":0,"text":"Liu et al. (2005)","origin":{"pointer":"/sections/6/paragraphs/0","offset":670,"length":17},"authors":[{"last":"Liu"},{"last":"al."}],"year":"2005","references":["/references/9"]},{"style":0,"text":"Matusov et al. (2006)","origin":{"pointer":"/sections/6/paragraphs/0","offset":834,"length":21},"authors":[{"last":"Matusov"},{"last":"al."}],"year":"2006","references":["/references/12"]},{"style":0,"text":"Lu and Ng (2010)","origin":{"pointer":"/sections/6/paragraphs/0","offset":1051,"length":16},"authors":[{"last":"Lu"},{"last":"Ng"}],"year":"2010","references":["/references/10"]},{"style":0,"text":"Roark et al. (2006)","origin":{"pointer":"/sections/6/paragraphs/3","offset":125,"length":19},"authors":[{"last":"Roark"},{"last":"al."}],"year":"2006","references":["/references/17"]},{"style":0,"text":"Favre et al. (2008)","origin":{"pointer":"/sections/6/paragraphs/3","offset":237,"length":19},"authors":[{"last":"Favre"},{"last":"al."}],"year":"2008","references":["/references/3","/references/4"]},{"style":0,"text":"Yamada and Matsumoto, 2003","origin":{"pointer":"/sections/6/paragraphs/4","offset":312,"length":26},"authors":[{"last":"Yamada"},{"last":"Matsumoto"}],"year":"2003","references":[]},{"style":0,"text":"Nivre et al., 2006b","origin":{"pointer":"/sections/6/paragraphs/4","offset":340,"length":19},"authors":[{"last":"Nivre"},{"last":"al."}],"year":"2006b","references":[]},{"style":0,"text":"Zhang and Clark, 2008","origin":{"pointer":"/sections/6/paragraphs/4","offset":361,"length":21},"authors":[{"last":"Zhang"},{"last":"Clark"}],"year":"2008","references":[]},{"style":0,"text":"Huang and Sagae, 2010","origin":{"pointer":"/sections/6/paragraphs/4","offset":384,"length":21},"authors":[{"last":"Huang"},{"last":"Sagae"}],"year":"2010","references":[]},{"style":0,"text":"Zhang and Nivre, 2011","origin":{"pointer":"/sections/6/paragraphs/4","offset":437,"length":21},"authors":[{"last":"Zhang"},{"last":"Nivre"}],"year":"2011","references":["/references/21"]},{"style":0,"text":"Hatori et al., 2011","origin":{"pointer":"/sections/6/paragraphs/5","offset":74,"length":19},"authors":[{"last":"Hatori"},{"last":"al."}],"year":"2011","references":["/references/6"]},{"style":0,"text":"Bohnet and Nivre, 2012","origin":{"pointer":"/sections/6/paragraphs/5","offset":95,"length":22},"authors":[{"last":"Bohnet"},{"last":"Nivre"}],"year":"2012","references":["/references/0"]},{"style":0,"text":"Zhang and Nivre, 2011","origin":{"pointer":"/sections/7/paragraphs/0","offset":143,"length":21},"authors":[{"last":"Zhang"},{"last":"Nivre"}],"year":"2011","references":["/references/21"]},{"style":0,"text":"Zhang and Nivre, 2011","origin":{"pointer":"/sections/7/paragraphs/0","offset":582,"length":21},"authors":[{"last":"Zhang"},{"last":"Nivre"}],"year":"2011","references":["/references/21"]},{"style":0,"text":"Zhang and Nivre, 2011","origin":{"pointer":"/sections/7/paragraphs/6","offset":245,"length":21},"authors":[{"last":"Zhang"},{"last":"Nivre"}],"year":"2011","references":["/references/21"]},{"style":0,"text":"Zhang and Clark, 2008","origin":{"pointer":"/sections/7/paragraphs/7","offset":126,"length":21},"authors":[{"last":"Zhang"},{"last":"Clark"}],"year":"2008","references":[]},{"style":0,"text":"Hatori et al., 2011","origin":{"pointer":"/sections/10/paragraphs/2","offset":95,"length":19},"authors":[{"last":"Hatori"},{"last":"al."}],"year":"2011","references":["/references/6"]},{"style":0,"text":"Bohnet and Nivre, 2012","origin":{"pointer":"/sections/10/paragraphs/2","offset":116,"length":22},"authors":[{"last":"Bohnet"},{"last":"Nivre"}],"year":"2012","references":["/references/0"]},{"style":0,"text":"Zhang and Nivre, 2011","origin":{"pointer":"/sections/11/paragraphs/0","offset":427,"length":21},"authors":[{"last":"Zhang"},{"last":"Nivre"}],"year":"2011","references":["/references/21"]},{"style":0,"text":"Zhang and Clark, 2008","origin":{"pointer":"/sections/11/paragraphs/14","offset":1,"length":21},"authors":[{"last":"Zhang"},{"last":"Clark"}],"year":"2008","references":[]},{"style":0,"text":"Zhang and Nivre, 2011","origin":{"pointer":"/sections/11/paragraphs/14","offset":24,"length":21},"authors":[{"last":"Zhang"},{"last":"Nivre"}],"year":"2011","references":["/references/21"]},{"style":0,"text":"Collins, 2002","origin":{"pointer":"/sections/11/paragraphs/16","offset":1,"length":13},"authors":[{"last":"Collins"}],"year":"2002","references":["/references/2"]},{"style":0,"text":"Marneffe et al., 2006","origin":{"pointer":"/sections/12/paragraphs/0","offset":225,"length":21},"authors":[{"last":"Marneffe"},{"last":"al."}],"year":"2006","references":["/references/11"]},{"style":0,"text":"Paul, 2009","origin":{"pointer":"/sections/12/paragraphs/1","offset":116,"length":10},"authors":[{"last":"Paul"}],"year":"2009","references":["/references/16"]},{"style":0,"text":"Collins, 2002","origin":{"pointer":"/sections/12/paragraphs/2","offset":175,"length":13},"authors":[{"last":"Collins"}],"year":"2002","references":["/references/2"]}]}
