{"sections":[{"title":"","paragraphs":["Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1073–1082, Sofia, Bulgaria, August 4-9 2013. c⃝2013 Association for Computational Linguistics"]},{"title":"Joint Word Alignment and Bilingual Named Entity Recognition Using Dual Decomposition Mengqiu Wang Stanford University Stanford, CA 94305 mengqiu@cs.stanford.edu Wanxiang Che Harbin Institute of Technology Harbin, China, 150001 car@ir.hit.edu.cn Christopher D. Manning Stanford University Stanford, CA 94305 manning@cs.stanford.edu Abstract","paragraphs":["Translated bi-texts contain complementary language cues, and previous work on Named Entity Recognition (NER) has demonstrated improvements in performance over monolingual taggers by promoting agreement of tagging decisions between the two languages. However, most previous approaches to bilingual tagging assume word alignments are given as fixed input, which can cause cascading errors. We observe that NER label information can be used to correct alignment mistakes, and present a graphical model that performs bilingual NER tagging jointly with word alignment, by combining two monolingual tagging models with two uni-directional alignment models. We introduce additional cross-lingual edge factors that encourage agreements between tagging and alignment decisions. We design a dual decomposition inference algorithm to perform joint decoding over the combined alignment and NER output space. Experiments on the OntoNotes dataset demonstrate that our method yields significant improvements in both NER and word alignment over state-of-the-art monolingual baselines."]},{"title":"1 Introduction","paragraphs":["We study the problem of Named Entity Recognition (NER) in a bilingual context, where the goal is to annotate parallel bi-texts with named entity tags. This is a particularly important problem for machine translation (MT) since entities such as person names, locations, organizations, etc. carry much of the information expressed in the source sentence. Recognizing them provides useful information for phrase detection and word sense disambiguation (e.g., “melody” as in a female name has a different translation from the word “melody” in a musical sense), and can be directly leveraged to improve translation quality (Babych and Hartley, 2003). We can also automatically construct a named entity translation lexicon by annotating and extracting entities from bi-texts, and use it to improve MT performance (Huang and Vogel, 2002; Al-Onaizan and Knight, 2002). Previous work such as Burkett et al. (2010b), Li et al. (2012) and Kim et al. (2012) have also demonstrated that bi-texts annotated with NER tags can provide useful additional training sources for improving the performance of standalone monolingual taggers.","Because human translation in general preserves semantic equivalence, bi-texts represent two perspectives on the same semantic content (Burkett et al., 2010b). As a result, we can find complementary cues in the two languages that help to disambiguate named entity mentions (Brown et al., 1991). For example, the English word “Jordan” can be either a last name or a country. Without sufficient context it can be difficult to distinguish the two; however, in Chinese, these two senses are disambiguated: “ 乔丹” as a last name, and “ 约旦” as a country name.","In this work, we first develop a bilingual NER model (denoted as BI-NER) by embedding two monolingual CRF-based NER models into a larger undirected graphical model, and introduce additional edge factors based on word alignment (WA). Because the new bilingual model contains many cyclic cliques, exact inference is intractable. We employ a dual decomposition (DD) inference algorithm (Bertsekas, 1999; Rush et al., 2010) for performing approximate inference. Unlike most 1073 f1 f2 f3 f4 f5 f6 e1 e2 e3 e4 e5 e6 Xinhua News Agency Beijing Feb 16 B-ORG I-ORG I-ORG [O] B-LOC O O 新华社 , 北京 , 二月 十六 B-ORG O B-GPE O O O Figure 1: Example of NER labels between two word-aligned bilingual parallel sentences. The [O] tag is an example of a wrong tag assignment. The dashed alignment link between e3 and f2 is an example of alignment error. previous applications of the DD method in NLP, where the model typically factors over two components and agreement is to be sought between the two (Rush et al., 2010; Koo et al., 2010; DeNero and Macherey, 2011; Chieu and Teow, 2012), our method decomposes the larger graphical model into many overlapping components where each alignment edge forms a separate factor. We design clique potentials over the alignment-based edges to encourage entity tag agreements. Our method does not require any manual annotation of word alignments or named entities over the bilingual training data.","The aforementioned BI-NER model assumes fixed alignment input given by an underlying word aligner. But the entity span and type predictions given by the NER models contain complementary information for correcting alignment errors. To capture this source of information, we present a novel extension that combines the BI-NER model with two uni-directional HMM-based alignment models, and perform joint decoding of NER and word alignments. The new model (denoted as BI-NER-WA) factors over five components: one NER model and one word alignment model for each language, plus a joint NER-alignment model which not only enforces NER label agreements but also facilitates message passing among the other four components. An extended DD decoding algorithm is again employed to perform approximate inference.","We give a formal definition of the Bi-NER model in Section 2, and then move to present the Bi-NER-WA model in Section 3."]},{"title":"2 Bilingual NER by Agreement","paragraphs":["The inputs to our models are parallel sentence pairs (see Figure 1 for an example in English and Chinese). We denote the sentences as e (for English) and f (for Chinese). We assume access to two monolingual linear-chain CRF-based NER models that are already trained. The English-side CRF model assigns the following probability for a tag sequence ye",":","PCRFe (ye |e) = ∏ vi∈Ve ψ(vi) ∏ (vi,vj)∈De ω(vi, vj)","Ze (e)","where Ve","is the set of vertices in the CRF and","De","is the set of edges. ψ(v","i) and ω(vi, vj) are the node and edge clique potentials, and Ze","(e) is the partition function for input sequence e under the English CRF model. We let k(ye",") be the un-normalized log-probability of tag sequence ye",", defined as:","k(ye ) = log   ∏ vi∈Ve ψ(vi) ∏ (vi,vj)∈De ω(vi, vj)   Similarly, we define model PCRFf and un-normalized log-probability l(yf",") for Chinese.","We also assume that a set of word alignments (A = {(i, j) : ei ↔ fj}) is given by a word aligner and remain fixed in our model.","For clarity, we assume ye","and yf","are binary variables in the description of our algorithms. The extension to the multi-class case is straight-forward and does not affect the core algorithms. 2.1 Hard Agreement We define a BI-NER model which imposes hard agreement of entity labels over aligned word pairs. At inference time, we solve the following opti-1074 mization problem: max ye",",yf","log (PCRFe (ye )) + log (","PCRFf ( yf))","= max ye",",yf","k(ye ) + l(yf",") − log Z e(e) − log Zf (f )","≃ max ye",",yf","k(ye ) + l(yf",")","∋ ye i = y f j ∀(i, j) ∈ A We dropped the Ze(e) and Zf(f ) terms because","they remain constant at inference time. The Lagrangian relaxation of this term is: L","(","ye",", yf , U)","=","k (ye ) + l (","yf)","+ ∑ (i,j)∈A u(i, j) ( ye i − y f j ) where u(i, j) are the Lagrangian multipliers.","Instead of solving the Lagrangian directly, we can form the dual of this problem and solve it using dual decomposition (Rush et al., 2010): min U ( max ye ","k (ye ) + ∑ (i,j)∈A","u(i, j)ye i  ","+ max yf  l ( yf)","− ∑ (i,j)∈A u(i, j)y f j   )","Similar to previous work, we solve this DD problem by iteratively updating the sub-gradient as depicted in Algorithm 1. T is the maximum number of iterations before early stopping, and αt is the learning rate at time t. We adopt a learning rate update rule from Koo et al. (2010) where αt is defined as 1","N , where N is the number of times we observed a consecutive dual value increase from iteration 1 to t.","A thorough introduction to the theoretical foundations of dual decomposition algorithms is be-yond the scope of this paper; we encourage unfamiliar readers to read Rush and Collins (2012) for a full tutorial. 2.2 Soft Agreement The previously discussed hard agreement model rests on the core assumption that aligned words must have identical entity tags. In reality, however, this assumption does not always hold. Firstly, as-suming words are correctly aligned, their entity tags may not agree due to inconsistency in annotation standards. In Figure 1, for example, the","Algorithm 1 DD inference algorithm for hard","agreement model. ∀(i, j) ∈ A : u(i, j) = 0 for t ← 1 to T do ye∗","← argmax k (ye",") + ∑","(i,j)∈A","u(i, j)ye i","yf∗ ← argmax l (","yf) − ∑ (i,j)∈A","u(i, j)yf j","if ∀(i, j) ∈ A : ye∗ i = yf∗","j then return ( ye∗",", yf∗) end if for all (i, j) ∈ A do","u(i, j) ← u(i, j) + αt ( yf∗ j − ye∗","i )","end for end for return ( ye∗ (T), yf∗","(T) ) word “Beijing” can be either a Geo-Political Entity (GPE) or a location. The Chinese annotation standard may enforce that “Beijing” should always be tagged as GPE when it is mentioned in isola-tion, while the English standard may require the annotator to judge based on word usage context. The assumption in the hard agreement model can also be violated if there are word alignment errors.","In order to model this uncertainty, we extend the two previously independent CRF models into a larger undirected graphical model, by introducing a cross-lingual edge factor φ(i, j) for every pair of word positions (i, j) ∈ A. We associate a clique potential function h(i,j)(ye","i , y f j) for φ(i, j): h(i,j) ( ye i , y f j ) = pmi ( ye i , y f j )P̂ (ei,fj)","where pmi(ye i , y","f","j) is the point-wise mutual information (PMI) of the tag pair, and we raise it to the power of a posterior alignment probability P̂ (ei, fj). For a pair of NEs that are aligned with low probability, we cannot be too sure about the association of the two NEs, therefore the model should not impose too much influence from the bilingual agreement model; instead, we will let the monolingual NE models make their decisions, and trust that those are the best estimates we can come up with when we do not have much confidence in their bilingual association. The use of the posterior alignment probability facilitates this purpose.","Initially, each of the cross-lingual edge factors will attempt to assign a pair of tags that has the highest PMI score, but if the monolingual taggers do not agree, a penalty will start accumulating over this pair, until some other pair that agrees better with the monolingual models takes the top spot. 1075 Simultaneously, the monolingual models will also be encouraged to agree with the cross-lingual edge factors. This way, the various components effectively trade penalties indirectly through the cross-lingual edges, until a tag sequence that maximizes the joint probability is achieved.","Since we assume no bilingually annotated NER corpus is available, in order to get an estimate of the PMI scores, we first tag a collection of unannotated bilingual sentence pairs using the monolingual CRF taggers, and collect counts of aligned entity pairs from this auto-generated tagged data.","Each of the φ(i, j) edge factors (e.g., the edge between node f3 and e4 in Figure 1) overlaps with each of the two CRF models over one vertex (e.g., f3 on Chinese side and e4 on English side), and we seek agreement with the Chinese CRF model over tag assignment of fj, and similarly for ei on English side. In other words, no direct agreement between the two CRF models is enforced, but they both need to agree with the bilingual edge factors. The updated optimization problem becomes: max","ye(k)","yf(l)","ye(h)","yf(h) k ( ye(k)) + l ( yf(l))","+ ∑ (i,j)∈A h(i,j) ( ye(h) i , y f(h) j ) ∋ ∀(i, j) ∈ A :","( ye(k) i = ye(h)","i ) ∧ ( y f(l) j = y f(h) j )","where the notation ye(k)","i denotes tag assignment to","word ei by the English CRF and ye(h)","i denotes assignment to word ei by the bilingual factor; y f(l) j","denotes the tag assignment to word fj by the Chinese CRF and y f(h) j denotes assignment to word","fj by the bilingual factor.","The updated DD algorithm is illustrated in Algorithm 2 (case 2). We introduce two separate sets of dual constraints we","and wf",", which range over the set of vertices on their respective half of the graph. Decoding the edge factor model h(i,j)(ye","i , y","f","j) simply involves finding the pair of","tag assignments that gives the highest PMI score,","subject to the dual constraints.","The way DD algorithms work in decomposing undirected graphical models is analogous to other message passing algorithms such as loopy belief propagation, but DD gives a stronger optimality guarantee upon convergence (Rush et al., 2010)."]},{"title":"3 Joint Alignment and NER Decoding","paragraphs":["In this section we develop an extended model in which NER information can in turn be used to improve alignment accuracy. Although we have seen more than a handful of recent papers that apply the dual decomposition method for joint inference problems, all of the past work deals with cases where the various model components have the same inference output space (e.g., dependency parsing (Koo et al., 2010), POS tagging (Rush et al., 2012), etc.). In our case the output space is the much more complex joint alignment and NER tagging space. We propose a novel dual decomposition variant for performing inference over this joint space.","Most commonly used alignment models, such as the IBM models and HMM-based aligner are unsupervised learners, and can only capture simple distortion features and lexical translational features due to the high complexity of the structure prediction space. On the other hand, the CRF-based NER models are trained on manually annotated data, and admit richer sequence and lexical features. The entity label predictions made by the NER model can potentially be leveraged to correct alignment mistakes. For example, in Figure 1, if the tagger knows that the word “Agency” is tagged I-ORG, and if it also knows that the first comma in the Chinese sentence is not part of any entity, then we can infer it is very unlikely that there exists an alignment link between “Agency” and the comma.","To capture this intuition, we extend the BI-NER model to jointly perform word alignment and NER decoding, and call the resulting model BI-NER-WA. As a first step, instead of taking the output from an aligner as fixed input, we incorporate two uni-directional aligners into our model. We name the Chinese-to-English aligner model as m(Be",") and the reverse directional model n(Bf","). Be","is a matrix that holds the output of the Chinese-to-English aligner. Each be","(i, j) binary variable in Be","indicates whether fj is aligned to ei; similarly we define output matrix Bf","and bf","(i, j) for Chinese. In our experiments, we used two HMM-based alignment models. But in principle we can adopt any alignment model as long as we can perform efficient inference over it.","We introduce a cross-lingual edge factor ζ(i, j) in the undirected graphical model for every pair of word indices (i, j), which predicts a binary vari-1076 Algorithm 2 DD inference algorithm for joint alignment and NER model. A line marked with (2) means it applies to the BI-NER model; a line marked with (3) means it applies to the BI-NER-WA model. S ← A (2) S ← {(i, j) : ∀i ∈ |e|, ∀j ∈ |f |} (3) ∀i ∈ |e| : we","i = 0; ∀j ∈ |f | : wf","j = 0 (2,3) ∀(i, j) ∈ S : de","(i, j) = 0, df","(i, j) = 0 (3) for t ← 1 to T do ye(k)∗ ← argmax k ( ye(k)) + ∑ i∈|e| we i ye(k)","i (2,3) yf(l)∗ ← argmax l ( yf(l)) + ∑ i∈|f| wf j yf(l)","j (2,3)","Be∗","←argmax m (Be ) + ∑ (i,j)de","(i, j)be","(i, j) (3)","Bf∗ ←argmax n (","Bf ) + ∑","(i,j)df (i, j)bf","(i, j) (3)","for all (i, j) ∈ S do","(ye(h)∗","i yf(h)∗","j )← −we","i ye(h)","i − wf","j yf(h)","j","+ argmax h(i,j)(ye(q) i yf(q)","j ) (2)","(ye(q)∗","i yf(q)∗","j a(i, j)∗",")← −we","i ye(q)","i − wf","j yf(q)","j","+ argmax q(i,j)(ye(q)","i yf(q)","j a(i, j))","− de","(i, j)a(i, j) − df","(i, j)a(i, j) (3)","end for","Conv = (ye(k) =ye(q) ∧ yf(l) =yf(q) ) (2)","Conv = (Be","=A=Bf","∧ ye(k)","=ye(q)","∧ yf(l) =yf(q) ) (3) if Conv = true , then return ( ye(k)∗ , yf(l)∗) (2) return ( ye(k)∗ , yf(l)∗ , A) (3)","else for all i ∈ |e| do we i ← we","i + αt ( ye(q|h)∗ i − ye(k)∗","i ) (2,3) end for for all j ∈ |f | do wf j ← wf","j + αt ( yf(q|h)∗ j − yf(l)∗","j ) (2,3)","end for","for all (i, j) ∈ S do","de","(i, j) ← de (i, j) + αt (ae∗","(i, j) − be∗","(i, j)) (3)","df","(i, j) ← df (i, j) + αt (","af∗","(i, j) − bf∗","(i, j)) (3) end for end if end for return","( ye(k)∗ (T) , yf(l)∗","(T) ) (2) return","( ye(k)∗ (T) , yf(l)∗","(T) , A(T) ) (3) able a(i, j) for an alignment link between ei and fj. The edge factor also predicts the entity tags for ei and fj.","The new edge potential q is defined as: q(i,j) ( ye i , y f j, a(i, j) ) =","log(P (a(i, j) = 1)) + S(ye i , y f j|a(i, j))P (a(i,j)=1)","S(ye i , y f j|a(i, j))=","{ pmi(ye","i , y f j), if a(i, j) = 1 0, else P (a(i, j) = 1) is the alignment probability assigned by the bilingual edge factor between node ei and fj. We initialize this value to P̂ (ei, fj) = 1 2 (Pm(ei, fj) + Pn(ei, fj)), where Pm(ei, fj) and Pn(ei, fj) are the posterior probabilities assigned by the HMM-aligners.","The joint optimization problem is defined as:","max","ye(k)","yf(l)","ye(h)","yf(h)","Be","Bf","A k(ye(k) ) + l(yf(l) )+","m(Be ) + n(Bf",") + ∑","(i∈|e|,j∈|f|)q(i,j)(yeh i , y f(h) j , a(i, j)) ∋ ∀(i, j) :","( be (i, j)=a(i, j))","∧ ( bf","(i, j)=a(i, j)) ∧ if a(i, j) = 1 then ( ye(k) i =ye(h)","i ) ∧ ( y f(l) j =y f(h) j ) We include two dual constraints de","(i, j) and df","(i, j) over alignments for every bilingual edge factor ζ(i, j), which are applied to the English and Chinese sides of the alignment space, respectively.","The DD algorithm used for this model is given in Algorithm 2 (case 3). One special note is that after each iteration when we consider updates to the dual constraint for entity tags, we only check tag agreements for cross-lingual edge factors that have an alignment assignment value of 1. In other words, cross-lingual edges that are not aligned do not affect bilingual NER tagging.","Similar to φ(i, j), ζ(i, j) factors do not provide that much additional information other than some selectional preferences via PMI score. But the real power of these cross-language edge cliques is that they act as a liaison between the NER and alignment models on each language side, and encourage these models to indirectly agree with each other by having them all agree with the edge cliques.","It is also worth noting that since we decode the alignment models with Viterbi inference, additional constraints such as the neighborhood constraint proposed by DeNero and Macherey (2011) can be easily integrated into our model. The neighborhood constraint enforces that if fj is aligned to ei, then fj can only be aligned to ei+1 or ei−1 (with a small penalty), but not any other word position. We report results of adding neighborhood constraints to our model in Section 6."]},{"title":"4 Experimental Setup","paragraphs":["We evaluate on the large OntoNotes (v4.0) corpus (Hovy et al., 2006) which contains manually 1077 annotated NER tags for both Chinese and English. Document pairs are sentence aligned using the Champollion Tool Kit (Ma, 2006). After discarding sentences with no aligned counterpart, a total of 402 documents and 8,249 parallel sentence pairs were used for evaluation. We will refer to this evaluation set as full-set. We use odd-numbered documents as the dev set and evennumbered documents as the blind test set. We did not perform parameter tuning on the dev set to optimize performance, instead we fix the initial learning rate to 0.5 and maximum iterations to 1,000 in all DD experiments. We only use the dev set for model development.","The Stanford CRF-based NER tagger was used as the monolingual component in our models (Finkel et al., 2005). It also serves as a state-of-the-art monolingual baseline for both English and Chinese. For English, we use the default tagger setting from Finkel et al. (2005). For Chinese, we use an improved set of features over the default tagger, which includes distributional similarity features trained on large amounts of non-overlapping data.1","We train the two CRF models on all portions of the OntoNotes corpus that are annotated with named entity tags, except the parallel-aligned portion which we reserve for development and test purposes. In total, there are about 660 training documents (∼16k sentences) for Chinese and 1,400 documents (∼39k sentences) for English.","Out of the 18 named entity types that are annotated in OntoNotes, which include person, location, date, money, and so on, we select the four most commonly seen named entity types for evaluation. They are person, location, organization and GPE. All entities of these four types are converted to the standard BIO format, and background to-kens and all other entity types are marked with tag O. When we consider label agreements over aligned word pairs in all bilingual agreement models, we ignore the distinction between B- and I-tags.","We report standard NER measures (entity precision (P), recall (R) and F1 score) on the test set. Statistical significance tests are done using the paired bootstrap resampling method (Efron and Tibshirani, 1993).","For alignment experiments, we train two uni-","1","The exact feature set and the CRF implementation can be found here: http://nlp.stanford.edu/ software/CRF-NER.shtml directional HMM models as our baseline and monolingual alignment models. The parameters of the HMM were initialized by IBM Model 1 using the agreement-based EM training algorithms from Liang et al. (2006). Each model is trained for 2 iterations over a parallel corpus of 12 million English words and Chinese words, almost twice as much data as used in previous work that yields state-of-the-art unsupervised alignment results (DeNero and Klein, 2008; Haghighi et al., 2009; DeNero and Macherey, 2011).","Word alignment evaluation is done over the sections of OntoNotes that have matching gold-standard word alignment annotations from GALE Y1Q4 dataset.2","This subset contains 288 documents and 3,391 sentence pairs. We will refer to this subset as wa-subset. This evaluation set is over 20 times larger than the 150 sentences set used in most past evaluations (DeNero and Klein, 2008; Haghighi et al., 2009; DeNero and Macherey, 2011).","Alignments input to the BI-NER model are produced by thresholding the averaged posterior probability at 0.5. In joint NER and alignment experiments, instead of posterior thresholding, we take the direct intersection of the Viterbi-best alignment of the two directional models. We report the standard P, R, F1 and Alignment Error Rate (AER) measures for alignment experiments.","An important past work to make comparisons with is Burkett et al. (2010b). Their method is similar to ours in that they also model bilingual agreement in conjunction with two CRF-based monolingual models. But instead of using just the PMI scores of bilingual NE pairs, as in our work, they employed a feature-rich log-linear model to capture bilingual correlations. Parameters in their log-linear model require training with bilingually annotated data, which is not readily available. To counter this problem, they proposed an “up-training” method which simulates a supervised learning environment by pairing a weak classifier with strong classifiers, and train the bilingual model to rank the output of the strong classifier highly among the N-best outputs of the weak classifier. In order to compare directly with their method, we obtained the code behind Burkett et al. (2010b) and reproduced their experimental setting for the OntoNotes data. An extra set of 5,000 unannotated parallel sentence pairs are used for 2 LDC Catalog No. LDC2006E86. 1078","Chinese English","P R F1 P R F1 Mono 76.89 61.64 68.42 81.98 74.59 78.11 Burkett 77.52 65.84 71.20 82.28 76.64 79.36 Bi-soft 79.14 71.55 75.15 82.58 77.96 80.20 Table 1: NER results on bilingual parallel test set. Best numbers on each measure that are statistically significantly better than the monolingual baseline and Burkett et al. (2010b) are highlighted in bold. training the reranker, and the reranker model selection was performed on the development dataset."]},{"title":"5 Bilingual NER Results","paragraphs":["The main results on bilingual NER over the test portion of full-set are shown in Table 1. We initially experimented with the hard agreement model, but it performs quite poorly for reasons we discussed in Section 2.2. The BI-NER model with soft agreement constraints, however, significantly outperforms all baselines. In particular, it achieves an absolute F1 improvement of 6.7% in Chinese and 2.1% in English over the CRF monolingual baselines.","A well-known issue with the DD method is that when the model does not necessarily converge, then the procedure could be very sensitive to hyper-parameters such as initial step size and early termination criteria. If a model only gives good performance with well-tuned hyper-parameters, then we must have manually annotated data for tuning, which would significantly reduce the applicability and portability of this method to other language pairs and tasks. To evaluate the parameter sensitivity of our model, we run the model from 50 to 3000 iterations before early stopping, and with 6 different initial step sizes from 0.01 to 1. The results are shown in Figure 2. The soft agreement model does not seem to be sensitive to initial step size and almost always converges to a superior solution than the baseline."]},{"title":"6 Joint NER and Alignment Results","paragraphs":["We present results for the BI-NER-WA model in Table 2. By jointly decoding NER with word alignment, our model not only maintains significant improvements in NER performance, but also yields significant improvements to alignment performance. Overall, joint decoding with NER alone yields a 10.8% error reduction in AER over the baseline HMM-aligners, and also gives improve-0 0.01 0.05 0.1 0.2 0.5 1 2 3000 1000 800 500 300 100 5073 74 75 76 77 78 79 80 initial step sizemax no. of iterations F1 score Figure 2: Performance variance of the soft agreement models on the Chinese dev dataset, as a function of step size (x-axis) and maximum number of iterations before early stopping (y-axis). ment over BI-NER in NER. Adding additional neighborhood constraints gives a further 6% error reduction in AER, at the cost of a small loss in Chinese NER. In terms of word alignment results, we see great increases in F1 and recall, but precision goes down significantly. This is because the joint decoding algorithm promotes an effect of “soft-union”, by encouraging the two uni-directional aligners to agree more often. Adding the neighborhood constraints further enhances this union effect."]},{"title":"7 Error Analysis and Discussion","paragraphs":["We can examine the example in Figure 3 to gain an understanding of the model’s performance. In this example, a snippet of a longer sentence pair is shown with NER and word alignment results. The monolingual Chinese tagger provides a strong cue that word f6 is a person name because the unique 4-character word pattern is commonly associated with foreign names in Chinese, and also the word is immediately preceded by the word “president”. The English monolingual tagger, however, confuses the aligned word e0 with a GPE.","Our bilingual NER model is able to correct this error as expected. Similarly, the bilingual model corrects the error over e11. However, the model also propagates labeling errors from the English side over the entity “Tibet Autonomous Region” to the Chinese side. Nevertheless, the resulting Chinese tags are arguably more useful than the original tags assigned by the baseline model.","In terms of word alignment, the HMM models failed badly on this example because of the long 1079","NER-Chinese NER-English word alignment","P R F1 P R F1 P R F1 AER HMM-WA - - - - - - 90.43 40.95 56.38 43.62 Mono-CRF 82.50 66.58 73.69 84.24 78.70 81.38 - - - - Bi-NER 84.87 75.30 79.80 84.47 81.45 82.93 - - - - Bi-NER-WA 84.42 76.34 80.18 84.25 82.20 83.21 77.45 50.43 61.09 38.91 Bi-NER-WA+NC 84.25 75.09 79.41 84.28 82.17 83.21 76.67 54.44 63.67 36.33 Table 2: Joint alignment and NER test results. +NC means incorporating additional neighbor constraints from DeNero and Macherey (2011) to the model. Best number in each column is highlighted in bold. f0 f1 f2 f3 f4 f5 f6 e0 e1 e2 e3 e4 e5 e6 e7 e8 e9 e10 e11Suolangdaji , president of Tibet Auto. Region branch of Bank of ChinaB-PER O O O B-GPE I-GPE I-GPE O O B-ORG I-ORG I-ORGB-PER O O O [B-LOC] [I-LOC] [I-LOC] O O B-ORG I-ORG I-ORG[B-GPE] O O O [B-LOC] [I-LOC] [I-LOC] O O [O] [O] [B-GPE] 中国 银行 西藏 自治区 分行 行长 索朗达吉 B-ORG I-ORG B-GPE O O O B-PER B-ORG I-ORG [B-LOC] [I-LOC] O O B-PER B-ORG I-ORG [O] O O O B-PER Figure 3: An example output of our BI-NER-WA model. Dotted alignment links are the oracle, dashed links are alignments from HMM baseline, and solid links are outputs of our model. Entity tags in the gold line (closest to nodes ei and fj) are the gold-standard tags; in the green line (second closest to nodes) are output from our model; and in the crimson line (furthest from nodes) are baseline output. distance swapping phenomena. The two unidirectional HMMs also have strong disagreements over the alignments, and the resulting baseline aligner output only recovers two links. If we were to take this alignment as fixed input, most likely we would not be able to recover the error over e11, but the joint decoding method successfully recovered 4 more links, and indirectly resulted in the NER tagging improvement discussed above."]},{"title":"8 Related Work","paragraphs":["The idea of employing bilingual resources to improve over monolingual systems has been explored by much previous work. For example, Huang et al. (2009) improved parsing performance using a bilingual parallel corpus. In the NER domain, Li et al. (2012) presented a cyclic CRF model very similar to our BI-NER model, and performed approximate inference using loopy belief propagation. The feature-rich CRF formulation of bilingual edge potentials in their model is much more powerful than our simple PMI-based bilingual edge model. Adding a richer bilingual edge model might well further improve our results, and this is a possible direction for further experimentation. However, a big drawback of this approach is that training such a feature-rich model requires manually annotated bilingual NER data, which can be prohibitively expensive to generate. How and where to obtain training signals with-out manual supervision is an interesting and open question. One of the most interesting papers in this regard is Burkett et al. (2010b), which explored an “up-training” mechanism by using the outputs from a strong monolingual model as ground-truth, and simulated a learning environment where a bilingual model is trained to help a “weakened” monolingual model to recover the results of the strong model. It is worth mentioning that since our method does not require additional training and can take pretty much any existing model as “black-box” during decoding, the richer and more accurate bilingual model learned from Burkett et al. (2010b) can be directly plugged into our model.","A similar dual decomposition algorithm to ours was proposed by Riedel and McCallum (2011) for biomedical event detection. In their Model 3, the trigger and argument extraction models are reminiscent of the two monolingual CRFs in our model; additional binding agreements are enforced over every protein pair, similar to how we enforce agreement between every aligned word 1080 pair. Martins et al. (2011b) presented a new DD method that combines the power of DD with the augmented Lagrangian method. They showed that their method can achieve faster convergence than traditional sub-gradient methods in models with many overlapping components (Martins et al., 2011a). This method is directly applicable to our work.","Another promising direction for improving NER performance is in enforcing global label consistency across documents, which is an idea that has been greatly explored in the past (Sutton and McCallum, 2004; Bunescu and Mooney, 2004; Finkel et al., 2005). More recently, Rush et al. (2012) and Chieu and Teow (2012) have shown that combining local prediction models with global consistency models, and enforcing agreement via DD is very effective. It is straight-forward to incorporate an additional global consistency model into our model for further improvements.","Our joint alignment and NER decoding approach is inspired by prior work on improving alignment quality through encouraging agreement between bi-directional models (Liang et al., 2006; DeNero and Macherey, 2011). Instead of enforcing agreement in the alignment space based on best sequences found by Viterbi, we could opt to encourage agreement between posterior probability distributions, which is related to the posterior regularization work by Graca̧ et al. (2008). Cromières and Kurohashi (2009) proposed an approach that takes phrasal bracketing constraints from parsing outputs, and uses them to enforce phrasal alignments. This idea is similar to our joint alignment and NER approach, but in our case the phrasal constraints are indirectly imposed by entity spans. We also differ in the implementation details, where in their case belief propagation is used in both training and Viterbi inference.","Burkett et al. (2010a) presented a supervised learning method for performing joint parsing and word alignment using log-linear models over parse trees and an ITG model over alignment. The model demonstrates performance improvements in both parsing and alignment, but shares the common limitations of other supervised work in that it requires manually annotated bilingual joint parsing and word alignment data.","Chen et al. (2010) also tackled the problem of joint alignment and NER. Their method employs a set of heuristic rules to expand a candidate named entity set generated by monolingual taggers, and then rank those candidates using a bilingual named entity dictionary. Our approach differs in that we provide a probabilistic formulation of the problem and do not require pre-existing NE dictionaries."]},{"title":"9 Conclusion","paragraphs":["We introduced a graphical model that combines two HMM word aligners and two CRF NER taggers into a joint model, and presented a dual decomposition inference method for performing efficient decoding over this model. Results from NER and word alignment experiments suggest that our method gives significant improvements in both NER and word alignment. Our techniques make minimal assumptions about the underlying monolingual components, and can be adapted for many other tasks such as parsing."]},{"title":"Acknowledgments","paragraphs":["The authors would like to thank Rob Voigt and the three anonymous reviewers for their valuable comments and suggestions. We gratefully acknowledge the support of the National Natural Science Foundation of China (NSFC) via grant 61133012, the National “863” Project via grant 2011AA01A207 and 2012AA011102, the Ministry of Education Research of Social Sciences Youth funded projects via grant 12YJCZH304, and the support of the U.S. Defense Advanced Research Projects Agency (DARPA) Broad Operational Language Translation (BOLT) program through IBM.","Any opinions, findings, and conclusion or recommendations expressed in this material are those of the authors and do not necessarily reflect the view of DARPA, or the US government."]},{"title":"References","paragraphs":["Yaser Al-Onaizan and Kevin Knight. 2002. Translat-ing named entities using monolingual and bilingual resources. In Proceedings of ACL.","Bogdan Babych and Anthony Hartley. 2003. Improving machine translation quality with automatic named entity recognition. In Proceedings of the 7th International EAMT workshop on MT and other Language Technology Tools, Improving MT through other Language Technology Tools: Resources and Tools for Building MT. 1081","Dimitri P. Bertsekas. 1999. Nonlinear Programming. Athena Scientific, New York.","Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1991. Word-sense disambiguation using statistical methods. In Proceedings of ACL.","Razvan Bunescu and Raymond J. Mooney. 2004. Collective information extraction with relational Markov networks. In Proceedings of ACL.","David Burkett, John Blitzer, and Dan Klein. 2010a. Joint parsing and alignment with weakly synchronized grammars. In Proceedings of NAACL-HLT.","David Burkett, Slav Petrov, John Blitzer, and Dan Klein. 2010b. Learning better monolingual models with unannotated bilingual text. In Proceedings of CoNLL.","Yufeng Chen, Chengqing Zong, and Keh-Yih Su. 2010. On jointly recognizing and aligning bilingual named entities. In Proceedings of ACL.","Hai Leong Chieu and Loo-Nin Teow. 2012. Combining local and non-local information with dual decomposition for named entity recognition from text. In Proceedings of 15th International Conference on Information Fusion (FUSION).","Fabien Cromières and Sadao Kurohashi. 2009. An alignment algorithm using belief propagation and a structure-based distortion model. In Proceedings of EACL/ IJCNLP.","John DeNero and Dan Klein. 2008. The complexity of phrase alignment problems. In Proceedings of ACL.","John DeNero and Klaus Macherey. 2011. Model-based aligner combination using dual decomposition. In Proceedings of ACL.","Brad Efron and Robert Tibshirani. 1993. An Introduc-tion to the Bootstrap. Chapman & Hall, New York.","Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by Gibbs sampling. In Proceedings of ACL.","Joao Graca̧, Kuzman Ganchev, and Ben Taskar. 2008. Expectation maximization and posterior constraints. In Proceedings of NIPS.","Aria Haghighi, John Blitzer, John DeNero, and Dan Klein. 2009. Better word alignments with supervised ITG models. In Proceedings of ACL.","Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance Ramshaw, and Ralph Weischedel. 2006. OntoNotes: the 90% solution. In Proceedings of NAACL-HLT.","Fei Huang and Stephan Vogel. 2002. Improved named entity translation and bilingual named entity extraction. In Proceedings of the 2002 International Conference on Multimodal Interfaces (ICMI).","Liang Huang, Wenbin Jiang, and Qun Liu. 2009. Bilingually-constrained (monolingual) shift-reduce parsing. In Proceedings of EMNLP.","Sungchul Kim, Kristina Toutanova, and Hwanjo Yu. 2012. Multilingual named entity recognition using parallel data and metadata from Wikipedia. In Proceedings of ACL.","Terry Koo, Alexander M. Rush, Michael Collins, Tommi Jaakkola, and David Sontag. 2010. Dual decomposition for parsing with non-projective head automata. In Proceedings of EMNLP.","Qi Li, Haibo Li, Heng Ji, Wen Wang, Jing Zheng, and Fei Huang. 2012. Joint bilingual name tagging for parallel corpora. In Proceedings of CIKM.","Percy Liang, Ben Taskar, and Dan Klein. 2006. Alignment by agreement. In Proceedings of HLT-NAACL.","Xiaoyi Ma. 2006. Champollion: A robust parallel text sentence aligner. In Proceedings of LREC.","André F. T. Martins, Noah A. Smith, Pedro M. Q. Aguiar, and Mário A. T. Figueiredo. 2011a. Dual decomposition with many overlapping components. In Proceedings of EMNLP.","Andre F. T. Martins, Noah A. Smith, Eric P. Xing, Pedro M. Q. Aguiar, and Mário A. T. Figueiredo. 2011b. Augmenting dual decomposition for map inference. In Proceedings of the International Workshop on Optimization for Machine Learning (OPT 2010).","Sebastian Riedel and Andrew McCallum. 2011. Fast and robust joint models for biomedical event extraction. In Proceedings of EMNLP.","Alexander M. Rush and Michael Collins. 2012. A tutorial on dual decomposition and Lagrangian relaxation for inference in natural language processing. JAIR, 45:305–362.","Alexander M. Rush, David Sontag, Michael Collins, and Tommi Jaakkola. 2010. On dual decomposition and linear programming relaxations for natural language processing. In Proceedings of EMNLP.","Alexander M. Rush, Roi Reichert, Michael Collins, and Amir Globerson. 2012. Improved parsing and POS tagging using inter-sentence consistency constraints. In Proceedings of EMNLP.","Charles Sutton and Andrew McCallum. 2004. Collective segmentation and labeling of distant entities in information extraction. In Proceedings of ICML Workshop on Statistical Relational Learning and Its connections to Other Fields. 1082"]}],"references":[{"authors":[{"first":"Yaser","last":"Al-Onaizan"},{"first":"Kevin","last":"Knight"}],"year":"2002","title":"Translat-ing named entities using monolingual and bilingual resources","source":"Yaser Al-Onaizan and Kevin Knight. 2002. Translat-ing named entities using monolingual and bilingual resources. In Proceedings of ACL."},{"authors":[{"first":"Bogdan","last":"Babych"},{"first":"Anthony","last":"Hartley"}],"year":"2003","title":"Improving machine translation quality with automatic named entity recognition","source":"Bogdan Babych and Anthony Hartley. 2003. Improving machine translation quality with automatic named entity recognition. In Proceedings of the 7th International EAMT workshop on MT and other Language Technology Tools, Improving MT through other Language Technology Tools: Resources and Tools for Building MT. 1081"},{"authors":[{"first":"Dimitri","middle":"P.","last":"Bertsekas"}],"year":"1999","title":"Nonlinear Programming","source":"Dimitri P. Bertsekas. 1999. Nonlinear Programming. Athena Scientific, New York."},{"authors":[{"first":"Peter","middle":"F.","last":"Brown"},{"first":"Stephen","middle":"A. Della","last":"Pietra"},{"first":"Vincent","middle":"J. Della","last":"Pietra"},{"first":"Robert","middle":"L.","last":"Mercer"}],"year":"1991","title":"Word-sense disambiguation using statistical methods","source":"Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1991. Word-sense disambiguation using statistical methods. In Proceedings of ACL."},{"authors":[{"first":"Razvan","last":"Bunescu"},{"first":"Raymond","middle":"J.","last":"Mooney"}],"year":"2004","title":"Collective information extraction with relational Markov networks","source":"Razvan Bunescu and Raymond J. Mooney. 2004. Collective information extraction with relational Markov networks. In Proceedings of ACL."},{"authors":[{"first":"David","last":"Burkett"},{"first":"John","last":"Blitzer"},{"first":"Dan","last":"Klein"}],"year":"2010a","title":"Joint parsing and alignment with weakly synchronized grammars","source":"David Burkett, John Blitzer, and Dan Klein. 2010a. Joint parsing and alignment with weakly synchronized grammars. In Proceedings of NAACL-HLT."},{"authors":[{"first":"David","last":"Burkett"},{"first":"Slav","last":"Petrov"},{"first":"John","last":"Blitzer"},{"first":"Dan","last":"Klein"}],"year":"2010b","title":"Learning better monolingual models with unannotated bilingual text","source":"David Burkett, Slav Petrov, John Blitzer, and Dan Klein. 2010b. Learning better monolingual models with unannotated bilingual text. In Proceedings of CoNLL."},{"authors":[{"first":"Yufeng","last":"Chen"},{"first":"Chengqing","last":"Zong"},{"first":"Keh-Yih","last":"Su"}],"year":"2010","title":"On jointly recognizing and aligning bilingual named entities","source":"Yufeng Chen, Chengqing Zong, and Keh-Yih Su. 2010. On jointly recognizing and aligning bilingual named entities. In Proceedings of ACL."},{"authors":[{"first":"Hai","middle":"Leong","last":"Chieu"},{"first":"Loo-Nin","last":"Teow"}],"year":"2012","title":"Combining local and non-local information with dual decomposition for named entity recognition from text","source":"Hai Leong Chieu and Loo-Nin Teow. 2012. Combining local and non-local information with dual decomposition for named entity recognition from text. In Proceedings of 15th International Conference on Information Fusion (FUSION)."},{"authors":[{"first":"Fabien","last":"Cromières"},{"first":"Sadao","last":"Kurohashi"}],"year":"2009","title":"An alignment algorithm using belief propagation and a structure-based distortion model","source":"Fabien Cromières and Sadao Kurohashi. 2009. An alignment algorithm using belief propagation and a structure-based distortion model. In Proceedings of EACL/ IJCNLP."},{"authors":[{"first":"John","last":"DeNero"},{"first":"Dan","last":"Klein"}],"year":"2008","title":"The complexity of phrase alignment problems","source":"John DeNero and Dan Klein. 2008. The complexity of phrase alignment problems. In Proceedings of ACL."},{"authors":[{"first":"John","last":"DeNero"},{"first":"Klaus","last":"Macherey"}],"year":"2011","title":"Model-based aligner combination using dual decomposition","source":"John DeNero and Klaus Macherey. 2011. Model-based aligner combination using dual decomposition. In Proceedings of ACL."},{"authors":[{"first":"Brad","last":"Efron"},{"first":"Robert","last":"Tibshirani"}],"year":"1993","title":"An Introduc-tion to the Bootstrap","source":"Brad Efron and Robert Tibshirani. 1993. An Introduc-tion to the Bootstrap. Chapman & Hall, New York."},{"authors":[{"first":"Jenny","middle":"Rose","last":"Finkel"},{"first":"Trond","last":"Grenager"},{"first":"Christopher","last":"Manning"}],"year":"2005","title":"Incorporating non-local information into information extraction systems by Gibbs sampling","source":"Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by Gibbs sampling. In Proceedings of ACL."},{"authors":[{"first":"Joao","last":"Graca̧"},{"first":"Kuzman","last":"Ganchev"},{"first":"Ben","last":"Taskar"}],"year":"2008","title":"Expectation maximization and posterior constraints","source":"Joao Graca̧, Kuzman Ganchev, and Ben Taskar. 2008. Expectation maximization and posterior constraints. In Proceedings of NIPS."},{"authors":[{"first":"Aria","last":"Haghighi"},{"first":"John","last":"Blitzer"},{"first":"John","last":"DeNero"},{"first":"Dan","last":"Klein"}],"year":"2009","title":"Better word alignments with supervised ITG models","source":"Aria Haghighi, John Blitzer, John DeNero, and Dan Klein. 2009. Better word alignments with supervised ITG models. In Proceedings of ACL."},{"authors":[{"first":"Eduard","last":"Hovy"},{"first":"Mitchell","last":"Marcus"},{"first":"Martha","last":"Palmer"},{"first":"Lance","last":"Ramshaw"},{"first":"Ralph","last":"Weischedel"}],"year":"2006","title":"OntoNotes: the 90% solution","source":"Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance Ramshaw, and Ralph Weischedel. 2006. OntoNotes: the 90% solution. In Proceedings of NAACL-HLT."},{"authors":[{"first":"Fei","last":"Huang"},{"first":"Stephan","last":"Vogel"}],"year":"2002","title":"Improved named entity translation and bilingual named entity extraction","source":"Fei Huang and Stephan Vogel. 2002. Improved named entity translation and bilingual named entity extraction. In Proceedings of the 2002 International Conference on Multimodal Interfaces (ICMI)."},{"authors":[{"first":"Liang","last":"Huang"},{"first":"Wenbin","last":"Jiang"},{"first":"Qun","last":"Liu"}],"year":"2009","title":"Bilingually-constrained (monolingual) shift-reduce parsing","source":"Liang Huang, Wenbin Jiang, and Qun Liu. 2009. Bilingually-constrained (monolingual) shift-reduce parsing. In Proceedings of EMNLP."},{"authors":[{"first":"Sungchul","last":"Kim"},{"first":"Kristina","last":"Toutanova"},{"first":"Hwanjo","last":"Yu"}],"year":"2012","title":"Multilingual named entity recognition using parallel data and metadata from Wikipedia","source":"Sungchul Kim, Kristina Toutanova, and Hwanjo Yu. 2012. Multilingual named entity recognition using parallel data and metadata from Wikipedia. In Proceedings of ACL."},{"authors":[{"first":"Terry","last":"Koo"},{"first":"Alexander","middle":"M.","last":"Rush"},{"first":"Michael","last":"Collins"},{"first":"Tommi","last":"Jaakkola"},{"first":"David","last":"Sontag"}],"year":"2010","title":"Dual decomposition for parsing with non-projective head automata","source":"Terry Koo, Alexander M. Rush, Michael Collins, Tommi Jaakkola, and David Sontag. 2010. Dual decomposition for parsing with non-projective head automata. In Proceedings of EMNLP."},{"authors":[{"first":"Qi","last":"Li"},{"first":"Haibo","last":"Li"},{"first":"Heng","last":"Ji"},{"first":"Wen","last":"Wang"},{"first":"Jing","last":"Zheng"},{"first":"Fei","last":"Huang"}],"year":"2012","title":"Joint bilingual name tagging for parallel corpora","source":"Qi Li, Haibo Li, Heng Ji, Wen Wang, Jing Zheng, and Fei Huang. 2012. Joint bilingual name tagging for parallel corpora. In Proceedings of CIKM."},{"authors":[{"first":"Percy","last":"Liang"},{"first":"Ben","last":"Taskar"},{"first":"Dan","last":"Klein"}],"year":"2006","title":"Alignment by agreement","source":"Percy Liang, Ben Taskar, and Dan Klein. 2006. Alignment by agreement. In Proceedings of HLT-NAACL."},{"authors":[{"first":"Xiaoyi","last":"Ma"}],"year":"2006","title":"Champollion: A robust parallel text sentence aligner","source":"Xiaoyi Ma. 2006. Champollion: A robust parallel text sentence aligner. In Proceedings of LREC."},{"authors":[{"first":"André","middle":"F. T.","last":"Martins"},{"first":"Noah","middle":"A.","last":"Smith"},{"first":"Pedro","middle":"M. Q.","last":"Aguiar"},{"first":"Mário","middle":"A. T.","last":"Figueiredo"}],"year":"2011a","title":"Dual decomposition with many overlapping components","source":"André F. T. Martins, Noah A. Smith, Pedro M. Q. Aguiar, and Mário A. T. Figueiredo. 2011a. Dual decomposition with many overlapping components. In Proceedings of EMNLP."},{"authors":[{"first":"Andre","middle":"F. T.","last":"Martins"},{"first":"Noah","middle":"A.","last":"Smith"},{"first":"Eric","middle":"P.","last":"Xing"},{"first":"Pedro","middle":"M. Q.","last":"Aguiar"},{"first":"Mário","middle":"A. T.","last":"Figueiredo"}],"year":"2011b","title":"Augmenting dual decomposition for map inference","source":"Andre F. T. Martins, Noah A. Smith, Eric P. Xing, Pedro M. Q. Aguiar, and Mário A. T. Figueiredo. 2011b. Augmenting dual decomposition for map inference. In Proceedings of the International Workshop on Optimization for Machine Learning (OPT 2010)."},{"authors":[{"first":"Sebastian","last":"Riedel"},{"first":"Andrew","last":"McCallum"}],"year":"2011","title":"Fast and robust joint models for biomedical event extraction","source":"Sebastian Riedel and Andrew McCallum. 2011. Fast and robust joint models for biomedical event extraction. In Proceedings of EMNLP."},{"authors":[{"first":"Alexander","middle":"M.","last":"Rush"},{"first":"Michael","last":"Collins"}],"year":"2012","title":"A tutorial on dual decomposition and Lagrangian relaxation for inference in natural language processing","source":"Alexander M. Rush and Michael Collins. 2012. A tutorial on dual decomposition and Lagrangian relaxation for inference in natural language processing. JAIR, 45:305–362."},{"authors":[{"first":"Alexander","middle":"M.","last":"Rush"},{"first":"David","last":"Sontag"},{"first":"Michael","last":"Collins"},{"first":"Tommi","last":"Jaakkola"}],"year":"2010","title":"On dual decomposition and linear programming relaxations for natural language processing","source":"Alexander M. Rush, David Sontag, Michael Collins, and Tommi Jaakkola. 2010. On dual decomposition and linear programming relaxations for natural language processing. In Proceedings of EMNLP."},{"authors":[{"first":"Alexander","middle":"M.","last":"Rush"},{"first":"Roi","last":"Reichert"},{"first":"Michael","last":"Collins"},{"first":"Amir","last":"Globerson"}],"year":"2012","title":"Improved parsing and POS tagging using inter-sentence consistency constraints","source":"Alexander M. Rush, Roi Reichert, Michael Collins, and Amir Globerson. 2012. Improved parsing and POS tagging using inter-sentence consistency constraints. In Proceedings of EMNLP."},{"authors":[{"first":"Charles","last":"Sutton"},{"first":"Andrew","last":"McCallum"}],"year":"2004","title":"Collective segmentation and labeling of distant entities in information extraction","source":"Charles Sutton and Andrew McCallum. 2004. Collective segmentation and labeling of distant entities in information extraction. In Proceedings of ICML Workshop on Statistical Relational Learning and Its connections to Other Fields. 1082"}],"cites":[{"style":0,"text":"Babych and Hartley, 2003","origin":{"pointer":"/sections/2/paragraphs/0","offset":619,"length":24},"authors":[{"last":"Babych"},{"last":"Hartley"}],"year":"2003","references":["/references/1"]},{"style":0,"text":"Huang and Vogel, 2002","origin":{"pointer":"/sections/2/paragraphs/0","offset":808,"length":21},"authors":[{"last":"Huang"},{"last":"Vogel"}],"year":"2002","references":["/references/17"]},{"style":0,"text":"Al-Onaizan and Knight, 2002","origin":{"pointer":"/sections/2/paragraphs/0","offset":831,"length":27},"authors":[{"last":"Al-Onaizan"},{"last":"Knight"}],"year":"2002","references":["/references/0"]},{"style":0,"text":"Burkett et al. (2010b)","origin":{"pointer":"/sections/2/paragraphs/0","offset":883,"length":22},"authors":[{"last":"Burkett"},{"last":"al."}],"year":"2010b","references":["/references/6"]},{"style":0,"text":"Li et al. (2012)","origin":{"pointer":"/sections/2/paragraphs/0","offset":907,"length":16},"authors":[{"last":"Li"},{"last":"al."}],"year":"2012","references":["/references/21"]},{"style":0,"text":"Kim et al. (2012)","origin":{"pointer":"/sections/2/paragraphs/0","offset":928,"length":17},"authors":[{"last":"Kim"},{"last":"al."}],"year":"2012","references":["/references/19"]},{"style":0,"text":"Burkett et al., 2010b","origin":{"pointer":"/sections/2/paragraphs/1","offset":135,"length":21},"authors":[{"last":"Burkett"},{"last":"al."}],"year":"2010b","references":["/references/6"]},{"style":0,"text":"Brown et al., 1991","origin":{"pointer":"/sections/2/paragraphs/1","offset":273,"length":18},"authors":[{"last":"Brown"},{"last":"al."}],"year":"1991","references":["/references/3"]},{"style":0,"text":"Bertsekas, 1999","origin":{"pointer":"/sections/2/paragraphs/2","offset":384,"length":15},"authors":[{"last":"Bertsekas"}],"year":"1999","references":["/references/2"]},{"style":0,"text":"Rush et al., 2010","origin":{"pointer":"/sections/2/paragraphs/2","offset":401,"length":17},"authors":[{"last":"Rush"},{"last":"al."}],"year":"2010","references":["/references/28"]},{"style":0,"text":"Rush et al., 2010","origin":{"pointer":"/sections/2/paragraphs/2","offset":980,"length":17},"authors":[{"last":"Rush"},{"last":"al."}],"year":"2010","references":["/references/28"]},{"style":0,"text":"Koo et al., 2010","origin":{"pointer":"/sections/2/paragraphs/2","offset":999,"length":16},"authors":[{"last":"Koo"},{"last":"al."}],"year":"2010","references":["/references/20"]},{"style":0,"text":"DeNero and Macherey, 2011","origin":{"pointer":"/sections/2/paragraphs/2","offset":1017,"length":25},"authors":[{"last":"DeNero"},{"last":"Macherey"}],"year":"2011","references":["/references/11"]},{"style":0,"text":"Chieu and Teow, 2012","origin":{"pointer":"/sections/2/paragraphs/2","offset":1044,"length":20},"authors":[{"last":"Chieu"},{"last":"Teow"}],"year":"2012","references":["/references/8"]},{"style":0,"text":"Rush et al., 2010","origin":{"pointer":"/sections/3/paragraphs/38","offset":120,"length":17},"authors":[{"last":"Rush"},{"last":"al."}],"year":"2010","references":["/references/28"]},{"style":0,"text":"Koo et al. (2010)","origin":{"pointer":"/sections/3/paragraphs/43","offset":262,"length":17},"authors":[{"last":"Koo"},{"last":"al."}],"year":"2010","references":["/references/20"]},{"style":0,"text":"Rush and Collins (2012)","origin":{"pointer":"/sections/3/paragraphs/45","offset":164,"length":23},"authors":[{"last":"Rush"},{"last":"Collins"}],"year":"2012","references":["/references/27"]},{"style":0,"text":"Rush et al., 2010","origin":{"pointer":"/sections/3/paragraphs/91","offset":216,"length":17},"authors":[{"last":"Rush"},{"last":"al."}],"year":"2010","references":["/references/28"]},{"style":0,"text":"Koo et al., 2010","origin":{"pointer":"/sections/4/paragraphs/0","offset":388,"length":16},"authors":[{"last":"Koo"},{"last":"al."}],"year":"2010","references":["/references/20"]},{"style":0,"text":"Rush et al., 2012","origin":{"pointer":"/sections/4/paragraphs/0","offset":420,"length":17},"authors":[{"last":"Rush"},{"last":"al."}],"year":"2012","references":["/references/29"]},{"style":0,"text":"DeNero and Macherey (2011)","origin":{"pointer":"/sections/4/paragraphs/101","offset":161,"length":26},"authors":[{"last":"DeNero"},{"last":"Macherey"}],"year":"2011","references":["/references/11"]},{"style":0,"text":"Hovy et al., 2006","origin":{"pointer":"/sections/5/paragraphs/0","offset":50,"length":17},"authors":[{"last":"Hovy"},{"last":"al."}],"year":"2006","references":["/references/16"]},{"style":0,"text":"Ma, 2006","origin":{"pointer":"/sections/5/paragraphs/0","offset":215,"length":8},"authors":[{"last":"Ma"}],"year":"2006","references":["/references/23"]},{"style":0,"text":"Finkel et al., 2005","origin":{"pointer":"/sections/5/paragraphs/1","offset":87,"length":19},"authors":[{"last":"Finkel"},{"last":"al."}],"year":"2005","references":["/references/13"]},{"style":0,"text":"Finkel et al. (2005)","origin":{"pointer":"/sections/5/paragraphs/1","offset":249,"length":20},"authors":[{"last":"Finkel"},{"last":"al."}],"year":"2005","references":["/references/13"]},{"style":0,"text":"Efron and Tibshirani, 1993","origin":{"pointer":"/sections/5/paragraphs/4","offset":183,"length":26},"authors":[{"last":"Efron"},{"last":"Tibshirani"}],"year":"1993","references":["/references/12"]},{"style":0,"text":"Liang et al. (2006)","origin":{"pointer":"/sections/5/paragraphs/7","offset":301,"length":19},"authors":[{"last":"Liang"},{"last":"al."}],"year":"2006","references":["/references/22"]},{"style":0,"text":"DeNero and Klein, 2008","origin":{"pointer":"/sections/5/paragraphs/7","offset":543,"length":22},"authors":[{"last":"DeNero"},{"last":"Klein"}],"year":"2008","references":["/references/10"]},{"style":0,"text":"Haghighi et al., 2009","origin":{"pointer":"/sections/5/paragraphs/7","offset":567,"length":21},"authors":[{"last":"Haghighi"},{"last":"al."}],"year":"2009","references":["/references/15"]},{"style":0,"text":"DeNero and Macherey, 2011","origin":{"pointer":"/sections/5/paragraphs/7","offset":590,"length":25},"authors":[{"last":"DeNero"},{"last":"Macherey"}],"year":"2011","references":["/references/11"]},{"style":0,"text":"DeNero and Klein, 2008","origin":{"pointer":"/sections/5/paragraphs/9","offset":206,"length":22},"authors":[{"last":"DeNero"},{"last":"Klein"}],"year":"2008","references":["/references/10"]},{"style":0,"text":"Haghighi et al., 2009","origin":{"pointer":"/sections/5/paragraphs/9","offset":230,"length":21},"authors":[{"last":"Haghighi"},{"last":"al."}],"year":"2009","references":["/references/15"]},{"style":0,"text":"DeNero and Macherey, 2011","origin":{"pointer":"/sections/5/paragraphs/9","offset":253,"length":25},"authors":[{"last":"DeNero"},{"last":"Macherey"}],"year":"2011","references":["/references/11"]},{"style":0,"text":"Burkett et al. (2010b)","origin":{"pointer":"/sections/5/paragraphs/11","offset":51,"length":22},"authors":[{"last":"Burkett"},{"last":"al."}],"year":"2010b","references":["/references/6"]},{"style":0,"text":"Burkett et al. (2010b)","origin":{"pointer":"/sections/5/paragraphs/11","offset":858,"length":22},"authors":[{"last":"Burkett"},{"last":"al."}],"year":"2010b","references":["/references/6"]},{"style":0,"text":"Burkett et al. (2010b)","origin":{"pointer":"/sections/5/paragraphs/13","offset":303,"length":22},"authors":[{"last":"Burkett"},{"last":"al."}],"year":"2010b","references":["/references/6"]},{"style":0,"text":"DeNero and Macherey (2011)","origin":{"pointer":"/sections/8/paragraphs/4","offset":423,"length":26},"authors":[{"last":"DeNero"},{"last":"Macherey"}],"year":"2011","references":["/references/11"]},{"style":0,"text":"Huang et al. (2009)","origin":{"pointer":"/sections/9/paragraphs/0","offset":132,"length":19},"authors":[{"last":"Huang"},{"last":"al."}],"year":"2009","references":["/references/18"]},{"style":0,"text":"Li et al. (2012)","origin":{"pointer":"/sections/9/paragraphs/0","offset":235,"length":16},"authors":[{"last":"Li"},{"last":"al."}],"year":"2012","references":["/references/21"]},{"style":0,"text":"Burkett et al. (2010b)","origin":{"pointer":"/sections/9/paragraphs/0","offset":1010,"length":22},"authors":[{"last":"Burkett"},{"last":"al."}],"year":"2010b","references":["/references/6"]},{"style":0,"text":"Burkett et al. (2010b)","origin":{"pointer":"/sections/9/paragraphs/0","offset":1517,"length":22},"authors":[{"last":"Burkett"},{"last":"al."}],"year":"2010b","references":["/references/6"]},{"style":0,"text":"Riedel and McCallum (2011)","origin":{"pointer":"/sections/9/paragraphs/1","offset":63,"length":26},"authors":[{"last":"Riedel"},{"last":"McCallum"}],"year":"2011","references":["/references/26"]},{"style":0,"text":"Martins et al. (2011b)","origin":{"pointer":"/sections/9/paragraphs/1","offset":383,"length":22},"authors":[{"last":"Martins"},{"last":"al."}],"year":"2011b","references":["/references/25"]},{"style":0,"text":"Martins et al., 2011a","origin":{"pointer":"/sections/9/paragraphs/1","offset":643,"length":21},"authors":[{"last":"Martins"},{"last":"al."}],"year":"2011a","references":["/references/24"]},{"style":0,"text":"Sutton and McCallum, 2004","origin":{"pointer":"/sections/9/paragraphs/2","offset":178,"length":25},"authors":[{"last":"Sutton"},{"last":"McCallum"}],"year":"2004","references":["/references/30"]},{"style":0,"text":"Bunescu and Mooney, 2004","origin":{"pointer":"/sections/9/paragraphs/2","offset":205,"length":24},"authors":[{"last":"Bunescu"},{"last":"Mooney"}],"year":"2004","references":["/references/4"]},{"style":0,"text":"Finkel et al., 2005","origin":{"pointer":"/sections/9/paragraphs/2","offset":231,"length":19},"authors":[{"last":"Finkel"},{"last":"al."}],"year":"2005","references":["/references/13"]},{"style":0,"text":"Rush et al. (2012)","origin":{"pointer":"/sections/9/paragraphs/2","offset":268,"length":18},"authors":[{"last":"Rush"},{"last":"al."}],"year":"2012","references":["/references/29"]},{"style":0,"text":"Chieu and Teow (2012)","origin":{"pointer":"/sections/9/paragraphs/2","offset":291,"length":21},"authors":[{"last":"Chieu"},{"last":"Teow"}],"year":"2012","references":["/references/8"]},{"style":0,"text":"Liang et al., 2006","origin":{"pointer":"/sections/9/paragraphs/3","offset":164,"length":18},"authors":[{"last":"Liang"},{"last":"al."}],"year":"2006","references":["/references/22"]},{"style":0,"text":"DeNero and Macherey, 2011","origin":{"pointer":"/sections/9/paragraphs/3","offset":184,"length":25},"authors":[{"last":"DeNero"},{"last":"Macherey"}],"year":"2011","references":["/references/11"]},{"style":0,"text":"Graca̧ et al. (2008)","origin":{"pointer":"/sections/9/paragraphs/3","offset":446,"length":20},"authors":[{"last":"Graca̧"},{"last":"al."}],"year":"2008","references":["/references/14"]},{"style":0,"text":"Cromières and Kurohashi (2009)","origin":{"pointer":"/sections/9/paragraphs/3","offset":468,"length":30},"authors":[{"last":"Cromières"},{"last":"Kurohashi"}],"year":"2009","references":["/references/9"]},{"style":0,"text":"Burkett et al. (2010a)","origin":{"pointer":"/sections/9/paragraphs/4","offset":0,"length":22},"authors":[{"last":"Burkett"},{"last":"al."}],"year":"2010a","references":["/references/5"]},{"style":0,"text":"Chen et al. (2010)","origin":{"pointer":"/sections/9/paragraphs/5","offset":0,"length":18},"authors":[{"last":"Chen"},{"last":"al."}],"year":"2010","references":["/references/7"]}]}
