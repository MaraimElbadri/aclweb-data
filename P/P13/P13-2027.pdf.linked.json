{"sections":[{"title":"","paragraphs":["Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 148–152, Sofia, Bulgaria, August 4-9 2013. c⃝2013 Association for Computational Linguistics"]},{"title":"Are Semantically Coherent Topic Models Useful for Ad Hoc Information Retrieval? Romain Deveaud Eric SanJuan University of Avignon - LIA Avignon, France","paragraphs":["romain.deveaud@univ-avignon.fr eric.sanjuan@univ-avignon.fr"]},{"title":"Patrice Bellot Aix-Marseille University - LSIS Marseille, France","paragraphs":["patrice.bellot@lsis.org"]},{"title":"Abstract","paragraphs":["The current topic modeling approaches for Information Retrieval do not allow to explicitly model query-oriented latent topics. More, the semantic coherence of the topics has never been considered in this field. We propose a model-based feedback approach that learns Latent Dirichlet Allocation topic models on the top-ranked pseudo-relevant feedback, and we measure the semantic coherence of those topics. We perform a first experimental evaluation using two major TREC test collections. Results show that retrieval performances tend to be better when using topics with higher semantic coherence."]},{"title":"1 Introduction","paragraphs":["Representing documents as mixtures of “topics” has always been a challenge and an objective for researchers working in text-related fields. Based on the words used within a document, topic models learn topic level relations by assuming that the document covers a small set of concepts. Learning the topics from a document collection can help to extract high level semantic information, and help humans to understand the meaning of documents. Latent Semantic Indexing (Deerwester et al., 1990) (LSI), probabilistic Latent Semantic Analysis (Hofmann, 2001) (pLSA) and Latent Dirichlet Allocation (Blei et al., 2003) (LDA) are the most famous approaches that tried to tackle this problem throughout the years. Topics produced by these methods are generally fancy and appealing, and often correlate well with human concepts. This is one of the reasons of the intensive use of topic models (and especially LDA) in current research in Natural Language Processing (NLP) related areas.","One main problem in ad hoc Information Retrieval (IR) is the difficulty for users to translate a complex information need into a keyword query. The most popular and effective approach to overcome this problem is to improve the representation of the query by adding query-related “concepts”. This approach mostly relies on pseudo-relevance feedback, where these so-called “concepts” are the most frequent words occurring in the top-ranked documents retrieved by the retrieval system (Lavrenko and Croft, 2001). From that perspective, topic models seem attractive in the sense that they can provide a descriptive and intuitive representation of concepts. But how can we quantify the usefulness of these topics with respect to an IR system? Recently, researchers developed measures which evaluate the semantic coherence of topic models (Newman et al., 2010; Mimno et al., 2011; Stevens et al., 2012). We adopt their view of semantic coherence and apply one of these measures to query-oriented topics.","Several studies concentrated on improving the quality of document ranking using topic models, especially probabilistic ones. The approach by Wei and Croft (2006) was the first to leverage LDA topics to improve the estimate of document language models and achieved good empirical results. Following this pioneering work, several studies explored the use of pLSA and LDA under different experimental settings (Park and Ramamohanarao, 2009; Yi and Allan, 2009; Andrzejewski and Buttler, 2011; Lu et al., 2011). The reported results suggest that the words and the probability distributions learned by probabilistic topic models are effective for query expansion. The main drawback of these approaches is that topics are learned on the whole target document collection prior to retrieval, thus leading to a static topical representation of the collection. Depending on the query and on its specificity, topics may either be too coarse or too fine to accurately represent the latent concepts of the query. Recently, Ye et al. (2011) proposed a method which uses 148 LDA and learns topics directly on a limited set of documents. While this approach is a first step towards modeling query-oriented topics, it lacks some theoretic principles and only aims to heuristically construct a “best” topic (from all learned topics) before expanding the query with its most probable words. More, none of the aforementioned works studied the semantic coherence of those generated topics. We tackle these issues by making the following contributions:","• we introduce Topic-Driven Relevance Models, a model-based feedback approach (Zhai and Lafferty, 2001) for integrating topic models into relevance models by learning topics on pseudo-relevant feedback documents (as opposed to the entire document collection),","• we explore the coherence of those generated topics using the queries of two major and well-established TREC test collections,","• we evaluate the effects coherent topics have on ad hoc IR using the same test collections."]},{"title":"2 Topic-Driven Relevance Models 2.1 Relevance Models","paragraphs":["The goal of relevance models is to improve the representation of a query Q by selecting terms from a set of initially retrieved documents (Lavrenko and Croft, 2001). As the concentration of relevant documents is usually higher in the top ranks of the ranking list, this is constituted by a number N of top-ranked documents. Relevance models usually perform better when combined with the original query model (or maximum likelihood estimate). Let θ̃Q be this maximum likelihood query estimate and θ̂Q a relevance model, the updated new query model is given by: P (w|θQ) = λ P (w|θ̃Q) + (1 − λ)P (w|θ̂Q) (1) where λ ∈ [0, 1] is a parameter that controls the tradeoff between the original query model and the relevance model. One of the most robust variants of the relevance models is computed as follows: P (w|θ̂Q) ∝ ∑ θD∈Θ P (θD)P (w|θD) ∏ t∈Q P (t|θD)","(2) where Θ is a set of pseudo-relevant feedback documents and θD is the language model of document D. This notion of estimating a query model is often referred to as model-based feedback (Zhai and Lafferty, 2001). We assume P (θD) to be uniform, resulting in an estimated relevance model based on a sum of document models weighted by the query likelihood score. The final, interpolated, estimate expressed in equation (1) is often referred in the literature as RM3. We tackle the null probabilities problem by smoothing the document language model using the well-known Dirichlet smoothing (Zhai and Lafferty, 2004). 2.2 LDA-based Feedback Model The estimation of the feedback model θ̂Q constitutes the first contribution of this work. We propose to explicitly model the latent topics (or concepts) that exist behind an information need, and to use them to improve the query representation. We consider Θ as the set of pseudo-relevant feedback documents from which the latent concepts would be extracted. The retrieval algorithm used to obtain these documents can be of any kind, the important point is that Θ is a reduced collection that contains the top documents ranked by an automatic and state-of-the-art retrieval process.","Instead of viewing Θ as a set of document language models that are likely to contain topical information about the query, we take a probabilistic topic modeling approach. We specifically focus on Latent Dirichlet Allocation (LDA), since it is currently one of the most representative. In LDA, each topic multinomial distribution φk is generated by a conjugate Dirichlet prior with parameter β, while each document multinomial distribution θd is generated by a conjugate Dirichlet prior with parameter α. In other words, θd,k is the probability of topic k occurring in document D (i.e. P (k|D)). Respectively, φk,w is the probability of word w belonging to topic k (i.e. P (w|k)). We use variational inference implemented in the LDA-C software1","to overcome intractability issues (Blei et al., 2003; Griffiths and Steyvers, 2004). Under this setting, we compute the topic-driven estimation of the query model using the following equation: P (w|θ̂Q) ∝ ∑ θD∈Θ ( P (θD)P (w|θD) PT M (w|D) ∏ t∈Q P (t|θD)) (3) where PT M (w|D) is the probability of word w occurring in document D using the previously 1","www.cs.princeton.edu/b̃lei/lda-c 149 5 10 20 30 40 50 9.4 9.6 9.8 10.0 10.2 Coherence Number of feedback documents","Number of topics 3 5 10 15 20 WT10g 5 10 20 30 40 50 9.4 9.6 9.8 10.0 10.2 Coherence Number of feedback documents","Number of topics 3 5 10 15 20 Robust04 Figure 1: Semantic coherence of the topic models for different values of K, in function of the number N of feedback documents. learned multinomial distributions. Let TΘ be a topic model learned on the Θ set of feedback documents, this probability is given by: PT M (w|D) = ∑ k∈TΘ φk,w · θD,k (4) High probabilities are thus given to words that are important in topic k, when k is an important topic in document D. In the remainder of this paper, we refer to this general approach as TDRM for Topic-Driven Relevance Models.","2.3 Measuring the coherence of query-oriented topics TDRM relies on two important parameters: the number of topics K that we want to learn, and the number of feedback documents N from which LDA learns the topics. Varying these two parameters can help to capture more information and to model finer topics, but how about their global semantic coherence?","Term similarities measured in restricted do-mains was the first step for evaluating semantic coherence (Gliozzo et al., 2007), and was a first basis for the development of several topic coherence evaluation measures (Newman et al., 2010). Computing the Pointwise Mutual Information (PMI) of all word pairs over Wikipedia was found to be an effective metric using news and books corpora. Recently, Stevens et al. (2012) used (among others) an aggregate version of this metric to evaluate large amounts of topic models. We use this method to evaluate the coherence of query-oriented topics. Specifically, the coherence","of a topic model T K Θ composed of K topics is:","c(T K Θ ) = 1 K K ∑ i=1 ∑","(w,w′ )∈ki log","P (w, w′",") + ε","P (w)P (w′",") (5) where probabilities of word occurrences and cooccurrences are estimated using an external reference corpus. Following Newman et al. (2010), we use Wikipedia to compute PMI and set ε = 1 as in (Stevens et al., 2012)."]},{"title":"3 Evaluation 3.1 Experimental setup","paragraphs":["We performed our evaluation using two main TREC2","collections: Robust04 and WT10g. Robust04 is composed 528,155 of news articles coming from three newspapers and the FBIS. It supported the TREC 2004 Robust track, from which we used the 250 query topics (numbers: 301-450, 601-700). The WT10g collection is composed of 1,692,096 web pages, and supported the TREC Web track for four years (2001-2004). We focus on the 2000 and 2001 ad-hoc query topics (numbers: 451-550). We used the open-source index-ing and retrieval system Indri3","to run our experiments. We indexed the two collections with the exact same parameters: tokens were stemmed with the well-known light Krovetz stemmer and stop-words were removed using the standard English stoplist embedded with Indri (417 words). 3.2 Semantic coherence evaluation","Most coherent topics are composed of rare words","that do not often occur in the reference corpus, but 2 trec.nist.gov 3 lemurproject.org/indri.php 150 0.200 0.205 0.210 0.215 0.220 5 10 20 30 40 50 MAP Number of feedback documents l l l l l l l l","l l l l l l","Number of topics 3 5 10 15 20 RM3 WT10g 0.260 0.265 0.270 0.275 0.280 0.285 0.290 5 10 20 30 40 50 MAP Number of feedback documents l l l l","l l","l l l l l l l l","Number of topics 3 5 10 15 20 RM3 Robust04 Figure 2: Retrieval performance in terms of Mean Average Precision (MAP) of the TDRM approach. Each line represent a different number of topics K, and the performance are reported in function the number N of feedback documents. The black, plain line represents the RM3 baseline. co-occur at lot together. We see on Figure 1 that very coherent topics are identified in the top 5 and 10 feedback documents for the WT10g collection, suggesting that closely related documents are retrieved in the top ranks. Results are quite different on the Robust04 collection, where topic models with 20 topics on 5 documents are the least coherent. However, when looking at the Robust04 documents, we see that they are on average almost twice smaller than the WT10g web pages. We hypothesize that the heterogeneous nature of the web allows to model very different topics covering several aspects of the query, while news articles are contributions focused on a single subject.","Overall, the more coherent topic models contain a reasonable amount of topics (10-15), thus allow-ing to fit with variable amounts of documents. The attentive reader will notice that the topic coherence scores are very high compared to those previously reported in the literature (Stevens et al., 2012). The TDRM approach captures topics that are centered around a specific information need, often with a limited vocabulary, which favors word co-occurrence. On the other hand, topics learned on entire collections are coarser than ours, which leads to lower coherence scores. 3.3 Document retrieval results Since TDRM is based on Relevance Models (Lavrenko and Croft, 2001), we take the RM3 approach presented in Section 2.1 as baseline. The λ parameter is common between RM3 and TDRM and is determined for each query using leave-one-query-out cross-validation (that is: learn the best parameter setting for all queries but one, and evaluate the held-out query using the previously learned parameter).","We report ad hoc document retrieval performances in Figure 2. We noticed in the previous section that the most coherent topic models were modeled using 5 feedback documents and 20 topics for the WT10g collection, and this parameter combination also achieves the best retrieval results. Overall, using 10, 15 or 20 topics allow it to achieve high and similar performance from 5 to 20 documents. We observe than using 20 topics for the Robust04 collection consistently achieves the highest results, with the topic model coherence growing as the number of feedback documents in-creases. Although topics coming from news articles may be limited, they benefit from the rich vocabulary of professional writers who are trained to avoid repetition. Their use of synonyms allows TDRM to model deep topics, with a comprehensive description of query aspects. Since synonyms are less likely to co-occur in encyclopedic articles like Wikipedia, we think that, in our case, the semantic coherence measure could be more accurate using other textual resources. This measure seems however to be effective when dealing with heterogeneously structured documents."]},{"title":"4 Conclusions & Future Work","paragraphs":["Overall, modeling query-oriented topic models and estimating the feedback query model using these topics greatly improves ad hoc Information Retrieval, compared to state-of-the-art relevance models. While semantically coherent topic mod-151 els do not seem to be effective in the context of a news articles search task, they are a good indicator of effectiveness in the context of web search. Measuring the semantic coherence of query topics could help predict query effectiveness or even choose the best query-representative topic model."]},{"title":"Acknowledgments","paragraphs":["This work was supported by the French Agency for Scientific Research (Agence Nationale de la Recherche) under CAAS project (ANR 2010 CORD 001 02)."]},{"title":"References","paragraphs":["David Andrzejewski and David Buttler. 2011. Latent Topic Feedback for Information Retrieval. In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data min-ing, KDD ’11, pages 600–608.","David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet Allocation. Journal of Machine Learning Research, 3:993–1022.","Scott Deerwester, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, and Richard Harshman. 1990. Indexing by Latent Semantic Analysis. Journal of the American Society for Information Science, 41(6):391–407.","Alfio Massimiliano Gliozzo, Marco Pennacchiotti, and Patrick Pantel. 2007. The Domain Restriction Hypothesis: Relating Term Similarity and Semantic Consistency. In Human Language Technologies: The 2007 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 131–138.","Thomas L Griffiths and Mark Steyvers. 2004. Finding scientific topics. Proceedings of the National Academy of Sciences of the United States of America, 101 Suppl.","Thomas Hofmann. 2001. Unsupervised Learning by Probabilistic Latent Semantic Analysis. Machine Learning, 42:177–196.","Victor Lavrenko and W. Bruce Croft. 2001. Relevance-Based Language Models. In Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’01, pages 120–127.","Yue Lu, Qiaozhu Mei, and ChengXiang Zhai. 2011. Investigating task performance of probabilistic topic models: an empirical study of PLSA and LDA. Information Retrieval, 14:178–203.","David Mimno, Hanna M. Wallach, Edmund Talley, Miriam Leenders, and Andrew McCallum. 2011. Optimizing Semantic Coherence in Topic Models. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11, pages 262–272.","David Newman, Jey Han Lau, Karl Grieser, and Timothy Baldwin. 2010. Automatic Evaluation of Topic Coherence. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 100–108.","Laurence A. Park and Kotagiri Ramamohanarao. 2009. The Sensitivity of Latent Dirichlet Allocation for Information Retrieval. In Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases, ECML PKDD ’09, pages 176–188.","Keith Stevens, Philip Kegelmeyer, David Andrzejewski, and David Buttler. 2012. Exploring Topic Coherence over Many Models and Many Topics. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ’12, pages 952–961.","Xing Wei and W. Bruce Croft. 2006. LDA-based Document Models for Ad-hoc Retrieval. In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’06, pages 178–185.","Zheng Ye, Jimmy Xiangji Huang, and Hongfei Lin. 2011. Finding a Good Query-Related Topic for Boosting Pseudo-Relevance Feedback. JASIST, 62(4):748–760.","Xing Yi and James Allan. 2009. A Comparative Study of Utilizing Topic Models for Information Retrieval. In Proceedings of the 31th European Conference on IR Research on Advances in Information Retrieval, ECIR ’09, pages 29–41. Springer-Verlag.","Chengxiang Zhai and John Lafferty. 2001. Model-based Feedback in the Language Modeling Approach to Information Retrieval. In Proceedings of the Tenth International Conference on Information and Knowledge Management, CIKM ’01, pages 403–410.","Chengxiang Zhai and John Lafferty. 2004. A Study of Smoothing Methods for Language Models Applied to Information Retrieval. ACM Transactions on Information Systems, 22(2):179–214. 152"]}],"references":[{"authors":[{"first":"David","last":"Andrzejewski"},{"first":"David","last":"Buttler"}],"year":"2011","title":"Latent Topic Feedback for Information Retrieval","source":"David Andrzejewski and David Buttler. 2011. Latent Topic Feedback for Information Retrieval. In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data min-ing, KDD ’11, pages 600–608."},{"authors":[{"first":"David","middle":"M.","last":"Blei"},{"first":"Andrew","middle":"Y.","last":"Ng"},{"first":"Michael I","middle":".","last":"Jordan"}],"year":"2003","title":"Latent Dirichlet Allocation","source":"David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet Allocation. Journal of Machine Learning Research, 3:993–1022."},{"authors":[{"first":"Scott","last":"Deerwester"},{"first":"Susan","middle":"T.","last":"Dumais"},{"first":"George","middle":"W.","last":"Furnas"},{"first":"Thomas","middle":"K.","last":"Landauer"},{"first":"Richard","last":"Harshman"}],"year":"1990","title":"Indexing by Latent Semantic Analysis","source":"Scott Deerwester, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, and Richard Harshman. 1990. Indexing by Latent Semantic Analysis. Journal of the American Society for Information Science, 41(6):391–407."},{"authors":[{"first":"Alfio","middle":"Massimiliano","last":"Gliozzo"},{"first":"Marco","last":"Pennacchiotti"},{"first":"Patrick","last":"Pantel"}],"year":"2007","title":"The Domain Restriction Hypothesis: Relating Term Similarity and Semantic Consistency","source":"Alfio Massimiliano Gliozzo, Marco Pennacchiotti, and Patrick Pantel. 2007. The Domain Restriction Hypothesis: Relating Term Similarity and Semantic Consistency. In Human Language Technologies: The 2007 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 131–138."},{"authors":[{"first":"Thomas","middle":"L","last":"Griffiths"},{"first":"Mark","last":"Steyvers"}],"year":"2004","title":"Finding scientific topics","source":"Thomas L Griffiths and Mark Steyvers. 2004. Finding scientific topics. Proceedings of the National Academy of Sciences of the United States of America, 101 Suppl."},{"authors":[{"first":"Thomas","last":"Hofmann"}],"year":"2001","title":"Unsupervised Learning by Probabilistic Latent Semantic Analysis","source":"Thomas Hofmann. 2001. Unsupervised Learning by Probabilistic Latent Semantic Analysis. Machine Learning, 42:177–196."},{"authors":[{"first":"Victor","last":"Lavrenko"},{"first":"W.","middle":"Bruce","last":"Croft"}],"year":"2001","title":"Relevance-Based Language Models","source":"Victor Lavrenko and W. Bruce Croft. 2001. Relevance-Based Language Models. In Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’01, pages 120–127."},{"authors":[{"first":"Yue","last":"Lu"},{"first":"Qiaozhu","last":"Mei"},{"first":"ChengXiang","last":"Zhai"}],"year":"2011","title":"Investigating task performance of probabilistic topic models: an empirical study of PLSA and LDA","source":"Yue Lu, Qiaozhu Mei, and ChengXiang Zhai. 2011. Investigating task performance of probabilistic topic models: an empirical study of PLSA and LDA. Information Retrieval, 14:178–203."},{"authors":[{"first":"David","last":"Mimno"},{"first":"Hanna","middle":"M.","last":"Wallach"},{"first":"Edmund","last":"Talley"},{"first":"Miriam","last":"Leenders"},{"first":"Andrew","last":"McCallum"}],"year":"2011","title":"Optimizing Semantic Coherence in Topic Models","source":"David Mimno, Hanna M. Wallach, Edmund Talley, Miriam Leenders, and Andrew McCallum. 2011. Optimizing Semantic Coherence in Topic Models. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11, pages 262–272."},{"authors":[{"first":"David","last":"Newman"},{"first":"Jey","middle":"Han","last":"Lau"},{"first":"Karl","last":"Grieser"},{"first":"Timothy","last":"Baldwin"}],"year":"2010","title":"Automatic Evaluation of Topic Coherence","source":"David Newman, Jey Han Lau, Karl Grieser, and Timothy Baldwin. 2010. Automatic Evaluation of Topic Coherence. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 100–108."},{"authors":[{"first":"Laurence","middle":"A.","last":"Park"},{"first":"Kotagiri","last":"Ramamohanarao"}],"year":"2009","title":"The Sensitivity of Latent Dirichlet Allocation for Information Retrieval","source":"Laurence A. Park and Kotagiri Ramamohanarao. 2009. The Sensitivity of Latent Dirichlet Allocation for Information Retrieval. In Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases, ECML PKDD ’09, pages 176–188."},{"authors":[{"first":"Keith","last":"Stevens"},{"first":"Philip","last":"Kegelmeyer"},{"first":"David","last":"Andrzejewski"},{"first":"David","last":"Buttler"}],"year":"2012","title":"Exploring Topic Coherence over Many Models and Many Topics","source":"Keith Stevens, Philip Kegelmeyer, David Andrzejewski, and David Buttler. 2012. Exploring Topic Coherence over Many Models and Many Topics. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ’12, pages 952–961."},{"authors":[{"first":"Xing","last":"Wei"},{"first":"W.","middle":"Bruce","last":"Croft"}],"year":"2006","title":"LDA-based Document Models for Ad-hoc Retrieval","source":"Xing Wei and W. Bruce Croft. 2006. LDA-based Document Models for Ad-hoc Retrieval. In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’06, pages 178–185."},{"authors":[{"first":"Zheng","last":"Ye"},{"first":"Jimmy","middle":"Xiangji","last":"Huang"},{"first":"Hongfei","last":"Lin"}],"year":"2011","title":"Finding a Good Query-Related Topic for Boosting Pseudo-Relevance Feedback","source":"Zheng Ye, Jimmy Xiangji Huang, and Hongfei Lin. 2011. Finding a Good Query-Related Topic for Boosting Pseudo-Relevance Feedback. JASIST, 62(4):748–760."},{"authors":[{"first":"Xing","last":"Yi"},{"first":"James","last":"Allan"}],"year":"2009","title":"A Comparative Study of Utilizing Topic Models for Information Retrieval","source":"Xing Yi and James Allan. 2009. A Comparative Study of Utilizing Topic Models for Information Retrieval. In Proceedings of the 31th European Conference on IR Research on Advances in Information Retrieval, ECIR ’09, pages 29–41. Springer-Verlag."},{"authors":[{"first":"Chengxiang","last":"Zhai"},{"first":"John","last":"Lafferty"}],"year":"2001","title":"Model-based Feedback in the Language Modeling Approach to Information Retrieval","source":"Chengxiang Zhai and John Lafferty. 2001. Model-based Feedback in the Language Modeling Approach to Information Retrieval. In Proceedings of the Tenth International Conference on Information and Knowledge Management, CIKM ’01, pages 403–410."},{"authors":[{"first":"Chengxiang","last":"Zhai"},{"first":"John","last":"Lafferty"}],"year":"2004","title":"A Study of Smoothing Methods for Language Models Applied to Information Retrieval","source":"Chengxiang Zhai and John Lafferty. 2004. A Study of Smoothing Methods for Language Models Applied to Information Retrieval. ACM Transactions on Information Systems, 22(2):179–214. 152"}],"cites":[{"style":0,"text":"Deerwester et al., 1990","origin":{"pointer":"/sections/4/paragraphs/0","offset":468,"length":23},"authors":[{"last":"Deerwester"},{"last":"al."}],"year":"1990","references":["/references/2"]},{"style":0,"text":"Hofmann, 2001","origin":{"pointer":"/sections/4/paragraphs/0","offset":540,"length":13},"authors":[{"last":"Hofmann"}],"year":"2001","references":["/references/5"]},{"style":0,"text":"Blei et al., 2003","origin":{"pointer":"/sections/4/paragraphs/0","offset":595,"length":17},"authors":[{"last":"Blei"},{"last":"al."}],"year":"2003","references":["/references/1"]},{"style":0,"text":"Lavrenko and Croft, 2001","origin":{"pointer":"/sections/4/paragraphs/1","offset":483,"length":24},"authors":[{"last":"Lavrenko"},{"last":"Croft"}],"year":"2001","references":["/references/6"]},{"style":0,"text":"Newman et al., 2010","origin":{"pointer":"/sections/4/paragraphs/1","offset":834,"length":19},"authors":[{"last":"Newman"},{"last":"al."}],"year":"2010","references":["/references/9"]},{"style":0,"text":"Mimno et al., 2011","origin":{"pointer":"/sections/4/paragraphs/1","offset":855,"length":18},"authors":[{"last":"Mimno"},{"last":"al."}],"year":"2011","references":["/references/8"]},{"style":0,"text":"Stevens et al., 2012","origin":{"pointer":"/sections/4/paragraphs/1","offset":875,"length":20},"authors":[{"last":"Stevens"},{"last":"al."}],"year":"2012","references":["/references/11"]},{"style":0,"text":"Wei and Croft (2006)","origin":{"pointer":"/sections/4/paragraphs/2","offset":141,"length":20},"authors":[{"last":"Wei"},{"last":"Croft"}],"year":"2006","references":["/references/12"]},{"style":0,"text":"Park and Ramamohanarao, 2009","origin":{"pointer":"/sections/4/paragraphs/2","offset":408,"length":28},"authors":[{"last":"Park"},{"last":"Ramamohanarao"}],"year":"2009","references":["/references/10"]},{"style":0,"text":"Yi and Allan, 2009","origin":{"pointer":"/sections/4/paragraphs/2","offset":438,"length":18},"authors":[{"last":"Yi"},{"last":"Allan"}],"year":"2009","references":["/references/14"]},{"style":0,"text":"Andrzejewski and Buttler, 2011","origin":{"pointer":"/sections/4/paragraphs/2","offset":458,"length":30},"authors":[{"last":"Andrzejewski"},{"last":"Buttler"}],"year":"2011","references":["/references/0"]},{"style":0,"text":"Lu et al., 2011","origin":{"pointer":"/sections/4/paragraphs/2","offset":490,"length":15},"authors":[{"last":"Lu"},{"last":"al."}],"year":"2011","references":["/references/7"]},{"style":0,"text":"Ye et al. (2011)","origin":{"pointer":"/sections/4/paragraphs/2","offset":1010,"length":16},"authors":[{"last":"Ye"},{"last":"al."}],"year":"2011","references":["/references/13"]},{"style":0,"text":"Zhai and Lafferty, 2001","origin":{"pointer":"/sections/4/paragraphs/3","offset":79,"length":23},"authors":[{"last":"Zhai"},{"last":"Lafferty"}],"year":"2001","references":["/references/15"]},{"style":0,"text":"Lavrenko and Croft, 2001","origin":{"pointer":"/sections/5/paragraphs/0","offset":139,"length":24},"authors":[{"last":"Lavrenko"},{"last":"Croft"}],"year":"2001","references":["/references/6"]},{"style":0,"text":"Zhai and Lafferty, 2001","origin":{"pointer":"/sections/5/paragraphs/1","offset":189,"length":23},"authors":[{"last":"Zhai"},{"last":"Lafferty"}],"year":"2001","references":["/references/15"]},{"style":0,"text":"Zhai and Lafferty, 2004","origin":{"pointer":"/sections/5/paragraphs/1","offset":591,"length":23},"authors":[{"last":"Zhai"},{"last":"Lafferty"}],"year":"2004","references":["/references/16"]},{"style":0,"text":"Blei et al., 2003","origin":{"pointer":"/sections/5/paragraphs/3","offset":35,"length":17},"authors":[{"last":"Blei"},{"last":"al."}],"year":"2003","references":["/references/1"]},{"style":0,"text":"Griffiths and Steyvers, 2004","origin":{"pointer":"/sections/5/paragraphs/3","offset":54,"length":28},"authors":[{"last":"Griffiths"},{"last":"Steyvers"}],"year":"2004","references":["/references/4"]},{"style":0,"text":"Gliozzo et al., 2007","origin":{"pointer":"/sections/5/paragraphs/8","offset":104,"length":20},"authors":[{"last":"Gliozzo"},{"last":"al."}],"year":"2007","references":["/references/3"]},{"style":0,"text":"Newman et al., 2010","origin":{"pointer":"/sections/5/paragraphs/8","offset":217,"length":19},"authors":[{"last":"Newman"},{"last":"al."}],"year":"2010","references":["/references/9"]},{"style":0,"text":"Stevens et al. (2012)","origin":{"pointer":"/sections/5/paragraphs/8","offset":397,"length":21},"authors":[{"last":"Stevens"},{"last":"al."}],"year":"2012","references":["/references/11"]},{"style":0,"text":"Newman et al. (2010)","origin":{"pointer":"/sections/5/paragraphs/15","offset":124,"length":20},"authors":[{"last":"Newman"},{"last":"al."}],"year":"2010","references":["/references/9"]},{"style":0,"text":"Stevens et al., 2012","origin":{"pointer":"/sections/5/paragraphs/15","offset":199,"length":20},"authors":[{"last":"Stevens"},{"last":"al."}],"year":"2012","references":["/references/11"]},{"style":0,"text":"Stevens et al., 2012","origin":{"pointer":"/sections/6/paragraphs/10","offset":281,"length":20},"authors":[{"last":"Stevens"},{"last":"al."}],"year":"2012","references":["/references/11"]},{"style":0,"text":"Lavrenko and Croft, 2001","origin":{"pointer":"/sections/6/paragraphs/10","offset":648,"length":24},"authors":[{"last":"Lavrenko"},{"last":"Croft"}],"year":"2001","references":["/references/6"]}]}
