{"sections":[{"title":"","paragraphs":["Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 212–216, Sofia, Bulgaria, August 4-9 2013. c⃝2013 Association for Computational Linguistics"]},{"title":"Latent Semantic Matching: Application to Cross-language Text Categorization without Alignment Information Tsutomu Hirao and Tomoharu Iwata and Masaaki Nagata NTT Communication Science Laboratories, NTT Corporation 2-4, Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237, Japan {hirao.tsutomu,iwata.tomoharu,nagata.masaaki}@lab.ntt.co.jp Abstract","paragraphs":["Unsupervised object matching (UOM) is a promising approach to cross-language natural language processing such as bilingual lexicon acquisition, parallel corpus construction, and cross-language text categorization, because it does not require labor-intensive linguistic resources. However, UOM only finds one-to-one correspondences from data sets with the same number of instances in source and target domains, and this prevents us from applying UOM to real-world cross-language natural language processing tasks. To alleviate these limitations, we proposes latent semantic matching, which embeds objects in both source and target language domains into a shared latent topic space. We demonstrate the effectiveness of our method on cross-language text categorization. The results show that our method outperforms conventional unsupervised object matching methods."]},{"title":"1 Introduction","paragraphs":["Unsupervised object matching is a method for finding one-to-one correspondences between objects across different domains without knowledge about the relation between the domains. Kernelized sorting (Novi et al., 2010) and canonical correlation analysis based methods (Haghighi et al., 2008; Tripathi et al., 2010) are two such examples of unsupervised object matching, which have been shown to be quite useful for cross-language natural language processing (NLP) tasks. One of the most important properties of the unsupervised object matching is that it does not require any linguistic resources which connects between the languages. This distinguishes it from other cross-language NLP methods such as machine translation based and projection based approaches (Dumais et al., 1996; Gliozzo and Strapparava, 2005; Platt et al., 2010), which we need bilingual dictionaries or parallel sentences.","When we apply unsupervised object matching methods to cross-language NLP tasks, there are two critical problems. The first is that they only find one-to-one matching. The second is they require the same size of source- and target-data. For example, the correct translation of a word is not always unique. French words ‘ maison’, ‘ appart-ment’ and ‘ domicile’ can be regarded as translation of an English word ‘home’. In addition, English vocabulary size is not equal to that of French.","These discussions motivate us to introduce a shared space in which both source and target domain objects will reside. If we can obtain such a shared space, we can match objects within the space, because we can use standard distance metrics on this space. This will also enable us to use various kinds of non-strict matching. For example, k-nearest objects in the source domain will be retrieved for a query object in the target domain. In this paper, we propose a simple but effective method to find the shared space by assuming that two languages have common latent topics, which we call latent semantic matching. With latent semantic matching, we first find latent topics in two domains independently. Then, the topics in two domains are aligned by kernelized sorting, and objects are embedded in a shared latent topic space. Latent topic representations are successfully used in a wide range of NLP tasks, such as information retrieval and text classification, because they represent intrinsic information of documents (Deerwester et al., 1990). By matching latent topics, we can find relation between source and target domains, and additionally we can handle different numbers of objects in two domains.","We compared latent semantic matching with conventional unsupervised object matching meth-212 ods on the task of cross-language text categorization, i.e. classifying target side unlabeled documents by label information obtained from source side documents. The results show that, with more source side documents, our method achieved the highest classification accuracy."]},{"title":"2 Related work","paragraphs":["Many cross-language text processing methods have been proposed that require correspondences between source and target languages. For example, (Dumais et al., 1996) proposed cross-lingual latent semantic indexing, and (Platt et al., 2010) employed oriented principle component analysis and canonical correlation analysis (CCA). They concatenate the document pairs (source document and its translation) obtained from a document-level parallel corpus. They then apply multivariate analysis to acquire the translingual projection. There are extensions of latent Dirichlet allocation (LDA) (Blei et al., 2003) for cross-language analysis, such as multilingual topic models (Boyd-Graber and Blei, 2009), joint LDA (Jagadeesh and Daume III, 2010) and multilingual LDA (Xiaochuan et al., 2011). They require a bilingual dictionary or document-level parallel corpora.","Unsupervised object matching methods have been proposed recently (Novi et al., 2010; Haghighi et al., 2008; Yamada and Sugiyama, 2011). These methods are promising in terms of language portability because they do not require external language resources. (Novi et al., 2010) proposed kernelized sorting (KS); it finds one-to-one correspondences between objects in different domains by permuting a set to maximize the dependence between two sets. Here, the Hilbert-Schmidt independence criterion is used for measuring dependence. (Djuric et al., 2012) proposed convex kernelized sorting as an extension of KS. (Yamada and Sugiyama, 2011) proposed leastsquares object matching which maximizes the squared-loss mutual information between matched pairs. (Haghighi et al., 2008) proposed another framework, matching CCA (MCCA), based on a probabilistic interpretation of CCA (Bach and Jordan, 2005). MCCA simultaneously finds latent variables that represent correspondences and latent features so that the latent features of corresponding examples exhibit the maximum correlation. However, these unsupervised object matching methods have limitations. They require that the source and target domains have the same data size, and they find one-to-one correspondences. There are critical weaknesses of these methods when we attempt to apply them to real world cross-language NLP applications."]},{"title":"3 Latent Semantic Matching","paragraphs":["We propose latent semantic matching to find a shared latent space by assuming that two languages have common latent topics. Our method consists of following four steps: (1) for both source and target domains, we map the documents to a K-dimensional latent topic space independently, (2) we find the one-to-one correspondences between topics across source and target domains by unsupervised object matching, (3) we permute topics of the target side according to the correspondences, while fixing the topics of the source side, and (4) finally, we map documents in the source and target domains to a shared latent space by using permuted and fixed topics. 3.1 Topic Extraction as Dimension Reduction Suppose that we have N documents in the source domain. sn=(sni)I","i=1 is the nth document represented as a multi-dimensional column vector in the domain, i.e. each document is represented as a bag-of-words vector. Here, each element of the vectors indicates the TF·IDF score of the corresponding word in the document. I is the size of the feature set, i.e., the vocabulary size in the source domain. Also, we have M documents in the target domain. tm=(tmj)J","j=1 is the mth document represented as a multi-dimensional vector. J is the vocabulary size in the target domain. Thus, the data set in the source domain is represented by an I × N matrix, S=(s1, · · · , sN ), the data set in the target is represented by a J × M matrix, T =(t1, · · · , tM ).","We factorize these matrices using nonnegative matrix factorization (Lee and Seung, 2000) to find topics as follows: S ≈ WSHS, (1) T ≈ WT HT . (2) WS is an I × K matrix that represents a set of topics, i.e. each column vector denotes word weights for each topic. HS is a K × N matrix that denotes a set of latent semantic representations of documents in the source domain, i.e. each row 213"]},{"title":"× × × ≈ ≈ ≈","paragraphs":["WS HS π* = 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0 0","ment.","I","is","the","size","of feature set, i.e",".,","the","size","of","v","ocab","ulary","in","the","source","domain.","Also, we ha v","e","M","documents","in","a","tar","get","domain.","t","m","=(","t","mj )J j =1 is the","m","-th","document","represented","as","a","multi-dimensional v ector . J","is","the","size","of","v","ocab","ulary","in","the","tar","get","domain.","Thus, the data","set","in","the","source","domain","is","represented","as","the","I","× N matrix,","S",",","the","data","set","in","the","tar","get","is","represented","as","the","J × M matrix,","T",".","Here,","we","assume","that these matrices","are","approximated","as","the","product","of","lo","w","rank matrices","as","follo","ws: S ≈ W S H","S",",","(1) T ≈ W T","H","T","(2)","W","S","is","I","×","K","matrix, which represents","a","set","of","topic","propor","-","tions","in","the","source","domain, i.e ., each","column","v","ector","denotes","topic","proportion.","H","S is K × N","matrix,","which","denotes","a","set","of","documents","in","the K -dimensional","latent","space","which","cor","-","responds","to","the","source domain, i.e",".,","each","ro","w","v","ector","denotes","the","document","in","the","latent space.","The","k","(1","≤","k","≤","K",")","-th","basis","in","the","latent","space","corresponds to","the","k","-th","topic","proportion.","W","T","is","I","×","K","matrix, which represents","a","set","of","topic","pro-","portions","in","the","tar","get domain. H","T","is","K","×","N","matrix,","which","denotes","a","set","of","documents in the","latent","topic","space","with","di-","mentionaly","K",".","K","is less than I ,","J",".","In","this","paper",",","we","emplo","y","Non-ne","g","ati","v","e","Matrix F actorization","(NMF)","[","Lee","and","Seung,","2000","]","to","f","actorize","the original matrices.","According","to","the","f","actorization","of","the","original","matrices,","we","can","map","the","documents in the source","and","tar","get","domain","to","latent","topic","space","with dimentionaly","K",",","independently",".","3.2","Finding","Optimal T o pic","Alignments","by","Unsuper","vised Object Matching","T","o","connect","the","dif","ferent latent space,","the","basis","of","the","space","ha","v","e","to","be","aligned","each other . That","is,","topic","proportion","e","x-","tracted","from","the","source language","must","be","aligned","that","from","the","tar","get","language.","This is reasonable","consideration","because","we","can","assume","the","same latent","concept","for","both","language.","F","or","e","xample,","a","topic proportion obtained","from","English","docu-","ments","can","be","aligned a topic proportion","obtained","from","French","documents.","F","or","all","k and k′ , k -th","column","v","ector","in","W","S","are","aligned","k′","-th","column v ector in W","T",".","Ho","we","v","er",",","we","can","not measure","similarity","between","the","topic","proportions","because we do not ha","v","e","an","y","language","resources","such","as","dictionary",".","Therefore, we","utilize","unsupervised","ob-","ject","matching","method to find one-to-one","correspondences","be-","tween","topic","proportions. In this paper",",","we","emplo","y","K","ernelized","Sorting","(KS)","[","No","vi","et al. , 2010 ] .","Of","cource,","we","can","replace","KS","to","another","unsupervised object","matching","sush","as","MCCA","[","Haghighi","et","al.",",","2008 ] , LSOM","[","Y","amada","and","Sugiyama,","2011","]",".","KS","finds","the","best","one-to-one matching","by","follo","wings:","π∗","= ar g m ax π ∈ Π K tr(","̄","G","S","πT","̄","G","T","π",")",",","s.t",". π 1 K =1 K","and","πT","1","K","=1","K",".","(3)","π","is","K","×","K","matrix which represents","one-to-one","correspon-","dence","between","topic proportion,","i.e",".,","π","ij","=1","indicates","i","-th","topic","proportion","in","the source language","corresponds","to","j","-th","one","of","the","tar","get","language. Π","indicates","set","of","all","possible","K","×","K","matrices","which store","one-to-one","corresponrence.","G","denotes","K","×","K","k","ernel matrix obtained","from","topic","proportion,","G","ij","=","K","(","WT","i,",":",",W",":",",j ) , and ̄ G is","the","centerd","matrix","of","G",".","K","(",",",")","is","a","k","ernel","function. 1 K is K -dimensional","column","v","ector","of","all","ones.","π∗","is","obtained by iterati","v","e","procedure.","According","to","π∗",",","we","can","permutate the basis","of","the","latent","space","obtained","from","source","language. See fig","hoge. S ≈ W","S","H","S",".","(4)","On","the","other","hand, we can directly","fomulate","objecti","v","e","func-","tion","of","unsupervised mapping",".","If","the","topic","proportions","are","aligned","each","other , the correlation","matrix","(or","gram","matrix)","obtained","from","source language","is","proportional","to","one","from","tar","get","language:","|| G S − α G T","||2","=0",".","(5)","α","denotes","the","h","yperparameter for","tuning","the","socore","range","be-","tween","tw","o","gram","matrices.","By","minimize","the error of the","matrix","f","actorization","(equa-","tion","(1),(2))","and","the dif ference between","correlation","matrices","(equation","(6)),","the","objecti v e function","is","defined","as","follo","w:","E = ∥ S −","W","S","H","S","∥2 + ∥ T −","W","T","H","T","∥2 + β || G S","−","α","G","T","||2",".","(6)","β","is","cost","parameter between","first,","second","ar","gu-","ment","and","third","ar gument.","The","optimal","parameters","(","W","S",",W","T",",H","S",",H","T ) are obtained","by","minimizing","the","objecti","v","e","function.","T o mimimize","the","objecti","v","e,","gradient","de-","scend","can","be","used.","b ut Ho we v er","that","is","not","con","v","e","x","function,","we","only","obtained","local optimal. Thefore,","we","emplo","yed","abo","v","e","tw","o","step","procedure??????","This","objecti","v","e","function is","not","con","v","e","x.","That","means","we","can","only","obtain local optimal","parameters.","By","min-","imizing","equation","(6), we can","obtain","a","set","of","parameter","(","W","S",",W","T",",H","S",",H","T ) for unsupervised","mapping.","we","could","be","emplo","yed","gradient based algorithm","b","ut,","as","the","first","step,","we","emplo","y","former","tw o step optimization","procedure.","3.3","Cr","oss-lingual T ext Categorization","via","Unsuper","vised Mapping","m","-th","document","in","the tar get domain","(","t","m",")","is","mapped","to","the","source","domain","as","follo ws,","s ( t m )= H T","⊤",":",",m","W","S",".","(7)","Here,","H","T",":",",m","denotes the m -th column","v","ector","of","H","T",",","s","(","t","m",")","is","I","dimentional","v","ector .","When","each","document in the","source","domain","has","a","class","label","y","n",",","we","can","train a classifier","on","the","training","data","set","{","s","n",",y","n","}N","n","=1",".","Therefore, the class","label","of","the","mapped","docu-","ment","in","the","tar","get","domain s ( t m )","is","assigned","by","the","classifier",".","In","the","later","e","xperiments, we emplo","y","k","(","=","10)","-NN","as","a","classi-","fier",".","ment.","I","is","the","size","of feature set, i.e",".,","the","size","of","v","ocab","ulary","in","the","source","domain.","Also, we ha v","e","M","documents","in","a","tar","get","domain.","t","m","=(","t","mj )J j =1 is the","m","-th","document","represented","as","a","multi-dimensional v ector . J","is","the","size","of","v","ocab","ulary","in","the","tar","get","domain.","Thus, the data","set","in","the","source","domain","is","represented","as","the","I","× N matrix,","S",",","the","data","set","in","the","tar","get","is","represented","as","the","J × M matrix,","T",".","Here,","we","assume","that these matrices","are","approximated","as","the","product","of","lo","w","rank matrices","as","follo","ws: S ≈ W S H","S",",","(1) T ≈ W T","H","T","(2)","W","S","is","I","×","K","matrix, which represents","a","set","of","topic","propor","-","tions","in","the","source","domain, i.e ., each","column","v","ector","denotes","topic","proportion.","H","S is K × N","matrix,","which","denotes","a","set","of","documents","in","the K -dimensional","latent","space","which","cor","-","responds","to","the","source domain, i.e",".,","each","ro","w","v","ector","denotes","the","document","in","the","latent space.","The","k","(1","≤","k","≤","K",")","-th","basis","in","the","latent","space","corresponds to","the","k","-th","topic","proportion.","W","T","is","I","×","K","matrix, which represents","a","set","of","topic","pro-","portions","in","the","tar","get domain. H","T","is","K","×","N","matrix,","which","denotes","a","set","of","documents in the","latent","topic","space","with","di-","mentionaly","K",".","K","is less than I ,","J",".","In","this","paper",",","we","emplo","y","Non-ne","g","ati","v","e","Matrix F actorization","(NMF)","[","Lee","and","Seung,","2000","]","to","f","actorize","the original matrices.","According","to","the","f","actorization","of","the","original","matrices,","we","can","map","the","documents in the source","and","tar","get","domain","to","latent","topic","space","with dimentionaly","K",",","independently",".","3.2","Finding","Optimal T o pic","Alignments","by","Unsuper","vised Object Matching","T","o","connect","the","dif","ferent latent space,","the","basis","of","the","space","ha","v","e","to","be","aligned","each other . That","is,","topic","proportion","e","x-","tracted","from","the","source language","must","be","aligned","that","from","the","tar","get","language.","This is reasonable","consideration","because","we","can","assume","the","same latent","concept","for","both","language.","F","or","e","xample,","a","topic proportion obtained","from","English","docu-","ments","can","be","aligned a topic proportion","obtained","from","French","documents.","F","or","all","k and k′ , k -th","column","v","ector","in","W","S","are","aligned","k′","-th","column v ector in W","T",".","Ho","we","v","er",",","we","can","not measure","similarity","between","the","topic","proportions","because we do not ha","v","e","an","y","language","resources","such","as","dictionary",".","Therefore, we","utilize","unsupervised","ob-","ject","matching","method to find one-to-one","correspondences","be-","tween","topic","proportions. In this paper",",","we","emplo","y","K","ernelized","Sorting","(KS)","[","No","vi","et al. , 2010 ] .","Of","cource,","we","can","replace","KS","to","another","unsupervised object","matching","sush","as","MCCA","[","Haghighi","et","al.",",","2008 ] , LSOM","[","Y","amada","and","Sugiyama,","2011","]",".","KS","finds","the","best","one-to-one matching","by","follo","wings:","π∗","= ar g m ax π ∈ Π K tr(","̄","G","S","πT","̄","G","T","π",")",",","s.t",". π 1 K =1 K","and","πT","1","K","=1","K",".","(3)","π","is","K","×","K","matrix which represents","one-to-one","correspon-","dence","between","topic proportion,","i.e",".,","π","ij","=1","indicates","i","-th","topic","proportion","in","the source language","corresponds","to","j","-th","one","of","the","tar","get","language. Π","indicates","set","of","all","possible","K","×","K","matrices","which store","one-to-one","corresponrence.","G","denotes","K","×","K","k","ernel matrix obtained","from","topic","proportion,","G","ij","=","K","(","WT","i,",":",",W",":",",j ) , and ̄ G is","the","centerd","matrix","of","G",".","K","(",",",")","is","a","k","ernel","function. 1 K is K -dimensional","column","v","ector","of","all","ones.","π∗","is","obtained by iterati","v","e","procedure.","According","to","π∗",",","we","can","permutate the basis","of","the","latent","space","obtained","from","source","language. See fig","hoge. S ≈ W","S","H","S",".","(4)","On","the","other","hand, we can directly","fomulate","objecti","v","e","func-","tion","of","unsupervised mapping",".","If","the","topic","proportions","are","aligned","each","other , the correlation","matrix","(or","gram","matrix)","obtained","from","source language","is","proportional","to","one","from","tar","get","language:","|| G S − α G T","||2","=0",".","(5)","α","denotes","the","h","yperparameter for","tuning","the","socore","range","be-","tween","tw","o","gram","matrices.","By","minimize","the error of the","matrix","f","actorization","(equa-","tion","(1),(2))","and","the dif ference between","correlation","matrices","(equation","(6)),","the","objecti v e function","is","defined","as","follo","w:","E = ∥ S −","W","S","H","S","∥2 + ∥ T −","W","T","H","T","∥2 + β || G S","−","α","G","T","||2",".","(6)","β","is","cost","parameter between","first,","second","ar","gu-","ment","and","third","ar gument.","The","optimal","parameters","(","W","S",",W","T",",H","S",",H","T ) are obtained","by","minimizing","the","objecti","v","e","function.","T o mimimize","the","objecti","v","e,","gradient","de-","scend","can","be","used.","b ut Ho we v er","that","is","not","con","v","e","x","function,","we","only","obtained","local optimal. Thefore,","we","emplo","yed","abo","v","e","tw","o","step","procedure??????","This","objecti","v","e","function is","not","con","v","e","x.","That","means","we","can","only","obtain local optimal","parameters.","By","min-","imizing","equation","(6), we can","obtain","a","set","of","parameter","(","W","S",",W","T",",H","S",",H","T ) for unsupervised","mapping.","we","could","be","emplo","yed","gradient based algorithm","b","ut,","as","the","first","step,","we","emplo","y","former","tw o step optimization","procedure.","3.3","Cr","oss-lingual T ext Categorization","via","Unsuper","vised Mapping","m","-th","document","in","the tar get domain","(","t","m",")","is","mapped","to","the","source","domain","as","follo ws,","s ( t m )= H T","⊤",":",",m","W","S",".","(7)","Here,","H","T",":",",m","denotes the m -th column","v","ector","of","H","T",",","s","(","t","m",")","is","I","dimentional","v","ector .","When","each","document in the","source","domain","has","a","class","label","y","n",",","we","can","train a classifier","on","the","training","data","set","{","s","n",",y","n","}N","n","=1",".","Therefore, the class","label","of","the","mapped","docu-","ment","in","the","tar","get","domain s ( t m )","is","assigned","by","the","classifier",".","In","the","later","e","xperiments, we emplo","y","k","(","=","10)","-NN","as","a","classi-","fier",".","ment.","I","is","the","size","of feature set, i.e",".,","the","size","of","v","ocab","ulary","in","the","source","domain.","Also, we ha v","e","M","documents","in","a","tar","get","domain.","t","m","=(","t","mj )J j =1 is the","m","-th","document","represented","as","a","multi-dimensional v ector . J","is","the","size","of","v","ocab","ulary","in","the","tar","get","domain.","Thus, the data","set","in","the","source","domain","is","represented","as","the","I","× N matrix,","S",",","the","data","set","in","the","tar","get","is","represented","as","the","J × M matrix,","T",".","Here,","we","assume","that these matrices","are","approximated","as","the","product","of","lo","w","rank matrices","as","follo","ws: S ≈ W S H","S",",","(1) T ≈ W T","H","T","(2)","W","S","is","I","×","K","matrix, which represents","a","set","of","topic","propor","-","tions","in","the","source","domain, i.e ., each","column","v","ector","denotes","topic","proportion.","H","S is K × N","matrix,","which","denotes","a","set","of","documents","in","the K -dimensional","latent","space","which","cor","-","responds","to","the","source domain, i.e",".,","each","ro","w","v","ector","denotes","the","document","in","the","latent space.","The","k","(1","≤","k","≤","K",")","-th","basis","in","the","latent","space","corresponds to","the","k","-th","topic","proportion.","W","T","is","I","×","K","matrix, which represents","a","set","of","topic","pro-","portions","in","the","tar","get domain. H","T","is","K","×","N","matrix,","which","denotes","a","set","of","documents in the","latent","topic","space","with","di-","mentionaly","K",".","K","is less than I ,","J",".","In","this","paper",",","we","emplo","y","Non-ne","g","ati","v","e","Matrix F actorization","(NMF)","[","Lee","and","Seung,","2000","]","to","f","actorize","the original matrices.","According","to","the","f","actorization","of","the","original","matrices,","we","can","map","the","documents in the source","and","tar","get","domain","to","latent","topic","space","with dimentionaly","K",",","independently",".","3.2","Finding","Optimal T o pic","Alignments","by","Unsuper","vised Object Matching","T","o","connect","the","dif","ferent latent space,","the","basis","of","the","space","ha","v","e","to","be","aligned","each other . That","is,","topic","proportion","e","x-","tracted","from","the","source language","must","be","aligned","that","from","the","tar","get","language.","This is reasonable","consideration","because","we","can","assume","the","same latent","concept","for","both","language.","F","or","e","xample,","a","topic proportion obtained","from","English","docu-","ments","can","be","aligned a topic proportion","obtained","from","French","documents.","F","or","all","k and k′ , k -th","column","v","ector","in","W","S","are","aligned","k′","-th","column v ector in W","T",".","Ho","we","v","er",",","we","can","not measure","similarity","between","the","topic","proportions","because we do not ha","v","e","an","y","language","resources","such","as","dictionary",".","Therefore, we","utilize","unsupervised","ob-","ject","matching","method to find one-to-one","correspondences","be-","tween","topic","proportions. In this paper",",","we","emplo","y","K","ernelized","Sorting","(KS)","[","No","vi","et al. , 2010 ] .","Of","cource,","we","can","replace","KS","to","another","unsupervised object","matching","sush","as","MCCA","[","Haghighi","et","al.",",","2008 ] , LSOM","[","Y","amada","and","Sugiyama,","2011","]",".","KS","finds","the","best","one-to-one matching","by","follo","wings:","π∗","= ar g m ax π ∈ Π K tr(","̄","G","S","πT","̄","G","T","π",")",",","s.t",". π 1 K =1 K","and","πT","1","K","=1","K",".","(3)","π","is","K","×","K","matrix which represents","one-to-one","correspon-","dence","between","topic proportion,","i.e",".,","π","ij","=1","indicates","i","-th","topic","proportion","in","the source language","corresponds","to","j","-th","one","of","the","tar","get","language. Π","indicates","set","of","all","possible","K","×","K","matrices","which store","one-to-one","corresponrence.","G","denotes","K","×","K","k","ernel matrix obtained","from","topic","proportion,","G","ij","=","K","(","WT","i,",":",",W",":",",j ) , and ̄ G is","the","centerd","matrix","of","G",".","K","(",",",")","is","a","k","ernel","function. 1 K is K -dimensional","column","v","ector","of","all","ones.","π∗","is","obtained by iterati","v","e","procedure.","According","to","π∗",",","we","can","permutate the basis","of","the","latent","space","obtained","from","source","language. See fig","hoge. S ≈ W","S","H","S",".","(4)","On","the","other","hand, we can directly","fomulate","objecti","v","e","func-","tion","of","unsupervised mapping",".","If","the","topic","proportions","are","aligned","each","other , the correlation","matrix","(or","gram","matrix)","obtained","from","source language","is","proportional","to","one","from","tar","get","language:","|| G S − α G T","||2","=0",".","(5)","α","denotes","the","h","yperparameter for","tuning","the","socore","range","be-","tween","tw","o","gram","matrices.","By","minimize","the error of the","matrix","f","actorization","(equa-","tion","(1),(2))","and","the dif ference between","correlation","matrices","(equation","(6)),","the","objecti v e function","is","defined","as","follo","w:","E = ∥ S −","W","S","H","S","∥2 + ∥ T −","W","T","H","T","∥2 + β || G S","−","α","G","T","||2",".","(6)","β","is","cost","parameter between","first,","second","ar","gu-","ment","and","third","ar gument.","The","optimal","parameters","(","W","S",",W","T",",H","S",",H","T ) are obtained","by","minimizing","the","objecti","v","e","function.","T o mimimize","the","objecti","v","e,","gradient","de-","scend","can","be","used.","b ut Ho we v er","that","is","not","con","v","e","x","function,","we","only","obtained","local optimal. Thefore,","we","emplo","yed","abo","v","e","tw","o","step","procedure??????","This","objecti","v","e","function is","not","con","v","e","x.","That","means","we","can","only","obtain local optimal","parameters.","By","min-","imizing","equation","(6), we can","obtain","a","set","of","parameter","(","W","S",",W","T",",H","S",",H","T ) for unsupervised","mapping.","we","could","be","emplo","yed","gradient based algorithm","b","ut,","as","the","first","step,","we","emplo","y","former","tw o step optimization","procedure.","3.3","Cr","oss-lingual T ext Categorization","via","Unsuper","vised Mapping","m","-th","document","in","the tar get domain","(","t","m",")","is","mapped","to","the","source","domain","as","follo ws,","s ( t m )= H T","⊤",":",",m","W","S",".","(7)","Here,","H","T",":",",m","denotes the m -th column","v","ector","of","H","T",",","s","(","t","m",")","is","I","dimentional","v","ector .","When","each","document in the","source","domain","has","a","class","label","y","n",",","we","can","train a classifier","on","the","training","data","set","{","s","n",",y","n","}N","n","=1",".","Therefore, the class","label","of","the","mapped","docu-","ment","in","the","tar","get","domain s ( t m )","is","assigned","by","the","classifier",".","In","the","later","e","xperiments, we emplo","y","k","(","=","10)","-NN","as","a","classi-","fier",".","ment.","I","is","the","size","of feature set, i.e",".,","the","size","of","v","ocab","ulary","in","the","source","domain.","Also, we ha v","e","M","documents","in","a","tar","get","domain.","t","m","=(","t","mj )J j =1 is the","m","-th","document","represented","as","a","multi-dimensional v ector . J","is","the","size","of","v","ocab","ulary","in","the","tar","get","domain.","Thus, the data","set","in","the","source","domain","is","represented","as","the","I","× N matrix,","S",",","the","data","set","in","the","tar","get","is","represented","as","the","J × M matrix,","T",".","Here,","we","assume","that these matrices","are","approximated","as","the","product","of","lo","w","rank matrices","as","follo","ws: S ≈ W S H","S",",","(1) T ≈ W T","H","T","(2)","W","S","is","I","×","K","matrix, which represents","a","set","of","topic","propor","-","tions","in","the","source","domain, i.e ., each","column","v","ector","denotes","topic","proportion.","H","S is K × N","matrix,","which","denotes","a","set","of","documents","in","the K -dimensional","latent","space","which","cor","-","responds","to","the","source domain, i.e",".,","each","ro","w","v","ector","denotes","the","document","in","the","latent space.","The","k","(1","≤","k","≤","K",")","-th","basis","in","the","latent","space","corresponds to","the","k","-th","topic","proportion.","W","T","is","I","×","K","matrix, which represents","a","set","of","topic","pro-","portions","in","the","tar","get domain. H","T","is","K","×","N","matrix,","which","denotes","a","set","of","documents in the","latent","topic","space","with","di-","mentionaly","K",".","K","is less than I ,","J",".","In","this","paper",",","we","emplo","y","Non-ne","g","ati","v","e","Matrix F actorization","(NMF)","[","Lee","and","Seung,","2000","]","to","f","actorize","the original matrices.","According","to","the","f","actorization","of","the","original","matrices,","we","can","map","the","documents in the source","and","tar","get","domain","to","latent","topic","space","with dimentionaly","K",",","independently",".","3.2","Finding","Optimal T o pic","Alignments","by","Unsuper","vised Object Matching","T","o","connect","the","dif","ferent latent space,","the","basis","of","the","space","ha","v","e","to","be","aligned","each other . That","is,","topic","proportion","e","x-","tracted","from","the","source language","must","be","aligned","that","from","the","tar","get","language.","This is reasonable","consideration","because","we","can","assume","the","same latent","concept","for","both","language.","F","or","e","xample,","a","topic proportion obtained","from","English","docu-","ments","can","be","aligned a topic proportion","obtained","from","French","documents.","F","or","all","k and k′ , k -th","column","v","ector","in","W","S","are","aligned","k′","-th","column v ector in W","T",".","Ho","we","v","er",",","we","can","not measure","similarity","between","the","topic","proportions","because we do not ha","v","e","an","y","language","resources","such","as","dictionary",".","Therefore, we","utilize","unsupervised","ob-","ject","matching","method to find one-to-one","correspondences","be-","tween","topic","proportions. In this paper",",","we","emplo","y","K","ernelized","Sorting","(KS)","[","No","vi","et al. , 2010 ] .","Of","cource,","we","can","replace","KS","to","another","unsupervised object","matching","sush","as","MCCA","[","Haghighi","et","al.",",","2008 ] , LSOM","[","Y","amada","and","Sugiyama,","2011","]",".","KS","finds","the","best","one-to-one matching","by","follo","wings:","π∗","= ar g m ax π ∈ Π K tr(","̄","G","S","πT","̄","G","T","π",")",",","s.t",". π 1 K =1 K","and","πT","1","K","=1","K",".","(3)","π","is","K","×","K","matrix which represents","one-to-one","correspon-","dence","between","topic proportion,","i.e",".,","π","ij","=1","indicates","i","-th","topic","proportion","in","the source language","corresponds","to","j","-th","one","of","the","tar","get","language. Π","indicates","set","of","all","possible","K","×","K","matrices","which store","one-to-one","corresponrence.","G","denotes","K","×","K","k","ernel matrix obtained","from","topic","proportion,","G","ij","=","K","(","WT","i,",":",",W",":",",j ) , and ̄ G is","the","centerd","matrix","of","G",".","K","(",",",")","is","a","k","ernel","function. 1 K is K -dimensional","column","v","ector","of","all","ones.","π∗","is","obtained by iterati","v","e","procedure.","According","to","π∗",",","we","can","permutate the basis","of","the","latent","space","obtained","from","source","language. See fig","hoge. S ≈ W","S","H","S",".","(4)","On","the","other","hand, we can directly","fomulate","objecti","v","e","func-","tion","of","unsupervised mapping",".","If","the","topic","proportions","are","aligned","each","other , the correlation","matrix","(or","gram","matrix)","obtained","from","source language","is","proportional","to","one","from","tar","get","language:","|| G S − α G T","||2","=0",".","(5)","α","denotes","the","h","yperparameter for","tuning","the","socore","range","be-","tween","tw","o","gram","matrices.","By","minimize","the error of the","matrix","f","actorization","(equa-","tion","(1),(2))","and","the dif ference between","correlation","matrices","(equation","(6)),","the","objecti v e function","is","defined","as","follo","w:","E = ∥ S −","W","S","H","S","∥2 + ∥ T −","W","T","H","T","∥2 + β || G S","−","α","G","T","||2",".","(6)","β","is","cost","parameter between","first,","second","ar","gu-","ment","and","third","ar gument.","The","optimal","parameters","(","W","S",",W","T",",H","S",",H","T ) are obtained","by","minimizing","the","objecti","v","e","function.","T o mimimize","the","objecti","v","e,","gradient","de-","scend","can","be","used.","b ut Ho we v er","that","is","not","con","v","e","x","function,","we","only","obtained","local optimal. Thefore,","we","emplo","yed","abo","v","e","tw","o","step","procedure??????","This","objecti","v","e","function is","not","con","v","e","x.","That","means","we","can","only","obtain local optimal","parameters.","By","min-","imizing","equation","(6), we can","obtain","a","set","of","parameter","(","W","S",",W","T",",H","S",",H","T ) for unsupervised","mapping.","we","could","be","emplo","yed","gradient based algorithm","b","ut,","as","the","first","step,","we","emplo","y","former","tw o step optimization","procedure.","3.3","Cr","oss-lingual T ext Categorization","via","Unsuper","vised Mapping","m","-th","document","in","the tar get domain","(","t","m",")","is","mapped","to","the","source","domain","as","follo ws,","s ( t m )= H T","⊤",":",",m","W","S",".","(7)","Here,","H","T",":",",m","denotes the m -th column","v","ector","of","H","T",",","s","(","t","m",")","is","I","dimentional","v","ector .","When","each","document in the","source","domain","has","a","class","label","y","n",",","we","can","train a classifier","on","the","training","data","set","{","s","n",",y","n","}N","n","=1",".","Therefore, the class","label","of","the","mapped","docu-","ment","in","the","tar","get","domain s ( t m )","is","assigned","by","the","classifier",".","In","the","later","e","xperiments, we emplo","y","k","(","=","10)","-NN","as","a","classi-","fier",".","ment.","I","is","the","size","of feature set, i.e",".,","the","size","of","v","ocab","ulary","in","the","source","domain.","Also, we ha v","e","M","documents","in","a","tar","get","domain.","t","m","=(","t","mj )J j =1 is the","m","-th","document","represented","as","a","multi-dimensional v ector . J","is","the","size","of","v","ocab","ulary","in","the","tar","get","domain.","Thus, the data","set","in","the","source","domain","is","represented","as","the","I","× N matrix,","S",",","the","data","set","in","the","tar","get","is","represented","as","the","J × M matrix,","T",".","Here,","we","assume","that these matrices","are","approximated","as","the","product","of","lo","w","rank matrices","as","follo","ws: S ≈ W S H","S",",","(1) T ≈ W T","H","T","(2)","W","S","is","I","×","K","matrix, which represents","a","set","of","topic","propor","-","tions","in","the","source","domain, i.e ., each","column","v","ector","denotes","topic","proportion.","H","S is K × N","matrix,","which","denotes","a","set","of","documents","in","the K -dimensional","latent","space","which","cor","-","responds","to","the","source domain, i.e",".,","each","ro","w","v","ector","denotes","the","document","in","the","latent space.","The","k","(1","≤","k","≤","K",")","-th","basis","in","the","latent","space","corresponds to","the","k","-th","topic","proportion.","W","T","is","I","×","K","matrix, which represents","a","set","of","topic","pro-","portions","in","the","tar","get domain. H","T","is","K","×","N","matrix,","which","denotes","a","set","of","documents in the","latent","topic","space","with","di-","mentionaly","K",".","K","is less than I ,","J",".","In","this","paper",",","we","emplo","y","Non-ne","g","ati","v","e","Matrix F actorization","(NMF)","[","Lee","and","Seung,","2000","]","to","f","actorize","the original matrices.","According","to","the","f","actorization","of","the","original","matrices,","we","can","map","the","documents in the source","and","tar","get","domain","to","latent","topic","space","with dimentionaly","K",",","independently",".","3.2","Finding","Optimal T o pic","Alignments","by","Unsuper","vised Object Matching","T","o","connect","the","dif","ferent latent space,","the","basis","of","the","space","ha","v","e","to","be","aligned","each other . That","is,","topic","proportion","e","x-","tracted","from","the","source language","must","be","aligned","that","from","the","tar","get","language.","This is reasonable","consideration","because","we","can","assume","the","same latent","concept","for","both","language.","F","or","e","xample,","a","topic proportion obtained","from","English","docu-","ments","can","be","aligned a topic proportion","obtained","from","French","documents.","F","or","all","k and k′ , k -th","column","v","ector","in","W","S","are","aligned","k′","-th","column v ector in W","T",".","Ho","we","v","er",",","we","can","not measure","similarity","between","the","topic","proportions","because we do not ha","v","e","an","y","language","resources","such","as","dictionary",".","Therefore, we","utilize","unsupervised","ob-","ject","matching","method to find one-to-one","correspondences","be-","tween","topic","proportions. In this paper",",","we","emplo","y","K","ernelized","Sorting","(KS)","[","No","vi","et al. , 2010 ] .","Of","cource,","we","can","replace","KS","to","another","unsupervised object","matching","sush","as","MCCA","[","Haghighi","et","al.",",","2008 ] , LSOM","[","Y","amada","and","Sugiyama,","2011","]",".","KS","finds","the","best","one-to-one matching","by","follo","wings:","π∗","= ar g m ax π ∈ Π K tr(","̄","G","S","πT","̄","G","T","π",")",",","s.t",". π 1 K =1 K","and","πT","1","K","=1","K",".","(3)","π","is","K","×","K","matrix which represents","one-to-one","correspon-","dence","between","topic proportion,","i.e",".,","π","ij","=1","indicates","i","-th","topic","proportion","in","the source language","corresponds","to","j","-th","one","of","the","tar","get","language. Π","indicates","set","of","all","possible","K","×","K","matrices","which store","one-to-one","corresponrence.","G","denotes","K","×","K","k","ernel matrix obtained","from","topic","proportion,","G","ij","=","K","(","WT","i,",":",",W",":",",j ) , and ̄ G is","the","centerd","matrix","of","G",".","K","(",",",")","is","a","k","ernel","function. 1 K is K -dimensional","column","v","ector","of","all","ones.","π∗","is","obtained by iterati","v","e","procedure.","According","to","π∗",",","we","can","permutate the basis","of","the","latent","space","obtained","from","source","language. See fig","hoge. S ≈ W","S","H","S",".","(4)","On","the","other","hand, we can directly","fomulate","objecti","v","e","func-","tion","of","unsupervised mapping",".","If","the","topic","proportions","are","aligned","each","other , the correlation","matrix","(or","gram","matrix)","obtained","from","source language","is","proportional","to","one","from","tar","get","language:","|| G S − α G T","||2","=0",".","(5)","α","denotes","the","h","yperparameter for","tuning","the","socore","range","be-","tween","tw","o","gram","matrices.","By","minimize","the error of the","matrix","f","actorization","(equa-","tion","(1),(2))","and","the dif ference between","correlation","matrices","(equation","(6)),","the","objecti v e function","is","defined","as","follo","w:","E = ∥ S −","W","S","H","S","∥2 + ∥ T −","W","T","H","T","∥2 + β || G S","−","α","G","T","||2",".","(6)","β","is","cost","parameter between","first,","second","ar","gu-","ment","and","third","ar gument.","The","optimal","parameters","(","W","S",",W","T",",H","S",",H","T ) are obtained","by","minimizing","the","objecti","v","e","function.","T o mimimize","the","objecti","v","e,","gradient","de-","scend","can","be","used.","b ut Ho we v er","that","is","not","con","v","e","x","function,","we","only","obtained","local optimal. Thefore,","we","emplo","yed","abo","v","e","tw","o","step","procedure??????","This","objecti","v","e","function is","not","con","v","e","x.","That","means","we","can","only","obtain local optimal","parameters.","By","min-","imizing","equation","(6), we can","obtain","a","set","of","parameter","(","W","S",",W","T",",H","S",",H","T ) for unsupervised","mapping.","we","could","be","emplo","yed","gradient based algorithm","b","ut,","as","the","first","step,","we","emplo","y","former","tw o step optimization","procedure.","3.3","Cr","oss-lingual T ext Categorization","via","Unsuper","vised Mapping","m","-th","document","in","the tar get domain","(","t","m",")","is","mapped","to","the","source","domain","as","follo ws,","s ( t m )= H T","⊤",":",",m","W","S",".","(7)","Here,","H","T",":",",m","denotes the m -th column","v","ector","of","H","T",",","s","(","t","m",")","is","I","dimentional","v","ector .","When","each","document in the","source","domain","has","a","class","label","y","n",",","we","can","train a classifier","on","the","training","data","set","{","s","n",",y","n","}N","n","=1",".","Therefore, the class","label","of","the","mapped","docu-","ment","in","the","tar","get","domain s ( t m )","is","assigned","by","the","classifier",".","In","the","later","e","xperiments, we emplo","y","k","(","=","10)","-NN","as","a","classi-","fier",". WT HT M I N J K K K"]},{"title":"T","paragraphs":["S"]},{"title":"T","paragraphs":["Figure 1: Topic alignments. vector denotes an embedding of a document in the K-dimensional latent space. Similarly, WT is an I × K matrix that represents a set of topics in the target domain, and HT is a K × M matrix that denotes a set of latent semantic representations of target documents. K is less than I and J .","By factorizing the original matrices, we can independently map the documents in the source and target domains to the latent topic spaces whose dimensionality is K.","3.2 Finding Optimal Topic Alignments by Unsupervised Object Matching To connect the different latent spaces, topics extracted from the source language must be aligned to one from the target language. This is reasonable because we can assume that both languages share the same latent concept.","However, we cannot quantify the similarity between the topics because we do not have any external language resources such as a dictionary. Therefore, we utilize unsupervised object matching method to find one-to-one correspondences between topics. In this paper, we employ kernelized sorting (KS) (Novi et al., 2010). KS finds the best one-to-one matching as follows:","π∗ = arg max","π∈ΠK","tr(GSπ⊤ G T π),","s.t. π1K =1K and π⊤ 1 K =1K . (3) Here, π is a K × K matrix that represents the one-to-one correspondence between topics, i.e. πij=1 indicates that the ith topic in the source language corresponds to the jth one of the target language.","Overall Average KS 0.252 ± 0.112 CKS 0.249 ± 0.033 LSOM 0.278 ± 0.086 LSM(300) 0.298 ± 0.077 LSM(600) 0.359 ± 0.062 Table 1: Average accuracy over all language pairs ΠK indicates the set of all possible matrices stor-ing one-to-one correspondences. G denotes the K × K kernel matrix obtained from topic proportion, Gij=K(W ⊤","i,: , W:,j), and G is the centered matrix of G. K(, ) is a kernel function. 1K is a K-dimensional column vector of all ones. π∗","is obtained by iterative procedure.","According to π∗",", we obtain permuted matrices, WT =WT π∗","and H","T =π∗⊤ H","T , and the product of permuted matrices is the same with that of un-permuted matrices as follows: T ≈ WT HT =WT HT . (4) Fig. 1 shows the topic alignment procedure.","Since documents from both domains are represented in a shared latent space, we can directly calculate the similarity between the nth document in the source domain and the mth document in the target domain based on HT :,m (mth column vector of HT ) and HS:,n (nth column vector of HS)."]},{"title":"4 Cross-language Text Categorization via Latent Semantic Matching","paragraphs":["Cross-language text categorization is the task of exploiting labeled documents in the source language (e.g. English) to classify documents in the target language (e.g. French). Suppose we have training data set {sn, yn}N","n=1 in the source language domain. yn ∈ Y is the class label for the nth document. We can train a classifier in the K-dimensional latent space with data set {H⊤","S:,n, yn}N","n=1. H⊤","S:,n is the projected vector of sn. Also, the mth document in the target language domain tm is projected into the latent space as H⊤","T :,m. Here, the documents in both domains are projected into the same size latent space and the basis vectors of the spaces are aligned. Therefore, we can classify a document in the target domain tm by a classifier trained with {H⊤","S:,n, yn}N","n=1. 214","Books English Hack, Parent, tale, subversion, Interesting, centre, Paper, T., prejudice, Murphy German Lydia, Sebastian, Seelenbrecher, Patient, Fitzek, Patrick, Fiktion, Patientenakte, Realitt, Klinik","Electronics English SD800, Angle, Digital, Optical, Silver, understnad, camra, 7.1MP, P3N, 10MP German *****, 550D, 600D, Objektiv, Canon, ablichten, Body, Werkzeug, Kamera, einliet","Kitchen English Briel, Electra-Craft, Chamonix, machine, Due, crema, supervisor, technician, espresso, tamp German ESGE, Prierkopf, Zauberstab, Gummikupplung, Suppe/Sauce, Braun , Bolognese, prieren, Testsieger, Topf","Music English Amy, Poison, Doherty, Schottin, Mid, Prince, Song, ausdrucksstark , Tempo, knocking German Norah, mini, ’Little, ’Rome, ’Come, Gardot, Lana, listenings , dreamlike, digipak","Watch","English watch, indicate, timex, HRM, month, icon, Timex, datum, troubleshooting, reasonable","German Orient, Diver, Lnette, Leuchtpunkt, Zahlenringes, Handgelenksdurchmesser, Stoppsekunde, Uhrforum,","Konsumbereiche, Schwingungen/Std Table 2: Examples of aligned latent topics"]},{"title":"5 Experimental Evaluation 5.1 Experimental Settings","paragraphs":["We compared our method, latent semantic matching (LSM), with three unsupervised object matching methods: Kernelized Sorting (KS), Convex Kernelized Sorting (CKS), Least-Squares Object Matching (LSOM). We set the number of the latent topics K to 100 and employed the k-nearest neighbor method (k=10) as the classifier.","For, KS, CKS and LSOM, we find the one-to-one correspondence between documents in the source language and documents in the target language. Then, we assign class labels of the target documents according to the correspondence.","In order to build a corpus with various language pairs for evaluation, we crawled product reviews from Amazon U.S., German, France and Japan with five categories: ‘Books’, ‘Electronics’, ‘Music’, ‘Kitchen’, ‘Watch’. The corpus is neither sentence level parallel nor comparable. For each category, we randomly select 60 documents as the test data (M =300) for all methods and 60 documents as the training data (N =300) for KS, CKS, LSOM and LSM(300). We also compared latent semantic matching with 120 training documents for each category (N =600), and called this method LSM(600). Note that since KS, CKS and LSOM require that the data sizes are the same for source and target domains, they cannot use training data more than test data. To avoid local optimum solutions of NMF, we executed our methods with 100 different initialization values and chose the solution that achieved the best objective function of KS. 5.2 Results and Discussion Table 1 shows average accuracies with standard division over all language pairs. From the table, classification accuracy of all methods significantly outperformed random classifier (accuracy=0.2). The results showed the effectiveness of both unsupervised object matching and latent semantic matching. When comparing LSM(300) with KS, CKS and LSOM, LSM(300) obtained better results than these unsupervised object matching methods. The result supports the effectiveness of the latent topic matching. Moreover, LSM(600) achieved the highest accuracy. There are large dif-ferences between LSM(600) and the others. This result implies not only the effectiveness of the latent topic matching but also increasing the number of source side documents (labeled training data) contributes to improving classification accuracy. This is natural in terms of supervised learning but only our method can deal with source side documents that are larger in number.","Table 2 shows examples of latent topics in English and German extracted and aligned by LSM(600). We can see that some author names, words related to camera, and cooking equipment appear in ‘Books’, ‘Electronics’ and ‘Kitchen’ topics, respectively. Similarity, there are some artists’ names in ‘Music’ and watch brands in ‘Watch’. 215"]},{"title":"6 Conclusion","paragraphs":["As an extension of unsupervised object matching, this paper proposed latent semantic matching that considers the shared latent space between two language domains. To generate such a space, topics of the target space are permuted by exploiting unsupervised object matching. We can measure distances between objects by standard metrics, which enable us retrieving k-nearest objects in the source domain for a query object in the target domain. This is a significant advantage over conventional unsupervised object matching methods. We used Amazon review corpus to demonstrate the effectiveness of our method on cross-language text categorization. The results showed that our method outperformed conventional object matching methods with the same number of training samples. Moreover, our method achieved even higher performance by utilizing more documents in the source domain."]},{"title":"Acknowledgements","paragraphs":["The authors would like to thank Nemanja Djuric for providing code for Convex Kernelized Sorting and the three anonymous reviewers for thoughtful suggestions."]},{"title":"References","paragraphs":["Francis Bach and Michael Jordan. 2005. A probabilistic interpretation of canonical correlation analysis. Technical report, Department of Statistics, University of California, Berkeley.","David Blei, Andrew Ng, and Michael Jordan. 2003. Latent Dirichlet allocation. JMLR, 3(Jan.):993– 1022.","Jordan Boyd-Graber and David Blei. 2009. Multilingual topic model for unaligned text. In Proc. of the 25th UAI, pages 75–82.","Scott Deerwester, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, and Richard Harshman. 1990. Indexing by latent semantic analysis. Journal of the American Society for Information Science, 41(6):391–407.","Nemanja Djuric, Mihajlo Grbovic, and Slobodan Vucetic. 2012. Convex kernelized sorting. In Proc. of the 26th AAAI, pages 893–899.","Susan Dumais, Lanauer Thomas, and Michael Littman. 1996. Automatic cross-linguistic information retrieval using latent semantic indexing. In Proc. of the Workshop on Cross-Linguistic Information Retieval in SIGIR, pages 16–23.","Alfio Gliozzo and Carlo Strapparava. 2005. Cross language text categorization by acquiring multilingual domain models from comparable corpora. In Proc. of the ACL Workshop on Building and Using Parallel Texts, pages 9–16.","Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick, and Dan Klein. 2008. Learning bilingual lexicons from monolingual corpora. In Proc. of ACL-08: HLT, pages 771–779.","Jagarlamudi Jagadeesh and Hal Daume III. 2010. Extracting multilingual topics from unaligned corpora. In Proc of the 32nd ECIR, pages 444–456.","Daniel Lee and Sebastian Seung. 2000. Algorithm for non-negative matrix factorization. In Advances in Neural Information Processing Systems 13, pages 556–562.","Quadrianto Novi, Smola Alexander, Song Le, and Tuytelaars Tinne. 2010. Kernelized sorting. IEEE Trans. on Pattern Analysis and Machine Intelligence, 32(10):1809–1821.","Jhon Platt, Kristina Toutanova, and Wen-tau Yih. 2010. Translingual document representation from discriminative projections. In Proc. of the 2010 Conference on EMNLP, pages 251–261.","Abhishek Tripathi, Arto Klami, and Sami Virpioja. 2010. Bilingual sentence matching using kernel CCA. In Proc. of the 2010 IEEE International Workshop on MLSP, pages 130–135.","Ni Xiaochuan, Sun Lian-Tao, Hu Jian, and Chen Zheng. 2011. Cross lingual text classification by mining multilingual topics from wikipedia. In Proc. of the 4th WSDM, pages 375–384.","Makoto Yamada and Masashi Sugiyama. 2011. Cross-domain object matching with model selection. In Proc. of the 14th AISTATS, pages 807–815. 216"]}],"references":[{"authors":[{"first":"Francis","last":"Bach"},{"first":"Michael","last":"Jordan"}],"year":"2005","title":"A probabilistic interpretation of canonical correlation analysis","source":"Francis Bach and Michael Jordan. 2005. A probabilistic interpretation of canonical correlation analysis. Technical report, Department of Statistics, University of California, Berkeley."},{"authors":[{"first":"David","last":"Blei"},{"first":"Andrew","last":"Ng"},{"first":"Michael","last":"Jordan"}],"year":"2003","title":"Latent Dirichlet allocation","source":"David Blei, Andrew Ng, and Michael Jordan. 2003. Latent Dirichlet allocation. JMLR, 3(Jan.):993– 1022."},{"authors":[{"first":"Jordan","last":"Boyd-Graber"},{"first":"David","last":"Blei"}],"year":"2009","title":"Multilingual topic model for unaligned text","source":"Jordan Boyd-Graber and David Blei. 2009. Multilingual topic model for unaligned text. In Proc. of the 25th UAI, pages 75–82."},{"authors":[{"first":"Scott","last":"Deerwester"},{"first":"Susan","middle":"T.","last":"Dumais"},{"first":"George","middle":"W.","last":"Furnas"},{"first":"Thomas","middle":"K.","last":"Landauer"},{"first":"Richard","last":"Harshman"}],"year":"1990","title":"Indexing by latent semantic analysis","source":"Scott Deerwester, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, and Richard Harshman. 1990. Indexing by latent semantic analysis. Journal of the American Society for Information Science, 41(6):391–407."},{"authors":[{"first":"Nemanja","last":"Djuric"},{"first":"Mihajlo","last":"Grbovic"},{"first":"Slobodan","last":"Vucetic"}],"year":"2012","title":"Convex kernelized sorting","source":"Nemanja Djuric, Mihajlo Grbovic, and Slobodan Vucetic. 2012. Convex kernelized sorting. In Proc. of the 26th AAAI, pages 893–899."},{"authors":[{"first":"Susan","last":"Dumais"},{"first":"Lanauer","last":"Thomas"},{"first":"Michael","last":"Littman"}],"year":"1996","title":"Automatic cross-linguistic information retrieval using latent semantic indexing","source":"Susan Dumais, Lanauer Thomas, and Michael Littman. 1996. Automatic cross-linguistic information retrieval using latent semantic indexing. In Proc. of the Workshop on Cross-Linguistic Information Retieval in SIGIR, pages 16–23."},{"authors":[{"first":"Alfio","last":"Gliozzo"},{"first":"Carlo","last":"Strapparava"}],"year":"2005","title":"Cross language text categorization by acquiring multilingual domain models from comparable corpora","source":"Alfio Gliozzo and Carlo Strapparava. 2005. Cross language text categorization by acquiring multilingual domain models from comparable corpora. In Proc. of the ACL Workshop on Building and Using Parallel Texts, pages 9–16."},{"authors":[{"first":"Aria","last":"Haghighi"},{"first":"Percy","last":"Liang"},{"first":"Taylor","last":"Berg-Kirkpatrick"},{"first":"Dan","last":"Klein"}],"year":"2008","title":"Learning bilingual lexicons from monolingual corpora","source":"Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick, and Dan Klein. 2008. Learning bilingual lexicons from monolingual corpora. In Proc. of ACL-08: HLT, pages 771–779."},{"authors":[{"first":"Jagarlamudi","last":"Jagadeesh"},{"first":"Hal","last":"Daume III"}],"year":"2010","title":"Extracting multilingual topics from unaligned corpora","source":"Jagarlamudi Jagadeesh and Hal Daume III. 2010. Extracting multilingual topics from unaligned corpora. In Proc of the 32nd ECIR, pages 444–456."},{"authors":[{"first":"Daniel","last":"Lee"},{"first":"Sebastian","last":"Seung"}],"year":"2000","title":"Algorithm for non-negative matrix factorization","source":"Daniel Lee and Sebastian Seung. 2000. Algorithm for non-negative matrix factorization. In Advances in Neural Information Processing Systems 13, pages 556–562."},{"authors":[{"first":"Quadrianto","last":"Novi"},{"first":"Smola","last":"Alexander"},{"first":"Song","last":"Le"},{"first":"Tuytelaars","last":"Tinne"}],"year":"2010","title":"Kernelized sorting","source":"Quadrianto Novi, Smola Alexander, Song Le, and Tuytelaars Tinne. 2010. Kernelized sorting. IEEE Trans. on Pattern Analysis and Machine Intelligence, 32(10):1809–1821."},{"authors":[{"first":"Jhon","last":"Platt"},{"first":"Kristina","last":"Toutanova"},{"first":"Wen-tau","last":"Yih"}],"year":"2010","title":"Translingual document representation from discriminative projections","source":"Jhon Platt, Kristina Toutanova, and Wen-tau Yih. 2010. Translingual document representation from discriminative projections. In Proc. of the 2010 Conference on EMNLP, pages 251–261."},{"authors":[{"first":"Abhishek","last":"Tripathi"},{"first":"Arto","last":"Klami"},{"first":"Sami","last":"Virpioja"}],"year":"2010","title":"Bilingual sentence matching using kernel CCA","source":"Abhishek Tripathi, Arto Klami, and Sami Virpioja. 2010. Bilingual sentence matching using kernel CCA. In Proc. of the 2010 IEEE International Workshop on MLSP, pages 130–135."},{"authors":[{"first":"Ni","last":"Xiaochuan"},{"first":"Sun","last":"Lian-Tao"},{"first":"Hu","last":"Jian"},{"first":"Chen","last":"Zheng"}],"year":"2011","title":"Cross lingual text classification by mining multilingual topics from wikipedia","source":"Ni Xiaochuan, Sun Lian-Tao, Hu Jian, and Chen Zheng. 2011. Cross lingual text classification by mining multilingual topics from wikipedia. In Proc. of the 4th WSDM, pages 375–384."},{"authors":[{"first":"Makoto","last":"Yamada"},{"first":"Masashi","last":"Sugiyama"}],"year":"2011","title":"Cross-domain object matching with model selection","source":"Makoto Yamada and Masashi Sugiyama. 2011. Cross-domain object matching with model selection. In Proc. of the 14th AISTATS, pages 807–815. 216"}],"cites":[{"style":0,"text":"Novi et al., 2010","origin":{"pointer":"/sections/2/paragraphs/0","offset":199,"length":17},"authors":[{"last":"Novi"},{"last":"al."}],"year":"2010","references":["/references/10"]},{"style":0,"text":"Haghighi et al., 2008","origin":{"pointer":"/sections/2/paragraphs/0","offset":268,"length":21},"authors":[{"last":"Haghighi"},{"last":"al."}],"year":"2008","references":["/references/7"]},{"style":0,"text":"Tripathi et al., 2010","origin":{"pointer":"/sections/2/paragraphs/0","offset":291,"length":21},"authors":[{"last":"Tripathi"},{"last":"al."}],"year":"2010","references":["/references/12"]},{"style":0,"text":"Dumais et al., 1996","origin":{"pointer":"/sections/2/paragraphs/0","offset":761,"length":19},"authors":[{"last":"Dumais"},{"last":"al."}],"year":"1996","references":["/references/5"]},{"style":0,"text":"Gliozzo and Strapparava, 2005","origin":{"pointer":"/sections/2/paragraphs/0","offset":782,"length":29},"authors":[{"last":"Gliozzo"},{"last":"Strapparava"}],"year":"2005","references":["/references/6"]},{"style":0,"text":"Platt et al., 2010","origin":{"pointer":"/sections/2/paragraphs/0","offset":813,"length":18},"authors":[{"last":"Platt"},{"last":"al."}],"year":"2010","references":["/references/11"]},{"style":0,"text":"Deerwester et al., 1990","origin":{"pointer":"/sections/2/paragraphs/2","offset":1023,"length":23},"authors":[{"last":"Deerwester"},{"last":"al."}],"year":"1990","references":["/references/3"]},{"style":0,"text":"Dumais et al., 1996","origin":{"pointer":"/sections/3/paragraphs/0","offset":143,"length":19},"authors":[{"last":"Dumais"},{"last":"al."}],"year":"1996","references":["/references/5"]},{"style":0,"text":"Platt et al., 2010","origin":{"pointer":"/sections/3/paragraphs/0","offset":218,"length":18},"authors":[{"last":"Platt"},{"last":"al."}],"year":"2010","references":["/references/11"]},{"style":0,"text":"Blei et al., 2003","origin":{"pointer":"/sections/3/paragraphs/0","offset":586,"length":17},"authors":[{"last":"Blei"},{"last":"al."}],"year":"2003","references":["/references/1"]},{"style":0,"text":"Boyd-Graber and Blei, 2009","origin":{"pointer":"/sections/3/paragraphs/0","offset":669,"length":26},"authors":[{"last":"Boyd-Graber"},{"last":"Blei"}],"year":"2009","references":["/references/2"]},{"style":0,"text":"Jagadeesh and Daume III, 2010","origin":{"pointer":"/sections/3/paragraphs/0","offset":709,"length":29},"authors":[{"last":"Jagadeesh"},{"last":"Daume III"}],"year":"2010","references":["/references/8"]},{"style":0,"text":"Xiaochuan et al., 2011","origin":{"pointer":"/sections/3/paragraphs/0","offset":762,"length":22},"authors":[{"last":"Xiaochuan"},{"last":"al."}],"year":"2011","references":["/references/13"]},{"style":0,"text":"Novi et al., 2010","origin":{"pointer":"/sections/3/paragraphs/1","offset":66,"length":17},"authors":[{"last":"Novi"},{"last":"al."}],"year":"2010","references":["/references/10"]},{"style":0,"text":"Haghighi et al., 2008","origin":{"pointer":"/sections/3/paragraphs/1","offset":85,"length":21},"authors":[{"last":"Haghighi"},{"last":"al."}],"year":"2008","references":["/references/7"]},{"style":0,"text":"Yamada and Sugiyama, 2011","origin":{"pointer":"/sections/3/paragraphs/1","offset":108,"length":25},"authors":[{"last":"Yamada"},{"last":"Sugiyama"}],"year":"2011","references":["/references/14"]},{"style":0,"text":"Novi et al., 2010","origin":{"pointer":"/sections/3/paragraphs/1","offset":255,"length":17},"authors":[{"last":"Novi"},{"last":"al."}],"year":"2010","references":["/references/10"]},{"style":0,"text":"Djuric et al., 2012","origin":{"pointer":"/sections/3/paragraphs/1","offset":529,"length":19},"authors":[{"last":"Djuric"},{"last":"al."}],"year":"2012","references":["/references/4"]},{"style":0,"text":"Yamada and Sugiyama, 2011","origin":{"pointer":"/sections/3/paragraphs/1","offset":609,"length":25},"authors":[{"last":"Yamada"},{"last":"Sugiyama"}],"year":"2011","references":["/references/14"]},{"style":0,"text":"Haghighi et al., 2008","origin":{"pointer":"/sections/3/paragraphs/1","offset":750,"length":21},"authors":[{"last":"Haghighi"},{"last":"al."}],"year":"2008","references":["/references/7"]},{"style":0,"text":"Bach and Jordan, 2005","origin":{"pointer":"/sections/3/paragraphs/1","offset":870,"length":21},"authors":[{"last":"Bach"},{"last":"Jordan"}],"year":"2005","references":["/references/0"]},{"style":0,"text":"Lee and Seung, 2000","origin":{"pointer":"/sections/4/paragraphs/3","offset":68,"length":19},"authors":[{"last":"Lee"},{"last":"Seung"}],"year":"2000","references":["/references/9"]},{"style":0,"text":"Novi et al., 2010","origin":{"pointer":"/sections/7/paragraphs/3","offset":298,"length":17},"authors":[{"last":"Novi"},{"last":"al."}],"year":"2010","references":["/references/10"]}]}
