{"sections":[{"title":"","paragraphs":["Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 137–142, Sofia, Bulgaria, August 4-9 2013. c⃝2013 Association for Computational Linguistics"]},{"title":"On the Predictability of Human Assessment: when Matrix Completion Meets NLP Evaluation Guillaume Wisniewski Université Paris Sud LIMSI–CNRS Orsay, France guillaume.wisniewski@limsi.fr Abstract","paragraphs":["This paper tackles the problem of collecting reliable human assessments. We show that knowing multiple scores for each example instead of a single score results in a more reliable estimation of a system quality. To reduce the cost of collecting these multiple ratings, we propose to use matrix completion techniques to predict some scores knowing only scores of other judges and some common ratings. Even if prediction performance is pretty low, decisions made using the predicted score proved to be more reliable than decision based on a single rating of each example."]},{"title":"1 Introduction","paragraphs":["Human assessment is often considered as the best, if not the only, way to evaluate ‘subjective’ NLP tasks like MT or speech generation. However, human evaluations are doomed to be noisy and, sometimes, even contradictory as they depend on individual perception and understanding of the score scale that annotators generally use in remarkably different ways (Koehn and Monz, 2006). Moreover, annotation is known to be a long and frustrating process and annotator fatigue has been identified as another source of noise (Pighin et al., 2012).","In addition to defining and enforcing stricter guidelines, several solutions have been proposed to reduce the annotation effort and produce more reliable ratings. For instance, to limit the impact of the score scale interpretation, in the WMT evaluation campaign (Callison-Burch et al., 2012), annotators are asked to rank translation hypotheses from best to worst instead of providing absolute scores (e.g. in terms of adequacy or fluency). Generalizing this approach, several works (Pighin et al., 2012; Lopez, 2012) have defined novel annotation protocols to reduce the number of judgments that need to be collected. However, all these methods suffer from several limitations: first, they provide no interpretable information about the quality of the system (only a relative comparison between two systems is possible); second, (Koehn, 2012) has recently shown that the ranking they induce is not reliable.","In this work, we study an alternative approach to the problem of collecting reliable human assessments. Our basic assumption, motivated by the success of ensemble methods, is that having several judgments for each example, even if they are noisy, will result in a more reliable decision than having a single judgment. An evaluation campaign should therefore aim at gathering a score matrix, in which each example is rated by all judges instead of having each judge rate only a small subset of examples, thereby minimizing redundancy. Obviously, the former approach requires a large annotation effort and is, in practice, not feasible. That is why, to reduce the number of judgments that must be collected, we propose to investigate the possibility of using matrix completion techniques to recover the entire score matrix from a sample of its entries. The question we try to answer is whether the missing scores of one judge can be predicted knowing only scores of other judges and some shared ratings.","The contributions of this paper are twofold: i) we show how knowing the full score matrix instead of a single score for each example provides a more reliable estimation of a system quality (Section 3); ii) we present preliminary experiments 137 showing that missing data techniques can be used to recover the score matrix from a sample of its entries despite the low inter-rater agreement (Section 4)."]},{"title":"2 Matrix Completion","paragraphs":["The recovering of a matrix from a sampling of its entries is a task of considerable interest (Candès and Recht, 2012). It can be used, for instance, in recommender systems: rows of the matrix represent users that are rating movies (columns of the matrix); the resulting matrix is mostly unknown (each user only rates a few movies) and the task consists in completing the matrix so that movies that any user is likely to like can be predicted.","Matrix completion generally relies on the low rank hypothesis: because of hidden factors between the observations (the columns of the matrix), the matrix has a low rank. For instance, in recommender systems it is commonly believed that only a few factors contribute to an individual’s tastes. Formally, recovering a matrix M amounts at solving: minimize rank X subject to Xij = Mij (i, j) ∈ Ω (1) where X is the decision variable and Ω is the set of known entries. This optimization problem seeks the simplest explanation fitting the observed data.","Solving the rank minimization problem has been proved to be NP-hard (Chistov and Grigor’ev, 1984). However several convex relaxations of this program have been proposed. In this work, we will consider the relaxation of the rank by the nuclear norm1","that can be efficiently solved by semidefinite programming (Becker et al., 2011). This relaxation enjoys many theoretical guarantees with respect to the optimality of its solution (under mild assumptions its solution is also the solution of the original problem), the conditions under which the matrix can be recovered and the number of entries that must be sampled to recover the original matrix. In our experiments we used TFOCS,2","a free implementation of this method. 1 The nuclear norm of a matrix is the sum of its singular","values; the relation between rank an nuclear norm is similar","to the one between l0 and l1 norms. 2 http://cvxr.com/tfocs/"]},{"title":"3 Corpora","paragraphs":["For our experiments we considered two publicly available corpora in which multiple human ratings (i.e. scores on an ordinal scale) were available. The CE Corpus The first corpus of human judgments we have considered has been collected for the WMT12 shared task on quality estimation (Callison-Burch et al., 2012).3","The data set is made of 2, 254 English sentences and their automatic translations in Spanish predicted by a standard Moses system. Each sentence pair is accompanied by three estimates in the range 1 to 5 of its translation quality expressed in terms of post-editing effort. These human grades are in the range 1 to 5, the latter standing for a very good translation that hardly requires post-editing, while the former identifies very poor automatic translations that are not deemed to be worth the post-editing effort.","As pointed out by the task organizers, despite the special care that was taken to ensure the quality of the data, the inter-raters agreement was much lower than what is typically observed in NLP tasks (Artstein and Poesio, 2008): the weighted κ ranged from 0.39 to 0.50 depending on the pair of annotators considered4","; the Fleiss coefficient (a generalization of κ to multi-raters) was 0.25 and the Kendall τb correlation coefficient5","between 0.64 and 0.68, meaning that, on average, two raters do not agree on the relative order of two translations almost two out of five times. In fact, as often observed for the sentence level human evaluation of MT outputs, the different judges have used the score scale differently: the second judge had a clear tendency to give more ‘medium’ scores than the others, and the variance of her scores was low. Because theirs distributions are different, standardizing the scores has only a very limited impact on the agreement.","If, as in many manual evaluations, each example had been rated by a single judge chosen randomly, the resulting scores would have been only moderately correlated with the average of the three scores which is, intuitively, a better estimate of the ‘true’ quality: the 95% confidence interval of the 3 The corpus is available from http://www.statmt.","org/wmt12/quality-estimation-task.html 4 The weighted κ is a generalization of the κ to ordinal","data; a linear weighting schema was used. 5 Note that, in statistics, agreement is a stronger notion","than correlation, as the former compare the actual values. 138 τb between the averaged scores and the ‘sampled’ score is 0.754–0.755. TIDES The second corpus considered was collected for the DARPA TIDES program: a team of human judges provided multiple assessments of adequacy and fluency for Arabic to English and Chinese to English automatic translations.6","For space reasons, only results on the Chinese to English fluency corpus will be presented; similar results were achieved on the other corpora.","In the considered corpus, 31 sets of automatic translations, generated by three systems, have been rated by two judges on a scale of 1 to 5. The inter-rater agreement is very low: depending on the pair of judges, the weighted κ is between -0.05 and 0.2, meaning that agreement occurs less often than predicted by chance alone. More importantly, if the ratings of a pair of judges were used to decide which is the best system among two, the two judges will disagree 36% of the time. This ‘agreement’ score is computed as follows: if mA,i is the mean of the scores given to system A by the i-th annotator, we say that there is no agreement in a pairwise comparison if mA,i > mB,i and mA,j < mB,j, i.e. if two judges rank two systems in a different order; the score is then the percentage of agreement when considering all pairs of systems and judges.","Considering the full scoring matrix instead of single scores has a large impact: if each example is rated by a single judge (chosen randomly), the resulting comparison between the two systems will be different from the decision made by averaging the two scores of the full score matrix in almost 20% of the comparisons."]},{"title":"4 Experimental Results 4.1 Testing the Low-Rank Hypothesis","paragraphs":["Matrix completion relies on the hypothesis that the matrix has a low rank. We first propose to test this hypothesis on simulated data, using a method similar to the one proposed in (Mathet et al., 2012), to evaluate the impact of noise in human judgments on the score matrix rank. Artificial ratings are generated as follows: a MT system is producing n translations the quality of which, qi, is estimated by a continuous value, that represents, for instance, a hTER score. This 6 These corpora are available from LDC under the refer-","ences ldc2003t17 and ldc2003t18 value is drawn from N ( μ, σ2)",". Based on this ‘intrinsic’ quality, two ratings, ai and bi, are generated according to three strategies: in the first, ai and bi are sampled from N (qi, θ); in the second, ai ∼ N (","qi + θ 2 , σ′2) and bi ∼ N (","qi − θ 2 , σ′2) and in the third, ai ∼ N (","qi, σ′2)","and the bi is drawn from a bimodal distribu-","tion 1","2 ( N (","qi − θ 2 , σ′2) + N (","qi + θ 2 , σ′2)) (with σ′2","< θ","2 ). θ describes the noise level.","Each of these strategies models a different kind of noise that has been observed in different evaluation campaigns (Koehn and Monz, 2006): the first one describes random noise in the ratings; the second a systematic difference in the annotators’ in-terpretation of the score scale and the third, the situation in which one annotator gives medium score while the other one tend to commit more strongly to whether she considered the translation good or bad. Stacking all these judgments results in a n×2 score matrix. To test whether this matrix has a low rank or not, we assess how close it is to its approximation by a rank 1 matrix. A well-known result (Lawson and Hanson, 1974) states that the Frobenius norm of the difference of these matrices is equal to the 2nd singular value of the original matrix; the quality of the approximation can thus be estimated by ρ, defined as the 2nd eigen-value of the matrix normalized by its norm (Leon, 1994). Intuitively, the smaller ρ, the better the approximation.","Figure 1 represents the impact of the noise level on the condition number. As a baseline, we have also represented ρ for a random matrix. All values are averaged over 100 simulations. As it could be expected, ρ is close to 0 for small noise level; but even for moderate noise level, the second eigen-value continue to be small, suggesting that the matrix can still be approximated by a matrix of rank 1 without much loss of information. As a comparison, on average, ρ = 0.08 for the CE score matrix, in spite of the low inter-rater agreement. 4.2 Prediction Performance We conducted several experiments to evaluate the possibility to use matrix completion to recover a score matrix. Experiments consist in choosing randomly k% of the entries of a matrix; these entries are considered unknown and predicted using the method introduced in Section 2 denoted pred in the following. In our experiments k varies from 10% to 40%. Note that, when, as in our exper-139 0 0.1 0.2 0.3 0.4 0 0.1 0.2 0.3 0.4 random θ ρ 1st strat. 2nd strat. 3rd strat. Figure 1: Evolution of the condition number ρ with the noise level θ for the different strategies (see text for details) iments, only two judges are involved, k = 50% would mean that each example is rated by a single judge. Two simple methods for handling missing data are used as baselines: in the first one, denoted rand, missing scores are chosen randomly; the second one, denoted mean, predicts for all the missing scores of a judge the mean of her known scores.","We propose to evaluate the quality of the recovery, first by comparing the predicted score to their true value and then by evaluating the decision that will be made when considering the recovered matrix instead of the full matrix. Prediction Performance Comparing the completed matrix to the original score matrix can be done in terms of Mean Absolute Error (MAE) defined as 1","N","∑N","i=1 |yi − ŷi| where ŷi is the predicted value and yi the corresponding ‘true’ value; the sum runs over all unknown values of the matrix.","Table 1 presents the results achieved by the different methods. All reported results are averaged over 10 runs (i.e.: sampling of the score matrix and prediction of the missing scores) and over all pairs of judges. All tables also report the 95% confidence interval. The MAE of therand method is almost constant, whatever the number of samples is. Performance of the matrix completion technique is not so good: predicted scores are quite different than true scores. In particular, performance falls quickly when the number of missing data increases. This observation is not surprising: when 40% of the scores are missing, only a few examples have more than a single score and many have no score at all. In these conditions recovering missing data pred mean","40% 0.78 ±6.21 × 10−3","0.72 ±8.86 × 10−3","30% 0.83 ±3.19 × 10−3","0.80 ±5.42 × 10−3","20% 0.88 ±2.49 × 10−3","0.87 ±3.54 × 10−3","10% 0.93 ±1.76 × 10−3","0.92 ±1.51 × 10−3 Table 2: Correlation between the rankings induced by the recovered matrix and the original score matrix for the CE corpus the matrix is almost impossible. The performance of the simple mean technique is, comparatively, pretty good, especially when only a few entries are known. However, the pred method always outperform the rand method showing that there are dependencies between the two ratings even if statistical measures of agreement are low. Impact on the Decision The negative results of the previous paragraph only provide indirect measure of the recovery quality as it is not the value of the score that is important but the decision that it will support. That is why, we also evaluated matrix recovery in a more task-oriented way by comparing the decision made when considering the recovered score matrix instead of the ‘true’ score matrix.","For the CE corpus, a task-oriented evaluation can be done by comparing the rankings induced by the recovered matrix and by the original matrix when examples are ordered according to their averaged score. Such a ranking can be used by a MT user to set a quality threshold granting her control over translation quality (Soricut and Echihabi, 2010). Table 2 shows the correlation between the two rankings as evaluated by τb. The two rankings appear to be highly correlated, the matrix completion technique outperforming slightly the mean baseline. More importantly, even when 40% of the data are missing, the ranking induced by the true scores is better correlated to the ranking induced by the predicted scores than to the ranking induced when each example is only rated once: as reported in Section 3, the τb is, in this case, 0.75.","For the TIDES corpus, we computed the number of pairs of judges for which the results of a pairwise comparison between two systems is different when the systems are evaluated using the predicted scores and the true scores. Results presented in Table 3 show that considering the predicted matrix is far better than having judges rate 140 QE TIDES k pred mean rand pred mean rand","40% 1.14 ±2.9 · 10−2","0.78 ±6.6 · 10−3 1.45 — — —","30% 0.94 ±2.9 · 10−2","0.78 ±7.4 · 10−3 1.44 0.95 ±2.7 · 10−2","0.43 ±2.6 · 10−2","1.37","20% 0.77 ±3.4 · 10−2","0.78 ±1.0 · 10−2 1.45 0.76 ±2.6 · 10−2","0.41 ±2.5 · 10−2","1.38","10% 0.65 ±2.1 · 10−2","0.79 ±1.9 · 10−2 1.47 0.48 ±3.0 · 10−2","0.41 ±2.5 · 10−2","1.36 Table 1: Completion performance as evaluated by the MAE for the three prediction methods and the three corpora considered. random samples of the examples: the number of disagreement falls from 20% (Sect. 3) to less than 4%. While the mean method outperforms the pred method, this result shows that, even in case of low inter-rater agreement, there is still enough information to predict the score of one annotator knowing only the score of the others.","For the tasks considered, decisions based on a recovered matrix are therefore more similar to decisions made considering the full score matrix than decisions based on a single rating of each example."]},{"title":"5 Conclusion","paragraphs":["This paper proposed a new way of collecting reliable human assessment. We showed, on two corpora, that knowing multiple scores for each example instead of a single score results in a more reliable estimation of the quality of a NLP system. We proposed to used matrix completion techniques to reduce the annotation effort required to collect these multiple ratings. Our experiments showed that while scores predicted using these techniques are pretty different from the true scores, decisions considering them are more reliable than decisions based on a single score.","Even if it can not predict scores accurately, we believe that the connection between NLP evaluation and matrix completion has many potential applications. For instance, it can be applied to identify errors made when collecting scores by comparing the predicted and actual scores."]},{"title":"6 Acknowledgments","paragraphs":["This work was partly supported by ANR project Trace (ANR-09-CORD-023). The author would like to thank Franco̧is Yvon and Nicolas Pécheux for their helpful questions and comments on the various drafts of this work. % missing data pred mean 30% 9.24% 3.53 % 20% 6.45% 2.10 % 10% 3.66% 1.20 % Table 3: Disagreements in a pairwise comparison of two systems of the TIDES corpus, when the systems are evaluated using the predicted scores and the true scores"]},{"title":"References","paragraphs":["Ron Artstein and Massimo Poesio. 2008. Inter-coder agreement for computational linguistics. Comput. Linguist., 34(4):555–596, December.","Stephen R. Becker, Emmanuel J. Candès, and Michael C. Grant. 2011. Templates for convex cone problems with applications to sparse signal recovery. Math. Prog. Comput., 3(3):165–218.","Chris Callison-Burch, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2012. Findings of the 2012 workshop on statistical machine translation. In Proc. of WMT, pages 10–51, Montréal, Canada, June. ACL.","Emmanuel Candès and Benjamin Recht. 2012. Exact matrix completion via convex optimization. Commun. ACM, 55(6):111–119, June.","A. Chistov and D. Grigor’ev. 1984. Complexity of quantifier elimination in the theory of algebraically closed fields. In M. Chytil and V. Koubek, editors, Math. Found. of Comp. Science, volume 176, pages 17–31. Springer Berlin / Heidelberg.","Philipp Koehn and Christof Monz. 2006. Manual and automatic evaluation of machine translation between european languages. In Proc. WMT, pages 102–121, New York City, June. ACL.","Philipp Koehn. 2012. Simulating human judgment in machine translation evaluation campaigns. In Proc. of IWSLT.","Charles L. Lawson and Richard J. Hanson. 1974. Solving Least Squares Problems. Prentice Hall. 141","Stephen J: Leon. 1994. Linear Algebra with Applications. Macmillan,.","Adam Lopez. 2012. Putting human assessments of machine translation systems in order. In Proc. of WMT, pages 1–9, Montréal, Canada, June. ACL.","Yann Mathet, Antoine Widlcher, Karën Fort, Claire Franco̧is, Olivier Galibert, Cyril Grouin, Juliette Kahn, Sophie Rosset, and Pierre Zweigenbaum. 2012. Manual corpus annotation: Giving meaning to the evaluation metrics. In Proceedings of COL-ING 2012: Posters, pages 809–818, Mumbai, India, December.","Daniele Pighin, Lluı́s Formiga, and Lluı́s Màrquez. 2012. A graph-based strategy to streamline translation quality assessments. In Proc. of AMTA.","Matthew Snover, Nitin Madnani, Bonnie Dorr, and Richard Schwartz. 2009. Fluency, adequacy, or HTER? Exploring different human judgments with a tunable MT metric. In Proc. of WMT, pages 259– 268, Athens, Greece, March. ACL.","Radu Soricut and Abdessamad Echihabi. 2010. Trustrank: Inducing trust in automatic translations via ranking. In Proc. of ACL, pages 612–621, Uppsala, Sweden, July. ACL. 142"]}],"references":[{"authors":[{"first":"Ron","last":"Artstein"},{"first":"Massimo","last":"Poesio"}],"year":"2008","title":"Inter-coder agreement for computational linguistics","source":"Ron Artstein and Massimo Poesio. 2008. Inter-coder agreement for computational linguistics. Comput. Linguist., 34(4):555–596, December."},{"authors":[{"first":"Stephen","middle":"R.","last":"Becker"},{"first":"Emmanuel","middle":"J.","last":"Candès"},{"first":"Michael","middle":"C.","last":"Grant"}],"year":"2011","title":"Templates for convex cone problems with applications to sparse signal recovery","source":"Stephen R. Becker, Emmanuel J. Candès, and Michael C. Grant. 2011. Templates for convex cone problems with applications to sparse signal recovery. Math. Prog. Comput., 3(3):165–218."},{"authors":[{"first":"Chris","last":"Callison-Burch"},{"first":"Philipp","last":"Koehn"},{"first":"Christof","last":"Monz"},{"first":"Matt","last":"Post"},{"first":"Radu","last":"Soricut"},{"first":"Lucia","last":"Specia"}],"year":"2012","title":"Findings of the 2012 workshop on statistical machine translation","source":"Chris Callison-Burch, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2012. Findings of the 2012 workshop on statistical machine translation. In Proc. of WMT, pages 10–51, Montréal, Canada, June. ACL."},{"authors":[{"first":"Emmanuel","last":"Candès"},{"first":"Benjamin","last":"Recht"}],"year":"2012","title":"Exact matrix completion via convex optimization","source":"Emmanuel Candès and Benjamin Recht. 2012. Exact matrix completion via convex optimization. Commun. ACM, 55(6):111–119, June."},{"authors":[{"first":"A.","last":"Chistov"},{"first":"D.","last":"Grigor’ev"}],"year":"1984","title":"Complexity of quantifier elimination in the theory of algebraically closed fields","source":"A. Chistov and D. Grigor’ev. 1984. Complexity of quantifier elimination in the theory of algebraically closed fields. In M. Chytil and V. Koubek, editors, Math. Found. of Comp. Science, volume 176, pages 17–31. Springer Berlin / Heidelberg."},{"authors":[{"first":"Philipp","last":"Koehn"},{"first":"Christof","last":"Monz"}],"year":"2006","title":"Manual and automatic evaluation of machine translation between european languages","source":"Philipp Koehn and Christof Monz. 2006. Manual and automatic evaluation of machine translation between european languages. In Proc. WMT, pages 102–121, New York City, June. ACL."},{"authors":[{"first":"Philipp","last":"Koehn"}],"year":"2012","title":"Simulating human judgment in machine translation evaluation campaigns","source":"Philipp Koehn. 2012. Simulating human judgment in machine translation evaluation campaigns. In Proc. of IWSLT."},{"authors":[{"first":"Charles","middle":"L.","last":"Lawson"},{"first":"Richard","middle":"J.","last":"Hanson"}],"year":"1974","title":"Solving Least Squares Problems","source":"Charles L. Lawson and Richard J. Hanson. 1974. Solving Least Squares Problems. Prentice Hall. 141"},{"authors":[{"first":"Stephen","middle":"J:","last":"Leon"}],"year":"1994","title":"Linear Algebra with Applications","source":"Stephen J: Leon. 1994. Linear Algebra with Applications. Macmillan,."},{"authors":[{"first":"Adam","last":"Lopez"}],"year":"2012","title":"Putting human assessments of machine translation systems in order","source":"Adam Lopez. 2012. Putting human assessments of machine translation systems in order. In Proc. of WMT, pages 1–9, Montréal, Canada, June. ACL."},{"authors":[{"first":"Yann","last":"Mathet"},{"first":"Antoine","last":"Widlcher"},{"first":"Karën","last":"Fort"},{"first":"Claire","last":"Franco̧is"},{"first":"Olivier","last":"Galibert"},{"first":"Cyril","last":"Grouin"},{"first":"Juliette","last":"Kahn"},{"first":"Sophie","last":"Rosset"},{"first":"Pierre","last":"Zweigenbaum"}],"year":"2012","title":"Manual corpus annotation: Giving meaning to the evaluation metrics","source":"Yann Mathet, Antoine Widlcher, Karën Fort, Claire Franco̧is, Olivier Galibert, Cyril Grouin, Juliette Kahn, Sophie Rosset, and Pierre Zweigenbaum. 2012. Manual corpus annotation: Giving meaning to the evaluation metrics. In Proceedings of COL-ING 2012: Posters, pages 809–818, Mumbai, India, December."},{"authors":[{"first":"Daniele","last":"Pighin"},{"first":"Lluı́s","last":"Formiga"},{"first":"Lluı́s","last":"Màrquez"}],"year":"2012","title":"A graph-based strategy to streamline translation quality assessments","source":"Daniele Pighin, Lluı́s Formiga, and Lluı́s Màrquez. 2012. A graph-based strategy to streamline translation quality assessments. In Proc. of AMTA."},{"authors":[{"first":"Matthew","last":"Snover"},{"first":"Nitin","last":"Madnani"},{"first":"Bonnie","last":"Dorr"},{"first":"Richard","last":"Schwartz"}],"year":"2009","title":"Fluency, adequacy, or HTER? Exploring different human judgments with a tunable MT metric","source":"Matthew Snover, Nitin Madnani, Bonnie Dorr, and Richard Schwartz. 2009. Fluency, adequacy, or HTER? Exploring different human judgments with a tunable MT metric. In Proc. of WMT, pages 259– 268, Athens, Greece, March. ACL."},{"authors":[{"first":"Radu","last":"Soricut"},{"first":"Abdessamad","last":"Echihabi"}],"year":"2010","title":"Trustrank: Inducing trust in automatic translations via ranking","source":"Radu Soricut and Abdessamad Echihabi. 2010. Trustrank: Inducing trust in automatic translations via ranking. In Proc. of ACL, pages 612–621, Uppsala, Sweden, July. ACL. 142"}],"cites":[{"style":0,"text":"Koehn and Monz, 2006","origin":{"pointer":"/sections/2/paragraphs/0","offset":358,"length":20},"authors":[{"last":"Koehn"},{"last":"Monz"}],"year":"2006","references":["/references/5"]},{"style":0,"text":"Pighin et al., 2012","origin":{"pointer":"/sections/2/paragraphs/0","offset":518,"length":19},"authors":[{"last":"Pighin"},{"last":"al."}],"year":"2012","references":["/references/11"]},{"style":0,"text":"Callison-Burch et al., 2012","origin":{"pointer":"/sections/2/paragraphs/1","offset":264,"length":27},"authors":[{"last":"Callison-Burch"},{"last":"al."}],"year":"2012","references":["/references/2"]},{"style":0,"text":"Pighin et al., 2012","origin":{"pointer":"/sections/2/paragraphs/1","offset":485,"length":19},"authors":[{"last":"Pighin"},{"last":"al."}],"year":"2012","references":["/references/11"]},{"style":0,"text":"Lopez, 2012","origin":{"pointer":"/sections/2/paragraphs/1","offset":506,"length":11},"authors":[{"last":"Lopez"}],"year":"2012","references":["/references/9"]},{"style":0,"text":"Koehn, 2012","origin":{"pointer":"/sections/2/paragraphs/1","offset":832,"length":11},"authors":[{"last":"Koehn"}],"year":"2012","references":["/references/6"]},{"style":0,"text":"Candès and Recht, 2012","origin":{"pointer":"/sections/3/paragraphs/0","offset":94,"length":22},"authors":[{"last":"Candès"},{"last":"Recht"}],"year":"2012","references":["/references/3"]},{"style":0,"text":"Chistov and Grigor’ev, 1984","origin":{"pointer":"/sections/3/paragraphs/2","offset":69,"length":27},"authors":[{"last":"Chistov"},{"last":"Grigor’ev"}],"year":"1984","references":["/references/4"]},{"style":0,"text":"Becker et al., 2011","origin":{"pointer":"/sections/3/paragraphs/3","offset":60,"length":19},"authors":[{"last":"Becker"},{"last":"al."}],"year":"2011","references":["/references/1"]},{"style":0,"text":"Callison-Burch et al., 2012","origin":{"pointer":"/sections/4/paragraphs/0","offset":284,"length":27},"authors":[{"last":"Callison-Burch"},{"last":"al."}],"year":"2012","references":["/references/2"]},{"style":0,"text":"Artstein and Poesio, 2008","origin":{"pointer":"/sections/4/paragraphs/2","offset":202,"length":25},"authors":[{"last":"Artstein"},{"last":"Poesio"}],"year":"2008","references":["/references/0"]},{"style":0,"text":"Mathet et al., 2012","origin":{"pointer":"/sections/5/paragraphs/0","offset":182,"length":19},"authors":[{"last":"Mathet"},{"last":"al."}],"year":"2012","references":["/references/10"]},{"style":0,"text":"Koehn and Monz, 2006","origin":{"pointer":"/sections/5/paragraphs/13","offset":116,"length":20},"authors":[{"last":"Koehn"},{"last":"Monz"}],"year":"2006","references":["/references/5"]},{"style":0,"text":"Lawson and Hanson, 1974","origin":{"pointer":"/sections/5/paragraphs/13","offset":655,"length":23},"authors":[{"last":"Lawson"},{"last":"Hanson"}],"year":"1974","references":["/references/7"]},{"style":0,"text":"Leon, 1994","origin":{"pointer":"/sections/5/paragraphs/13","offset":936,"length":10},"authors":[{"last":"Leon"}],"year":"1994","references":["/references/8"]},{"style":0,"text":"Soricut and Echihabi, 2010","origin":{"pointer":"/sections/5/paragraphs/28","offset":318,"length":26},"authors":[{"last":"Soricut"},{"last":"Echihabi"}],"year":"2010","references":["/references/13"]}]}
