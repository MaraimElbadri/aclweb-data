{"sections":[{"title":"","paragraphs":["Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 855–859, Sofia, Bulgaria, August 4-9 2013. c⃝2013 Association for Computational Linguistics"]},{"title":"Identifying Sentiment Words Using an Optimization-based Model without Seed Words Hongliang Yu","paragraphs":["1"]},{"title":", Zhi-Hong Deng","paragraphs":["2∗"]},{"title":", Shiyingxue Li","paragraphs":["3"]},{"title":"Key Laboratory of Machine Perception (Ministry of Education), School of Electronics Engineering and Computer Science, Peking University, Beijing 100871, China","paragraphs":["1"]},{"title":"yuhongliang324@gmail.com","paragraphs":["2"]},{"title":"zhdeng@cis.pku.edu.cn","paragraphs":["3"]},{"title":"rachellieinspace@gmail.com Abstract","paragraphs":["Sentiment Word Identification (SWI) is a basic technique in many sentiment analysis applications. Most existing research-es exploit seed words, and lead to low robustness. In this paper, we propose a novel optimization-based model for SWI. Unlike previous approaches, our model exploits the sentiment labels of documents instead of seed words. Several experiments on real datasets show that WEED is effective and outperforms the state-of-the-art methods with seed words."]},{"title":"1 Introduction","paragraphs":["In recent years, sentiment analysis (Pang et al., 2002) has become a hotspot in opinion mining and attracted much attention. Sentiment analysis is to classify a text span into different sentiment polarities, i.e. positive, negative or neutral. Sentiment Word Identification (SWI) is a basic technique in sentiment analysis. According to (Ku et al., 2006)(Chen et al., 2012)(Fan et al., 2011), SWI can be applied to many fields, such as determin-ing critics opinions about a given product, tweeter classification, summarization of reviews, and message filtering, etc. Thus in this paper, we focus on SWI.","Here is a simple example of how SWI is applied to comment analysis. The sentence below is an movie review in IMDB database:","• Bored performers and a lackluster plot and script, do not make a good action movie. In order to judge the sentence polarity (thus we can learn about the preference of this user), one must recognize which words are able to express sentiment. In this sentence, “bored” and “lackluster” are negative while “good” should be positive, yet","∗","Corresponding author its polarity is reversed by “not”. By such analysis, we then conclude such movie review is a negative comment. But how do we recognize sentiment words?","To achieve this, previous supervised approaches need labeled polarity words, also called seed words, usually manually selected. The words to be classified by their sentiment polarities are called candidate words. Prior works study the relations between labeled seed words and unlabeled candidate words, and then obtain sentiment polarities of candidate words by these relations. There are many ways to generate word relations. The authors of (Turney and Littman, 2003) and (Kaji and Kitsuregawa, 2007) use statistical measures, such as point wise mutual information (PMI), to compute similarities in words or phrases. Kanayama and Nasukawa (2006) assume sentiment words successively appear in the text, so one could find sentiment words in the context of seed words (Kanayama and Nasukawa, 2006). In (Hassan and Radev, 2010) and (Hassan et al., 2011), a Markov random walk model is applied to a large word relatedness graph, constructed according to the synonyms and hypernyms in WordNet (Miller, 1995).","However, approaches based on seed words has obvious shortcomings. First, polarities of seed words are not reliable for various domains. As a simple example, “rise” is a neutral word most often, but becomes positive in stock market. Second, manually selection of seed words can be very subjective even if the application domain is determined. Third, algorithms using seed words have low robustness. Any missing key word in the set of seed words could lead to poor performance. Therefore, the seed word set of such algorithms demands high completeness (by containing common polarity words as many as possible).","Unlike the previous research work, we identify sentiment words without any seed words in this paper. Instead, the documents’ bag-of-words in-855 formation and their polarity labels are exploited in the identification process. Intuitively, polarities of the document and its most component sentiment words are the same. We call such phenomenon as “sentiment matching”. Moreover, if a word is found mostly in positive documents, it is very likely a positive word, and vice versa.","We present an optimization-based model, called WEED, to exploit the phenomenon of “sentiment matching”. We first measure the importance of the component words in the labeled documents semantically. Here, the basic assumption is that important words are more sentiment related to the document than those less important. Then, we estimate the polarity of each document using its component words’ importance along with their sentiment values, and compare the estimation to the real polarity. After that, we construct an optimization model for the whole corpus to weigh the overall estimation error, which is minimized by the best sentiment values of candidate words. Finally, several experiments demonstrate the effectiveness of our approach. To the best of our knowledge, this paper is the first work that identifies sentiment words without seed words."]},{"title":"2 The Proposed Approach 2.1 Preliminary","paragraphs":["We formulate the sentiment word identification problem as follows. Let D = {d1, . . . , dn} denote document set. Vector ⃗l =    l1 ... ln    represents their labels. If document di is a positive sample, then li = 1; if di is negative, then li = −1. We use the notation C = {c1, . . . , cV } to represent candidate word set, and V is the number of candidate words. Each document is formed by consecutive words in C. Our task is to predict the sentiment polarity of each word cj ∈ C. 2.2 Word Importance We assume each document di ∈ D is presented by a bag-of-words feature vector ⃗fi =    fi1 ... fiV   , where fij describes the importance of cj to di. A high value of fij indicates word cj contributes a lot to document di in semantic view, and vice versa. Note that fij > 0 if cj appears in di, while fij = 0 if not. For simplicity, every ⃗fi is normalized to a unit vector, such that features of different documents are relatively comparable.","There are several ways to define the word importance, and we choose normalized TF-IDF (Jones, 1972). Therefore, we have fij ∝ T F −IDF (di, cj), and ∥⃗fi∥ = 1. 2.3 Polarity Value In the above description, the sentiment polarity has only two states, positive or negative. We extend both word and document polarities to polarity values in this section. Definition 1 Word Polarity Value: For each word cj ∈ C, we denote its word polarity value as w(cj). w(cj) > 0 indicates cj is a positive word, while w(cj) < 0 indicates cj is a negative word. |w(cj)| indicates the strength of the belief of cj’s polarity. Denote w(cj) as wj, and the word polarity value vector ⃗w =    w1 ... wV   . For example, if w(“bad”) < w(“greedy”) < 0, we can say “bad” is more likely to be a negative word than “greedy”. Definition 2 Document Polarity Value: For each document di, document polarity value is y(di) = cosine(⃗fi, ⃗w) = ⃗fiT","· ⃗w ∥ ⃗w∥ . (1) We denote y(di) as yi for short.","Here, we can regard yi as a polarity estimate for di based on ⃗w. To explain this, Table 1 shows an example. “MR1”, “MR2” and “MR3” are three movie review documents, and “compelling” and “boring” are polarity words in the vocabulary. we simply use TF to construct the document feature vectors without normalization. In the table, these three vectors, ⃗f1, ⃗f2 and ⃗f3, are (3, 1), (2, 1) and (1, 3) respectively. Similarly, we can get ⃗w = (1, −1), indicating “compelling” is a positive word while “boring” is negative. After normaliz-ing ⃗f1, ⃗f2 and ⃗f3, and calculating their cosine similarities with ⃗w, we obtain y1 > y2 > 0 > y3. These inequalities tell us the first two reviews are positive, while the last review is negative. Further-more, we believe that “MR1” is more positive than “MR2”. 856","“compelling” “boring” MR1 3 1 MR2 2 1 MR3 1 3 w 1 -1 Table 1: Three rows in the middle shows the feature vectors of three movie reviews, and the last row shows the word polarity value vector ⃗w. For simplicity, we use TF value to represent the word importance feature. 2.4 Optimization Model As mentioned above, we can regard yi as a polarity estimate for document di. A precise prediction makes the positive document’s estimator close to 1, and the negative’s close to -1. We define the polarity estimate error for document di as: ei = |yi − li| = | ⃗fiT","· ⃗w ∥ ⃗w∥ − li|. (2) Our learning procedure tries to decrease ei. We obtain ⃗w by minimizing the overall estimation error of all document samples n∑ i=1 e2 i . Thus, the optimization problem can be described as min ⃗w n ∑ i=1 ( ⃗fiT","· ⃗w ∥ ⃗w∥","− li)2 . (3) After solving this problem, we not only obtain the polarity of each word cj according to the sign of wj, but also its polarity belief based on |wj|. 2.5 Model Solution We use normalized vector ⃗x to substitute ⃗w","∥ ⃗w∥ , and derive an equivalent optimization problem: min ⃗x E(⃗x) = n ∑ i=1 (⃗fiT · ⃗x − li)2 s.t. ∥⃗x∥ = 1. (4)","The equality constraint of above model makes the problem non-convex. We relax the equality constraint to ∥⃗x∥ ≤ 1, then the problem becomes convex. We can rewrite the objective function as the form of least square regression: E(⃗x) = ∥F · ⃗x − ⃗l∥2",", where F is the feature matrix, and equals to     ⃗f1T ... ⃗fnT    .","Now we can solve the problem by convex optimization algorithms (Boyd and Vandenberghe, 2004), such as gradient descend method. In each iteration step, we update ⃗x by ∆⃗x = η · (−∇E) = 2η · (F T⃗l − F T","F ⃗x), where η > 0 is the learning rate."]},{"title":"3 Experiment 3.1 Experimental Setup","paragraphs":["We leverage two widely used document datasets. The first dataset is the Cornell Movie Review Data 1",", containing 1,000 positive and 1,000 negative processed reviews. The other is the Stanford Large Dataset 2","(Maas et al., 2011), a collection of 50,000 comments from IMDB, evenly divided into training and test sets.","The ground-truth is generated with the help of a sentiment lexicon, MPQA subjective lexicon 3",". We randomly select 20% polarity words as the seed words, and the remaining are candidate ones. Here, the seed words are provided for the baseline methods but not for ours. In order to increase the difficulty of our task, several non-polarity words are added to the candidate word set. Table 2 shows the word distribution of two datasets. Dataset Word Set pos neg non total Cornell","seed 135 201 - 336 candidate 541 806 1232 2579 Stanford","seed 202 343 - 545 candidate 808 1370 2566 4744 Table 2: Word Distribution","In order to demonstrate the effectiveness of our model, we select two baselines, SO-PMI (Turney and Littman, 2003) and COM (Chen et al., 2012). Both of them need seed words. 3.2 Top-K Test In face of the long lists of recommended polarity words, people are only concerned about the top-ranked words with the highest sentiment value. In this experiment we consider the accuracy of the top K polarity words. The quality of a polarity word list is measured by p@K =","Nright,K","K , where","Nright,K is the number of top-K words which are","correctly recommended. 1 http://www.cs.cornell.edu/people/pabo/movie-review-","data/ 2 http://ai.stanford.edu/ amaas/data/sentiment/ 3 http://www.cs.pitt.edu/mpqa/ 857","WEED SO-PMI COM positive words negative words positive words negative words positive words negative words great excellent bad stupid destiny lush cheap worst best great ridiculous bad perfect perfectly worst mess brilliant skillfully ridiculous annoying will star plot evil","terrific best boring ridiculous courtesy courtesy damn pathetic bad fun star garish true wonderfully awful plot gorgeous magnificent inconsistencies fool better plot dreadfully stupid brilliant outstanding worse terrible temptation marvelously desperate giddy love horror pretty fun Table 3: Case Study (a) Cornell Dataset (b) Stanford Dataset 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 p@10 p@20 p@50 p@100 WEED SO_PMI COM 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 p@10 p@20 p@50 p@100 WEED SO_PMI COM Figure 1: Top-K Test","Figure 1 shows the final result of p@K, which is the average score of the positive and negative list. We can see that in both datasets, our approach highly outperforms two baselines, and the precision is 14.4%-33.0% higher than the best baseline. p@10s of WEED for Cornell and Stanford datasets reach to 93.5% and 89.0%, and it shows the top 10 words in our recommended list is exceptionally reliable. As the size of K increases, the accuracy of all methods falls accordingly. This shows three approaches rank the most probable polarity words in the front of the word list. Compared with the small dataset, we obtain a better result with large K on the Stanford dataset. 3.3 Case Study We conduct an experiment to illustrate the characteristics of three methods. Table 3 shows top-10 positive and negative words for each method, where the bold words are the ones with correct polarities. From the first two columns, we can see the accuracy of WEED is very high, where positive words are absolutely correct and negative word list makes only one mistake, “plot”. The other columns of this table shows the baseline methods both achieve reasonable results but do not perform as well as WEED.","Our approach is able to identify frequently used sentiment words, which are vital for the applications without prior sentiment lexicons. The sentiment words identified by SO-PMI are not so representative as WEED and COM. For example, “skillfully” and “giddy” are correctly classified but they are not very frequently used. COM tends to assign wrong polarities to the sentiment words al-though these words are often used. In the 5th","and 6th","columns of Table 3, “bad” and “horror” are recognized as positive words, while “pretty” and “fun” are recognized as negative ones. These concrete results show that WEED captures the generality of the sentiment words, and achieves a higher accuracy than the baselines."]},{"title":"4 Conclusion and Future Work","paragraphs":["We propose an effective optimization-based model, WEED, to identify sentiment words from the corpus without seed words. The algorithm exploits the sentiment information provided by the documents. To the best of our knowledge, this paper is the first work that identifies sentiment words without any seed words. Several experiments on real datasets show that WEED outperforms the state-of-the-art methods with seed words.","Our work can be considered as the first step of building a domain-specific sentiment lexicon. Once some sentiment words are obtained in a certain domain, our future work is to improve WEED by utilizing these words. 858"]},{"title":"Acknowledgments","paragraphs":["This work is partially supported by National Natural Science Foundation of China (Grant No. 61170091)."]},{"title":"References","paragraphs":["S. Boyd and L. Vandenberghe. 2004. Convex optimization. Cambridge university press.","L. Chen, W. Wang, M. Nagarajan, S. Wang, and A.P. Sheth. 2012. Extracting diverse sentiment expressions with target-dependent polarity from twitter. In Proceedings of the Sixth International AAAI Conference on Weblogs and Social Media (ICWSM), pages 50–57.","Wen Fan, Shutao Sun, and Guohui Song. 2011. Probability adjustment naı̈ve bayes algorithm based on nondomain-specific sentiment and evaluation word for domain-transfer sentiment analysis. In Fuzzy Systems and Knowledge Discovery (FSKD), 2011 Eighth International Conference on, volume 2, pages 1043–1046. IEEE.","A. Hassan and D. Radev. 2010. Identifying text polarity using random walks. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 395–403. Association for Computational Linguistics.","A. Hassan, A. Abu-Jbara, R. Jha, and D. Radev. 2011. Identifying the semantic orientation of foreign words. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 592–597.","K.S. Jones. 1972. A statistical interpretation of term specificity and its application in retrieval. Journal of documentation, 28(1):11–21.","N. Kaji and M. Kitsuregawa. 2007. Building lexicon for sentiment analysis from massive collection of html documents. In Proceedings of the joint conference on empirical methods in natural language processing and computational natural language learning (EMNLP-CoNLL), pages 1075–1083.","H. Kanayama and T. Nasukawa. 2006. Fully automatic lexicon expansion for domain-oriented sentiment analysis. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Process-ing, pages 355–363. Association for Computational Linguistics.","Lun-Wei Ku, Yu-Ting Liang, and Hsin-Hsi Chen. 2006. Opinion extraction, summarization and track-ing in news and blog corpora. In Proceedings of AAAI-2006 spring symposium on computational approaches to analyzing weblogs, volume 2001.","A.L. Maas, R.E. Daly, P.T. Pham, D. Huang, A.Y. Ng, and C. Potts. 2011. Learning word vectors for sentiment analysis. In Proceedings of the 49th annual meeting of the association for computational Linguistics (acL-2011).","Miller. 1995. Wordnet: a lexical database for english. Communications of the ACM, 38(11):39–41.","B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs up?: sentiment classification using machine learning techniques. In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10, pages 79–86. Association for Computational Linguistics.","P. Turney and M.L. Littman. 2003. Measuring praise and criticism: Inference of semantic orientation from association. 859"]}],"references":[{"authors":[{"first":"S.","last":"Boyd"},{"first":"L.","last":"Vandenberghe"}],"year":"2004","title":"Convex optimization","source":"S. Boyd and L. Vandenberghe. 2004. Convex optimization. Cambridge university press."},{"authors":[{"first":"L.","last":"Chen"},{"first":"W.","last":"Wang"},{"first":"M.","last":"Nagarajan"},{"first":"S.","last":"Wang"},{"first":"A.","middle":"P.","last":"Sheth"}],"year":"2012","title":"Extracting diverse sentiment expressions with target-dependent polarity from twitter","source":"L. Chen, W. Wang, M. Nagarajan, S. Wang, and A.P. Sheth. 2012. Extracting diverse sentiment expressions with target-dependent polarity from twitter. In Proceedings of the Sixth International AAAI Conference on Weblogs and Social Media (ICWSM), pages 50–57."},{"authors":[{"first":"Wen","last":"Fan"},{"first":"Shutao","last":"Sun"},{"first":"Guohui","last":"Song"}],"year":"2011","title":"Probability adjustment naı̈ve bayes algorithm based on nondomain-specific sentiment and evaluation word for domain-transfer sentiment analysis","source":"Wen Fan, Shutao Sun, and Guohui Song. 2011. Probability adjustment naı̈ve bayes algorithm based on nondomain-specific sentiment and evaluation word for domain-transfer sentiment analysis. In Fuzzy Systems and Knowledge Discovery (FSKD), 2011 Eighth International Conference on, volume 2, pages 1043–1046. IEEE."},{"authors":[{"first":"A.","last":"Hassan"},{"first":"D.","last":"Radev"}],"year":"2010","title":"Identifying text polarity using random walks","source":"A. Hassan and D. Radev. 2010. Identifying text polarity using random walks. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 395–403. Association for Computational Linguistics."},{"authors":[{"first":"A.","last":"Hassan"},{"first":"A.","last":"Abu-Jbara"},{"first":"R.","last":"Jha"},{"first":"D.","last":"Radev"}],"year":"2011","title":"Identifying the semantic orientation of foreign words","source":"A. Hassan, A. Abu-Jbara, R. Jha, and D. Radev. 2011. Identifying the semantic orientation of foreign words. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 592–597."},{"authors":[{"first":"K.","middle":"S.","last":"Jones"}],"year":"1972","title":"A statistical interpretation of term specificity and its application in retrieval","source":"K.S. Jones. 1972. A statistical interpretation of term specificity and its application in retrieval. Journal of documentation, 28(1):11–21."},{"authors":[{"first":"N.","last":"Kaji"},{"first":"M.","last":"Kitsuregawa"}],"year":"2007","title":"Building lexicon for sentiment analysis from massive collection of html documents","source":"N. Kaji and M. Kitsuregawa. 2007. Building lexicon for sentiment analysis from massive collection of html documents. In Proceedings of the joint conference on empirical methods in natural language processing and computational natural language learning (EMNLP-CoNLL), pages 1075–1083."},{"authors":[{"first":"H.","last":"Kanayama"},{"first":"T.","last":"Nasukawa"}],"year":"2006","title":"Fully automatic lexicon expansion for domain-oriented sentiment analysis","source":"H. Kanayama and T. Nasukawa. 2006. Fully automatic lexicon expansion for domain-oriented sentiment analysis. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Process-ing, pages 355–363. Association for Computational Linguistics."},{"authors":[{"first":"Lun-Wei","last":"Ku"},{"first":"Yu-Ting","last":"Liang"},{"first":"Hsin-Hsi","last":"Chen"}],"year":"2006","title":"Opinion extraction, summarization and track-ing in news and blog corpora","source":"Lun-Wei Ku, Yu-Ting Liang, and Hsin-Hsi Chen. 2006. Opinion extraction, summarization and track-ing in news and blog corpora. In Proceedings of AAAI-2006 spring symposium on computational approaches to analyzing weblogs, volume 2001."},{"authors":[{"first":"A.","middle":"L.","last":"Maas"},{"first":"R.","middle":"E.","last":"Daly"},{"first":"P.","middle":"T.","last":"Pham"},{"first":"D.","last":"Huang"},{"first":"A.","middle":"Y.","last":"Ng"},{"first":"C.","last":"Potts"}],"year":"2011","title":"Learning word vectors for sentiment analysis","source":"A.L. Maas, R.E. Daly, P.T. Pham, D. Huang, A.Y. Ng, and C. Potts. 2011. Learning word vectors for sentiment analysis. In Proceedings of the 49th annual meeting of the association for computational Linguistics (acL-2011)."},{"authors":[{"last":"Miller"}],"year":"1995","title":"Wordnet: a lexical database for english","source":"Miller. 1995. Wordnet: a lexical database for english. Communications of the ACM, 38(11):39–41."},{"authors":[{"first":"B.","last":"Pang"},{"first":"L.","last":"Lee"},{"first":"S.","last":"Vaithyanathan"}],"year":"2002","title":"Thumbs up?: sentiment classification using machine learning techniques","source":"B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs up?: sentiment classification using machine learning techniques. In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10, pages 79–86. Association for Computational Linguistics."},{"authors":[{"first":"P.","last":"Turney"},{"first":"M.","middle":"L.","last":"Littman"}],"year":"2003","title":"Measuring praise and criticism: Inference of semantic orientation from association","source":"P. Turney and M.L. Littman. 2003. Measuring praise and criticism: Inference of semantic orientation from association. 859"}],"cites":[{"style":0,"text":"Pang et al., 2002","origin":{"pointer":"/sections/8/paragraphs/0","offset":37,"length":17},"authors":[{"last":"Pang"},{"last":"al."}],"year":"2002","references":["/references/11"]},{"style":0,"text":"Ku et al., 2006","origin":{"pointer":"/sections/8/paragraphs/0","offset":338,"length":15},"authors":[{"last":"Ku"},{"last":"al."}],"year":"2006","references":["/references/8"]},{"style":0,"text":"Chen et al., 2012","origin":{"pointer":"/sections/8/paragraphs/0","offset":355,"length":17},"authors":[{"last":"Chen"},{"last":"al."}],"year":"2012","references":["/references/1"]},{"style":0,"text":"Fan et al., 2011","origin":{"pointer":"/sections/8/paragraphs/0","offset":374,"length":16},"authors":[{"last":"Fan"},{"last":"al."}],"year":"2011","references":["/references/2"]},{"style":0,"text":"Turney and Littman, 2003","origin":{"pointer":"/sections/8/paragraphs/5","offset":443,"length":24},"authors":[{"last":"Turney"},{"last":"Littman"}],"year":"2003","references":["/references/12"]},{"style":0,"text":"Kaji and Kitsuregawa, 2007","origin":{"pointer":"/sections/8/paragraphs/5","offset":474,"length":26},"authors":[{"last":"Kaji"},{"last":"Kitsuregawa"}],"year":"2007","references":["/references/6"]},{"style":0,"text":"Kanayama and Nasukawa (2006)","origin":{"pointer":"/sections/8/paragraphs/5","offset":618,"length":28},"authors":[{"last":"Kanayama"},{"last":"Nasukawa"}],"year":"2006","references":["/references/7"]},{"style":0,"text":"Kanayama and Nasukawa, 2006","origin":{"pointer":"/sections/8/paragraphs/5","offset":767,"length":27},"authors":[{"last":"Kanayama"},{"last":"Nasukawa"}],"year":"2006","references":["/references/7"]},{"style":0,"text":"Hassan and Radev, 2010","origin":{"pointer":"/sections/8/paragraphs/5","offset":801,"length":22},"authors":[{"last":"Hassan"},{"last":"Radev"}],"year":"2010","references":["/references/3"]},{"style":0,"text":"Hassan et al., 2011","origin":{"pointer":"/sections/8/paragraphs/5","offset":830,"length":19},"authors":[{"last":"Hassan"},{"last":"al."}],"year":"2011","references":["/references/4"]},{"style":0,"text":"Miller, 1995","origin":{"pointer":"/sections/8/paragraphs/5","offset":989,"length":12},"authors":[{"last":"Miller"}],"year":"1995","references":["/references/10"]},{"style":0,"text":"Jones, 1972","origin":{"pointer":"/sections/9/paragraphs/1","offset":87,"length":11},"authors":[{"last":"Jones"}],"year":"1972","references":["/references/5"]},{"style":0,"text":"Boyd and Vandenberghe, 2004","origin":{"pointer":"/sections/9/paragraphs/11","offset":64,"length":27},"authors":[{"last":"Boyd"},{"last":"Vandenberghe"}],"year":"2004","references":["/references/0"]},{"style":0,"text":"Maas et al., 2011","origin":{"pointer":"/sections/10/paragraphs/2","offset":1,"length":17},"authors":[{"last":"Maas"},{"last":"al."}],"year":"2011","references":["/references/9"]},{"style":0,"text":"Turney and Littman, 2003","origin":{"pointer":"/sections/10/paragraphs/7","offset":89,"length":24},"authors":[{"last":"Turney"},{"last":"Littman"}],"year":"2003","references":["/references/12"]},{"style":0,"text":"Chen et al., 2012","origin":{"pointer":"/sections/10/paragraphs/7","offset":124,"length":17},"authors":[{"last":"Chen"},{"last":"al."}],"year":"2012","references":["/references/1"]}]}
