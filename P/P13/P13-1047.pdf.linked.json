{"sections":[{"title":"","paragraphs":["Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 476–485, Sofia, Bulgaria, August 4-9 2013. c⃝2013 Association for Computational Linguistics"]},{"title":"Leveraging Synthetic Discourse Data via Multi-task Learning for Implicit Discourse Relation Recognition Man Lan and Yu Xu Department of Computer Science and Technology East China Normal University Shanghai, P.R.China mlan@cs.ecnu.edu.cn 51101201049@ecnu.cn Zheng-Yu Niu Baidu Inc. Beijing, P.R.China niuzhengyu@baidu.com Abstract","paragraphs":["To overcome the shortage of labeled data for implicit discourse relation recognition, previous works attempted to automatically generate training data by removing explicit discourse connectives from sentences and then built models on these synthetic implicit examples. However, a previous study (Sporleder and Lascarides, 2008) showed that models trained on these synthetic data do not generalize very well to natural (i.e. genuine) implicit discourse data. In this work we revisit this issue and present a multi-task learning based system which can effectively use synthetic data for implicit discourse relation recognition. Results on PDTB data show that under the multi-task learning framework our models with the use of the prediction of explicit discourse connectives as auxiliary learning tasks, can achieve an averaged F1 improvement of 5.86% over baseline models."]},{"title":"1 Introduction","paragraphs":["The task of implicit discourse relation recognition is to identify the type of discourse relation (a.k.a. rhetorical relation) hold between two spans of text, where there is no discourse connective (a.k.a. discourse marker, e.g., but, and) in context to explicitly mark their discourse relation (e.g., Contrast or Explanation). It can be of great benefit to many downstream NLP applications, such as question answering (QA) (Verberne et al., 2007), information extraction (IE) (Cimiano et al., 2005), and machine translation (MT), etc. This task is quite challenging due to two reasons. First, without discourse connective in text, the task is quite difficult in itself. Second, implicit discourse relation is quite frequent in text. For example, almost half the sentences in the British National Corpus held implicit discourse relations (Sporleder and Lascarides, 2008). Therefore, the task of implicit discourse relation recognition is the key to improving end-to-end discourse parser performance.","To overcome the shortage of manually annotated training data, (Marcu and Echihabi, 2002) proposed a pattern-based approach to automatically generate training data from raw corpora. This line of research was followed by (Sporleder and Lascarides, 2008) and (Blair-Goldensohn, 2007). In these works, sentences containing certain words or phrases (e.g. but, although) were selected out from raw corpora using a pattern-based approach and then these words or phrases were removed from these sentences. Thus the resulting sentences were used as synthetic training examples for implicit discourse relation recognition. Since there is ambiguity of a word or phrase serving for discourse connective (i.e., the ambiguity between discourse and non-discourse usage or the ambiguity between two or more discourse relations if the word or phrase is used as a discourse connective), the synthetic implicit data would contain a lot of noises. Later, with the release of manually annotated corpus, such as Penn Discourse Treebank 2.0 (PDTB) (Prasad et al., 2008), recent studies performed implicit discourse relation recognition on natural (i.e., genuine) implicit discourse data (Pitler et al., 2009) (Lin et al., 2009) (Wang et al., 2010) with the use of linguistically informed features and machine learning algorithms.","(Sporleder and Lascarides, 2008) conducted a study of the pattern-based approach presented by (Marcu and Echihabi, 2002) and showed that the model built on synthetical implicit data has not generalize well on natural implicit data. They found some evidence that this behavior is largely independent of the classifiers used and seems to lie in the data itself (e.g., marked and unmarked examples may be too dissimilar linguistically and 476 removing unambiguous markers in the automatic labelling process may lead to a meaning shift in the examples). We state that in some cases it is true while in other cases it may not always be so. A simple example is given here:","(E1) a. We can’t win. b. [but] We must keep trying. We may find that in this example whether the insertion or the removal of connective but would not lead to a redundant or missing information between the above two sentences. That is, discourse connectives can be inserted between or removed from two sentences without changing the semantic relations between them in some cases. Another similar observation is in the annotation procedure of PDTB. To label implicit discourse relation, annotators inserted connective which can best express the relation between sentences without any redundancy1",". We see that there should be some linguistical similarities between explicit and implicit discourse examples. Therefore, the first question arises: can we exploit this kind of linguistic similarity between explicit and implicit discourse examples to improve implicit discourse relation recognition?","In this paper, we propose a multi-task learning based method to improve the performance of implicit discourse relation recognition (as main task) with the help of relevant auxiliary tasks. Specif-ically, the main task is to recognize the implicit discourse relations based on genuine implicit discourse data and the auxiliary task is to recognize the implicit discourse relations based on synthetic implicit discourse data. According to the principle of multi-task learning, the learning model can be optimized by the shared part of the main task and the auxiliary tasks without bring unnecessary noise. That means, the model can learn from synthetic implicit data while it would not bring unnecessary noise from synthetic implicit data.","Although (Sporleder and Lascarides, 2008) did not mention, we speculate that another possible reason for the reported worse performance may result from noises in synthetic implicit discourse data. These synthetic data can be generated from two sources: (1) raw corpora with the use of pattern-based approach in (Marcu and Echihabi,","1","According to the PDTB Annotation Manual (PDTB-Group, 2008), if the insertion of connective leads to “redundancy”, the relation is annotated as Alternative lexicalizations (AltLex), not implicit. 2002) and (Sporleder and Lascarides, 2008), and (2) manually annotated explicit data with the removal of explicit discourse connectives. Obviously, the data generated from the second source is cleaner and more reliable than that from the first source. Therefore, the second question to address in this work is: whether synthetic implicit discourse data generated from explicit discourse data source (i.e., the second source) can lead to a better performance than that from raw corpora (i.e., the first source)? To answer this question, we will make a comparison of synthetic discourse data generated from two corpora, i.e., the BILLIP corpus and the explicit discourse data annotated in PDTB.","The rest of this paper is organized as follows. Section 2 reviews related work on implicit discourse relation classification and multi-task learning. Section 3 presents our proposed multi-task learning method for implicit discourse relation classification. Section 4 provides the implementation technique details of the proposed multi-task method. Section 5 presents experiments and discusses results. Section 6 concludes this work."]},{"title":"2 Related Work 2.1 Implicit discourse relation classification 2.1.1 Unsupervised approaches","paragraphs":["Due to the lack of benchmark data for implicit discourse relation analysis, earlier work used unlabeled data to generate synthetic implicit discourse data. For example, (Marcu and Echihabi, 2002) proposed an unsupervised method to recognize four discourse relations, i.e., Contrast, Explanation-evidence, Condition and Elaboration. They first used unambiguous pattern to extract explicit discourse examples from raw corpus. Then they generated synthetic implicit discourse data by removing explicit discourse connectives from sentences extracted. In their work, they collected word pairs from synthetic data set as features and used machine learning method to classify implicit discourse relation. Based on this work, several researchers have extended the work to improve the performance of relation classification. For example, (Saito et al., 2006) showed that the use of phrasal patterns as additional features can help a word-pair based system for discourse relation prediction on a Japanese corpus. Further-more, (Blair-Goldensohn, 2007) improved previous work with the use of parameter optimization, 477 topic segmentation and syntactic parsing. However, (Sporleder and Lascarides, 2008) showed that the training model built on a synthetic data set, like the work of (Marcu and Echihabi, 2002), may not be a good strategy since the linguistic dissimilarity between explicit and implicit data may hurt the performance of a model on natural data when being trained on synthetic data. 2.1.2 Supervised approaches This line of research work approaches this relation prediction problem by recasting it as a classification problem. (Soricut and Marcu, 2003) parsed the discourse structures of sentences on RST Bank data set (Carlson et al., 2001) which is annotated based on Rhetorical Structure Theory (Mann and Thompson, 1988). (Wellner et al., 2006) presented a study of discourse relation disambiguation on GraphBank (Wolf et al., 2005). Recently, (Pitler et al., 2009) (Lin et al., 2009) and (Wang et al., 2010) conducted discourse relation study on PDTB (Prasad et al., 2008) which has been widely used in this field. 2.1.3 Semi-supervised approaches Research work in this category exploited both labeled and unlabeled data for discourse relation prediction. (Hernault et al., 2010) presented a semi-supervised method based on the analysis of co-occurring features in labeled and unlabeled data. Very recently, (Hernault et al., 2011) in-troduced a semi-supervised work using structure learning method for discourse relation classification, which is quite relevant to our work. However, they performed discourse relation classification on both explicit and implicit data. And their work is different from our work in many aspects, such as, feature sets, auxiliary task, auxiliary data, class labels, learning framework, and so on. Furthermore, there is no explicit conclusion or evidence in their work to address the two questions raised in Section 1.","Unlike their previous work, our previous work (Zhou et al., 2010) presented a method to predict the missing connective based on a language model trained on an unannotated corpus. The predicted connective was then used as a feature to classify the implicit relation. 2.2 Multi-task learning Multi-task learning is a kind of machine learning method, which learns a main task together with other related auxiliary tasks at the same time, using a shared representation. This often leads to a better model for the main task, because it allows the learner to use the commonality among the tasks. Many multi-task learning methods have been proposed in recent years, (Ando and Zhang, 2005a), (Argyriou et al., 2008), (Jebara, 2004), (Bonilla et al., 2008), (Evgeniou and Pontil, 2004), (Baxter, 2000), (Caruana, 1997), (Thrun, 1996). One group uses task relations as regularization terms in the objective function to be optimized. For example, in (Evgeniou and Pontil, 2004) the regularization terms make the parameters of models closer for similar tasks. Another group is proposed to find the common structure from data and then utilize the learned structure for multi-task learning (Argyriou et al., 2008) (Ando and Zhang, 2005b)."]},{"title":"3 Multi-task Learning for Discourse Relation Prediction 3.1 Motivation","paragraphs":["The idea of using multi-task learning for implicit discourse relation classification is motivated by the observations that we have made on implicit discourse relation.","On one hand, since building a hand-annotated implicit discourse relation corpus is costly and time consuming, most previous work attempted to use synthetic implicit discourse examples as training data. However, (Sporleder and Lascarides, 2008) found that the model trained on synthetic implicit data has not performed as well as expected in natural implicit data. They stated that the reason is linguistic dissimilarity between explicit and implicit discourse data. This indicates that straightly using synthetic implicit data as training data may not be helpful.","On the other hand, as shown in Section 1, we observe that in some cases explicit discourse relation and implicit discourse relation can express the same meaning with or without a discourse connective. This indicates that in certain degree they must be similar to each other. If it is true, the synthetic implicit relations are expected to be helpful for implicit discourse relation classification. Therefore, what we have to do is to find a way to train a model which has the capabilities to learn from their similarity and to ignore their dissimilarity as well.","To solve it, we propose a multi-task learning method for implicit discourse relation classi-478 fication, where the classification model seeks the shared part through jointly learning main task and multiple auxiliary tasks. As a result, the model can be optimized by the similar shared part without bringing noise in the dissimilar part. Specifically, in this work, we use alternating structure optimization (ASO) (Ando and Zhang, 2005a) to construct the multi-task learning framework. ASO has been shown to be useful in a semi-supervised learning configuration for several NLP applications, such as, text chunking (Ando and Zhang, 2005b) and text classification (Ando and Zhang, 2005a). 3.2 Multi-task learning and ASO Generally, multi-task learning(MTL) considers m prediction problems indexed by l ∈ {1, ..., m}, each with nl samples (Xl","i , Y l","i ) for i ∈ {1, ...nl} (Xi are input feature vectors and Yi are corresponding classification labels) and assumes that there exists a common predictive structure shared by these m problems. Generally, the joint linear model for MTL is to predict problem l in the following form:","fl(Θ, X) = wT","l X + vT","l ΘX, ΘΘT","= I, (1) where I is the identity matrix, wl and vl are weight vectors specific to each problem l, and Θ is the structure matrix shared by all the m predictors. The main goal of MTL is to learn a common good feature map ΘX for all the m problems. Several MTL methods have been presented to learn ΘX for all the m problems. In this work, we adopt the ASO method.","Specifically, the ASO method adopted singular value decomposition (SVD) to obtain Θ and m predictors that minimize the empirical risk summed over all the m problems. Thus, the problem of optimization becomes the minimization of the joint empirical risk written as: m∑ l=1 ( nl ∑ i=1","L(fl(Θ, Xl i ), Yi) nl + λ||Wl||2 ) (2) where loss function L(.) quantifies the difference between the prediction f (Xi) and the true out-put Yi for each predictor, and λ is a regularization parameter for square regularization to control the model complexity. To minimize the empirical risk, ASO repeats the following alternating optimization procedure until a convergence criterion is met:","1) Fix (Θ, Vl), and find m predictors fl that minimize the above joint empirical risk.","2) Fix m predictors fl, and find (Θ, Vl) that minimizes the above joint empirical risk. 3.3 Auxiliary tasks There are two main principles to create auxiliary tasks. First, the auxiliary tasks should be automatically labeled in order to reduce the cost of manual labeling. Second, since the MTL model learns from the shared part of main task and auxiliary tasks, the auxiliary tasks should be quite relevant/similar to the main task. It is generally be-lieved that the more the auxiliary tasks are relevant to the main task, the more the main task can benefit from the auxiliary tasks. Following these two principles, we create the auxiliary tasks by generating automatically labeled data as follows.","Previous work (Marcu and Echihabi, 2002) and (Sporleder and Lascarides, 2008) adopted predefined pattern-based approach to generate synthetic labeled data, where each predefined pattern has one discourse relation label. In contrast, we adopt an automatic approach to generate synthetic labeled data, where each discourse connective between two texts serves as their relation label. The reason lies in the very strong connection between discourse connectives and discourse relations. For example, the connective but always indicates a contrast relation between two texts. And (Pitler et al., 2008) proved that using only connective itself, the accuracy of explicit discourse relation classification is over 93%.","To build the mapping between discourse connective and discourse relation, for each connective, we count the times it appears in each relation and regard the relation in which it appears most frequently as its most relevant relation. Based on this mapping between connective and relation, we extract the synthetic labeled data containing the connective as training data for auxiliary tasks.","For example, and appears 3, 000 times in PDTB as a discourse connective. Among them, it is manually annotated as an Expansion relation for 2, 938 times. So we regard the Expansion relation as its most relevant relation and generate a mapping pattern like: “and → Expansion”. Then we extract all sentences which contain discourse “and” and remove this connective “and” from sentences to generate synthetic implicit data. The resulting sentences are used in auxiliary task and automatically 479 marked as Expansion relation."]},{"title":"4 Implementation Details of Multi-task Learning Method 4.1 Data sets for main and auxiliary tasks","paragraphs":["To examine whether there is a difference in synthetic implicit data generated from unannotated and annotated corpus, we use two corpora. One is a hand-annotated explicit discourse corpus, i.e., the explicit discourse relations in PDTB, denoted as exp. Another is an unannotated corpus, i.e., BLLIP (David McClosky and Johnson., 2008). 4.1.1 Penn Discourse Treebank PDTB (Prasad et al., 2008) is the largest hand-annotated corpus of discourse relation so far. It contains 2, 312 Wall Street Journal (WSJ) articles. The sense label of discourse relations is hierarchically with three levels, i.e., class, type and subtype. The top level contains four major semantic classes: Comparison (denoted as Comp.), Contingency (Cont.), Expansion (Exp.) and Temporal (Temp.). For each class, a set of types is used to refine relation sense. The set of subtypes is to further specify the semantic contribution of each argument. In this paper, we focus on the top level (class) and the second level (type) relations because the subtype relations are too fine-grained and only appear in some relations.","Both explicit and implicit discourse relations are labeled in PDTB. In our experiment, the implicit discourse relations are used in the main task and for evaluation. While the explicit discourse relations are used in the auxiliary task. A detailed description of the data sources for different tasks is given below.","Data set for main task Following previous work in (Pitler et al., 2009) and (Zhou et al., 2010), the implicit relations in sections 2-20 are used as training data for the main task (denoted as imp) and the implicit relations in sections 21-22 are for evaluation. Table 1 shows the distribution of implicit relations. There are too few training instances for six second level relations (indicated by * in Table 1), so we removed these six relations in our experiments.","Data set for auxiliary task All explicit instances in sections 00-24 in PDTB, i.e., 18, 459 instances, are used for auxiliary task (denoted as exp). Following the method described in Section 3.3, we build the mapping patterns between con-","Top level Second level train test","Temp 736 83 Synchrony 203 28 Asynchronous 532 55","Cont 3333 279 Cause 3270 272 Pragmatic Cause* 64 7 Condition* 1 0 Pragmatic condition* 1 0","Comp 1939 152 Contrast 1607 134 Pragmatic contrast* 4 0 Concession 183 17 Pragmatic concession* 1 0","Exp 6316 567 Conjunction 2872 208 Instantiation 1063 119 Restatement 2405 213 Alternative 147 9 Exception* 0 0 List 338 12 Table 1: Distribution of implicit discourse relations in the top and second level of PDTB nectives and relations in PDTB and generate synthetic labeled data by removing the connectives. According to the most relevant relation sense of connective removed, the resulting instances are grouped into different data sets. 4.1.2 BLLIP BLLIP North American News Text (Complete) is used as unlabeled data source to generate synthetic labeled data. In comparison with the synthetic labeled data generated from the explicit relations in PDTB, the synthetic labeled data from BLLIP contains more noise. This is because the former data is manually annotated whether a word serves as discourse connective or not, while the latter does not manually disambiguate two types of ambiguity, i.e., whether a word serves as discourse connective or not, and the type of discourse relation if it is a discourse connective. Finally, we extract 26, 412 instances from BLLIP (denoted as BLLIP) and use them for auxiliary task. 4.2 Feature representation For both main task and auxiliary tasks, we adopt the following three feature types. These features are chosen due to their superior performance in previous work (Pitler et al., 2009) and our previous work (Zhou et al., 2010).","Verbs: Following (Pitler et al., 2009), we extract the pairs of verbs from both text spans. The number of verb pairs which have the same highest 480 Levin verb class levels (Levin, 1993) is counted as a feature. Besides, the average length of verb phrases in each argument is included as a feature. In addition, the part of speech tags of the main verbs (e.g., base form, past tense, 3rd person singular present, etc.) in each argument, i.e., MD, VB, VBD, VBG, VBN, VBP, VBZ, are recorded as features, where we simply use the first verb in each argument as the main verb.","Polarity: This feature records the number of positive, negated positive, negative and neutral words in both arguments and their cross product as well. For negated positives, we first locate the negated words in text span and then define the closely behind positive word as negated positive. The polarity of each word in arguments is derived from Multi-perspective Question Answering Opinion Corpus (MPQA) (Wilson et al., 2009).","Modality: We examine six modal words (i.e., can, may, must, need, shall, will) including their various tenses or abbreviation forms in both arguments. This feature records the presence or absence of modal words in both arguments and their cross product. 4.3 Classifiers used multi-task learning We extract the above linguistically informed features from two synthetic implicit data sets (i.e., BLLIP and exp) to learn the auxiliary classifier and from the natural implicit data set (i.e., imp) to learn the main classifier. Under the ASO-based multi-task learning framework, the model of main task learns from the shared part of main task and auxiliary tasks. Specifically, we adopt multiple binary classification to build model for main task. That is, for each discourse relation, we build a binary classifier."]},{"title":"5 Experiments and Results 5.1 Experiments","paragraphs":["Although previous work has been done on PDTB (Pitler et al., 2009) and (Lin et al., 2009), we cannot make a direct comparison with them because various experimental conditions, such as, different classification strategies (multi-class classification, multiple binary classification), different data preparation (feature extraction and selection), different benchmark data collections (different sections for training and test, different levels of discourse relations), different classifiers with various parameters (MaxEnt, Naı̈ve Bayes, SVM, etc) and even different evaluation methods (F1, accuracy) have been adopted by different researchers.","Therefore, to address the two questions raised in Section 1 and to make the comparison reliable and reasonable, we performed experiments on the top and second level of PDTB using single task learning and multi-task learning, respectively. The systems using single task learning serve as baseline systems. Under the single task learning, various combinations of exp and BLLIP data are incorporated with imp data for the implicit discourse relation classification task.","We hypothesize that synthetical implicit data would contribute to the main task, i.e., the implicit discourse relation classification. Specifically, the natural implicit data (i.e., imp) are used to create main task and the synthetical implicit data (exp or BLLIP) are used to create auxiliary tasks for the purpose of optimizing the objective functions of main task. If the hypothesis is correct, the performance of main task would be improved by auxiliary tasks created from synthetical implicit data. Thus in the experiments of multi-task learning, only natural implicit examples (i.e., imp) data are used for main task training while different combinations of synthetical implicit examples (exp and BLLIP) are used for auxiliary task training.","We adopt precision, recall and their combination F1 for performance evaluation. We also perform one-tailed t-test to validate if there is significant difference between two methods in terms of F1 performance analysis. 5.2 Results Table 2 summarizes the experimental results under single and multi-task learning on the top level of four PDTB relations with respect to different combinations of synthetic implicit data. For each relation, the first three rows indicate the results of using different single training data under single task learning and the last three rows indicate the results using different combinations of training data under single task and multi-task learning. The best F1 for every relation is shown in bold font. From this table, we can find that on four relations, our multi-task learning systems achieved the best performance using the combination of exp and BLLIP synthetic data.","Table 3 summarizes the best single task and the best multi-task learning results on the second level of PDTB. For four relations, i.e., Synchrony, Con-481","Single-task Multi-task","Level 1 class Data P R F1 Data Data P R F1","(main) (aux)","Comp. imp 21.43 37.50 27.27 - - - - - BLLIP 12.68 53.29 20.48 - - - - - exp 15.25 50.66 23.44 - - - - - imp + exp 16.94 40.13 23.83 imp exp 22.94 49.34 30.90 imp + BLLIP 13.56 44.08 20.74 imp BLLIP 20.47 63.16 30.92 imp + exp + BLLIP 14.54 38.16 21.05 imp exp + BLLIP 23.47 48.03 31.53","Cont. imp 37.65 43.73 40.46 - - - - - BLLIP 33.72 31.18 32.40 - - - - - exp 35.24 26.52 30.27 - - - - - imp + exp 39.00 13.98 20.58 imp exp 39.94 45.52 42.55 imp + BLLIP 37.30 24.73 29.74 imp BLLIP 37.80 63.80 47.47 imp + exp + BLLIP 39.37 31.18 34.80 imp exp + BLLIP 35.90 70.25 47.52","Exp. imp 56.59 66.67 61.21 - - - - - BLLIP 53.29 40.04 45.72 - - - - - exp 57.97 58.38 58.17 - - - - - imp + exp 57.32 65.61 61.18 imp exp 59.14 67.90 63.22 imp + BLLIP 56.28 65.61 60.59 imp BLLIP 53.80 99.82 69.92 imp + exp + BLLIP 55.81 65.26 60.16 imp exp + BLLIP 53.90 99.82 70.01","Temp. imp 16.46 63.86 26.17 - - - - - BLLIP 17.31 43.37 24.74 - - - - - exp 15.46 36.14 21.66 - - - - - imp + exp 15.35 39.76 22.15 imp exp 18.60 63.86 28.80 imp + BLLIP 14.74 33.73 20.51 imp BLLIP 18.12 67.47 28.57 imp + exp + BLLIP 15.94 39.76 22.76 imp exp + BLLIP 19.08 65.06 29.51 Table 2: Performance of precision, recall and F1 for 4 Level 1 relation classes. “-” indicates N.A.","Single-task Multi-task Level 2 type Data P R F1 Data Data P R F1","(main) (aux) Asynchronous imp 11.36 74.55 19.71 imp exp + BLLIP 23.08 21.82 22.43 Synchrony imp - - - imp exp + BLLIP - - - Cause imp 36.38 64.34 46.48 imp exp + BLLIP 36.01 67.65 47.00 Contrast imp 20.07 42.54 27.27 imp exp + BLLIP 20.70 52.99 29.77 Concession imp - - - imp exp + BLLIP - - - Conjunction imp 26.35 63.46 37.24 imp exp + BLLIP 26.29 73.56 38.73 Instantiation imp 22.78 53.78 32.00 imp exp + BLLIP 22.55 57.98 32.47 Restatement imp 23.11 67.61 34.45 imp exp + BLLIP 26.93 53.99 35.94 Alternative imp - - - imp exp + BLLIP - - - List imp - - - imp exp + BLLIP - - - Table 3: Performance of precision, recall and F1 for 10 Level 2 relation types. “-” indicates 0.00. cession, Alternative and List, the classifier labels no instances due to the small percentages for these four types.","Table 4 summarizes the one-tailed t-test results on the top level of PDTB between the best single task learning system (i.e., imp) and three multi-task learning systems (imp:exp+BLLIP indicates that imp is used for main task and the combination of exp and BLLIP are for auxiliary task). The systems with insignificant performance differences are grouped into one set and ”>” and ”>>” denote better than at significance level 0.01 and 0.001 respectively. 5.3 Discussion From Table 2 to Table 4, several findings can be found as follows.","We can see that the multi-task learning systems perform consistently better than the single task learning systems for the prediction of implicit discourse relations. Our best multi-task learning system achieves an averaged F1 improvement of 5.86% over the best single task learning system on the top level of PDTB relations. Specifically, for 482 Class One-tailed t-test results Comp. (imp:exp+BLLIP, imp:exp, imp:BLLIP) >> (imp) Cont. (imp:exp+BLLIP, imp:BLLIP) >> (imp:exp) > (imp) Exp. (imp:exp+BLLIP, imp:BLLIP) >> (imp:exp) > (imp) Temp. (imp:exp+BLLIP, imp:exp, imp:BLLIP) >> (imp) Table 4: Statistical significance tests results. the relations Comp., Cont., Exp., Temp., our best multi-task learning system achieve 4.26%, 7.06%, 8.8% and 3.34% F1 improvements over the best single task learning system. It indicates that using synthetic implicit data as auxiliary task greatly improves the performance of the main task. This is confirmed by the following t-tests in Table 4.","In contrast to the performance of multi-task learning, the performance of the best single task learning system has been achieved on natural implicit discourse data alone. This finding is consistent with (Sporleder and Lascarides, 2008). It indicates that under single task learning, directly adding synthetic implicit data to increase the number of training data cannot be helpful to implicit discourse relation classification. The possible reasons result from (1) the different nature of implicit and explicit discourse data in linguistics and (2) the noise brought from synthetic implicit data.","Based on the above analysis, we state that it is the way of utilizing synthetic implicit data that is important for implicit discourse relation classification.","Although all three multi-task learning systems outperformed single task learning systems, we find that the two synthetic implicit data sets have not been shown a universally consistent performance on four top level PDTB relations. On one hand, for the relations Comp. and Temp., the performance of the two synthetic implicit data sets alone and their combination are comparable to each other and there is no significant difference between them. On the other hand, for the relations Cont. and Exp., the performance of exp data is inferior to that of BLLIP and their combination. This is contrary to our original expectation that exp data which has been manually annotated for discourse connective disambiguation should outperform BLLIP which contains a lot of noise. This finding indicates that under the multi-task learning, it may not be worthy of using manually annotated corpus to generate auxiliary data. It is quite promising since it can provide benefits to reducing the cost of human efforts on corpus annotation. 5.4 Ambiguity Analysis Although our experiments show that synthetic implicit data can help implicit discourse relation classification under multi-task learning framework, the overall performance is still quite low (44.64% in F1). Therefore, we analyze the types of ambiguity in relations and connectives in order to motivate possible future work. 5.4.1 Ambiguity of implicit relation Without explicit discourse connective, the implicit discourse relation instance can be understood in two or more different ways. Given the example E2 in PDTB, the PDTB annotators explain it as Contingency or Expansion relation and manually insert corresponding implicit connective for one thing or because to express its relation.","(E2) Arg1:Now the stage is set for the battle to play out Arg2:The anti-programmers are getting some helpful thunder from Congress Connective1:because Sense1:Contingency.Cause.Reason Connective2:for one thing Sense2:Expansion.Instantiation (wsj 0118)","Thus the ambiguity of implicit discourse relations makes this task difficult in itself. 5.4.2 Ambiguity of discourse connectives As we mentioned before, even given an explicit discourse connective in text, its discourse relation still can be explained in two or more different ways. And for different connectives, the ambiguity of relation senses is quite different. That is, the most frequent sense is not always the only sense that a connective expresses. In example E3, “since” is explained by annotators to express Temporal or Contingency relation.","(E3) Arg1:MiniScribe has been on the rocks Arg2:since it disclosed early this year that its earnings reports for 1988 weren’t accurate. 483 Sense1:Temporal.Asynchronous.Succession Sense2:Contingency.Cause.Reason (wsj 0003)","In PDTB, “since” appears 184 times in explicit discourse relations. It expresses Temporal relation for 80 times, Contingency relation for 94 times and both Temporal and Contingency for 10 time (like example E3). Therefore, although we use its most frequent sense, i.e., Contingency, to automatically extract sentences and label them, almost less than half of them actually express Temporal relation. Thus the ambiguity of discourse connectives is another source which has brought noise to data when we generate synthetical implicit discourse relation."]},{"title":"6 Conclusions","paragraphs":["In this paper, we present a multi-task learning method to improve implicit discourse relation classification by leveraging synthetic implicit discourse data. Results on PDTB show that under the framework of multi-task learning, using synthetic discourse data as auxiliary task significantly improves the performance of main task. Our best multi-task learning system achieves an averaged F1 improvement of 5.86% over the best single task learning system on the top level of PDTB relations. Specifically, for the relations Comp., Cont., Exp., Temp., our best multi-task learning system achieves 4.26%, 7.06%, 8.8%, and 3.34% F1 improvements over a state of the art baseline system. This indicates that it is the way of utilizing synthetic discourse examples that is important for implicit discourse relation classification."]},{"title":"Acknowledgements","paragraphs":["This research is supported by grants from National Natural Science Foundation of China (No.60903093), Shanghai Pujiang Talent Program (No.09PJ1404500), Doctoral Fund of Ministry of Education of China (No. 20090076120029) and Shanghai Knowledge Service Platform Project (No. ZF1213)."]},{"title":"References","paragraphs":["R.K. Ando and T. Zhang. 2005a. A framework for learning predictive structures from multiple tasks and unlabeled data. The Journal of Machine Learn-ing Research, 6:1817–1853.","R.K. Ando and T. Zhang. 2005b. A high-performance semi-supervised learning method for text chunking. pages 1–9. Association for Computational Linguistics. Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics.","A. Argyriou, C.A. Micchelli, M. Pontil, and Y. Ying. 2008. A spectral regularization framework for multi-task structure learning. Advances in Neural Information Processing Systems, 20:2532.","J. Baxter. 2000. A model of inductive bias learning. J. Artif. Intell. Res. (JAIR), 12:149–198.","S.J. Blair-Goldensohn. 2007. Long-answer question answering and rhetorical-semantic relations. Ph.D. thesis.","E. Bonilla, K.M. Chai, and C. Williams. 2008. Multi-task gaussian process prediction. Advances in Neural Information Processing Systems, 20(October).","L. Carlson, D. Marcu, and M.E. Okurowski. 2001. Building a discourse-tagged corpus in the framework of rhetorical structure theory. pages 1–10. Association for Computational Linguistics. Proceedings of the Second SIGdial Workshop on Discourse and Dialogue-Volume 16.","R. Caruana. 1997. Multitask learning. Machine Learning, 28(1):41–75.","P. Cimiano, U. Reyle, and J. Saric. 2005. Ontologydriven discourse analysis for information extraction. Data and Knowledge Engineering, 55(1):59–83.","Eugene Charniak David McClosky and Mark Johnson. 2008. Bllip north american news text, complete.","T. Evgeniou and M. Pontil. 2004. Regularized multi– task learning. pages 109–117. ACM. Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining.","H. Hernault, D. Bollegala, and M. Ishizuka. 2010. A semi-supervised approach to improve classification of infrequent discourse relations using feature vector extension. pages 399–409. Association for Computational Linguistics. Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing.","H. Hernault, D. Bollegala, and M. Ishizuka. 2011. Semi-supervised discourse relation classification with structural learning. In Proceedings of the 12th international conference on Computational linguistics and intelligent text processing - Volume Part I, CICLing’11, pages 340–352, Berlin, Heidelberg. Springer-Verlag.","T. Jebara. 2004. Multi-task feature and kernel selection for svms. page 55. ACM. Proceedings of the twenty-first international conference on Machine learning.","B. Levin. 1993. English verb classes and alternations: A preliminary investigation, volume 348. University of Chicago press Chicago, IL:. 484","Z. Lin, M.Y. Kan, and H.T. Ng. 2009. Recogniz-ing implicit discourse relations in the penn discourse treebank. pages 343–351. Association for Computational Linguistics. Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1-Volume 1.","W.C. Mann and S.A. Thompson. 1988. Rhetorical structure theory: Toward a functional theory of text organization. Text-Interdisciplinary Journal for the Study of Discourse, 8(3):243–281.","D. Marcu and A. Echihabi. 2002. An unsupervised approach to recognizing discourse relations. pages 368–375. Association for Computational Linguistics. Proceedings of the 40th Annual Meeting on Association for Computational Linguistics.","PDTB-Group. 2008. The penn discourse treebank 2.0 annotation manual. Technical report, Institute for Research in Cognitive Science, University of Pennsylvania.","E. Pitler, M. Raghupathy, H. Mehta, A. Nenkova, A. Lee, and A. Joshi. 2008. Easily identifiable discourse relations. Citeseer. Proceedings of the 22nd International Conference on Computational Linguistics (COLING 2008), Manchester, UK, August.","E. Pitler, A. Louis, and A. Nenkova. 2009. Automatic sense prediction for implicit discourse relations in text. pages 683–691. Association for Computational Linguistics. Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2-Volume 2.","Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind Joshi, and Bonnie Webber. 2008. The penn discourse treebank 2.0. In In Proceedings of LREC.","M. Saito, K. Yamamoto, and S. Sekine. 2006. Using phrasal patterns to identify discourse relations. pages 133–136. Association for Computational Linguistics. Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers on XX.","R. Soricut and D. Marcu. 2003. Sentence level discourse parsing using syntactic and lexical information. pages 149–156. Association for Computational Linguistics. Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1.","C. Sporleder and A. Lascarides. 2008. Using automatically labelled examples to classify rhetorical relations: An assessment. Natural Language Engineer-ing, 14(03):369–416.","S. Thrun. 1996. Is learning the n-th thing any easier than learning the first? Advances in Neural Information Processing Systems, pages 640–646.","S. Verberne, L. Boves, N. Oostdijk, and P.A. Coppen. 2007. Evaluating discourse-based answer extraction for why-question answering. pages 735–736. ACM. Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval.","W.T. Wang, J. Su, and C.L. Tan. 2010. Kernel based discourse relation recognition with temporal order-ing information. pages 710–719. Association for Computational Linguistics. Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics.","B. Wellner, J. Pustejovsky, C. Havasi, A. Rumshisky, and R. Sauri. 2006. Classification of discourse coherence relations: An exploratory study using multiple knowledge sources. pages 117–125. Association for Computational Linguistics. Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue.","T. Wilson, J. Wiebe, and P. Hoffmann. 2009. Recognizing contextual polarity: An exploration of features for phrase-level sentiment analysis. Computational Linguistics, 35(3):399–433.","F. Wolf, E. Gibson, A. Fisher, and M. Knight. 2005. The discourse graphbank: A database of texts annotated with coherence relations. Linguistic Data Consortium.","Z.M. Zhou, Y. Xu, Z.Y. Niu, M. Lan, J. Su, and C.L. Tan. 2010. Predicting discourse connectives for implicit discourse relation recognition. pages 1507– 1514. Association for Computational Linguistics. Proceedings of the 23rd International Conference on Computational Linguistics: Posters. 485"]}],"references":[{"authors":[{"first":"R.","middle":"K.","last":"Ando"},{"first":"T.","last":"Zhang"}],"year":"2005a","title":"A framework for learning predictive structures from multiple tasks and unlabeled data","source":"R.K. Ando and T. Zhang. 2005a. A framework for learning predictive structures from multiple tasks and unlabeled data. The Journal of Machine Learn-ing Research, 6:1817–1853."},{"authors":[{"first":"R.","middle":"K.","last":"Ando"},{"first":"T.","last":"Zhang"}],"year":"2005b","title":"A high-performance semi-supervised learning method for text chunking","source":"R.K. Ando and T. Zhang. 2005b. A high-performance semi-supervised learning method for text chunking. pages 1–9. Association for Computational Linguistics. Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics."},{"authors":[{"first":"A.","last":"Argyriou"},{"first":"C.","middle":"A.","last":"Micchelli"},{"first":"M.","last":"Pontil"},{"first":"Y.","last":"Ying"}],"year":"2008","title":"A spectral regularization framework for multi-task structure learning","source":"A. Argyriou, C.A. Micchelli, M. Pontil, and Y. Ying. 2008. A spectral regularization framework for multi-task structure learning. Advances in Neural Information Processing Systems, 20:2532."},{"authors":[{"first":"J.","last":"Baxter"}],"year":"2000","title":"A model of inductive bias learning","source":"J. Baxter. 2000. A model of inductive bias learning. J. Artif. Intell. Res. (JAIR), 12:149–198."},{"authors":[{"first":"S.","middle":"J.","last":"Blair-Goldensohn"}],"year":"2007","title":"Long-answer question answering and rhetorical-semantic relations","source":"S.J. Blair-Goldensohn. 2007. Long-answer question answering and rhetorical-semantic relations. Ph.D. thesis."},{"authors":[{"first":"E.","last":"Bonilla"},{"first":"K.","middle":"M.","last":"Chai"},{"first":"C.","last":"Williams"}],"year":"2008","title":"Multi-task gaussian process prediction","source":"E. Bonilla, K.M. Chai, and C. Williams. 2008. Multi-task gaussian process prediction. Advances in Neural Information Processing Systems, 20(October)."},{"authors":[{"first":"L.","last":"Carlson"},{"first":"D.","last":"Marcu"},{"first":"M.","middle":"E.","last":"Okurowski"}],"year":"2001","title":"Building a discourse-tagged corpus in the framework of rhetorical structure theory","source":"L. Carlson, D. Marcu, and M.E. Okurowski. 2001. Building a discourse-tagged corpus in the framework of rhetorical structure theory. pages 1–10. Association for Computational Linguistics. Proceedings of the Second SIGdial Workshop on Discourse and Dialogue-Volume 16."},{"authors":[{"first":"R.","last":"Caruana"}],"year":"1997","title":"Multitask learning","source":"R. Caruana. 1997. Multitask learning. Machine Learning, 28(1):41–75."},{"authors":[{"first":"P.","last":"Cimiano"},{"first":"U.","last":"Reyle"},{"first":"J.","last":"Saric"}],"year":"2005","title":"Ontologydriven discourse analysis for information extraction","source":"P. Cimiano, U. Reyle, and J. Saric. 2005. Ontologydriven discourse analysis for information extraction. Data and Knowledge Engineering, 55(1):59–83."},{"authors":[{"first":"Eugene","middle":"Charniak David","last":"McClosky"},{"first":"Mark","last":"Johnson"}],"year":"2008","title":"Bllip north american news text, complete","source":"Eugene Charniak David McClosky and Mark Johnson. 2008. Bllip north american news text, complete."},{"authors":[{"first":"T.","last":"Evgeniou"},{"first":"M.","last":"Pontil"}],"year":"2004","title":"Regularized multi– task learning","source":"T. Evgeniou and M. Pontil. 2004. Regularized multi– task learning. pages 109–117. ACM. Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining."},{"authors":[{"first":"H.","last":"Hernault"},{"first":"D.","last":"Bollegala"},{"first":"M.","last":"Ishizuka"}],"year":"2010","title":"A semi-supervised approach to improve classification of infrequent discourse relations using feature vector extension","source":"H. Hernault, D. Bollegala, and M. Ishizuka. 2010. A semi-supervised approach to improve classification of infrequent discourse relations using feature vector extension. pages 399–409. Association for Computational Linguistics. Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing."},{"authors":[{"first":"H.","last":"Hernault"},{"first":"D.","last":"Bollegala"},{"first":"M.","last":"Ishizuka"}],"year":"2011","title":"Semi-supervised discourse relation classification with structural learning","source":"H. Hernault, D. Bollegala, and M. Ishizuka. 2011. Semi-supervised discourse relation classification with structural learning. In Proceedings of the 12th international conference on Computational linguistics and intelligent text processing - Volume Part I, CICLing’11, pages 340–352, Berlin, Heidelberg. Springer-Verlag."},{"authors":[{"first":"T.","last":"Jebara"}],"year":"2004","title":"Multi-task feature and kernel selection for svms","source":"T. Jebara. 2004. Multi-task feature and kernel selection for svms. page 55. ACM. Proceedings of the twenty-first international conference on Machine learning."},{"authors":[{"first":"B.","last":"Levin"}],"year":"1993","title":"English verb classes and alternations: A preliminary investigation, volume 348","source":"B. Levin. 1993. English verb classes and alternations: A preliminary investigation, volume 348. University of Chicago press Chicago, IL:. 484"},{"authors":[{"first":"Z.","last":"Lin"},{"first":"M.","middle":"Y.","last":"Kan"},{"first":"H.","middle":"T.","last":"Ng"}],"year":"2009","title":"Recogniz-ing implicit discourse relations in the penn discourse treebank","source":"Z. Lin, M.Y. Kan, and H.T. Ng. 2009. Recogniz-ing implicit discourse relations in the penn discourse treebank. pages 343–351. Association for Computational Linguistics. Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1-Volume 1."},{"authors":[{"first":"W.","middle":"C.","last":"Mann"},{"first":"S.","middle":"A.","last":"Thompson"}],"year":"1988","title":"Rhetorical structure theory: Toward a functional theory of text organization","source":"W.C. Mann and S.A. Thompson. 1988. Rhetorical structure theory: Toward a functional theory of text organization. Text-Interdisciplinary Journal for the Study of Discourse, 8(3):243–281."},{"authors":[{"first":"D.","last":"Marcu"},{"first":"A.","last":"Echihabi"}],"year":"2002","title":"An unsupervised approach to recognizing discourse relations","source":"D. Marcu and A. Echihabi. 2002. An unsupervised approach to recognizing discourse relations. pages 368–375. Association for Computational Linguistics. Proceedings of the 40th Annual Meeting on Association for Computational Linguistics."},{"authors":[{"last":"PDTB-Group"}],"year":"2008","title":"The penn discourse treebank 2","source":"PDTB-Group. 2008. The penn discourse treebank 2.0 annotation manual. Technical report, Institute for Research in Cognitive Science, University of Pennsylvania."},{"authors":[{"first":"E.","last":"Pitler"},{"first":"M.","last":"Raghupathy"},{"first":"H.","last":"Mehta"},{"first":"A.","last":"Nenkova"},{"first":"A.","last":"Lee"},{"first":"A.","last":"Joshi"}],"year":"2008","title":"Easily identifiable discourse relations","source":"E. Pitler, M. Raghupathy, H. Mehta, A. Nenkova, A. Lee, and A. Joshi. 2008. Easily identifiable discourse relations. Citeseer. Proceedings of the 22nd International Conference on Computational Linguistics (COLING 2008), Manchester, UK, August."},{"authors":[{"first":"E.","last":"Pitler"},{"first":"A.","last":"Louis"},{"first":"A.","last":"Nenkova"}],"year":"2009","title":"Automatic sense prediction for implicit discourse relations in text","source":"E. Pitler, A. Louis, and A. Nenkova. 2009. Automatic sense prediction for implicit discourse relations in text. pages 683–691. Association for Computational Linguistics. Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2-Volume 2."},{"authors":[{"first":"Rashmi","last":"Prasad"},{"first":"Nikhil","last":"Dinesh"},{"first":"Alan","last":"Lee"},{"first":"Eleni","last":"Miltsakaki"},{"first":"Livio","last":"Robaldo"},{"first":"Aravind","last":"Joshi"},{"first":"Bonnie","last":"Webber"}],"year":"2008","title":"The penn discourse treebank 2","source":"Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind Joshi, and Bonnie Webber. 2008. The penn discourse treebank 2.0. In In Proceedings of LREC."},{"authors":[{"first":"M.","last":"Saito"},{"first":"K.","last":"Yamamoto"},{"first":"S.","last":"Sekine"}],"year":"2006","title":"Using phrasal patterns to identify discourse relations","source":"M. Saito, K. Yamamoto, and S. Sekine. 2006. Using phrasal patterns to identify discourse relations. pages 133–136. Association for Computational Linguistics. Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers on XX."},{"authors":[{"first":"R.","last":"Soricut"},{"first":"D.","last":"Marcu"}],"year":"2003","title":"Sentence level discourse parsing using syntactic and lexical information","source":"R. Soricut and D. Marcu. 2003. Sentence level discourse parsing using syntactic and lexical information. pages 149–156. Association for Computational Linguistics. Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1."},{"authors":[{"first":"C.","last":"Sporleder"},{"first":"A.","last":"Lascarides"}],"year":"2008","title":"Using automatically labelled examples to classify rhetorical relations: An assessment","source":"C. Sporleder and A. Lascarides. 2008. Using automatically labelled examples to classify rhetorical relations: An assessment. Natural Language Engineer-ing, 14(03):369–416."},{"authors":[{"first":"S.","last":"Thrun"}],"year":"1996","title":"Is learning the n-th thing any easier than learning the first? Advances in Neural Information Processing Systems, pages 640–646","source":"S. Thrun. 1996. Is learning the n-th thing any easier than learning the first? Advances in Neural Information Processing Systems, pages 640–646."},{"authors":[{"first":"S.","last":"Verberne"},{"first":"L.","last":"Boves"},{"first":"N.","last":"Oostdijk"},{"first":"P.","middle":"A.","last":"Coppen"}],"year":"2007","title":"Evaluating discourse-based answer extraction for why-question answering","source":"S. Verberne, L. Boves, N. Oostdijk, and P.A. Coppen. 2007. Evaluating discourse-based answer extraction for why-question answering. pages 735–736. ACM. Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval."},{"authors":[{"first":"W.","middle":"T.","last":"Wang"},{"first":"J.","last":"Su"},{"first":"C.","middle":"L.","last":"Tan"}],"year":"2010","title":"Kernel based discourse relation recognition with temporal order-ing information","source":"W.T. Wang, J. Su, and C.L. Tan. 2010. Kernel based discourse relation recognition with temporal order-ing information. pages 710–719. Association for Computational Linguistics. Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics."},{"authors":[{"first":"B.","last":"Wellner"},{"first":"J.","last":"Pustejovsky"},{"first":"C.","last":"Havasi"},{"first":"A.","last":"Rumshisky"},{"first":"R.","last":"Sauri"}],"year":"2006","title":"Classification of discourse coherence relations: An exploratory study using multiple knowledge sources","source":"B. Wellner, J. Pustejovsky, C. Havasi, A. Rumshisky, and R. Sauri. 2006. Classification of discourse coherence relations: An exploratory study using multiple knowledge sources. pages 117–125. Association for Computational Linguistics. Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue."},{"authors":[{"first":"T.","last":"Wilson"},{"first":"J.","last":"Wiebe"},{"first":"P.","last":"Hoffmann"}],"year":"2009","title":"Recognizing contextual polarity: An exploration of features for phrase-level sentiment analysis","source":"T. Wilson, J. Wiebe, and P. Hoffmann. 2009. Recognizing contextual polarity: An exploration of features for phrase-level sentiment analysis. Computational Linguistics, 35(3):399–433."},{"authors":[{"first":"F.","last":"Wolf"},{"first":"E.","last":"Gibson"},{"first":"A.","last":"Fisher"},{"first":"M.","last":"Knight"}],"year":"2005","title":"The discourse graphbank: A database of texts annotated with coherence relations","source":"F. Wolf, E. Gibson, A. Fisher, and M. Knight. 2005. The discourse graphbank: A database of texts annotated with coherence relations. Linguistic Data Consortium."},{"authors":[{"first":"Z.","middle":"M.","last":"Zhou"},{"first":"Y.","last":"Xu"},{"first":"Z.","middle":"Y.","last":"Niu"},{"first":"M.","last":"Lan"},{"first":"J.","last":"Su"},{"first":"C.","middle":"L.","last":"Tan"}],"year":"2010","title":"Predicting discourse connectives for implicit discourse relation recognition","source":"Z.M. Zhou, Y. Xu, Z.Y. Niu, M. Lan, J. Su, and C.L. Tan. 2010. Predicting discourse connectives for implicit discourse relation recognition. pages 1507– 1514. Association for Computational Linguistics. Proceedings of the 23rd International Conference on Computational Linguistics: Posters. 485"}],"cites":[{"style":0,"text":"Sporleder and Lascarides, 2008","origin":{"pointer":"/sections/1/paragraphs/0","offset":296,"length":30},"authors":[{"last":"Sporleder"},{"last":"Lascarides"}],"year":"2008","references":["/references/24"]},{"style":0,"text":"Verberne et al., 2007","origin":{"pointer":"/sections/2/paragraphs/0","offset":425,"length":21},"authors":[{"last":"Verberne"},{"last":"al."}],"year":"2007","references":["/references/26"]},{"style":0,"text":"Cimiano et al., 2005","origin":{"pointer":"/sections/2/paragraphs/0","offset":478,"length":20},"authors":[{"last":"Cimiano"},{"last":"al."}],"year":"2005","references":["/references/8"]},{"style":0,"text":"Sporleder and Lascarides, 2008","origin":{"pointer":"/sections/2/paragraphs/0","offset":839,"length":30},"authors":[{"last":"Sporleder"},{"last":"Lascarides"}],"year":"2008","references":["/references/24"]},{"style":0,"text":"Marcu and Echihabi, 2002","origin":{"pointer":"/sections/2/paragraphs/1","offset":63,"length":24},"authors":[{"last":"Marcu"},{"last":"Echihabi"}],"year":"2002","references":["/references/17"]},{"style":0,"text":"Sporleder and Lascarides, 2008","origin":{"pointer":"/sections/2/paragraphs/1","offset":220,"length":30},"authors":[{"last":"Sporleder"},{"last":"Lascarides"}],"year":"2008","references":["/references/24"]},{"style":0,"text":"Blair-Goldensohn, 2007","origin":{"pointer":"/sections/2/paragraphs/1","offset":257,"length":22},"authors":[{"last":"Blair-Goldensohn"}],"year":"2007","references":["/references/4"]},{"style":0,"text":"Prasad et al., 2008","origin":{"pointer":"/sections/2/paragraphs/1","offset":1026,"length":19},"authors":[{"last":"Prasad"},{"last":"al."}],"year":"2008","references":["/references/21"]},{"style":0,"text":"Pitler et al., 2009","origin":{"pointer":"/sections/2/paragraphs/1","offset":1165,"length":19},"authors":[{"last":"Pitler"},{"last":"al."}],"year":"2009","references":["/references/20"]},{"style":0,"text":"Lin et al., 2009","origin":{"pointer":"/sections/2/paragraphs/1","offset":1187,"length":16},"authors":[{"last":"Lin"},{"last":"al."}],"year":"2009","references":["/references/15"]},{"style":0,"text":"Wang et al., 2010","origin":{"pointer":"/sections/2/paragraphs/1","offset":1206,"length":17},"authors":[{"last":"Wang"},{"last":"al."}],"year":"2010","references":["/references/27"]},{"style":0,"text":"Sporleder and Lascarides, 2008","origin":{"pointer":"/sections/2/paragraphs/2","offset":1,"length":30},"authors":[{"last":"Sporleder"},{"last":"Lascarides"}],"year":"2008","references":["/references/24"]},{"style":0,"text":"Marcu and Echihabi, 2002","origin":{"pointer":"/sections/2/paragraphs/2","offset":95,"length":24},"authors":[{"last":"Marcu"},{"last":"Echihabi"}],"year":"2002","references":["/references/17"]},{"style":0,"text":"Sporleder and Lascarides, 2008","origin":{"pointer":"/sections/2/paragraphs/6","offset":10,"length":30},"authors":[{"last":"Sporleder"},{"last":"Lascarides"}],"year":"2008","references":["/references/24"]},{"style":0,"text":"PDTB-Group, 2008","origin":{"pointer":"/sections/2/paragraphs/8","offset":41,"length":16},"authors":[{"last":"PDTB-Group"}],"year":"2008","references":["/references/18"]},{"style":0,"text":"Sporleder and Lascarides, 2008","origin":{"pointer":"/sections/2/paragraphs/8","offset":206,"length":30},"authors":[{"last":"Sporleder"},{"last":"Lascarides"}],"year":"2008","references":["/references/24"]},{"style":0,"text":"Marcu and Echihabi, 2002","origin":{"pointer":"/sections/3/paragraphs/0","offset":170,"length":24},"authors":[{"last":"Marcu"},{"last":"Echihabi"}],"year":"2002","references":["/references/17"]},{"style":0,"text":"Saito et al., 2006","origin":{"pointer":"/sections/3/paragraphs/0","offset":830,"length":18},"authors":[{"last":"Saito"},{"last":"al."}],"year":"2006","references":["/references/22"]},{"style":0,"text":"Blair-Goldensohn, 2007","origin":{"pointer":"/sections/3/paragraphs/0","offset":1018,"length":22},"authors":[{"last":"Blair-Goldensohn"}],"year":"2007","references":["/references/4"]},{"style":0,"text":"Sporleder and Lascarides, 2008","origin":{"pointer":"/sections/3/paragraphs/0","offset":1161,"length":30},"authors":[{"last":"Sporleder"},{"last":"Lascarides"}],"year":"2008","references":["/references/24"]},{"style":0,"text":"Marcu and Echihabi, 2002","origin":{"pointer":"/sections/3/paragraphs/0","offset":1273,"length":24},"authors":[{"last":"Marcu"},{"last":"Echihabi"}],"year":"2002","references":["/references/17"]},{"style":0,"text":"Soricut and Marcu, 2003","origin":{"pointer":"/sections/3/paragraphs/0","offset":1632,"length":23},"authors":[{"last":"Soricut"},{"last":"Marcu"}],"year":"2003","references":["/references/23"]},{"style":0,"text":"Carlson et al., 2001","origin":{"pointer":"/sections/3/paragraphs/0","offset":1724,"length":20},"authors":[{"last":"Carlson"},{"last":"al."}],"year":"2001","references":["/references/6"]},{"style":0,"text":"Mann and Thompson, 1988","origin":{"pointer":"/sections/3/paragraphs/0","offset":1803,"length":23},"authors":[{"last":"Mann"},{"last":"Thompson"}],"year":"1988","references":["/references/16"]},{"style":0,"text":"Wellner et al., 2006","origin":{"pointer":"/sections/3/paragraphs/0","offset":1830,"length":20},"authors":[{"last":"Wellner"},{"last":"al."}],"year":"2006","references":["/references/28"]},{"style":0,"text":"Wolf et al., 2005","origin":{"pointer":"/sections/3/paragraphs/0","offset":1921,"length":17},"authors":[{"last":"Wolf"},{"last":"al."}],"year":"2005","references":["/references/30"]},{"style":0,"text":"Pitler et al., 2009","origin":{"pointer":"/sections/3/paragraphs/0","offset":1952,"length":19},"authors":[{"last":"Pitler"},{"last":"al."}],"year":"2009","references":["/references/20"]},{"style":0,"text":"Lin et al., 2009","origin":{"pointer":"/sections/3/paragraphs/0","offset":1974,"length":16},"authors":[{"last":"Lin"},{"last":"al."}],"year":"2009","references":["/references/15"]},{"style":0,"text":"Wang et al., 2010","origin":{"pointer":"/sections/3/paragraphs/0","offset":1997,"length":17},"authors":[{"last":"Wang"},{"last":"al."}],"year":"2010","references":["/references/27"]},{"style":0,"text":"Prasad et al., 2008","origin":{"pointer":"/sections/3/paragraphs/0","offset":2060,"length":19},"authors":[{"last":"Prasad"},{"last":"al."}],"year":"2008","references":["/references/21"]},{"style":0,"text":"Hernault et al., 2010","origin":{"pointer":"/sections/3/paragraphs/0","offset":2265,"length":21},"authors":[{"last":"Hernault"},{"last":"al."}],"year":"2010","references":["/references/11"]},{"style":0,"text":"Hernault et al., 2011","origin":{"pointer":"/sections/3/paragraphs/0","offset":2417,"length":21},"authors":[{"last":"Hernault"},{"last":"al."}],"year":"2011","references":["/references/12"]},{"style":0,"text":"Zhou et al., 2010","origin":{"pointer":"/sections/3/paragraphs/1","offset":47,"length":17},"authors":[{"last":"Zhou"},{"last":"al."}],"year":"2010","references":["/references/31"]},{"style":0,"text":"Ando and Zhang, 2005a","origin":{"pointer":"/sections/3/paragraphs/1","offset":660,"length":21},"authors":[{"last":"Ando"},{"last":"Zhang"}],"year":"2005a","references":["/references/0"]},{"style":0,"text":"Argyriou et al., 2008","origin":{"pointer":"/sections/3/paragraphs/1","offset":685,"length":21},"authors":[{"last":"Argyriou"},{"last":"al."}],"year":"2008","references":["/references/2"]},{"style":0,"text":"Jebara, 2004","origin":{"pointer":"/sections/3/paragraphs/1","offset":710,"length":12},"authors":[{"last":"Jebara"}],"year":"2004","references":["/references/13"]},{"style":0,"text":"Bonilla et al., 2008","origin":{"pointer":"/sections/3/paragraphs/1","offset":726,"length":20},"authors":[{"last":"Bonilla"},{"last":"al."}],"year":"2008","references":["/references/5"]},{"style":0,"text":"Evgeniou and Pontil, 2004","origin":{"pointer":"/sections/3/paragraphs/1","offset":750,"length":25},"authors":[{"last":"Evgeniou"},{"last":"Pontil"}],"year":"2004","references":["/references/10"]},{"style":0,"text":"Baxter, 2000","origin":{"pointer":"/sections/3/paragraphs/1","offset":779,"length":12},"authors":[{"last":"Baxter"}],"year":"2000","references":["/references/3"]},{"style":0,"text":"Caruana, 1997","origin":{"pointer":"/sections/3/paragraphs/1","offset":795,"length":13},"authors":[{"last":"Caruana"}],"year":"1997","references":["/references/7"]},{"style":0,"text":"Thrun, 1996","origin":{"pointer":"/sections/3/paragraphs/1","offset":812,"length":11},"authors":[{"last":"Thrun"}],"year":"1996","references":["/references/25"]},{"style":0,"text":"Evgeniou and Pontil, 2004","origin":{"pointer":"/sections/3/paragraphs/1","offset":940,"length":25},"authors":[{"last":"Evgeniou"},{"last":"Pontil"}],"year":"2004","references":["/references/10"]},{"style":0,"text":"Argyriou et al., 2008","origin":{"pointer":"/sections/3/paragraphs/1","offset":1177,"length":21},"authors":[{"last":"Argyriou"},{"last":"al."}],"year":"2008","references":["/references/2"]},{"style":0,"text":"Ando and Zhang, 2005b","origin":{"pointer":"/sections/3/paragraphs/1","offset":1201,"length":21},"authors":[{"last":"Ando"},{"last":"Zhang"}],"year":"2005b","references":["/references/1"]},{"style":0,"text":"Sporleder and Lascarides, 2008","origin":{"pointer":"/sections/4/paragraphs/1","offset":212,"length":30},"authors":[{"last":"Sporleder"},{"last":"Lascarides"}],"year":"2008","references":["/references/24"]},{"style":0,"text":"Ando and Zhang, 2005a","origin":{"pointer":"/sections/4/paragraphs/3","offset":415,"length":21},"authors":[{"last":"Ando"},{"last":"Zhang"}],"year":"2005a","references":["/references/0"]},{"style":0,"text":"Ando and Zhang, 2005b","origin":{"pointer":"/sections/4/paragraphs/3","offset":616,"length":21},"authors":[{"last":"Ando"},{"last":"Zhang"}],"year":"2005b","references":["/references/1"]},{"style":0,"text":"Ando and Zhang, 2005a","origin":{"pointer":"/sections/4/paragraphs/3","offset":664,"length":21},"authors":[{"last":"Ando"},{"last":"Zhang"}],"year":"2005a","references":["/references/0"]},{"style":0,"text":"Marcu and Echihabi, 2002","origin":{"pointer":"/sections/4/paragraphs/14","offset":15,"length":24},"authors":[{"last":"Marcu"},{"last":"Echihabi"}],"year":"2002","references":["/references/17"]},{"style":0,"text":"Sporleder and Lascarides, 2008","origin":{"pointer":"/sections/4/paragraphs/14","offset":46,"length":30},"authors":[{"last":"Sporleder"},{"last":"Lascarides"}],"year":"2008","references":["/references/24"]},{"style":0,"text":"Pitler et al., 2008","origin":{"pointer":"/sections/4/paragraphs/14","offset":576,"length":19},"authors":[{"last":"Pitler"},{"last":"al."}],"year":"2008","references":["/references/19"]},{"style":0,"text":"McClosky and Johnson., 2008","origin":{"pointer":"/sections/5/paragraphs/0","offset":305,"length":27},"authors":[{"last":"McClosky"},{"last":"Johnson."}],"year":"2008","references":[]},{"style":0,"text":"Prasad et al., 2008","origin":{"pointer":"/sections/5/paragraphs/0","offset":371,"length":19},"authors":[{"last":"Prasad"},{"last":"al."}],"year":"2008","references":["/references/21"]},{"style":0,"text":"Pitler et al., 2009","origin":{"pointer":"/sections/5/paragraphs/2","offset":51,"length":19},"authors":[{"last":"Pitler"},{"last":"al."}],"year":"2009","references":["/references/20"]},{"style":0,"text":"Zhou et al., 2010","origin":{"pointer":"/sections/5/paragraphs/2","offset":77,"length":17},"authors":[{"last":"Zhou"},{"last":"al."}],"year":"2010","references":["/references/31"]},{"style":0,"text":"Pitler et al., 2009","origin":{"pointer":"/sections/5/paragraphs/8","offset":1313,"length":19},"authors":[{"last":"Pitler"},{"last":"al."}],"year":"2009","references":["/references/20"]},{"style":0,"text":"Zhou et al., 2010","origin":{"pointer":"/sections/5/paragraphs/8","offset":1357,"length":17},"authors":[{"last":"Zhou"},{"last":"al."}],"year":"2010","references":["/references/31"]},{"style":0,"text":"Pitler et al., 2009","origin":{"pointer":"/sections/5/paragraphs/9","offset":18,"length":19},"authors":[{"last":"Pitler"},{"last":"al."}],"year":"2009","references":["/references/20"]},{"style":0,"text":"Levin, 1993","origin":{"pointer":"/sections/5/paragraphs/9","offset":174,"length":11},"authors":[{"last":"Levin"}],"year":"1993","references":["/references/14"]},{"style":0,"text":"Wilson et al., 2009","origin":{"pointer":"/sections/5/paragraphs/10","offset":406,"length":19},"authors":[{"last":"Wilson"},{"last":"al."}],"year":"2009","references":["/references/29"]},{"style":0,"text":"Pitler et al., 2009","origin":{"pointer":"/sections/6/paragraphs/0","offset":46,"length":19},"authors":[{"last":"Pitler"},{"last":"al."}],"year":"2009","references":["/references/20"]},{"style":0,"text":"Lin et al., 2009","origin":{"pointer":"/sections/6/paragraphs/0","offset":72,"length":16},"authors":[{"last":"Lin"},{"last":"al."}],"year":"2009","references":["/references/15"]},{"style":0,"text":"Sporleder and Lascarides, 2008","origin":{"pointer":"/sections/6/paragraphs/16","offset":204,"length":30},"authors":[{"last":"Sporleder"},{"last":"Lascarides"}],"year":"2008","references":["/references/24"]}]}
