{"sections":[{"title":"","paragraphs":["Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 424–428, Sofia, Bulgaria, August 4-9 2013. c⃝2013 Association for Computational Linguistics"]},{"title":"Minimum Bayes Risk based Answer Re-ranking for Question Answering Nan Duan Natural Language Computing Microsoft Research Asia nanduan@microsoft.com Abstract","paragraphs":["This paper presents two minimum Bayes risk (MBR) based Answer Re-ranking (MBRAR) approaches for the question answering (QA) task. The first approach re-ranks single QA system’s outputs by using a traditional MBR model, by measuring correlations between answer candidates; while the second approach re-ranks the combined outputs of multiple QA systems with heterogenous answer extraction components by using a mixture model-based MBR model. Evaluations are performed on factoid questions selected from two different domains: Jeopardy! and Web, and significant improve-ments are achieved on all data sets."]},{"title":"1 Introduction","paragraphs":["Minimum Bayes Risk (MBR) techniques have been successfully applied to a wide range of natural language processing tasks, such as statistical machine translation (Kumar and Byrne, 2004), automatic speech recognition (Goel and Byrne, 2000), parsing (Titov and Henderson, 2006), etc. This work makes further exploration along this line of research, by applying MBR technique to question answering (QA).","The function of a typical factoid question answering system is to automatically give answers to questions in most case asking about entities, which usually consists of three key components: question understanding, passage retrieval, and answer extraction. In this paper, we propose two MBR-based Answer Re-ranking (MBRAR) approaches, aiming to re-rank answer candidates from either single and multiple QA systems. The first one re-ranks answer outputs from single QA system based on a traditional MBR model by measuring the correlations between each answer candidates and all the other candidates; while the second one re-ranks the combined answer outputs from multiple QA systems based on a mixture model-based MBR model. The key contribution of this work is that, our MBRAR approaches assume little about QA systems and can be easily applied to QA systems with arbitrary sub-components.","The remainder of this paper is organized as follows: Section 2 gives a brief review of the QA task and describe two types of QA systems with different pros and cons. Section 3 presents two MBRAR approaches that can re-rank the answer candidates from single and multiple QA systems respectively. The relationship between our approach and previous work is discussed in Section 4. Section 5 evaluates our methods on large scale questions selected from two domains (Jeopardy! and Web) and shows promising results. Section 6 concludes this paper."]},{"title":"2 Question Answering 2.1 Overview","paragraphs":["Formally, given an input question Q, a typical factoid QA system generates answers on the basis of the following three procedures:","(1) Question Understanding, which determines the answer type and identifies necessory information contained in Q, such as question focus and lexical answer type (LAT). Such information will be encoded and used by the following procedures.","(2) Passage Retrieval, which formulates queries based on Q, and retrieves passages from offline corpus or online search engines (e.g. Google and Bing).","(3) Answer Extraction, which first extracts answer candidates from retrieved passages, and then ranks them based on specific ranking models. 424 2.2 Two Types of QA Systems We present two different QA sysytems, which are distinguished from three aspects: answer typing, answer generation, and answer ranking.","The 1st","QA system is denoted as Type-Dependent QA engine (TD-QA). In answer typing phase, TD-QA assigns the most possible answer type T̂ to a given question Q based on:","T̂ = argmax T P (T |Q) P (T |Q) is a probabilistic answer-typing model that is similar to Pinchak and Lin (2006)’s work. In answer generation phase, TD-QA uses a CRF-based Named Entity Recognizer to detect all named entities contained in retrieved passages with the type T̂ , and treat them as the answer candidate space H(Q): H(Q) = ⋃ k Ak In answer ranking phase, the decision rule described below is used to rank answer candidate space H(Q):","Â = argmax A∈H(Q) P (A| T̂ , Q)","= argmax A∈H(Q) ∑ i λi · hi(A, T̂ , Q) where {hi(·)} is a set of ranking features that measure the correctness of answer candidates, and {λi} are their corresponding feature weights.","The 2ed","QA system is denoted as Type-Independent QA engine (TI-QA). In answer typing phase, TI-QA assigns top N , instead of the best, answer types TN (Q) for each question Q. The probability of each type candidate is maintained as well. In answer generation phase, TI-QA extracts all answer candidates from retrieved passages based on answer types in TN (Q), by the same NER used in TD-QA. In answer ranking phase, TI-QA considers the probabilities of different answer types as well:","Â = argmax A∈H(Q) P (A|Q)","= argmax A∈H(Q) ∑ T ∈TN (Q) P (A|T, Q) · P (T |Q)","On one hand, TD-QA can achieve relative high ranking precision, as using a unique answer type greatly reduces the size of the candidate list for ranking. However, as the answer-typing model is far from perfect, if prediction errors happen, TD-QA can no longer give correct answers at all.","On the other hand, TI-QA can provide higher answer coverage, as it can extract answer candidates with multiple answer types. However, more answer candidates with different types bring more difficulties to the answer ranking model to rank the correct answer to the top 1 position. So the ranking precision of TI-QA is not as good as TD-QA."]},{"title":"3 MBR-based Answering Re-ranking 3.1 MBRAR for Single QA System","paragraphs":["MBR decoding (Bickel and Doksum, 1977) aims to select the hypothesis that minimizes the expected loss in classification. In MBRAR, we replace the loss function with the gain function that measure the correlation between answer candidates. Thus, the objective of the MBRAR approach for single QA system is to find the answer candidate that is most supported by other candidates under QA system’s distribution, which can be formally written as:","Â = argmax A∈H(Q) ∑ Ak∈H(Q) G(A, Ak) · P (Ak|H(Q))","P (Ak|H(Q)) denotes the hypothesis distribution estimated on the search space H(Q) based on the following log-linear formulation: P (Ak|H(Q)) = exp(β · P (Ak|Q))","∑ A′","∈H exp(β · P (A′ |Q)) P (Ak|Q) is the posterior probability of the answer candidate Ak based on QA system’s ranking model, β is a scaling factor which controls the distribution P (·) sharp (when β > 1) or smooth (when β < 1).","G(A, Ak) is the gain function that denotes the degree of how Ak supports A. This function can be further expanded as a weighted combination of a set of correlation features as:","∑","j λj · hj(A, Ak).","The following correlation features are used in","G(·): • answer-level n-gram correlation feature: hanswer(A, Ak) = ∑ ω∈A #ω(Ak) where ω denotes an n-gram in A, #ω(Ak) denotes the number of times that ω occurs in Ak. 425 • passage-level n-gram correlation feature: hpassage(A, Ak) = ∑ ω∈PA #ω(PAk ) where PA denotes passages from which A are extracted. This feature measures the degree of Ak supports A from the context perspective. • answer-type agreement feature: htype(A, Ak) = δ(TA, TAi) δ(TA, TAk ) denotes an indicator function that equals to 1 when the answer types of A and Ak are the same, and 0 otherwise.","• answer-length feature that is used to penalize long answer candidates.","• averaged passage-length feature that is used to penalize passages with a long averaged length. 3.2 MBRAR for Multiple QA Systems Aiming to apply MBRAR to the outputs from N QA systems, we modify MBR components as follows.","First, the hypothesis space HC(Q) is built by merging answer candidates of multiple QA systems: HC(Q) = ⋃ i Hi(Q)","Second, the hypothesis distribution is defined as a probability distribution over the combined search space of N component QA systems and computed as a weighted sum of component model distributions: P (A|HC(Q)) = N∑ i=1 αi · P (A|Hi(Q)) where α1, ..., αN are coefficients with following constraints holds1",": 0 ≤ α i ≤ 1 and","∑N","i=1 αi = 1,","P (A|Hi(Q)) is the posterior probability of A esti-","mated on the ith","QA system’s search space Hi(Q). Third, the features used in the gain function G(·)","can be grouped into two categories, including: • system-independent features, which includes all features described in Section 3.1 for single system based MBRAR method; 1 For simplicity, the coefficients are equally set: αi =","1/N.","• system-dependent features, which measure the correctness of answer candidates based on information provided by multiple QA systems:","– system indicator feature hsys(A, QAi), which equals to 1 when A is generated by the ith","system QA i, and 0 otherwise;","– system ranking feature hrank(A, QAi), which equals to the reciprocal of the rank position of A predicted by QAi. If QAi fails to generate A, then it equals to 0;","– ensemble feature hcons(A), which equals to 1 when A can be generated by all individual QA system, and 0 other-wise.","Thus, the MBRAR for multiple QA systems can be finally formulated as follows:","Â = argmax A∈HC(Q) ∑ Ai∈HC(Q) G(A, Ai) · P (Ai|HC(Q)) where the training process of the weights in the gain function is carried out with Ranking SVM2 based on the method described in Verberne et al. (2009)."]},{"title":"4 Related Work","paragraphs":["MBR decoding have been successfully applied to many NLP tasks, e.g. machine translation, parsing, speech recognition and etc. As far as we know, this is the first work that applies MBR principle to QA.","Yaman et al. (2009) proposed a classifica-tion based method for QA task that jointly uses multiple 5-W QA systems by selecting one optimal QA system for each question. Comparing to their work, our MBRAR approaches assume few about the question types, and all QA systems contribute in the re-ranking model. Tellez-Valero et al. (2008) presented an answer validation method that helps individual QA systems to automatically detect its own errors based on information from multiple QA systems. Chu-Carroll et al. (2003) presented a multi-level answer resolution algorithm to merge results from the answering agents at the question, passage, and answer levels. Grappy et al. 2 We use SV MRank","(Joachims, 2006) that can be found-","ed at www.cs.cornell.edu/people/tj/svm light/svm rank.html/ 426 (2012) proposed to use different score combinations to merge answers from different QA systems. Although all methods mentioned above leverage information provided by multiple QA systems, our work is the first time to explore the usage of MBR principle for the QA task."]},{"title":"5 Experiments 5.1 Data and Metric","paragraphs":["Questions from two different domains are used as our evaluation data sets: the first data set includes 10,051 factoid question-answer pairs selected from the Jeopardy! quiz show3","; while the second data set includes 360 celebrity-asking web questions4","selected from a commercial search engine, the answers for each question is labeled by human annotators.","The evaluation metric Succeed@n is defined as the number of questions whose correct answers are successfully ranked to the top n answer candidates. 5.2 MBRAR for Single QA System We first evaluate the effectiveness of our MBRAR for single QA system. Given the N-best answer outputs from each single QA system, together with their ranking scores assigned by the corresponding ranking components, we further perform MBRAR to re-rank them and show resulting numbers on two evaluation data sets in Table 1 and 2 respectively.","Both Table 1 and Table 2 show that, by leveraging our MBRAR method on individual QA systems, the rankings of correct answers are consistently improved on both Jeopardy! and web questions. Joepardy! Succeed@1 Succeed@2 Succeed@3 TD-QA 2,289 2,693 2,885 MBRAR 2,372 2,784 2,982 TI-QA 2,527 3,397 3,821 MBRAR 2,628 3,500 3,931 Table 1: Impacts of MBRAR for single QA system on Jeopardy! questions.","We also notice TI-QA performs significantly better than TD-QA on Jeopardy! questions, but worse on web questions. This is due to fact that when the answer type is fixed (PERSON for 3 http://www.jeopardy.com/ 4 The answers of such questions are person names. Web Succeed@1 Succeed@2 Succeed@3 TD-QA 97 128 146 MBRAR 99 130 148 TI-QA 95 122 136 MBRAR 97 126 143 Table 2: Impacts of MBRAR for single QA system on web questions. celebrity-asking questions), TI-QA will generate candidates with wrong answer types, which will definitely deteriorate the ranking accuracy. 5.3 MBRAR for Multiple QA Systems We then evaluate the effectiveness of our MBRAR for multiple QA systems. The mixture model-based MBRAR method described in Section 3.2 is used to rank the combined answer outputs from TD-QA and TI-QA, with ranking results shown in Table 3 and 4.","From Table 3 and Table 4 we can see that, comparing to the ranking performances of single QA systems TD-QA and TI-QA, MBRAR using two QA systems’ outputs shows significant improve-ments on both Jeopardy! and web questions. Furthermore, comparing to MBRAR on single QA system, MBRAR on multiple QA systems can provide extra gains on both questions sets as well. Jeopardy! Succeed@1 Succeed@2 Succeed@3 TD-QA 2,289 2,693 2,885 TI-QA 2,527 3,397 3,821 MBRAR 2,891 3,668 4,033 Table 3: Impacts of MBRAR for multiple QA systems on Jeopardy! questions. Web Succeed@1 Succeed@2 Succeed@3 TD-QA 97 128 146 TI-QA 95 122 136 MBRAR 108 137 152 Table 4: Impacts of MBRAR for multiple QA systems on web questions."]},{"title":"6 Conclusions and Future Work","paragraphs":["In this paper, we present two MBR-based answer re-ranking approaches for QA. Comparing to previous methods, MBRAR provides a systematic way to re-rank answers from either single or multiple QA systems, without considering their heterogeneous implementations of internal components. 427 Experiments on questions from two different domains show that, our proposed method can significantly improve the ranking performances. In future, we will add more QA systems into our MBRAR framework, and design more features for the MBR gain function."]},{"title":"References","paragraphs":["P. J. Bickel and K. A. Doksum. 1977. Mathematical Statistics: Basic Ideas and Selected Topics. Holden-Day Inc.","Jennifer Chu-Carroll, Krzysztof Czuba, John Prager, and Abraham Ittycheriah. 2003. In Question Answering, Two Heads Are Better Than One. In proceeding of HLT-NAACL.","Vaibhava Goel and William Byrne. 2000. Minimum bayes-risk automatic speech recognition, Computer Speech and Language.","Arnaud Grappy, Brigitte Grau, and Sophie Ros-set. 2012. Methods Combination and ML-based Re-ranking of Multiple Hypothesis for Question-Answering Systems, In proceeding of EACL.","Thorsten Joachims. 2006. Training Linear SVMs in Linear Time, In proceeding of KDD.","Shankar Kumar and William Byrne. 2004. Minimum Bayes-Risk Decoding for Statisti-cal Machine Translation. In proceeding of HLT-NAACL.","Christopher Pinchak and Dekang Lin. 2006. A Probabilistic Answer Type Model. In proceeding of EA-CL.","Ivan Titov and James Henderson. 2006. Bayes Risk Minimization in Natural Language Parsing. Technical report.","Alberto Tellez-Valero, Manuel Montes-y-Gomez, Luis Villasenor-Pineda, and Anselmo Penas. 2008. Improving Question Answering by Combining Multiple Systems via Answer Validation. In proceeding of CI-CLing.","Suzan Verberne, Clst Ru Nijmegen, Hans Van Halteren, Clst Ru Nijmegen, Daphne Theijssen, Ru Nijmegen, Stephan Raaijmakers, Lou Boves, and Clst Ru Nijmegen. 2009. Learning to rank qa data. evaluating machine learning techniques for ranking answers to why-questions. In proceeding of SIGIR workshop.","Sibel Yaman, Dilek Hakkani-Tur, Gokhan Tur, Ralph Grishman, Mary Harper, Kathleen R. McKeown, Adam Meyers, Kartavya Sharma. 2009. Classification-Based Strategies for Combining Multiple 5-W Question Answering Systems. In proceeding of INTERSPEECH. 428"]}],"references":[{"authors":[{"first":"P.","middle":"J.","last":"Bickel"},{"first":"K.","middle":"A.","last":"Doksum"}],"year":"1977","title":"Mathematical Statistics: Basic Ideas and Selected Topics","source":"P. J. Bickel and K. A. Doksum. 1977. Mathematical Statistics: Basic Ideas and Selected Topics. Holden-Day Inc."},{"authors":[{"first":"Jennifer","last":"Chu-Carroll"},{"first":"Krzysztof","last":"Czuba"},{"first":"John","last":"Prager"},{"first":"Abraham","last":"Ittycheriah"}],"year":"2003","title":"In Question Answering, Two Heads Are Better Than One","source":"Jennifer Chu-Carroll, Krzysztof Czuba, John Prager, and Abraham Ittycheriah. 2003. In Question Answering, Two Heads Are Better Than One. In proceeding of HLT-NAACL."},{"authors":[{"first":"Vaibhava","last":"Goel"},{"first":"William","last":"Byrne"}],"year":"2000","title":"Minimum bayes-risk automatic speech recognition, Computer Speech and Language","source":"Vaibhava Goel and William Byrne. 2000. Minimum bayes-risk automatic speech recognition, Computer Speech and Language."},{"authors":[{"first":"Arnaud","last":"Grappy"},{"first":"Brigitte","last":"Grau"},{"first":"Sophie","last":"Ros-set"}],"year":"2012","title":"Methods Combination and ML-based Re-ranking of Multiple Hypothesis for Question-Answering Systems, In proceeding of EACL","source":"Arnaud Grappy, Brigitte Grau, and Sophie Ros-set. 2012. Methods Combination and ML-based Re-ranking of Multiple Hypothesis for Question-Answering Systems, In proceeding of EACL."},{"authors":[{"first":"Thorsten","last":"Joachims"}],"year":"2006","title":"Training Linear SVMs in Linear Time, In proceeding of KDD","source":"Thorsten Joachims. 2006. Training Linear SVMs in Linear Time, In proceeding of KDD."},{"authors":[{"first":"Shankar","last":"Kumar"},{"first":"William","last":"Byrne"}],"year":"2004","title":"Minimum Bayes-Risk Decoding for Statisti-cal Machine Translation","source":"Shankar Kumar and William Byrne. 2004. Minimum Bayes-Risk Decoding for Statisti-cal Machine Translation. In proceeding of HLT-NAACL."},{"authors":[{"first":"Christopher","last":"Pinchak"},{"first":"Dekang","last":"Lin"}],"year":"2006","title":"A Probabilistic Answer Type Model","source":"Christopher Pinchak and Dekang Lin. 2006. A Probabilistic Answer Type Model. In proceeding of EA-CL."},{"authors":[{"first":"Ivan","last":"Titov"},{"first":"James","last":"Henderson"}],"year":"2006","title":"Bayes Risk Minimization in Natural Language Parsing","source":"Ivan Titov and James Henderson. 2006. Bayes Risk Minimization in Natural Language Parsing. Technical report."},{"authors":[{"first":"Alberto","last":"Tellez-Valero"},{"first":"Manuel","last":"Montes-y-Gomez"},{"first":"Luis","last":"Villasenor-Pineda"},{"first":"Anselmo","last":"Penas"}],"year":"2008","title":"Improving Question Answering by Combining Multiple Systems via Answer Validation","source":"Alberto Tellez-Valero, Manuel Montes-y-Gomez, Luis Villasenor-Pineda, and Anselmo Penas. 2008. Improving Question Answering by Combining Multiple Systems via Answer Validation. In proceeding of CI-CLing."},{"authors":[{"first":"Suzan","last":"Verberne"},{"first":"Clst","middle":"Ru","last":"Nijmegen"},{"first":"Hans","last":"Van Halteren"},{"first":"Clst","middle":"Ru","last":"Nijmegen"},{"first":"Daphne","last":"Theijssen"},{"first":"Ru","last":"Nijmegen"},{"first":"Stephan","last":"Raaijmakers"},{"first":"Lou","last":"Boves"},{"first":"Clst","middle":"Ru","last":"Nijmegen"}],"year":"2009","title":"Learning to rank qa data","source":"Suzan Verberne, Clst Ru Nijmegen, Hans Van Halteren, Clst Ru Nijmegen, Daphne Theijssen, Ru Nijmegen, Stephan Raaijmakers, Lou Boves, and Clst Ru Nijmegen. 2009. Learning to rank qa data. evaluating machine learning techniques for ranking answers to why-questions. In proceeding of SIGIR workshop."},{"authors":[{"first":"Sibel","last":"Yaman"},{"first":"Dilek","last":"Hakkani-Tur"},{"first":"Gokhan","last":"Tur"},{"first":"Ralph","last":"Grishman"},{"first":"Mary","last":"Harper"},{"first":"Kathleen","middle":"R.","last":"McKeown"},{"first":"Adam","last":"Meyers"},{"first":"Kartavya","last":"Sharma"}],"year":"2009","title":"Classification-Based Strategies for Combining Multiple 5-W Question Answering Systems","source":"Sibel Yaman, Dilek Hakkani-Tur, Gokhan Tur, Ralph Grishman, Mary Harper, Kathleen R. McKeown, Adam Meyers, Kartavya Sharma. 2009. Classification-Based Strategies for Combining Multiple 5-W Question Answering Systems. In proceeding of INTERSPEECH. 428"}],"cites":[{"style":0,"text":"Kumar and Byrne, 2004","origin":{"pointer":"/sections/2/paragraphs/0","offset":162,"length":21},"authors":[{"last":"Kumar"},{"last":"Byrne"}],"year":"2004","references":["/references/5"]},{"style":0,"text":"Goel and Byrne, 2000","origin":{"pointer":"/sections/2/paragraphs/0","offset":216,"length":20},"authors":[{"last":"Goel"},{"last":"Byrne"}],"year":"2000","references":["/references/2"]},{"style":0,"text":"Titov and Henderson, 2006","origin":{"pointer":"/sections/2/paragraphs/0","offset":248,"length":25},"authors":[{"last":"Titov"},{"last":"Henderson"}],"year":"2006","references":["/references/7"]},{"style":0,"text":"Pinchak and Lin (2006)","origin":{"pointer":"/sections/3/paragraphs/6","offset":90,"length":22},"authors":[{"last":"Pinchak"},{"last":"Lin"}],"year":"2006","references":["/references/6"]},{"style":0,"text":"Bickel and Doksum, 1977","origin":{"pointer":"/sections/4/paragraphs/0","offset":14,"length":23},"authors":[{"last":"Bickel"},{"last":"Doksum"}],"year":"1977","references":["/references/0"]},{"style":0,"text":"Verberne et al. (2009)","origin":{"pointer":"/sections/4/paragraphs/28","offset":183,"length":22},"authors":[{"last":"Verberne"},{"last":"al."}],"year":"2009","references":["/references/9"]},{"style":0,"text":"Yaman et al. (2009)","origin":{"pointer":"/sections/5/paragraphs/1","offset":0,"length":19},"authors":[{"last":"Yaman"},{"last":"al."}],"year":"2009","references":["/references/10"]},{"style":0,"text":"Tellez-Valero et al. (2008)","origin":{"pointer":"/sections/5/paragraphs/1","offset":306,"length":27},"authors":[{"last":"Tellez-Valero"},{"last":"al."}],"year":"2008","references":["/references/8"]},{"style":0,"text":"Chu-Carroll et al. (2003)","origin":{"pointer":"/sections/5/paragraphs/1","offset":491,"length":25},"authors":[{"last":"Chu-Carroll"},{"last":"al."}],"year":"2003","references":["/references/1"]},{"style":0,"text":"Joachims, 2006","origin":{"pointer":"/sections/5/paragraphs/2","offset":1,"length":14},"authors":[{"last":"Joachims"}],"year":"2006","references":["/references/4"]}]}
