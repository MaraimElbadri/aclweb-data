{"sections":[{"title":"","paragraphs":["Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 780–790, Sofia, Bulgaria, August 4-9 2013. c⃝2013 Association for Computational Linguistics"]},{"title":"An Infinite Hierarchical Bayesian Model of Phrasal Translation Trevor Cohn Department of Computer Science The University of Sheffield Sheffield, United Kingdom t.cohn@sheffield.ac.uk Gholamreza Haffari Faculty of Information Technology Monash University Clayton, Australia reza@monash.edu Abstract","paragraphs":["Modern phrase-based machine translation systems make extensive use of word-based translation models for inducing alignments from parallel corpora. This is problematic, as the systems are incapable of accurately modelling many translation phenomena that do not decompose into word-for-word translation. This paper presents a novel method for inducing phrase-based translation units directly from parallel data, which we frame as learning an inverse transduction grammar (ITG) using a recursive Bayesian prior. Overall this leads to a model which learns translations of entire sentences, while also learning their decomposition into smaller units (phrase-pairs) recursively, terminating at word translations. Our experiments on Arabic, Urdu and Farsi to English demonstrate improvements over competitive baseline systems."]},{"title":"1 Introduction","paragraphs":["The phrase-based approach (Koehn et al., 2003) to machine translation (MT) has transformed MT from a narrow research topic into a truly useful technology to end users. Leading translation systems (Chiang, 2007; Koehn et al., 2007; Marcu et al., 2006) all use some kind of multi-word translation unit, which allows translations to be produced from large canned units of text from the training corpus. Larger phrases allow for the lexical context to be considered in choosing the translation, and also limit the number of reordering decisions required to produce a full translation.","Word-based translation models (Brown et al., 1993) remain central to phrase-based model training, where they are used to infer word-level alignments from sentence aligned parallel data, from which phrasal translation units are extracted using a heuristic. Although this approach demonstrably works, it suffers from a number of shortcomings. Firstly, many phrase-based phenomena which do not decompose into word translations (e.g., idioms) will be missed, as the underlying word-based alignment model is unlikely to propose the correct alignments. Secondly, the relationship between different phrase-pairs is not considered, such as between single word translations and larger multi-word phrase-pairs or where one large phrase-pair subsumes another.","This paper develops a phrase-based translation model which aims to address the above shortcomings of the phrase-based translation pipeline. Specifically, we formulate translation using inverse transduction grammar (ITG), and seek to learn an ITG from parallel corpora. The novelty of our approach is that we develop a Bayesian prior over the grammar, such that a nonterminal becomes a ‘cache’ learning each production and its complete yield, which in turn is recursively composed of its child constituents. This is closely related to adaptor grammars (Johnson et al., 2007a), which also generate full tree rewrites in a monolingual setting. Our model learns translations of entire sentences while also learning their decomposition into smaller units (phrase-pairs) recursively, terminating at word translations. The model is richly parameterised, such that it can describe phrase-based phenomena while also explicitly modelling the relationships between phrase-pairs and their component expansions, thus ameliorating the disconnect between the treatment of words versus phrases in the current MT pipeline. We develop a Bayesian approach using a Pitman-Yor process prior, which is capable of modelling a diverse range of geometrically decaying distributions over infinite event spaces (here translation phrase-pairs), an approach shown to be state of the art for language modelling (Teh, 2006). 780","We are not the first to consider this idea; Neubig et al. (2011) developed a similar approach for learning an ITG using a form of Pitman-Yor adaptor grammar. However Neubig et al.’s work was flawed in a number of respects, most notably in terms of their heuristic beam sampling algorithm which does not meet either of the Markov Chain Monte Carlo criteria of ergodicity or detailed balance. Consequently their approach does not constitute a valid Bayesian model. In contrast, this paper provides a more rigorous and theoretically sound method. Moreover our approach results in consistent translation improvements across a number of translation tasks compared to Neubig et al.’s method, and a competitive phrase-based baseline."]},{"title":"2 Related Work","paragraphs":["Inversion transduction grammar (or ITG) (Wu, 1997) is a well studied synchronous grammar formalism. Terminal productions of the form X → e/f generate a word in two languages, and non-terminal productions allow phrasal movement in the translation process. Straight productions, denoted by their non-terminals inside square brackets [...], generate their symbols in the given order in both languages, while inverted productions, indicated by angled brackets ⟨...⟩, generate their symbols in the reverse order in the target language.","In the context of machine translation, ITG has been explored for statistical word alignment in both unsupervised (Zhang and Gildea, 2005; Cherry and Lin, 2007; Zhang et al., 2008; Pauls et al., 2010) and supervised (Haghighi et al., 2009; Cherry and Lin, 2006) settings, and for decoding (Petrov et al., 2008). Our paper fits into the recent line of work for jointly inducing the phrase table and word alignment (DeNero and Klein, 2010; Neubig et al., 2011). The work of DeNero and Klein (2010) presents a supervised approach to this problem, whereas our work is unsupervised hence more closely related to Neubig et al. (2011) which we describe in detail below.","A number of other approaches have been developed for learning phrase-based models from bilingual data, starting with Marcu and Wong (2002) who developed an extension to IBM model 1 to handle multi-word units. This pioneer-ing approach suffered from intractable inference and moreover, suffers from degenerate solutions (DeNero and Klein, 2010). Our approach is similar to these previous works, except that we impose additional constraints on how phrase-pairs can be tiled to produce a sentence pair, and moreover, we seek to model the embedding of phrase-pairs in one another, something not considered by this prior work. Another strand of related research is in estimating a broader class of synchronous grammars than ITGs, such as SCFGs (Blunsom et al., 2009b; Levenberg et al., 2012). Conceptually, our work could be readily adapted to general SCFGs using similar techniques.","This work was inspired by adaptor grammars (Johnson et al., 2007a), a monolingual grammar formalism whereby a non-terminal rewrites in a single step as a complete subtree. The model prior allows for trees to be generated as a mixture of a cache and a base adaptor grammar. In our case, we have generalised to a bilingual setting using an ITG. Additionally, we have extended the model to allow recursive nesting of adapted non-terminals, such that we end up with an infinitely recursive formulation where the top-level and base distributions are explicitly linked together.","As mentioned above, ours is not the first work attempting to generalise adaptor grammars for machine translation; (Neubig et al., 2011) also developed a similar approach based around ITG using a Pitman-Yor Process prior. Our approach improves upon theirs in terms of the model and inference, and critically, this is borne out in our experiments where we show uniform improvements in translation quality over a baseline system, as compared to their almost entirely negative results. We believe that their approach had a number of flaws: For inference they use a beam-search, which may speed up processing but means that they are no longer sampling from the true distribution, nor a distribution with the same support as the posterior. Moreover they include a Metropolis-Hastings correction step, which is required to correct the samples to account for repeated substructures which will be otherwise underrepresented. Consequently their approach does not constitute a Markov Chain Monte Carlo sampler, but rather a complex heuristic.","The other respect in which this work differs from Neubig et al. (2011) is in terms of model formulation. They develop an ITG which generates phrase-pairs as terminals, while we employ a more restrictive word-based model which forces the decomposition of every phrase-pair. This is an important restriction as it means that we jointly learn 781 a word and phrase based model, such that word based phenomena can affect the phrasal structures. Finally our approach models separately the three different types of ITG production (monotone, swap and lexical emission), allowing for a richer parameterisation which the model exploits by learning different hyper-parameter values."]},{"title":"3 Model","paragraphs":["The generative process of the model follows that of ITG with the following simple grammar X → [X X] | ⟨X X⟩ X → e/f | e/⊥ | ⊥/f , where [·] denotes monotone ordering and ⟨·⟩ denotes a swap in one language. The symbol ⊥ denotes the empty string. This corresponds to a simple generative story, with each stage being a non-terminal rewrite starting with X and terminating when there are no frontier non-terminals.","A popular variant is a phrasal ITG, where the leaves of the ITG tree are phrase-pairs and the training seeks to learn a segmentation of the source and target which yields good phrases. We would not expect this model to do very well as it cannot consider overlapping phrases, but instead is forced into selecting between many competing – and often equally viable – options. Our approach improves over the phrasal model by recursively generating complete phrases. This way we don’t insist on a single tiling of phrases for a sentence pair, but explicitly model the set of hierarchically nested phrases as defined by an ITG derivation. This approach is closer in spirit to the phrase-extraction heuristic, which defines a set of ‘atomic’ terminal phrase-pairs and then extracts every combination of these atomic phase-pairs which is contiguous in the source and target.1","The generative process is that we draw a complete ITG tree, t ∼ P2(·), as follows:","1. choose the rule type, r ∼ R, where r ∈","{mono, swap, emit}","2. for r = mono","(a) draw the complete subtree expansion,","t = X → [. . .] ∼ TM","3. for r = swap","(a) draw the complete subtree expansion,","t = X → ⟨. . .⟩ ∼ TS 1 Our technique considers the subset of phrase-pairs which","are consistent with the ITG tree.","4. for r = emit","(a) draw a pair of strings, (e, f ) ∼ E","(b) set t = X → e/f Note that we split the problem of drawing a tree into two steps: first choosing the top-level rule type and then drawing a rule of that type. This gives us greater control than simply drawing a tree of any type from one distribution, due to our parameterisation of the priors over the model parameters TM , TS and E.","To complete the generative story, we need to specify the prior distributions for TM , TS and E. First, we deal with the emission distribution, E which we drawn from a Dirichlet Process prior E ∼ DP(bE, P0). We restrict the emission rules to generate word pairs rather than phrase pairs.2","For the base distribution, P","0, we use a simple uniform distribution over word pairs, P0(e, f ) =    η2 1 VEVF e ̸= ⊥, f ̸= ⊥ η(1 − η) 1","VF e = ⊥, f ̸= ⊥ η(1 − η) 1","VE e ̸= ⊥, f = ⊥ , where the constant η denotes the binomial probability of a word being aligned.3","We use Pitman-Yor Process priors for the TM and TS parameters TM ∼ PYP(aM , bM , P1(·|r = mono)) TS ∼ PYP(aS, bS, P1(·|r = swap)) where P1(t1, t2|r) is a distribution over a pair of trees (the left and right children of a monotone or swap production). P1 is defined as follows:","1. choose the complete left subtree t1 ∼ P2,","2. choose the complete right subtree t2 ∼ P2,","3. set t = X → [t1 t2] or t = X → ⟨t1 t2⟩","depending on r This generative process is mutually recursive: P2 makes draws from P1 and P1 makes draws from P2. The recursion is terminated when the rule type r = emit is drawn.","Following standard practice in Bayesian models, we integrate out R, TM , TS and E. This means draws from P2 (or P1) are no longer iid: for any non-trivial tree, computing its probability under this model is complicated by the fact","2","Note that we could allow phrases here, but given the model can already reason over phrases by way of its hierarchical formulation, this is an unnecessary complication.","3","We also experimented with using word translation probabilities from IBM model 1, based on the prior used by Levenberg et al. (2012), however we found little empirical differ-ence compared with this simpler uniform model. 782 that the probability of its two subtrees are interdependent. This is best understood in terms of the Chinese Restaurant Franchise (CRF; Teh et al. (2006)), which describes the posterior distribution after integrating out the model parameters. In our case we can consider the process of drawing a tree from P2 as a customer entering a restaurant and choosing where to sit, from an infinite set of tables. The seating decision is based on the number of other customers at each table, such that popular tables are more likely to be joined than unpopular or empty ones. If the customer chooses an occupied table, the identity of the tree is then set to be the same as for the other customers also seated there. For empty tables the tree must be sampled from the base distribution P1. In the standard CRF analogy, this leads to another customer entering the restaurant one step up in the hierarchy, and this process can be chained many times. In our case, however, every new table leads to new customers reentering the original restaurant – these correspond to the left and right child trees of a monotone or swap rule. The recursion terminates when a table is shared, or a new table is labelled with a emit rule. 3.1 Inference The probability of a tree (i.e., a draw from P2) under the model is P2(t) = P (r)P2(t|r) (1) where r is the rule type, one of mono, swap or emit. The distribution over types, P (r), is defined as P (r) = n T,− r + bT 1","3 nT,−","+ bT where nT,−","are the counts over rules of types.4","The second component in (1), P2(t|r), is defined separately for each rule type. For r = mono or r = swap rules, it is defined as P2(t|r) = n− t,r − K−","t,rar n− r + br +","K− r ar + br n− r + br P1(t1, t2|r) , (2)","where n− t,r is the count for tree t in the other train-","ing sentences, K−","t,r is the table count for t and n−","r 4 The conditioning on event and table counts, n−",", K−","is","omitted for clarity. and K−","r are the total count of trees and tables, respectively. Finally, the probability for r = emit is given by P2(t|r = emit) =","n− t,E + bEP0(e, f ) n−","r + br , where t = X → e/f .","To complete the derivation we still need to define P1, which is formulated as P1(t1, t2) = P2(t1)P2(t2|t1) , where the conditioning of the second recursive call to P2 reflects that the counts n−","and K−","may be affected by the first draw from P2. Although these two draws are assumed iid in the prior, after marginalising out T they are no longer independent. For this reason, evaluating P2(t) is computationally expensive, requiring tracking of repeated substructures in descendent sub-trees of t, which may affect other descendants. This results in an asymptotic complexity exponential in the number of nodes in the tree. For this reason we consider trees annotated with binary values denoting their table assignment, namely whether they share a table or are seated alone. Given this, the calculation is greatly simplified, and has linear complexity.5","We construct an approximating ITG following the technique used for sampling trees from monolingual tree-substitution grammars (Cohn et al., 2010). To do so we encode the first term from (2) separately from the second term (correspond-ing to draws from P1). Summing together these two alternate paths – i.e., during inside inference – we recover P2 as shown in (2). The full grammar transform for inside inference is shown in Table 1.","The sampling algorithm closely follows the process for sampling derivations from Bayesian PCFGs (Johnson et al., 2007b). For each sentence-pair, we first decrement the counts associated with its current tree, and then sample a new derivation. This involves first constructing the inside lattice using the productions in Table 1, and then performing a top-down sampling pass. After sampling each derivation from the approximating grammar, we then convert this into its correspond-ing ITG tree, which we then score with the full model and accept or reject the sample using the","5","To support this computation, we track explicit table as-signments for every training tree and their component subtrees. We also sample trees labelled with seating indicator variables. 783 T ype X → M P (r = mono) X → S P (r = swap) X → E P (r = emit) Base M → [XX] K− MaM+bM n− M+bM S → ⟨XX⟩ K− S aS+bS n− S +bS Count For every tree, t, of type r = mono, with nt,M > 0: M → sig(t) n− t,M−K−","t,Mar n− M+bM","sig(t) → yield(t) 1 For every tree, t, of type r = swap, with nt,S > 0: S → sig(t) n− t,S−K−","t,SaS n− S +bS sig(t) → yield(t) 1 Emit For every word pair, e/f in sentence pair, where one of e, f can be ⊥:","E → e/f P2(t) Table 1: Grammar transformation rules for MAP inside inference. The function sig(t) returns a unique identifier for the complete tree t, and the function yield(t) returns the pair of terminal strings from the yield of t. Metropolis-Hastings algorithm.6","Accepted samples then replace the old tree (otherwise the old tree is retained) and the model counts are incremented. This process is then repeated for each sentence pair in the corpus in a random order."]},{"title":"4 Experiments Datasets","paragraphs":["We train our model across three language pairs: Urdu→English (UR-EN), Farsi→English (FA-EN), and Arabic→English (AR-EN). The corpora statistics of these translation tasks are summarised in Table 2. The UR-EN corpus comes from NIST 2009 translation evaluation.7","The AR-EN training data consists of the eTIRR corpus (LDC2004E72), the Arabic news corpus (LDC2004T17), the Ummah corpus (LDC2004T18), and the sentences with confidence c > 0.995 in the ISI automatically extracted web parallel corpus (LDC2006T02). For FA-EN, we use TEP8","Tehran English-Persian Parallel corpus (Pilevar and Faili, 2011), which consists of conversational/informal text extracted 6 The full model differs from the approximating grammar","in that it accounts for inter-dependencies between subtrees","by recursively tracking the changes in the customer and table","counts while scoring the tree. Around 98% of samples were","accepted in our experiments. 7 http://www.itl.nist.gov/iad/mig/tests/mt/2009 8 http://ece.ut.ac.ir/NLP/resources.htm source target sentences UR-EN 745K 575K 148K FA-EN 4.7M 4.4M 498K AR-EN 1.94M 2.08M 113K Table 2: Corpora statistics showing numbers of parallel sentences and source and target words for the training sets. from 1600 movie subtitles. We tokenized this corpus, removed noisy single-word sentences, randomly selected the development and test sets, and used the rest of the corpus as the training set. We discard sentences with length above 30 from the datasets for all experiments.9 Sampler configuration Samplers are initialised with trees created from GIZA++ alignments constructed using a SCFG factorisation method (Blunsom et al., 2009a). This algorithm represents the translation of a sentence as a large SCFG rule, which it then factorises into lower rank SCFG rules, a process akin to rule binarisation commonly used in SCFG decoding. Rules that cannot be reduced to a rank-2 SCFG are simplified by dropping alignment edges until they can be factorised, the net result being an ITG derivation largely respecting the alignments.10","The blocked sampler was run 1000 iterations for UR-EN, 100 iterations for FA-EN and AR-EN. After each full sampling iteration, we resam-ple all the hyper-parameters using slice-sampling, with the following priors: a ∼ Beta(1, 1), b ∼ Gamma(10, 0.1). Figure 1 shows the posterior probability improves with each full sampling iterations. The alignment probability was set to η = 0.99. The sampling was repeated for 5 independent runs, and we present results where we combine the outputs of these runs. This is a form of Monte Carlo integration which allows us to represent the uncertainty in the posterior, while also representing multiple modes, if present.","The time complexity of our inference algorithm is O(n6","), which can be prohibitive for large scale machine translation tasks. We reduce the complexity by constraining the inside inference to consider only derivations which are compatible 9 Hence the BLEU scores we get for the baselines may","appear lower than what reported in the literature. 10 Using the factorised alignments directly in a translation","system resulted in a slight loss in BLEU versus using the un-","factorised alignments. Our baseline system uses the latter. 784 0 100 200 300 400 500−9100000 −9050000 −9000000 −8950000 iteration log poster ior Figure 1: Training progress on the UR-EN corpus, showing the posterior probability improving with each full sampling iteration. Different colours denote independent sampling runs. lllll l llllll l l lllll","l lllllll lllllll lllllllll","lllllll lll llllllll lllllllll","ll l llllllllll l","llllllllll","lllllllllllll","llllllllllllll","lllllllllllllll","lllllllllllllll l","lllllllllllllllll","llllllllllllllllll","lllllllllllllllllll l lllllllllllllllllll l lllllllllllllllllllll lllllllllllllllllllll llllllllllllllllllll","l l lllllllllllllllllll","llllllllllllllllllll","l","l llllllllllllllllllll","lllllllllllllllllll","lllllllllllllllllll","lllllllllllllllll","l","l lllllllllllllllllll 0 5 10 15 20 25 30 1e−05 1e−03 1e−01 average sentence length time (s) Figure 2: The runtime cost of bottom-up inside inference and top-down sampling as a function of sentence length (UR-EN), with time shown on a logarithmic scale. Full ITG inference is shown with red circles, and restricted inference using the intersection constraints with blue triangles. The average time complexity for the latter is roughly O(l4","), as plotted in green t = 2 × 10−7","l4",". with high confidence alignments from GIZA++.11 Figure 2 shows the sampling time with respect to the average sentence length, showing that our alignment-constrained sampling algorithm is better than the unconstrained algorithm with empirical complexity of n4",". However, the time complexity is still high, so we set the maximum sentence length to 30 to keep our experiments practicable. Presumably other means of inference may be more efficient, such as Gibbs sampling (Levenberg et al., 2012) or auxiliary variable sampling (Blunsom and Cohn, 2010); we leave these extensions to future work. Baselines. Following (Levenberg et al., 2012; Neubig et al., 2011), we evaluate our model by using its output word alignments to construct a phrase table. As a baseline, we train a phrase-based model using the moses toolkit12","based on the word alignments obtained using GIZA++ in both directions and symmetrized using the growdiag-final-and heuristic13","(Koehn et al., 2003). This alignment is used as input to the rule factorisation algorithm, producing the ITG trees with which we initialise our sampler. To put our results in the context of the previous work, we also compare against pialign (Neubig et al., 2011), an ITG algorithm using a Pitman-Yor process prior, as described in Section 2.14","In the end-to-end MT pipeline we use a standard set of features: relative-frequency and lexical translation model probabilities in both directions; distance-based distortion model; language model and word count. We set the distortion limit to 6 and max-phrase-length to 7 in all experiments. We train 3-gram language models using modified Kneser-Ney smoothing. For AR-EN experiments the language model is trained on English data as (Blunsom et al., 2009a), and for FA-EN and UR-EN the English data are the target sides of the bilingual training data. We use minimum error rate training (Och, 2003) with nbest list size 100 to optimize the feature weights for maximum development BLEU. 11 These are taken from the final model 4 word alignments,","using the intersection of the source-target and target-source","models. These alignments are very high precision (but have","low recall), and therefore are unlikely to harm the model. 12 http://www.statmt.org/moses 13 We use the default parameter settings in both moses and","GIZA++. 14 http://www.phontron.com/pialign 785","Baselines This paper GIZA++ pialign individual combination UR-EN 16.95 15.65 16.68 ± .12 16.97 FA-EN 20.69 21.41 21.36 ± .17 21.50 AR-EN MT03 44.05 43.30 44.8 ± .28 45.10 MT04 38.15 37.78 38.4 ± .08 38.4 MT05 42.81 42.18 43.13 ± .23 43.45 MT08 32.43 33.00 32.7 ± .15 32.80 Table 3: The BLEU scores for the translation tasks of three language pairs. The individual column show the average and 95% confidence intervals for 5 independent runs, whereas the combination column show the results for combining the phrase tables of all these runs. The baselines are GIZA++ alignments and those generated by the pialign (Neubig et al., 2011) bold: the best result. 1 2 5 10 20 50 100 1e−05 1e−03 1e−01 rule frequency fr action of gr ammar monotone swap emit Figure 3: Fraction of rules with a given frequency, using a single sample grammar (UR-EN). 4.1 Results Table 3 shows the BLEU scores for the three translation tasks UR/AR/FA→EN based on our method against the baselines. For our models, we report the average BLEU score of the 5 independent runs as well as that of the aggregate phrase table generated by these 5 independent runs. There are a number of interesting observations in Table 3. Firstly, combining the phrase tables from independent runs results in increased BLEU scores, possibly due to the representation of uncertainty in the outputs, and the representation of different modes captured by the individual models. We believe this type of Monte Carlo model averaging should be considered in general when sampling techniques are employed for grammatical inference, e.g. in parsing and translation. Secondly, our approach consistently improves over the Giza++ baseline often by a large margin, whereas pialign underperforms the GIZA++ baseline in many cases. Thirdly, our model consistently outperforms pialign (except in AR-EN MT08 which is very close). This highlights the modeling and inference differences between our method and the pialign."]},{"title":"5 Analysis","paragraphs":["In this section, we present some insights about the learned grammar and the model hyper-parameters. Firstly, we start by presenting various statistics about different learned grammars. Figure 3 shows the fraction of rules with a given frequency for each of the three rule types. The three types of rule exhibit differing amounts of high versus low frequency rules, and all roughly follow power laws. As expected, there is a higher tendency to reuse high-frequency emissions (or single-word translation) compared to other rule types, which are the basic building blocks to compose larger rules (or phrases). Table 4 lists the high frequency monotone and swap rules in the learned grammar. We observe the high frequency swap rules capture reordering in verb clusters, preposition-noun inversions and adjective-noun reordering. Similar patterns are seen in the monotone rules, along with some common canned phrases. Note that “in Iraq” appears twice, once as an inversion in UR-EN and another time in monotone order for AR-EN.","Secondly, we analyse the values learned for the model hyper-parameters; Figure 4.(a) shows the posterior distribution over the hyper-parameter values. There is very little spread in the inferred values, suggesting the sampling chains may have converged. Furthermore, there is a large differ-ence between the learned hyper-parameters for the monotone rules versus the swap rules. For the Pitman-Yor Process prior, the values of the hyper-786","6‘bB@1M;HBb?","TBHB;M bm‘2fKhK M-+?B27f‘ vb- vQm bm‘2fKhK Mv- BK bm‘2fKhK MK- #Qbbf‘ vb- KF2 bm‘2fKhK M- ‘2 vQm bm‘2fKhK Mv- Mvrvf?‘ >H- T‘2bB/2Mif‘ vb DK?r‘- MQi bm‘2fKhK M MvbiK- BK bm‘2fKhK MK F?- BK bm‘2fKhK MK- bm‘2fKhK M","Qm‘ K2i?Q/ bm‘2fKhK }- ?p2f/ $ i?- #2f# $- ?p2f/ $ i? # $- H2i K2f ‘- #2+mb2 Q7fth‘- bm‘2fKhK } M- /QfF‘ ‘- +QK2 QMfxr/ #- 2t+mb2 K2f##t- FBHHf‘ #F- +QK2 QMfxr/#- KQ‘2 i?Mf$ i‘- #2?BM/fT $- r?i /QfKMwr‘i- r?i /Q vQmfKMwr‘i - FBHHfF $- /QMi rQ‘‘vfM;‘M M#- Bb Bif$ /?- r2H+QK2ftr $ %- +?B27f‘ } vb- KF2 bm‘2fKhK- BbfKv $- KF2 bm‘2fKhK }- KF2 bm‘2fKhK} M- BK bQ‘‘vf##t- H27if; - B7f;‘ $","‘#B+@1M;HBb?","TBHB;M bB/ Xf - bii2bf- mMBi2/f- H@r7/f - 2zQ‘ibf- Q7 Kbb /2bi‘m+iBQMf - vQmKf- DBMiQf - HK H vQmKf - H@BiiB?/ -f- i?2 }2H/ Q7f- BbHK#/f- b+?2/mH2/f - H@HK H@vQmKf - f H@?vif - T2MBMbmHf - K2Mr?BH2f - T‘BK2f- i?2 BMpBiiBQMf - f H@?vi -f - FQ‘2M T2MBMbmHf - H@M?‘ f DD- /2T‘iK2Mif - +Qi2f- b TQbbB#H2f - H HK H vQmKf -- H@HK H@vQmK -f - i i?2 BMpBiiBQMf - D+[m2bf - r2HH bf-TQBMibf- pH/BKB‘ TmiBMf - ;2Q‘;2 rX #mb?f","Qm‘ K2i?Q/ i?2 mMBi2/f- mb /QHH‘bf- T‘BK2f - +?BM f- bTQF2bKMf - KMvf- Bb 2tT2+i2/f- Bb 2tT2+i2/ iQf- i H2bif- QM im2b/vf- 2;vTi f- i?m‘b/vf- i?2 mMf- QM i?m‘b/vf- 7‘B/vf- QM 7‘B/vf- iQf-H@r7/ -f - i?2 mbf- 7Q‘f - }‘bi iBK2f - 7m‘i?2‘f- B‘[ f- Bb‘2HB T‘BK2f - i?2 irQf- QM bim‘/vf- QM bmM/vfmXbXf- pB2rbf- b?‘QM f- +QmMi‘v f- ?2 bB/f- Bb‘2H f- T2QTH2 f- ?2‘2f - +?BM f- - ?2 bB/f - 2‘HB2‘f - +?BM f-i H2bif - i?2 mXbXf- i?2 ;xf- i?2 ;x bi‘BTf- ?2 //2/f- ‘2 2tT2+i2/f- ‘2 2tT2+i2/ iQf- ‘2 2tT2+i2/ iQf - KBHHBQM mXbXf- ++Q‘/BM;f- iQf- Q‘/2‘f- BM Q‘/2‘f- ?2 TQBMi2/f- K7 - b?‘[ Hf - K7 - b?‘[ H rbif - ‘7i f k Table 5: Good phrase pairs in the top-100 high frequency phrase pairs specific to the phrase tables coming from our method vs that of pialign for FA-EN and AR-EN translation tasks. parameters affects the rate at which the number of types grows compared to the number of tokens. Specifically, as the discount a or the concentra-tion b parameters increases we expect for a rela-tive increase in the number of types. If the number of observed monotone and swap rules were equal, then there would be a higher chance in reusing the monotone rules. However, the number of observed monotone and swap rules are not equal, as plotted in Figure 4.(b). Similar results were observed for the other language pairs (figures omitted for space reasons).","Thirdly, we performed a manual evaluation for the quality of the phrase-pairs learned exclusively by our method vs pialign. For each method, we considered the top-100 high frequency phrase-pairs which are specific to that method. Then we asked a bilingual human expert to identify reasonably well phrase-pairs among these top-100 phrase-pairs. The results are summarized in Table 5, and show that we learn roughly twice as many reasonably good phrase-pairs for AR-EN and FA-EN compared to pialign."]},{"title":"Conclusions","paragraphs":["We have presented a novel method for learning a phrase-based model of translation directly from parallel data which we have framed as learning an inverse transduction grammar (ITG) using a recursive Bayesian prior. This has led to a model which learns translations of entire sentences, while also learning their decomposition into smaller units (phrase-pairs) recursively, terminating at word translations. We have presented a Metropolis-Hastings sampling algorithm for blocked inference in our non-parametric ITG. Our experiments on Urdu-English, Arabic-English, and Farsi-English translation tasks all demonstrate improvements over competitive baseline systems."]},{"title":"Acknowledgements","paragraphs":["The first author was supported by the EPSRC (grant EP/I034750/1) and an Erasmus-Mundus scholarship funding a research visit to Melbourne. The second author was supported by an early career research award from Monash University. 787 0.905 0.910 0.915 0.920 0.925 0 200 400 600 800 1000 a m and a","s Density 1000 2000 3000 4000 0.0000 0.0010 0.0020 b m and b","s Density 0 5 10 15 20 25 30 0.00 0.01 0.02 0.03 0.04 0.05 0.06 b e Density 65000 65500 66000 0.0000 0.0005 0.0010 0.0015 b t Density (a) 291000 292000 293000 0.0000 0.0004 0.0008 0.0012 monotone 176000 177000 0.0000 0.0004 0.0008 0.0012 swap Density (b) Figure 4: (a) Posterior over the hyper-parameters, aM , aS, bM , bS, bE, bT , measured for UR-EN using samples 400–500 for 3 independent sampling chains, and the intersection constraints. (b) Posterior over the number of monotone and swap rules in the resultant grammars. The distribution for emission rules was also peaked about 147k rules. key ( ( ( ?2fM?rL ) ( ⊥fMu ) ) ( bB/fF? ) ) kkk ( ⟨ ( bB/fF? ) ( ⊥fMu ) ⟩ ( i?ifF? ) ) kRN ( ( ( ( ?2fM?rL ) ( ⊥fMu ) )","( bB/fF? ) ) ( i?ifF? ) ) R93 ( ( ( B7f;‘ ) ( ⊥f% ) ) ( vQmfT ) ) Ry3 ( ( ?2fM?rL ) ⟨ ( bB/fF? ) ( ⊥fMu ) ⟩ ) R3k ⟨ ( rBHHf; ) ( #2f?r ) ⟩ RkN ⟨ ( Bbf?u ) ( MQifM?vL ) ⟩ Rkj ⟨ ( ?bf?u ) ( #22Mf;v ) ⟩ Ry9 ⟨ ( rBHHf; ) ( #2fDvu ) ⟩ Ryj ⟨ ( BMfKvL ) ( B‘[f1‘[ ) ⟩","l‘/m@1M;HBb? 3Ny ( ( QM2fvFv ) ( Q7fx ) ) 39j ( ⟨ ( v2?f‘? ) ( ⊥f% ) ⟩ ( XfX ) ) dj3 ( ( rBi?f# ) ( K2fKM ) ) e99 ( ( ( ( QFvf# ) ( ⊥f$ ))(⊥f? ) ) ( XfX ) ) ey3 ( ( iQf#? ) ( K2fKM ) ) k8R ⟨ ( Bbf/? ) ( Bif$ ) ⟩ kky ⟨ ( i2HHf#;r ) ( K2fKM ) ⟩ RNN ⟨ I ( Bf⊥ ) ( +MfirMK ) ⟩ ( ifMKv ) = RNy ⟨ ( ( r?QfFv ) ( ‘2f?biv ) ) ( vQmfir ) ⟩ R3d ⟨ ( iQH/f;7i ) ( K2fKM ) ⟩","6‘bB@1M;HBb? 8ee ( ( BMf ) ( B‘[f )) 9R9 ( ( BMf ) ( 2;vTif )) jNR ( ( i?Bbf ) ( v2‘f )) j8e ( ( b?‘[f ) ( H@rbif )) jyy ( ( BMf ) ( B‘[f )) 9yk9 ⟨ ( Xf ) ( f ) ⟩ RjRk ⟨ I ( i?2f ) ( mMBi2/f ) ⟩","( bii2bf )= ee8 ⟨ ( mMBi2/f ) ( bii2bf ) ⟩ e8y ⟨ ( Hbif ) ( v2‘f ) ⟩ 9ed ⟨ I ( i?2f ) ( mMBi2/f ) ⟩","( MiBQMbf )=","‘#B+@1M;HBb?","aQK2 HiBM i2ti M/ BMHBM2 ‘#B+,","l‘/m- 6‘bB- ‘#B+Table 4: Top 5 monotone and swap productions and their counts. Rules with mostly punctuation or encoding 1:many or many:1 alignments were omitted. 788"]},{"title":"References","paragraphs":["Phil Blunsom and Trevor Cohn. 2010. Inducing synchronous grammars with slice sampling. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 238–241, Los Angeles, California, June. Association for Computational Linguistics.","Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Osborne. 2009a. A Gibbs sampler for phrasal synchronous grammar induction. In ACL2009, Singapore, August.","Phil Blunsom, Trevor Cohn, and Miles Osborne. 2009b. Bayesian synchronous grammar induction. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors, Advances in Neural Information Processing Systems 21, pages 161–168. MIT Press.","P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and R. L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311.","Colin Cherry and Dekang Lin. 2006. Soft syntactic constraints for word alignment through discrimina-tive training. In Proceedings of COLING/ACL. Association for Computational Linguistics.","Colin Cherry and Dekany Lin. 2007. Inversion transduction grammar for joint phrasal translation modeling. In Proc. of the HLT-NAACL Workshop on Syntax and Structure in Statistical Translation (SSST 2007), Rochester, USA.","David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.","Trevor Cohn, Phil Blunsom, and Sharon Goldwater. 2010. Inducing tree-substitution grammars. Journal of Machine Learning Research, pages 3053–3096.","John DeNero and Dan Klein. 2010. Discriminative modeling of extraction sets for machine translation. In The 48th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL).","Aria Haghighi, John Blitzer, and Dan Klein. 2009. Better word alignments with supervised itg models. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, Suntec, Singapore. Association for Computational Linguistics.","Mark Johnson, Thomas L. Griffiths, and Sharon Goldwater. 2007a. Adaptor grammars: A framework for specifying compositional nonparametric bayesian models. In B. Schölkopf, J. Platt, and T. Hoffman, editors, Advances in Neural Information Processing Systems 19, pages 641–648. MIT Press, Cambridge, MA.","Mark Johnson, Thomas L Griffiths, and Sharon Goldwater. 2007b. Bayesian inference for PCFGs via Markov chain Monte Carlo. In Proc. of the 7th International Conference on Human Language Technology Research and 8th Annual Meeting of the NAACL (HLT-NAACL 2007), pages 139–146.","Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proc. of the 3rd International Conference on Human Language Technology Research and 4th Annual Meeting of the NAACL (HLT-NAACL 2003), pages 81–88, Edmonton, Canada, May.","Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proc. of the 45th Annual Meeting of the ACL (ACL-2007), Prague.","Abby Levenberg, Chris Dyer, and Phil Blunsom. 2012. A Bayesian model for learning SCFGs with discontiguous rules. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 223–232, Jeju Island, Korea, July. Association for Computational Linguistics.","Daniel Marcu and William Wong. 2002. A phrase-based, joint probability model for statistical machine translation. In Proc. of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP-2002), pages 133–139, Philadelphia, July. Association for Computational Linguistics.","Daniel Marcu, Wei Wang, Abdessamad Echihabi, and Kevin Knight. 2006. SPMT: Statistical machine translation with syntactified target language phrases. In Proc. of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP-2006), pages 44–52, Sydney, Australia, July.","Graham Neubig, Taro Watanabe, Eiichiro Sumita, Shinsuke Mori, and Tatsuya Kawahara. 2011. An unsupervised model for joint phrase alignment and extraction. In The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT), pages 632–641, Portland, Oregon, USA, 6.","Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proc. of the 41st Annual Meeting of the ACL (ACL-2003), pages 160– 167, Sapporo, Japan.","Adam Pauls, Dan Klein, David Chiang, and Kevin Knight. 2010. Unsupervised syntactic alignment with inversion transduction grammars. In Proceedings of the North American Conference of the Association for Computational Linguistics (NAACL). Association for Computational Linguistics. 789","Slav Petrov, Aria Haghighi, and Dan Klein. 2008. Coarse-to-fine syntactic machine translation using language projections. In Proceedings of EMNLP. Association for Computational Linguistics.","M. T. Pilevar and H. Faili. 2011. Tep: Tehran englishpersian parallel corpus. In Proc. International Conference on Intelligent Text Processing and Computational Linguistics (CICLing).","Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei. 2006. Hierarchical Dirichlet processes. Journal of the American Statistical Association, 101(476):1566–1581.","Y. W. Teh. 2006. A hierarchical Bayesian language model based on Pitman-Yor processes. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 985–992.","Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377–403.","Hao Zhang and Daniel Gildea. 2005. Stochastic lexicalized inversion transduction grammar for alignment. In Proceedings of the 43rd Annual Conference of the Association for Computational Linguistics (ACL). Association for Computational Linguistics.","Hao Zhang, Chris Quirk, Robert C. Moore, and Daniel Gildea. 2008. Bayesian learning of noncompositional phrases with synchronous parsing. In Proc. of the 46th Annual Conference of the Association for Computational Linguistics: Human Language Technologies (ACL-08:HLT), pages 97–105, Columbus, Ohio, June. 790"]}],"references":[{"authors":[{"first":"Phil","last":"Blunsom"},{"first":"Trevor","last":"Cohn"}],"year":"2010","title":"Inducing synchronous grammars with slice sampling","source":"Phil Blunsom and Trevor Cohn. 2010. Inducing synchronous grammars with slice sampling. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 238–241, Los Angeles, California, June. Association for Computational Linguistics."},{"authors":[{"first":"Phil","last":"Blunsom"},{"first":"Trevor","last":"Cohn"},{"first":"Chris","last":"Dyer"},{"first":"Miles","last":"Osborne"}],"year":"2009a","title":"A Gibbs sampler for phrasal synchronous grammar induction","source":"Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Osborne. 2009a. A Gibbs sampler for phrasal synchronous grammar induction. In ACL2009, Singapore, August."},{"authors":[{"first":"Phil","last":"Blunsom"},{"first":"Trevor","last":"Cohn"},{"first":"Miles","last":"Osborne"}],"year":"2009b","title":"Bayesian synchronous grammar induction","source":"Phil Blunsom, Trevor Cohn, and Miles Osborne. 2009b. Bayesian synchronous grammar induction. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors, Advances in Neural Information Processing Systems 21, pages 161–168. MIT Press."},{"authors":[{"first":"P.","middle":"F.","last":"Brown"},{"first":"S.","middle":"A. Della","last":"Pietra"},{"first":"V.","middle":"J. Della","last":"Pietra"},{"first":"R.","middle":"L.","last":"Mercer"}],"year":"1993","title":"The mathematics of statistical machine translation: Parameter estimation","source":"P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and R. L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311."},{"authors":[{"first":"Colin","last":"Cherry"},{"first":"Dekang","last":"Lin"}],"year":"2006","title":"Soft syntactic constraints for word alignment through discrimina-tive training","source":"Colin Cherry and Dekang Lin. 2006. Soft syntactic constraints for word alignment through discrimina-tive training. In Proceedings of COLING/ACL. Association for Computational Linguistics."},{"authors":[{"first":"Colin","last":"Cherry"},{"first":"Dekany","last":"Lin"}],"year":"2007","title":"Inversion transduction grammar for joint phrasal translation modeling","source":"Colin Cherry and Dekany Lin. 2007. Inversion transduction grammar for joint phrasal translation modeling. In Proc. of the HLT-NAACL Workshop on Syntax and Structure in Statistical Translation (SSST 2007), Rochester, USA."},{"authors":[{"first":"David","last":"Chiang"}],"year":"2007","title":"Hierarchical phrase-based translation","source":"David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228."},{"authors":[{"first":"Trevor","last":"Cohn"},{"first":"Phil","last":"Blunsom"},{"first":"Sharon","last":"Goldwater"}],"year":"2010","title":"Inducing tree-substitution grammars","source":"Trevor Cohn, Phil Blunsom, and Sharon Goldwater. 2010. Inducing tree-substitution grammars. Journal of Machine Learning Research, pages 3053–3096."},{"authors":[{"first":"John","last":"DeNero"},{"first":"Dan","last":"Klein"}],"year":"2010","title":"Discriminative modeling of extraction sets for machine translation","source":"John DeNero and Dan Klein. 2010. Discriminative modeling of extraction sets for machine translation. In The 48th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL)."},{"authors":[{"first":"Aria","last":"Haghighi"},{"first":"John","last":"Blitzer"},{"first":"Dan","last":"Klein"}],"year":"2009","title":"Better word alignments with supervised itg models","source":"Aria Haghighi, John Blitzer, and Dan Klein. 2009. Better word alignments with supervised itg models. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, Suntec, Singapore. Association for Computational Linguistics."},{"authors":[{"first":"Mark","last":"Johnson"},{"first":"Thomas","middle":"L.","last":"Griffiths"},{"first":"Sharon","last":"Goldwater"}],"year":"2007a","title":"Adaptor grammars: A framework for specifying compositional nonparametric bayesian models","source":"Mark Johnson, Thomas L. Griffiths, and Sharon Goldwater. 2007a. Adaptor grammars: A framework for specifying compositional nonparametric bayesian models. In B. Schölkopf, J. Platt, and T. Hoffman, editors, Advances in Neural Information Processing Systems 19, pages 641–648. MIT Press, Cambridge, MA."},{"authors":[{"first":"Mark","last":"Johnson"},{"first":"Thomas","middle":"L","last":"Griffiths"},{"first":"Sharon","last":"Goldwater"}],"year":"2007b","title":"Bayesian inference for PCFGs via Markov chain Monte Carlo","source":"Mark Johnson, Thomas L Griffiths, and Sharon Goldwater. 2007b. Bayesian inference for PCFGs via Markov chain Monte Carlo. In Proc. of the 7th International Conference on Human Language Technology Research and 8th Annual Meeting of the NAACL (HLT-NAACL 2007), pages 139–146."},{"authors":[{"first":"Philipp","last":"Koehn"},{"first":"Franz","middle":"Josef","last":"Och"},{"first":"Daniel","last":"Marcu"}],"year":"2003","title":"Statistical phrase-based translation","source":"Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proc. of the 3rd International Conference on Human Language Technology Research and 4th Annual Meeting of the NAACL (HLT-NAACL 2003), pages 81–88, Edmonton, Canada, May."},{"authors":[{"first":"Philipp","last":"Koehn"},{"first":"Hieu","last":"Hoang"},{"first":"Alexandra","last":"Birch"},{"first":"Chris","last":"Callison-Burch"},{"first":"Marcello","last":"Federico"},{"first":"Nicola","last":"Bertoldi"},{"first":"Brooke","last":"Cowan"},{"first":"Wade","last":"Shen"},{"first":"Christine","last":"Moran"},{"first":"Richard","last":"Zens"},{"first":"Chris","last":"Dyer"},{"first":"Ondrej","last":"Bojar"},{"first":"Alexandra","last":"Constantin"},{"first":"Evan","last":"Herbst"}],"year":"2007","title":"Moses: Open source toolkit for statistical machine translation","source":"Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proc. of the 45th Annual Meeting of the ACL (ACL-2007), Prague."},{"authors":[{"first":"Abby","last":"Levenberg"},{"first":"Chris","last":"Dyer"},{"first":"Phil","last":"Blunsom"}],"year":"2012","title":"A Bayesian model for learning SCFGs with discontiguous rules","source":"Abby Levenberg, Chris Dyer, and Phil Blunsom. 2012. A Bayesian model for learning SCFGs with discontiguous rules. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 223–232, Jeju Island, Korea, July. Association for Computational Linguistics."},{"authors":[{"first":"Daniel","last":"Marcu"},{"first":"William","last":"Wong"}],"year":"2002","title":"A phrase-based, joint probability model for statistical machine translation","source":"Daniel Marcu and William Wong. 2002. A phrase-based, joint probability model for statistical machine translation. In Proc. of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP-2002), pages 133–139, Philadelphia, July. Association for Computational Linguistics."},{"authors":[{"first":"Daniel","last":"Marcu"},{"first":"Wei","last":"Wang"},{"first":"Abdessamad","last":"Echihabi"},{"first":"Kevin","last":"Knight"}],"year":"2006","title":"SPMT: Statistical machine translation with syntactified target language phrases","source":"Daniel Marcu, Wei Wang, Abdessamad Echihabi, and Kevin Knight. 2006. SPMT: Statistical machine translation with syntactified target language phrases. In Proc. of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP-2006), pages 44–52, Sydney, Australia, July."},{"authors":[{"first":"Graham","last":"Neubig"},{"first":"Taro","last":"Watanabe"},{"first":"Eiichiro","last":"Sumita"},{"first":"Shinsuke","last":"Mori"},{"first":"Tatsuya","last":"Kawahara"}],"year":"2011","title":"An unsupervised model for joint phrase alignment and extraction","source":"Graham Neubig, Taro Watanabe, Eiichiro Sumita, Shinsuke Mori, and Tatsuya Kawahara. 2011. An unsupervised model for joint phrase alignment and extraction. In The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT), pages 632–641, Portland, Oregon, USA, 6."},{"authors":[{"first":"Franz","middle":"Josef","last":"Och"}],"year":"2003","title":"Minimum error rate training in statistical machine translation","source":"Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proc. of the 41st Annual Meeting of the ACL (ACL-2003), pages 160– 167, Sapporo, Japan."},{"authors":[{"first":"Adam","last":"Pauls"},{"first":"Dan","last":"Klein"},{"first":"David","last":"Chiang"},{"first":"Kevin","last":"Knight"}],"year":"2010","title":"Unsupervised syntactic alignment with inversion transduction grammars","source":"Adam Pauls, Dan Klein, David Chiang, and Kevin Knight. 2010. Unsupervised syntactic alignment with inversion transduction grammars. In Proceedings of the North American Conference of the Association for Computational Linguistics (NAACL). Association for Computational Linguistics. 789"},{"authors":[{"first":"Slav","last":"Petrov"},{"first":"Aria","last":"Haghighi"},{"first":"Dan","last":"Klein"}],"year":"2008","title":"Coarse-to-fine syntactic machine translation using language projections","source":"Slav Petrov, Aria Haghighi, and Dan Klein. 2008. Coarse-to-fine syntactic machine translation using language projections. In Proceedings of EMNLP. Association for Computational Linguistics."},{"authors":[{"first":"M.","middle":"T.","last":"Pilevar"},{"first":"H.","last":"Faili"}],"year":"2011","title":"Tep: Tehran englishpersian parallel corpus","source":"M. T. Pilevar and H. Faili. 2011. Tep: Tehran englishpersian parallel corpus. In Proc. International Conference on Intelligent Text Processing and Computational Linguistics (CICLing)."},{"authors":[{"first":"Y.","middle":"W.","last":"Teh"},{"first":"M.","middle":"I.","last":"Jordan"},{"first":"M.","middle":"J.","last":"Beal"},{"first":"D.","middle":"M.","last":"Blei"}],"year":"2006","title":"Hierarchical Dirichlet processes","source":"Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei. 2006. Hierarchical Dirichlet processes. Journal of the American Statistical Association, 101(476):1566–1581."},{"authors":[{"first":"Y.","middle":"W.","last":"Teh"}],"year":"2006","title":"A hierarchical Bayesian language model based on Pitman-Yor processes","source":"Y. W. Teh. 2006. A hierarchical Bayesian language model based on Pitman-Yor processes. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 985–992."},{"authors":[{"first":"Dekai","last":"Wu"}],"year":"1997","title":"Stochastic inversion transduction grammars and bilingual parsing of parallel corpora","source":"Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377–403."},{"authors":[{"first":"Hao","last":"Zhang"},{"first":"Daniel","last":"Gildea"}],"year":"2005","title":"Stochastic lexicalized inversion transduction grammar for alignment","source":"Hao Zhang and Daniel Gildea. 2005. Stochastic lexicalized inversion transduction grammar for alignment. In Proceedings of the 43rd Annual Conference of the Association for Computational Linguistics (ACL). Association for Computational Linguistics."},{"authors":[{"first":"Hao","last":"Zhang"},{"first":"Chris","last":"Quirk"},{"first":"Robert","middle":"C.","last":"Moore"},{"first":"Daniel","last":"Gildea"}],"year":"2008","title":"Bayesian learning of noncompositional phrases with synchronous parsing","source":"Hao Zhang, Chris Quirk, Robert C. Moore, and Daniel Gildea. 2008. Bayesian learning of noncompositional phrases with synchronous parsing. In Proc. of the 46th Annual Conference of the Association for Computational Linguistics: Human Language Technologies (ACL-08:HLT), pages 97–105, Columbus, Ohio, June. 790"}],"cites":[{"style":0,"text":"Koehn et al., 2003","origin":{"pointer":"/sections/2/paragraphs/0","offset":27,"length":18},"authors":[{"last":"Koehn"},{"last":"al."}],"year":"2003","references":["/references/12"]},{"style":0,"text":"Chiang, 2007","origin":{"pointer":"/sections/2/paragraphs/0","offset":197,"length":12},"authors":[{"last":"Chiang"}],"year":"2007","references":["/references/6"]},{"style":0,"text":"Koehn et al., 2007","origin":{"pointer":"/sections/2/paragraphs/0","offset":211,"length":18},"authors":[{"last":"Koehn"},{"last":"al."}],"year":"2007","references":["/references/13"]},{"style":0,"text":"Marcu et al., 2006","origin":{"pointer":"/sections/2/paragraphs/0","offset":231,"length":18},"authors":[{"last":"Marcu"},{"last":"al."}],"year":"2006","references":["/references/16"]},{"style":0,"text":"Brown et al., 1993","origin":{"pointer":"/sections/2/paragraphs/1","offset":31,"length":18},"authors":[{"last":"Brown"},{"last":"al."}],"year":"1993","references":["/references/3"]},{"style":0,"text":"Johnson et al., 2007a","origin":{"pointer":"/sections/2/paragraphs/2","offset":552,"length":21},"authors":[{"last":"Johnson"},{"last":"al."}],"year":"2007a","references":["/references/10"]},{"style":0,"text":"Teh, 2006","origin":{"pointer":"/sections/2/paragraphs/2","offset":1382,"length":9},"authors":[{"last":"Teh"}],"year":"2006","references":["/references/23"]},{"style":0,"text":"Neubig et al. (2011)","origin":{"pointer":"/sections/2/paragraphs/3","offset":44,"length":20},"authors":[{"last":"Neubig"},{"last":"al."}],"year":"2011","references":["/references/17"]},{"style":0,"text":"Wu, 1997","origin":{"pointer":"/sections/3/paragraphs/0","offset":41,"length":8},"authors":[{"last":"Wu"}],"year":"1997","references":["/references/24"]},{"style":0,"text":"Zhang and Gildea, 2005","origin":{"pointer":"/sections/3/paragraphs/1","offset":114,"length":22},"authors":[{"last":"Zhang"},{"last":"Gildea"}],"year":"2005","references":["/references/25"]},{"style":0,"text":"Cherry and Lin, 2007","origin":{"pointer":"/sections/3/paragraphs/1","offset":138,"length":20},"authors":[{"last":"Cherry"},{"last":"Lin"}],"year":"2007","references":["/references/5"]},{"style":0,"text":"Zhang et al., 2008","origin":{"pointer":"/sections/3/paragraphs/1","offset":160,"length":18},"authors":[{"last":"Zhang"},{"last":"al."}],"year":"2008","references":["/references/26"]},{"style":0,"text":"Pauls et al., 2010","origin":{"pointer":"/sections/3/paragraphs/1","offset":180,"length":18},"authors":[{"last":"Pauls"},{"last":"al."}],"year":"2010","references":["/references/19"]},{"style":0,"text":"Haghighi et al., 2009","origin":{"pointer":"/sections/3/paragraphs/1","offset":216,"length":21},"authors":[{"last":"Haghighi"},{"last":"al."}],"year":"2009","references":["/references/9"]},{"style":0,"text":"Cherry and Lin, 2006","origin":{"pointer":"/sections/3/paragraphs/1","offset":239,"length":20},"authors":[{"last":"Cherry"},{"last":"Lin"}],"year":"2006","references":["/references/4"]},{"style":0,"text":"Petrov et al., 2008","origin":{"pointer":"/sections/3/paragraphs/1","offset":289,"length":19},"authors":[{"last":"Petrov"},{"last":"al."}],"year":"2008","references":["/references/20"]},{"style":0,"text":"DeNero and Klein, 2010","origin":{"pointer":"/sections/3/paragraphs/1","offset":413,"length":22},"authors":[{"last":"DeNero"},{"last":"Klein"}],"year":"2010","references":["/references/8"]},{"style":0,"text":"Neubig et al., 2011","origin":{"pointer":"/sections/3/paragraphs/1","offset":437,"length":19},"authors":[{"last":"Neubig"},{"last":"al."}],"year":"2011","references":["/references/17"]},{"style":0,"text":"DeNero and Klein (2010)","origin":{"pointer":"/sections/3/paragraphs/1","offset":471,"length":23},"authors":[{"last":"DeNero"},{"last":"Klein"}],"year":"2010","references":["/references/8"]},{"style":0,"text":"Neubig et al. (2011)","origin":{"pointer":"/sections/3/paragraphs/1","offset":606,"length":20},"authors":[{"last":"Neubig"},{"last":"al."}],"year":"2011","references":["/references/17"]},{"style":0,"text":"Marcu and Wong (2002)","origin":{"pointer":"/sections/3/paragraphs/2","offset":117,"length":21},"authors":[{"last":"Marcu"},{"last":"Wong"}],"year":"2002","references":["/references/15"]},{"style":0,"text":"DeNero and Klein, 2010","origin":{"pointer":"/sections/3/paragraphs/2","offset":320,"length":22},"authors":[{"last":"DeNero"},{"last":"Klein"}],"year":"2010","references":["/references/8"]},{"style":0,"text":"Blunsom et al., 2009b","origin":{"pointer":"/sections/3/paragraphs/2","offset":740,"length":21},"authors":[{"last":"Blunsom"},{"last":"al."}],"year":"2009b","references":["/references/2"]},{"style":0,"text":"Levenberg et al., 2012","origin":{"pointer":"/sections/3/paragraphs/2","offset":763,"length":22},"authors":[{"last":"Levenberg"},{"last":"al."}],"year":"2012","references":["/references/14"]},{"style":0,"text":"Johnson et al., 2007a","origin":{"pointer":"/sections/3/paragraphs/3","offset":44,"length":21},"authors":[{"last":"Johnson"},{"last":"al."}],"year":"2007a","references":["/references/10"]},{"style":0,"text":"Neubig et al., 2011","origin":{"pointer":"/sections/3/paragraphs/4","offset":115,"length":19},"authors":[{"last":"Neubig"},{"last":"al."}],"year":"2011","references":["/references/17"]},{"style":0,"text":"Neubig et al. (2011)","origin":{"pointer":"/sections/3/paragraphs/5","offset":50,"length":20},"authors":[{"last":"Neubig"},{"last":"al."}],"year":"2011","references":["/references/17"]},{"style":0,"text":"Levenberg et al. (2012)","origin":{"pointer":"/sections/4/paragraphs/29","offset":108,"length":23},"authors":[{"last":"Levenberg"},{"last":"al."}],"year":"2012","references":["/references/14"]},{"style":0,"text":"Teh et al. (2006)","origin":{"pointer":"/sections/4/paragraphs/29","offset":361,"length":17},"authors":[{"last":"Teh"},{"last":"al."}],"year":"2006","references":["/references/22"]},{"style":0,"text":"Cohn et al., 2010","origin":{"pointer":"/sections/4/paragraphs/49","offset":127,"length":17},"authors":[{"last":"Cohn"},{"last":"al."}],"year":"2010","references":["/references/7"]},{"style":0,"text":"Johnson et al., 2007b","origin":{"pointer":"/sections/4/paragraphs/50","offset":97,"length":21},"authors":[{"last":"Johnson"},{"last":"al."}],"year":"2007b","references":["/references/11"]},{"style":0,"text":"Pilevar and Faili, 2011","origin":{"pointer":"/sections/5/paragraphs/2","offset":40,"length":23},"authors":[{"last":"Pilevar"},{"last":"Faili"}],"year":"2011","references":["/references/21"]},{"style":0,"text":"Blunsom et al., 2009a","origin":{"pointer":"/sections/5/paragraphs/6","offset":733,"length":21},"authors":[{"last":"Blunsom"},{"last":"al."}],"year":"2009a","references":["/references/1"]},{"style":0,"text":"Levenberg et al., 2012","origin":{"pointer":"/sections/5/paragraphs/36","offset":210,"length":22},"authors":[{"last":"Levenberg"},{"last":"al."}],"year":"2012","references":["/references/14"]},{"style":0,"text":"Blunsom and Cohn, 2010","origin":{"pointer":"/sections/5/paragraphs/36","offset":266,"length":22},"authors":[{"last":"Blunsom"},{"last":"Cohn"}],"year":"2010","references":["/references/0"]},{"style":0,"text":"Levenberg et al., 2012","origin":{"pointer":"/sections/5/paragraphs/36","offset":355,"length":22},"authors":[{"last":"Levenberg"},{"last":"al."}],"year":"2012","references":["/references/14"]},{"style":0,"text":"Neubig et al., 2011","origin":{"pointer":"/sections/5/paragraphs/36","offset":379,"length":19},"authors":[{"last":"Neubig"},{"last":"al."}],"year":"2011","references":["/references/17"]},{"style":0,"text":"Koehn et al., 2003","origin":{"pointer":"/sections/5/paragraphs/38","offset":1,"length":18},"authors":[{"last":"Koehn"},{"last":"al."}],"year":"2003","references":["/references/12"]},{"style":0,"text":"Neubig et al., 2011","origin":{"pointer":"/sections/5/paragraphs/38","offset":242,"length":19},"authors":[{"last":"Neubig"},{"last":"al."}],"year":"2011","references":["/references/17"]},{"style":0,"text":"Blunsom et al., 2009a","origin":{"pointer":"/sections/5/paragraphs/39","offset":433,"length":21},"authors":[{"last":"Blunsom"},{"last":"al."}],"year":"2009a","references":["/references/1"]},{"style":0,"text":"Och, 2003","origin":{"pointer":"/sections/5/paragraphs/39","offset":587,"length":9},"authors":[{"last":"Och"}],"year":"2003","references":["/references/18"]},{"style":0,"text":"Neubig et al., 2011","origin":{"pointer":"/sections/5/paragraphs/44","offset":612,"length":19},"authors":[{"last":"Neubig"},{"last":"al."}],"year":"2011","references":["/references/17"]}]}
