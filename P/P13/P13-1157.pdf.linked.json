{"sections":[{"title":"","paragraphs":["Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1597–1607, Sofia, Bulgaria, August 4-9 2013. c⃝2013 Association for Computational Linguistics"]},{"title":"Machine Translation Detection from Monolingual Web-Text Yuki Arase Microsoft Research Asia No. 5 Danling St., Haidian Dist. Beijing, P.R. China yukiar@microsoft.com Ming Zhou Microsoft Research Asia No. 5 Danling St., Haidian Dist. Beijing, P.R. China mingzhou@microsoft.com Abstract","paragraphs":["We propose a method for automatically detecting low-quality Web-text translated by statistical machine translation (SMT) systems. We focus on the phrase salad phenomenon that is observed in existing SMT results and propose a set of computationally inexpensive features to effectively detect such machine-translated sentences from a large-scale Web-mined text. Unlike previous approaches that require bilingual data, our method uses only monolingual text as input; therefore it is applicable for refining data produced by a variety of Web-mining activities. Evaluation results show that the proposed method achieves an accuracy of 95.8% for sentences and 80.6% for text in noisy Web pages."]},{"title":"1 Introduction","paragraphs":["The Web provides an extremely large volume of textual content on diverse topics and areas. Such data is beneficial for constructing a large scale monolingual (Microsoft Web N-gram Services, 2010; Google N-gram Corpus, 2006) and bilingual (Nie et al., 1999; Shi et al., 2006; Ishisaka et al., 2009; Jiang et al., 2009) corpus that can be used for training statistical models for NLP tools, as well as for building a large-scale knowledge-base (Suchanek et al., 2007; Zhu et al., 2009; Fader et al., 2011; Nakashole et al., 2012). With recent advances in statistical machine translation (SMT) systems and their wide adoption in Web services through APIs (Microsoft Translator, 2009; Google Translate, 2006), a large amount of text in Web pages is translated by SMT systems. According to Rarrick et al. (2011), their Web crawler finds that more than 15% of English-Japanese parallel documents are machine translation. Machine-translated sentences are useful if they are of sufficient quality and indistinguishable from human-generated sentences; however, the quality of these machine-translated sentences is generally much lower than sentences generated by native speakers and professional translators. Therefore, a method to detect and filter such SMT results is desired to best make use of Web-mined data.","To solve this problem, we propose a method for automatically detecting Web-text translated by SMT systems1",". We especially target machine-translated text produced through the Web APIs that is rapidly increasing. We focus on the phrase salad phenomenon (Lopez, 2008), which characterizes translations by existing SMT systems, i.e., each phrase in a sentence is semantically and syntactically correct but becomes incorrect when combined with other phrases in the sentence. Based on this trait, we propose features for evaluating the likelihood of machine-translated sentences and use a classifier to determine whether the sentence is generated by the SMT systems.","The primary contributions of the proposed method are threefold. First, unlike previous studies that use parallel text and bilingual features, such as (Rarrick et al., 2011), our method only requires monolingual text as input. Therefore, our method can be used in monolingual Web data mining where bilingual information is unavailable. Second, the proposed features are designed to be computationally light so that the method is suitable for handling a large-scale Web-mined data. Our method determines if an input sentence contains phrase salads using a simple yet effective features, i.e., language models (LMs) and automatically obtained non-contiguous phrases that are frequently used by people but difficult for SMT systems to generate. Third, our method computes features using both human-generated text and SMT 1 In this paper, the term machine-translated is used for in-","dicating translation by SMT systems. 1597 results to capture a phrase salad by contrasting these features, which significantly improves detection accuracy.","We evaluate our method using Japanese and English datasets, including a human evaluation to assess its performance. The results show that our method achieves an accuracy of 95.8% for sentences and 80.6% for noisy Web-text."]},{"title":"2 Related Work","paragraphs":["Previous methods for detecting machine-translated text are mostly designed for bilingual corpus construction. Antonova and Misyurev (2011) design a phrase-based decoder for detecting machine-translated documents in Russian-English Web data. By evaluating the BLEU score (Papineni et al., 2002) of translated documents (by their decoder) against the target-side documents, machine translation (MT) results are detected. Rarrick et al. (2011) extract a variety of features, such as the number of tokens and character types, from sentences in both the source and target languages to capture words that are mis-translated by MT systems. With these features, the likelihood of a bilingual sentence pair being machine-translated can be determined.","Confidence estimation of MT results is also a related area. These studies aim to precisely replicate human judgment in terms of the quality of machine-translated sentences based on features extracted using a syntactic parser (Corston-Oliver et al., 2001; Gamon et al., 2005; Avramidis et al., 2011) or essay scoring system (Parton et al., 2011), assuming that their input is always machine-translated. In contrast, our method aims at making a binary judgment to distinguish machine-translated sentences from a mixture of machine-translated and human-generated sentences. In addition, although methods for confidence estimation can assume sentences of a known source language and reference translations as inputs, these are unavailable in our problem setting.","Another related area is automatic grammatical error detection for English as a second language (ESL) learners (Leacock et al., 2010). We use common features that are also used in this area. They target specific error types commonly made by ESL learners, such as errors in prepositions and subject-verb agreement. In contrast, our method does not specify error types and aims to detect machine-translated sentences focusing on the phrase salad phenomenon produced by SMT systems. In addition, errors generated by ESL learners and SMT systems are different. ESL learners make spelling and grammar mistakes at the word level but their sentence are generally structured while SMT results are unstructured due to phrase salads. Works on translationese detection (Baroni and Bernardini, 2005; Kurokawa et al., 2009; Ilisei et al., 2010) aim to automatically identify human-translated text by professionals using text generated by native speakers. These are related, but our work focuses on machine-translated text.","The closest to our approach is the method proposed by Moore and Lewis (2010). It automatically selects data for creating a domain-specific LM. Specifically, the method constructs LMs using corpora of target and non-target domains and computes a cross-entropy score of an input sentence for estimating the likelihood that the input sentence belongs to the target or non-target domains. While the context is different, our work uses a similar idea of data selection for the purpose of detecting low-quality sentences translated by SMT systems."]},{"title":"3 Proposed Method","paragraphs":["When APIs of SMT services are used for machine-translating an Web page, they typically insert specific tags into the HTML source. Utilizing such tags makes MT detection trivial. However, the actual situation is more complicated in real Web data. When people manually copy and paste machine-translated sentences, such tags are lost. In addition, human-generated and machine-translated sentences are often mixed together even in a single paragraph. To observe the distribution of machine-translated sentences in such difficult cases, we examine 3K sentences collected by our in-house Web crawler. Among them, exclud-ing the pages with the tags of MT APIs, 6.7% of them are found to be clearly machine translation. Our goal is to automatically identify these sentences that cannot be simply detected by the tags, except when the sentences are of sufficient quality to be indistinguishable from human-generated sentences. 3.1 Phrase Salad Phenomenon Fig. 1 illustrates the phrase salad phenomenon that characterizes a sentence translated by an existing 1598 | Of surprise | was up | foreigners flocked | overseas | as well, | they publicized not only | Japan, | saw an article from the news. | Natural English: The news was broadcasted not only in Japan but also overseas, and it surprised foreigners who read the article. Unnatural phrase sequence Natural phrase| | Missing combinational word Figure 1: The phrase salad phenomenon in a sentence translated by an SMT system; each (segmented) phrase is correct and fluent, but dotted arcs show unnatural sequences of phrases and the boxed phrase shows an incomplete non-contiguous phrase. SMT system. Each phrase, a sequence of consecutive words, is fluent and grammatically correct; however, the fluency and grammar correct-ness are both poor in inter-phrases. In addition, a phrase salad becomes obvious by observing distant phrases. For example, the boxed phrase in Fig. 1 is a part of the non-contiguous phrase “not only ⋆ but also2",";” however, it lacks the latter part of the phrase (“but also”) that is also necessary for composing a meaning. Such non-contiguous phrases are difficult for most SMT systems to generate, since these phrases require insertion of sub-phrases in distant parts of the sentence.","Based on the observation of these characteristics, we define features to capture a phrase salad by examining local and distant phrases. These features evaluate (1) fluency (Sec. 3.2), (2) grammaticality (Sec. 3.3), and (3) completeness of non-contiguous phrases in a sentence (Sec. 3.4). Furthermore, humans can distinguish machine-translated text because they have prior knowledge of how a human-generated sentence would look like, which has been accumulated by observing a lot of examples through their life. This knowledge makes phrase-salads, e.g., missing objects and influent sequence of words, obvious for humans since they rarely appear on human-generated sentences. Based on this assumption, we extract these features using both human-generated and machine-translated text. Features extracted from human-generated text represent the similarity to human-generated text. Likewise, features extracted from machine-translated text depict the similarity to machine-translated text. By contrasting these feature weights, we can effectively capture phrase salads in the sentence. 3.2 Fluency Feature In a machine-translated sentence, fluency becomes poor among phrases where a phrase salad occurs. We capture this influency using two independent LM scores; fw,H and fw,MT . The former LM is 2 We use the symbol ⋆ to represent a gap in which any","word or phrase can be placed. trained with human-generated sentences and the latter one is trained with machine-translated sentences. We input a sentence into both of the LMs and use the scores as the fluency features. 3.3 Grammaticality Feature In a sentence with phrase salads, its grammaticality is poor because tense and voice become in-consistent among phrases. We capture this using LMs trained with part-of-speech (POS) sequences of human-generated and machine-translated sentences, and the features of fpos,H and fpos,MT are respectively computed. In a similar manner with a word-based LM, such grammatical inconsistency among phrases is detectable when computing a POS LM score, since the score becomes worse when an N -gram covers inter-phrases where a phrase salad occurs. This approach achieves computational efficiency since it only requires a POS tagger.","Since a phrase salad may occur among distant phrases of a sentence, it is also effective to evaluate combinations of phrases that cannot be covered by the span of N -gram. For this purpose, we make use of function words that sparsely appear in a sentence where their combinations are syntactically constrained. For example, the same preposition rarely appears many times in a human-generated sentence, while it does in a machine-translated sentence due to the phrase salad. Similar to the POS LM, we first analyze sentences generated by human or SMT by a POS tagger, extract sequences of function words, and finally train LMs with the sequences. We use these LMs to obtain scores that are used as features ffw,H and ffw,MT . 3.4 Gappy-Phrase Feature There are a lot of common non-contiguous phrases that consist of sub-phrases (contiguous word string) and gaps, which we refer to as gappy-phrases (Bansal et al., 2011). We specifically use gappy-phrases of 2-tuple, i.e., phrases consisting of two sub-phrases and one gap in the middle. Let us take an English example “not only ⋆ but 1599","Sequences World population not only grows , but grows old . A press release not only informs but also teases . Hazelnuts are not only for food , but also fuel . The coalition must not only listen but also act . Table 1: Example of a sequence database also.” When a sentence contains the phrase “not only,” the phrase “but also” is likely to appear in human-generated setences. Such a gappy-phrase is difficult for SMT systems to correctly generate and causes a phrase salad. Therefore, we define a feature to evaluate how likely a sentence contains gappy-phrases in a complete form without missing sub-phrases. This feature is effective to complement LMs that capture characteristics in N -grams. Sequential Pattern Mining It is costly to manually collect a lot of such gappy-phrases. Therefore, we regard the task as sequential pattern mining and apply PrefixSpan proposed by Pei et al. (2001), which is a widely used sequential pattern mining method3",".","Given a set of sequences and a user-specified min support ∈ N threshold, the sequential pattern mining finds all frequent subsequences whose occurrence frequency is no less than min support. For example, given a sequence database like Table 1, the sequential pattern mining finds all frequent subsequences, e.g., “not only,” “not only ⋆ but also,” “not ⋆ but ⋆,” and etc.","To capture a phrase salad by contrasting appear-ance of gappy-phrases in human-generated and machine-translated text, we independently extract gappy-phrases from each of them using PrefixSpan. We then compute features fg,H and fg,MT using the obtained phrases. Observation of Extracted Gappy-Phrases Based on a preliminary experiment, we set the parameter min support of PrefixSpan to 100 for computational efficiency. We extract gappy-phrases (of 2-tuple) from our development dataset described in Sec. 4.1 that includes 254K human-generated and 134K machine-translated sentences in Japanese, and 210K human-generated and 159K machine-translated sentences in English.","Regarding the Japanese dataset, we obtain about 104K and 64K gappy-phrases from human-3 Due to the severe space limitation, readers are referred to","that paper. generated and machine-translated sentences, respectively. According to our observation of the extracted phrases, 21K phrases commonly appear in human-generated and machine-translated sentences. Many of these common phrases are incomplete forms of gappy-phrases that lack semantic meaning to humans, such as “not only ⋆ the” and “not only ⋆ and.” On the other hand, complete forms of gappy-phrases that preserve semantic meaning exclusively appear in phrases extracted from human-generated sentences. We also obtain about 74K and 42K phrases from human-generated and machine-translated sentences in the English dataset (21K of them are common). Phrase Selection As a result of sequential pattern mining, we can gather a huge number of gappy-phrases from human-generated and machine-translated text, but as we described above, many of them are common. In addition, it is computationally expensive to use all of them. Therefore, our method selects useful phrases for detecting machine-translated sentences.","Although there are several approaches for feature selection, e.g., (Sebastiani, 2002), we use a method that is suitable for handling a large number of feature candidates. Specifically, we evaluate gappy-phrases based on the information gain that measures the amount of information in bits obtained for class prediction when knowing the presence or absence of a phrase and the corresponding class distribution. This corresponds to measuring an expected reduction in entropy, i.e., uncertainty associated with a random factor. The information gain G ∈ R for a gappy-phrase g is defined as G(g)",".","= H(C) − P (X1","g )H(C|X1","g )","−P (X0 g )H(C|X0","g ), where H(C) represents the entropy of the classification, C is a stochastic variable taking a class, Xg is a stochastic variable representing the presence (X1","g ) or absence (X0","g ) of the phrase g, P (Xg) represents the probability of presence or absence of the phrase g, and H(C|Xg) is the conditional entropy due to the phrase g. We use top-k phrases based on the information gain G. Specifically, we use the top 40% of phrases to compute the feature values. Table 2 shows examples of gappy-phrases extracted from human-generated and machine-translated text in our development dataset and remain after feature selection. 1600 in the early ⋆ period after ⋆ after the","known as ⋆ to and also ⋆ and","Human more ⋆ than MT and ⋆ but the not only ⋆ but also no ⋆ not with ⋆ as well as not ⋆ not Table 2: Example of gappy-phrases extracted from human-generated and machine-translated text; phrases preserving semantic meaning are extracted only from human-generated text.","The gappy-phrases depend on each other, and the more phrases extracted from human-generated (machine-translated) text are found in a sentence, the more likely the sentence is human-generated (machine-translated). Therefore, we compute the feature as fc(s) = ∑ i∈k wiδ(i, s), where wi is a weight of the i-th phrase, and δ(i, s) is a Kronecker’s delta function that takes 1 if the sentence s includes the i-th phrase and takes 0 otherwise. We may set the weight wi according to the importance of the phrase, such as the information gain. In this work, we set wi to 1 for simplicity. 3.5 Classification Table 3 summarizes the features employed in our method. In addition to the discussed features, we use the length of a sentence as a feature flen to avoid the bias of LM-based features that favor shorter sentences. The proposed method takes a monolingual sentence from Web data as input and computes a feature vector of f = (fw,H , . . . , flen) ∈ R9",". Each feature is finally normalized to have a zero-mean and unit variance distribution. In the feature space, a support vector machine (SVM) classifier (Vapnik, 1995) is used to determine the likelihoods of machine-translated and human-generated sentences."]},{"title":"4 Experiments","paragraphs":["We evaluate our method using both Japanese and English datasets from various aspects and investigate its characteristics. In this section, we describe our experiment settings. 4.1 Data Preparation For the purpose of evaluation, we use human-generated and machine-translated sentences for Feature Notation Fluency fw,H, fw,MT","Grammaticality fpos,H, fpos,MT","ffw,H, ffw,MT","Gappy-phrase fg,H, fg,MT Length flen Table 3: List of proposed features and their notations constructing LMs, extracting gappy-phrases, and training a classifier. These sentences should be ensured to be human-generated or machine-translated, and the human-generated and machine-translated sentences express the same content for fairness of evaluation to avoid effects due to vocabulary difference.","As a dataset that meets these requirements, we use parallel text in public websites (this is for fair evaluation and our method can be trained using nonparallel text on an actual deployment). Eight popular sites having Japanese and English parallel pages are crawled, whose text is manually verified to be human-generated. The main textual content of these 131K parallel pages are extracted, and the sentences are aligned using (Ma, 2006). As illustrated in Fig. 2, the text in one language is fed to the Bing translator, Google Translate, and an in-house SMT system4","implemented based on (Chiang, 2005) by ourselves for obtaining sentences translated by SMT systems. Due to a severe limitation on the number of requests to the APIs, we randomly subsample sentences before sending them to these SMT systems. We use text in the other language as human-generated sentences5",".","In this manner, we prepare 508K human-generated and 268K machine-translated sentences as a Japanese dataset, and 420K human-generated and 318K machine-translated sentences as an English dataset. We split each of them into two even datasets and use one for development and the other for evaluation. 4.2 Experiment Setting For the fluency and grammaticality features, we train 4-gram LMs using the development dataset with the SRI toolkit (Stolcke, 2002). To obtain the POS information, we use Mecab (Kudo et al., 2004) for Japanese and a POS tagger developed by Toutanova et al. (2003) for English. We evaluate 4 A preliminary evaluation of the in-house SMT system","shows that it has comparable quality with Bing translator. 5 These are a mixture of sentences generated by native","speakers and professional translators/editors. 1601 Parallel sentences","MT systems Machine translated sentences Human-generated sentences Human-generated sentences Human-generated sentences ... English Japanese Japanese Japanese ... ... ... Figure 2: Experimental data preparation; text in one language is fed to SMT systems and the other is used as human-generated sentences. the effect of the sizes of N -grams and development dataset in the experiments.","Using the proposed features, we train an SVM classifier for detecting machine-translated sentences. We use an implementation of LIB-SVM (Chang and Lin, 2011) with a radial basis function kernel due to the relatively small number of features in the proposed method. We set appropriate parameters by grid search in a preliminary experiment.","We evaluate the performance of MT detection based on accuracy6","that is a broadly used evaluation metric for classification problems: accuracy =","nT P + nT N n , where nT P and nT N are the numbers of truepositives and true-negatives, respectively, and n is the total number of exemplars. The accuracy scores that we report in Sec. 5 are all based on 10-fold cross validation. 4.3 Comparison Method We compare our method with the method of (Moore and Lewis, 2010) (Cross-Entropy). Although the Cross-Entropy method is designed for the task of domain adaptation of an LM, our problem is a variant of their original problem and thus their method is directly relevant. In our context, the method computes the cross-entropy scores IMT (s) and IH (s) of an input sentence s against LMs trained on machine-translated and human-generated sentences. Cross-entropy and perplexity are monotonically related, as perplexity of s according to an LM M is simply ob-","6","Although we also examine precision and recall of classification results, they are similar to accuracy reported in this paper. Method Accuracy","Cross-Entropy 90.7","Lexical Feature 87.8","Proposed feature Word LMs 94.1","POS LMs 91.3","FW LMs 82.7","GPs 85.7 Table 4: Accuracy (%) of individual features and comparison methods tained by bIM (s)","where IM (s) is cross-entropy score and b is a base with regard to which the cross-entropy is measured. The method scores the sentence according to the cross-entropy difference, i.e., IMT (s) − IH (s), and decides that the sentence is machine-translated when the score is lower than a predefined threshold. The classification is performed by 10-fold cross validation. We find the best performing threshold on a training set and evaluate the accuracy with a test set using the determined threshold.","Additionally, we compare our method to a method that uses a feature indicating presence or absence of unigrams, which we call Lexical Feature. This feature is commonly used for translationese detection and shows the best performance as a single feature in (Baroni and Bernardini, 2005). It is also used by Rarrick et al. (2011) and shows the best performance by itself in detecting machine-translated sentences in English-Japanese translation in the setting of bilingual input. We implement the feature and use it against a monolingual input to fit our problem setting."]},{"title":"5 Results and Discussions","paragraphs":["In this section, we analyze and discuss the experiment results in detail. 5.1 Accuracy on Japanese Dataset We evaluate the sentence-level and document-level accuracy of our method using the Japanese dataset. Specifically, we evaluate effects of individual features and their combinations, compare with human annotations, and assess performance variations across different sentence lengths and various settings on LM training. Effect of Individual Feature Table 4 shows the accuracy scores of individual features and comparison methods. We refer to features for fluency (fw,H , fw,MT ) as Word LMs, grammaticality using POS LMs (fpos,H , fpos,MT ) as POS LMs 1602","Method Accuracy Word LMs + GPs 94.7 Word LMs + POS LMs 95.1","Word LMs + POS LMs + GPs 95.4","Word LMs + POS LMs + FW LMs 95.5","All 95.8 Table 5: Accuracy (%) of feature combinations; there are significant differences (p ≪ .01) against the accuracy score of Word LMs. and function word LMs (ffw,H , ffw,MT ) as FW LMs, respectively, and for completeness of gappy-phrases (fg,H , fg,MT ) as GPs. The Word LMs show the best accuracy that outperforms Cross-Entropy by 3.4% and Lexical Feature by 6.3%. This high accuracy is achieved by contrasting fluency in human-generated and machine-translated text to capture the phrase salad phenomenon. The accuracy of Word LM trained only on human-generated sentences is limited to 65.5%. On the other hand, the accuracy of Word LM trained on machine-translated sentences shows a better performance (84.4%). By combining these into a single feature vector f = (fw,H , fw,MT , flen), the accuracy is largely improved.","It is interesting that Lexical Feature achieves a high accuracy of 87.8% despite its simplicity. Since Lexical Feature is a bag-of-words model, it can consider distant words in a sentence. This is effective for capturing a phrase salad that occurs among distant phrases, which N -gram cannot cover. As for Cross-Entropy, a simple subtraction of cross-entropy scores cannot well contrast the fluency in human-generated and machine-translated text and results in poorer accuracy than Word LMs.","The accuracy of POS LMs (91.3%) is slightly lower than that of Word LMs due to the limited vocabulary, i.e., the number of POSs. The accuracy of FW LMs and GPs are even lower. This is convincing since these features cannot have reasonable values when a sentence does not include a function word and gappy-phrase. However, these features are complementary to Word LMs as we will see in the next paragraph. Effect of Feature Combination Table 5 shows the accuracy when combining features. Sign tests show that the accuracy scores of these feature combinations are significantly different (p ≪ .01) against the accuracy of Word LMs. The results show that the features complement each other. The","Error Ratio Accuracy (%) Word","LMs All Has wrong content words 37.8 93.1 95.0 Misses content words 12.2 91.8 96.5 Has wrong function words 19.7 92.7 97.1 Misses function words 13.0 93.3 95.6 Has wrong inflections 10.8 97.3 98.7 Table 6: Distribution (%) of machine translation errors and accuracy (%) of proposed method on the different errors combination of all features reaches an accuracy of 95.8%, which improves the accuracy of Word LMs by 1.7%. This result supports that FW LMs and GPs are effective to capture a phrase salad occurring in distant phrases and complement the evidence in N -grams that is captured by LMs. This effect becomes more obvious in the human evaluation.","We also evaluate the accuracy of the proposed method at a document level. Due to the high accuracy at the sentence-level, we use a voting method to judge a document, i.e., deciding if the document is machine-translated when γ% of its sentences are judged as machine-translated. We use all features and find that our method achieves 99% precision and recall with γ = 50. Human Evaluation To further investigate the characteristics of our method, we conduct a human evaluation. We sample Japanese sentences and ask three native speakers to 1) judge whether a sentence is human-generated or machine-translated and 2) list errors that the sentence contains. Regarding the task 1), we allow the annotators to as-sign “hard to determine” for difficult cases. We allocate about 230 sentences for each annotator (in total 700 sentences) without overlapping annota-tion sets.","The accuracy of annotations is found to be 88.2%, which shows that our method is even superior to native speakers. Agreement between the annotators and our method (with all features) is 85.1%. As we interview the annotators, we find that human annotations are strongly affected by the annotators’ domain knowledge. For example, technical sentences are more often misclassified by the annotators.","Table 6 shows the distribution of errors on machine-translated sentences found by the annotators (on sentences that they correctly classified) with the accuracy of Word LMs and all features on 1603 0 2 4 6 8 10 70 75 80 85 90 95 100 6 10 14 18 22 26 30 34 38 42 46 50 54 58 62 66 70 74 78 Rati o (% ) A ccu racy (% ) Num. of words in a sentence Proposed Method Cross-Entropy Lexical Feature Human Length distribution Figure 3: Accuracy (%) across different sentence lengths (the primary axis) and distribution (%) of sentence lengths in the evaluation dataset (the secondly axis) these sentences (a sentence may contain multiple errors). It indicates that the accuracy of Word LMs is improved by feature combination; from 1.4% on sentences of “Has wrong inflections” to 4.7% on sentences of “Misses content words”. Effect of Sentence Length The accuracy of the proposed method is significantly affected by sentence length (the number of words in a sentence). Fig. 3 shows the accuracy of the proposed method (with all features) and comparison methods w.r.t. sentence lengths (with the primary axis), as well as the distribution of sentence lengths in the evaluation dataset (with the secondly axis). We aggregate the classification results on each cross-validation (test results). It also shows the accuracy of human annotations w.r.t. sentence lengths, which we obtain for the 700 sentences in the human evaluation. The accuracy drops on all methods when sentences are short; the accuracy of our method is 91.6% when a sentence contains less than or equal to 10 words. The proposed method shows the similar trend with the human annotations, and even the accuracy of human annotations significantly drops on such short sentences. This result indicates that SMT results on short sentences tend to be of sufficient quality and indistinguishable from human-generated sentences. Since such high-quality machine-translations do not harm the quality of Web-mined data, we do not need to detect them. Effect of Setting on LM Training We evaluate the performance variation w.r.t. the sizes of N -grams and development dataset. Fig. 4 shows the accuracy of the LM based features and feature combination when changing sizes of N -grams. The performance of Word LMs is stabilized after 78 83 88 93 98 1 2 3 4 A ccu racy  (% ) N-gram Word LMs POS LMs FW LMs ALL Figure 4: Effect of the sizes of N-grams on MT detection accuracy (%) 3-gram while that of POS LMs is still improved at 4-gram. This is because POS LMs need more evidence to compensate for their limited vocabulary. FW LMs become stable at 3-gram because the possible number of function words in a sentence should be small.","When we change the size of the development dataset with 10% increments, the accuracy curve is stabilized when the size is 40% of all set. Considering the fact that the overall development dataset is small, it shows that our method is deployable with a small dataset. 5.2 Accuracy on English Dataset To investigate the applicability of our method to other languages, we apply the same method to the English dataset. Because English is a configurational language, function words are less flexible than case markers in Japanese. Therefore, SMT systems may better handle English function words, which potentially decreases the effect of FW LMs in our method. In addition, because English is a morphologically poor language, the effect of POS LMs may be reduced.","Nevertheless, in our experiment, all features are shown to be effective even with the English dataset. The combination of all features achieves the best performance, with an accuracy of 93.1%, which outperforms Cross-Entropy by 1.9%, and Lexical Feature by 8.5%. Even though improvements by POS LMs and FW LMs are smaller than Japanese case, their effects are still positive. We also find that GPs stably contribute to the accuracy. These results show the applicability of our method to other languages. 5.3 Accuracy on Raw Web Pages To avoid unmodeled factors affecting the evaluation, we have carefully removed noise from our experiment datasets. However, real Web pages are 1604 more complex; there are often instances of sentence fragments, such as captions and navigational link text. To evaluate the accuracy of our method on real Web pages, we conduct experiments using the dataset generated by Rarrick et al. (2011) that contains randomly crawled Web pages annotated by two annotators to judge if a page is human-generated or machine-translated. We use Japanese sentences extracted from 69 pages (43 human-generated and 26 machine-translated pages) where the annotators’ judgments agree; 3, 312 sentences consisting of 1, 399 machine-translated and 1, 913 human-generated sentences. To replicate the situation in real Web pages, we conduct a minimal preprocessing, i.e., simply removing HTML tags, and then feed all the remaining text to our method.","An SVM classifier is trained with features obtained by the LMs and gappy-phrases computed from the data described in Sec. 4.1. Our method shows 80.6% accuracy at a sentence level and 82.4% accuracy at a document level using the voting method. One factor for this performance difference is again sentence lengths, as SMT results of short phrases in Web pages can be of high-quality. Another factor is the noise in Web pages. We find that experimental pages contain lots of non-sentences, such as fragments of scripts and product codes. The results show that we need a preprocessing to remove typical noise in Web text before SMT detection to handle noisy Web pages. 5.4 Quality of Cleaned Data Finally, we briefly demonstrate the effect of machine-translation filtering in an end-to-end scenario, taking LM construction as an example. We construct LMs reusing the Japanese evaluation dataset described in Sec. 4.1 where machine-translated sentences are removed by the proposed method (LM-Proposed), Lexical Feature (LM-LF), and Cross-Entropy (LM-CE), as well as an LM with all sentences, i.e., with machine-translated sentences (LM-All). As a result of 5-fold cross-validation, LM-Proposed has 17.8%, 17.1%, and 16.3% lower perplexities on average compared to LM-All, LM-LF, and LM-CE, respectively. These results show that our method is useful for improving the quality of Web-mined data."]},{"title":"6 Conclusion","paragraphs":["We propose a method for detecting machine-translated sentences from monolingual Web-text focusing on the phrase salad phenomenon produced by existing SMT systems. The experimental results show that our method achieves an accuracy of 95.8% for sentences and 80.6% for noisy Web text.","We plan to extend our method to detect machine-translated sentences produced by different MT systems, e.g., a rule-based system, and develop a unified framework for cleaning various types of noise in Web-mined data. In addition, we will investigate the effect of source and target languages on translation in terms of MT detection. As Lopez (2008) describes, a phrase-salad is a common phenomenon that characterizes current SMT results. Therefore, we expect that our method is basically effective on different language pairs. We will conduct experiments to evaluate performance difference using various language pairs."]},{"title":"Acknowledgments","paragraphs":["We sincerely appreciate Spencer Rarrick and Will Lewis for active discussion and sharing the experimental data with us. We thank Junichi Tsujii for his valuable feedback to improve our work."]},{"title":"References","paragraphs":["Alexandra Antonova and Alexey Misyurev. 2011. Building a web-based parallel corpus and filtering out machine translated text. In Proceedings of the Workshop on Building and Using Comparable Corpora, pages 136–144.","Eleftherios Avramidis, Maja Popovic, David Vilar Torres, and Aljoscha Burchardt. 2011. Evaluate with confidence estimation: Machine ranking of translation outputs using grammatical features. In Proceedings of the Workshop on Statistical Machine Translation (WMT 2011), pages 65–70.","Mohit Bansal, Chris Quirk, and Robert C. Moore. 2011. Gappy phrasal alignment by agreement. In Proceedings of the Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT 2011), pages 1308– 1317.","Marco Baroni and Silvia Bernardini. 2005. A new approach to the study of translationese: Machine-learning the difference between original and translated text. Literary and Linguistic Computing, 21(3):259–274. 1605","Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-SVM : a library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2(3):27:1–27:27.","David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL 2005), pages 263–270.","Simon Corston-Oliver, Michael Gamon, and Chris Brockett. 2001. A machine learning approach to the automatic evaluation of machine translation. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL 2001), pages 148–155.","Anthony Fader, Stephen Soderland, and Oren Etzioni. 2011. Identifying relations for open information extraction. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP 2011), pages 1535–1545.","Michael Gamon, Anthony Aue, and Martine Smets. 2005. Sentence-level MT evaluation without reference translations: Beyond language modeling. In Proceedings of European Association for Machine Translation (EAMT 2005).","Google N-gram Corpus. 2006. http://www.ldc. upenn.edu/Catalog/CatalogEntry. jsp?catalogId=LDC2006T13.","Google Translate. 2006. http://code.google. com/apis/language/.","Iustina Ilisei, Diana Inkpen, Gloria Corpas Pastor, and Ruslan Mitkov. 2010. Identification of translationese: A machine learning approach. In Proceedings of the International Conference on Intelligent Text Processing and Computational Linguistics (CI-CLing 2010), pages 503–511.","Tatsuya Ishisaka, Masao Utiyama, Eiichiro Sumita, and Kazuhide Yamamoto. 2009. Development of a Japanese-English software manual parallel corpus. In Proceedings of the Machine Translation Summit (MT Summit XII).","Long Jiang, Shiquan Yang, Ming Zhou, Xiaohua Liu, and Qingsheng Zhu. 2009. Mining bilingual data from the web with adaptively learnt patterns. In Proceedings of the Joint Conference of the Annual Meeting of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing of the Asian Federa-tion of Natural Language Processing (ACL-IJCNLP 2009), pages 870–878.","Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto. 2004. Applying conditional random fields to Japanese morphological analysis. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP 2004), pages 230– 237.","David Kurokawa, Cyril Goutte, and Pierre Isabelle. 2009. Automatic detection of translated text and its impact on machine translation. In Proceedings of the Machine Translation Summit (MT-Summit XII).","Claudia Leacock, Martin Chodorow, Michael Gamon, and Joel Tetreault. 2010. Automated Grammatical Error Detection for Language Learners. Morgan and Claypool Publishers.","Adam Lopez. 2008. Statistical machine translation. ACM Computing Surveys, 40(3):1–49.","Xiaoyi Ma. 2006. Champollion: a robust parallel text sentence aligner. In Proceedings of the International Conference on Language Resources and Evaluation (LREC 2006), pages 489–492.","Microsoft Translator. 2009. http://www. microsofttranslator.com/dev/.","Microsoft Web N-gram Services. 2010. http:// research.microsoft.com/web-ngram.","Robert Moore and William Lewis. 2010. Intelligent selection of language model training data. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL 2010), pages 220–224.","Ndapandula Nakashole, Gerhard Weikum, and Fabian M. Suchanek. 2012. PATTY: A taxonomy of relational patterns with semantic types. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL 2012), pages 1135–1145.","Jian-Yun Nie, Michel Simard, Pierre Isabelle, and Richard Durand. 1999. Cross-language information retrieval based on parallel texts and automatic mining of parallel texts from the web. In Proceedings of the Annual International ACM SIGIR Conference (SIGIR 1999), pages 74–81.","Kishore Papineni, Salim Roukos, Todd Ward, and Wei jing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL 2002), pages 311–318.","Kristen Parton, Joel Tetreault, Nitin Madnani, and Martin Chodorow. 2011. E-rating machine translation. In Proceedings of the Workshop on Statistical Machine Translation (WMT 2011), pages 108–115.","Jian Pei, Jiawei Han, Behzad Mortazavi-Asl, Helen Pinto, Qiming Chen, Umeshwar Dayal, and Mei-Chun Hsu. 2001. PrefixSpan: Mining sequential patterns efficiently by prefix-projected pattern growth. In Proceedings of the International Conference on Data Engineering (ICDE 2001), pages 215–224. 1606","Spencer Rarrick, Chris Quirk, and Will Lewis. 2011. MT detection in web-scraped parallel corpora. In Proceedings of the Machine Translation Summit (MT Summit XIII).","Fabrizio Sebastiani. 2002. Machine learning in automated text categorization. ACM Computing Surveys, 34(1):1–47.","Lei Shi, Cheng Niu, Ming Zhou, and Jianfeng Gao. 2006. A DOM tree alignment model for mining parallel data from the web. In Proceedings of the International Conference on Computational Linguistics and the Annual Meeting of the Association for Computational Linguistics (COLING-ACL 2006), pages 489–496.","Andreas Stolcke. 2002. SRILM-an extensible language modeling toolkit. In Proceedings of the International Conference on Spoken Language Processing (ICSLP 2002), pages 901–904.","Fabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2007. Yago: a core of semantic knowledge. In Proceedings of International Conference on World Wide Web (WWW 2007), pages 697–706.","Kristina Toutanova, Dan Klein, Christopher Manning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency network. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology (HLT-NAACL 2003), pages 252–259.","Vladimir N. Vapnik. 1995. The nature of statistical learning theory. Springer.","Jun Zhu, Zaiqing Nie, Xiaojiang Liu, Bo Zhang, and Ji-Rong Wen. 2009. StatSnowball: a statistical approach to extracting entity relationships. In Proceedings of International Conference on World Wide Web (WWW 2009), pages 101–110. 1607"]}],"references":[{"authors":[{"first":"Alexandra","last":"Antonova"},{"first":"Alexey","last":"Misyurev"}],"year":"2011","title":"Building a web-based parallel corpus and filtering out machine translated text","source":"Alexandra Antonova and Alexey Misyurev. 2011. Building a web-based parallel corpus and filtering out machine translated text. In Proceedings of the Workshop on Building and Using Comparable Corpora, pages 136–144."},{"authors":[{"first":"Eleftherios","last":"Avramidis"},{"first":"Maja","last":"Popovic"},{"first":"David","middle":"Vilar","last":"Torres"},{"first":"Aljoscha","last":"Burchardt"}],"year":"2011","title":"Evaluate with confidence estimation: Machine ranking of translation outputs using grammatical features","source":"Eleftherios Avramidis, Maja Popovic, David Vilar Torres, and Aljoscha Burchardt. 2011. Evaluate with confidence estimation: Machine ranking of translation outputs using grammatical features. In Proceedings of the Workshop on Statistical Machine Translation (WMT 2011), pages 65–70."},{"authors":[{"first":"Mohit","last":"Bansal"},{"first":"Chris","last":"Quirk"},{"first":"Robert","middle":"C.","last":"Moore"}],"year":"2011","title":"Gappy phrasal alignment by agreement","source":"Mohit Bansal, Chris Quirk, and Robert C. Moore. 2011. Gappy phrasal alignment by agreement. In Proceedings of the Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT 2011), pages 1308– 1317."},{"authors":[{"first":"Marco","last":"Baroni"},{"first":"Silvia","last":"Bernardini"}],"year":"2005","title":"A new approach to the study of translationese: Machine-learning the difference between original and translated text","source":"Marco Baroni and Silvia Bernardini. 2005. A new approach to the study of translationese: Machine-learning the difference between original and translated text. Literary and Linguistic Computing, 21(3):259–274. 1605"},{"authors":[{"first":"Chih-Chung","last":"Chang"},{"first":"Chih-Jen","last":"Lin"}],"year":"2011","title":"LIB-SVM : a library for support vector machines","source":"Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-SVM : a library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2(3):27:1–27:27."},{"authors":[{"first":"David","last":"Chiang"}],"year":"2005","title":"A hierarchical phrase-based model for statistical machine translation","source":"David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL 2005), pages 263–270."},{"authors":[{"first":"Simon","last":"Corston-Oliver"},{"first":"Michael","last":"Gamon"},{"first":"Chris","last":"Brockett"}],"year":"2001","title":"A machine learning approach to the automatic evaluation of machine translation","source":"Simon Corston-Oliver, Michael Gamon, and Chris Brockett. 2001. A machine learning approach to the automatic evaluation of machine translation. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL 2001), pages 148–155."},{"authors":[{"first":"Anthony","last":"Fader"},{"first":"Stephen","last":"Soderland"},{"first":"Oren","last":"Etzioni"}],"year":"2011","title":"Identifying relations for open information extraction","source":"Anthony Fader, Stephen Soderland, and Oren Etzioni. 2011. Identifying relations for open information extraction. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP 2011), pages 1535–1545."},{"authors":[{"first":"Michael","last":"Gamon"},{"first":"Anthony","last":"Aue"},{"first":"Martine","last":"Smets"}],"year":"2005","title":"Sentence-level MT evaluation without reference translations: Beyond language modeling","source":"Michael Gamon, Anthony Aue, and Martine Smets. 2005. Sentence-level MT evaluation without reference translations: Beyond language modeling. In Proceedings of European Association for Machine Translation (EAMT 2005)."},{"authors":[{"first":"Google","middle":"N-gram","last":"Corpus"}],"year":"2006","title":"http://www","source":"Google N-gram Corpus. 2006. http://www.ldc. upenn.edu/Catalog/CatalogEntry. jsp?catalogId=LDC2006T13."},{"authors":[{"first":"Google","last":"Translate"}],"year":"2006","title":"http://code","source":"Google Translate. 2006. http://code.google. com/apis/language/."},{"authors":[{"first":"Iustina","last":"Ilisei"},{"first":"Diana","last":"Inkpen"},{"first":"Gloria","middle":"Corpas","last":"Pastor"},{"first":"Ruslan","last":"Mitkov"}],"year":"2010","title":"Identification of translationese: A machine learning approach","source":"Iustina Ilisei, Diana Inkpen, Gloria Corpas Pastor, and Ruslan Mitkov. 2010. Identification of translationese: A machine learning approach. In Proceedings of the International Conference on Intelligent Text Processing and Computational Linguistics (CI-CLing 2010), pages 503–511."},{"authors":[{"first":"Tatsuya","last":"Ishisaka"},{"first":"Masao","last":"Utiyama"},{"first":"Eiichiro","last":"Sumita"},{"first":"Kazuhide","last":"Yamamoto"}],"year":"2009","title":"Development of a Japanese-English software manual parallel corpus","source":"Tatsuya Ishisaka, Masao Utiyama, Eiichiro Sumita, and Kazuhide Yamamoto. 2009. Development of a Japanese-English software manual parallel corpus. In Proceedings of the Machine Translation Summit (MT Summit XII)."},{"authors":[{"first":"Long","last":"Jiang"},{"first":"Shiquan","last":"Yang"},{"first":"Ming","last":"Zhou"},{"first":"Xiaohua","last":"Liu"},{"first":"Qingsheng","last":"Zhu"}],"year":"2009","title":"Mining bilingual data from the web with adaptively learnt patterns","source":"Long Jiang, Shiquan Yang, Ming Zhou, Xiaohua Liu, and Qingsheng Zhu. 2009. Mining bilingual data from the web with adaptively learnt patterns. In Proceedings of the Joint Conference of the Annual Meeting of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing of the Asian Federa-tion of Natural Language Processing (ACL-IJCNLP 2009), pages 870–878."},{"authors":[{"first":"Taku","last":"Kudo"},{"first":"Kaoru","last":"Yamamoto"},{"first":"Yuji","last":"Matsumoto"}],"year":"2004","title":"Applying conditional random fields to Japanese morphological analysis","source":"Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto. 2004. Applying conditional random fields to Japanese morphological analysis. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP 2004), pages 230– 237."},{"authors":[{"first":"David","last":"Kurokawa"},{"first":"Cyril","last":"Goutte"},{"first":"Pierre","last":"Isabelle"}],"year":"2009","title":"Automatic detection of translated text and its impact on machine translation","source":"David Kurokawa, Cyril Goutte, and Pierre Isabelle. 2009. Automatic detection of translated text and its impact on machine translation. In Proceedings of the Machine Translation Summit (MT-Summit XII)."},{"authors":[{"first":"Claudia","last":"Leacock"},{"first":"Martin","last":"Chodorow"},{"first":"Michael","last":"Gamon"},{"first":"Joel","last":"Tetreault"}],"year":"2010","title":"Automated Grammatical Error Detection for Language Learners","source":"Claudia Leacock, Martin Chodorow, Michael Gamon, and Joel Tetreault. 2010. Automated Grammatical Error Detection for Language Learners. Morgan and Claypool Publishers."},{"authors":[{"first":"Adam","last":"Lopez"}],"year":"2008","title":"Statistical machine translation","source":"Adam Lopez. 2008. Statistical machine translation. ACM Computing Surveys, 40(3):1–49."},{"authors":[{"first":"Xiaoyi","last":"Ma"}],"year":"2006","title":"Champollion: a robust parallel text sentence aligner","source":"Xiaoyi Ma. 2006. Champollion: a robust parallel text sentence aligner. In Proceedings of the International Conference on Language Resources and Evaluation (LREC 2006), pages 489–492."},{"authors":[{"first":"Microsoft","last":"Translator"}],"year":"2009","title":"http://www","source":"Microsoft Translator. 2009. http://www. microsofttranslator.com/dev/."},{"authors":[{"first":"Microsoft","middle":"Web N-gram","last":"Services"}],"year":"2010","title":"http:// research","source":"Microsoft Web N-gram Services. 2010. http:// research.microsoft.com/web-ngram."},{"authors":[{"first":"Robert","last":"Moore"},{"first":"William","last":"Lewis"}],"year":"2010","title":"Intelligent selection of language model training data","source":"Robert Moore and William Lewis. 2010. Intelligent selection of language model training data. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL 2010), pages 220–224."},{"authors":[{"first":"Ndapandula","last":"Nakashole"},{"first":"Gerhard","last":"Weikum"},{"first":"Fabian","middle":"M.","last":"Suchanek"}],"year":"2012","title":"PATTY: A taxonomy of relational patterns with semantic types","source":"Ndapandula Nakashole, Gerhard Weikum, and Fabian M. Suchanek. 2012. PATTY: A taxonomy of relational patterns with semantic types. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL 2012), pages 1135–1145."},{"authors":[{"first":"Jian-Yun","last":"Nie"},{"first":"Michel","last":"Simard"},{"first":"Pierre","last":"Isabelle"},{"first":"Richard","last":"Durand"}],"year":"1999","title":"Cross-language information retrieval based on parallel texts and automatic mining of parallel texts from the web","source":"Jian-Yun Nie, Michel Simard, Pierre Isabelle, and Richard Durand. 1999. Cross-language information retrieval based on parallel texts and automatic mining of parallel texts from the web. In Proceedings of the Annual International ACM SIGIR Conference (SIGIR 1999), pages 74–81."},{"authors":[{"first":"Kishore","last":"Papineni"},{"first":"Salim","last":"Roukos"},{"first":"Todd","last":"Ward"},{"first":"Wei","middle":"jing","last":"Zhu"}],"year":"2002","title":"BLEU: a method for automatic evaluation of machine translation","source":"Kishore Papineni, Salim Roukos, Todd Ward, and Wei jing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL 2002), pages 311–318."},{"authors":[{"first":"Kristen","last":"Parton"},{"first":"Joel","last":"Tetreault"},{"first":"Nitin","last":"Madnani"},{"first":"Martin","last":"Chodorow"}],"year":"2011","title":"E-rating machine translation","source":"Kristen Parton, Joel Tetreault, Nitin Madnani, and Martin Chodorow. 2011. E-rating machine translation. In Proceedings of the Workshop on Statistical Machine Translation (WMT 2011), pages 108–115."},{"authors":[{"first":"Jian","last":"Pei"},{"first":"Jiawei","last":"Han"},{"first":"Behzad","last":"Mortazavi-Asl"},{"first":"Helen","last":"Pinto"},{"first":"Qiming","last":"Chen"},{"first":"Umeshwar","last":"Dayal"},{"first":"Mei-Chun","last":"Hsu"}],"year":"2001","title":"PrefixSpan: Mining sequential patterns efficiently by prefix-projected pattern growth","source":"Jian Pei, Jiawei Han, Behzad Mortazavi-Asl, Helen Pinto, Qiming Chen, Umeshwar Dayal, and Mei-Chun Hsu. 2001. PrefixSpan: Mining sequential patterns efficiently by prefix-projected pattern growth. In Proceedings of the International Conference on Data Engineering (ICDE 2001), pages 215–224. 1606"},{"authors":[{"first":"Spencer","last":"Rarrick"},{"first":"Chris","last":"Quirk"},{"first":"Will","last":"Lewis"}],"year":"2011","title":"MT detection in web-scraped parallel corpora","source":"Spencer Rarrick, Chris Quirk, and Will Lewis. 2011. MT detection in web-scraped parallel corpora. In Proceedings of the Machine Translation Summit (MT Summit XIII)."},{"authors":[{"first":"Fabrizio","last":"Sebastiani"}],"year":"2002","title":"Machine learning in automated text categorization","source":"Fabrizio Sebastiani. 2002. Machine learning in automated text categorization. ACM Computing Surveys, 34(1):1–47."},{"authors":[{"first":"Lei","last":"Shi"},{"first":"Cheng","last":"Niu"},{"first":"Ming","last":"Zhou"},{"first":"Jianfeng","last":"Gao"}],"year":"2006","title":"A DOM tree alignment model for mining parallel data from the web","source":"Lei Shi, Cheng Niu, Ming Zhou, and Jianfeng Gao. 2006. A DOM tree alignment model for mining parallel data from the web. In Proceedings of the International Conference on Computational Linguistics and the Annual Meeting of the Association for Computational Linguistics (COLING-ACL 2006), pages 489–496."},{"authors":[{"first":"Andreas","last":"Stolcke"}],"year":"2002","title":"SRILM-an extensible language modeling toolkit","source":"Andreas Stolcke. 2002. SRILM-an extensible language modeling toolkit. In Proceedings of the International Conference on Spoken Language Processing (ICSLP 2002), pages 901–904."},{"authors":[{"first":"Fabian","middle":"M.","last":"Suchanek"},{"first":"Gjergji","last":"Kasneci"},{"first":"Gerhard","last":"Weikum"}],"year":"2007","title":"Yago: a core of semantic knowledge","source":"Fabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2007. Yago: a core of semantic knowledge. In Proceedings of International Conference on World Wide Web (WWW 2007), pages 697–706."},{"authors":[{"first":"Kristina","last":"Toutanova"},{"first":"Dan","last":"Klein"},{"first":"Christopher","last":"Manning"},{"first":"Yoram","last":"Singer"}],"year":"2003","title":"Feature-rich part-of-speech tagging with a cyclic dependency network","source":"Kristina Toutanova, Dan Klein, Christopher Manning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency network. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology (HLT-NAACL 2003), pages 252–259."},{"authors":[{"first":"Vladimir","middle":"N.","last":"Vapnik"}],"year":"1995","title":"The nature of statistical learning theory","source":"Vladimir N. Vapnik. 1995. The nature of statistical learning theory. Springer."},{"authors":[{"first":"Jun","last":"Zhu"},{"first":"Zaiqing","last":"Nie"},{"first":"Xiaojiang","last":"Liu"},{"first":"Bo","last":"Zhang"},{"first":"Ji-Rong","last":"Wen"}],"year":"2009","title":"StatSnowball: a statistical approach to extracting entity relationships","source":"Jun Zhu, Zaiqing Nie, Xiaojiang Liu, Bo Zhang, and Ji-Rong Wen. 2009. StatSnowball: a statistical approach to extracting entity relationships. In Proceedings of International Conference on World Wide Web (WWW 2009), pages 101–110. 1607"}],"cites":[{"style":0,"text":"Services, 2010","origin":{"pointer":"/sections/2/paragraphs/0","offset":180,"length":14},"authors":[{"last":"Services"}],"year":"2010","references":["/references/20"]},{"style":0,"text":"Corpus, 2006","origin":{"pointer":"/sections/2/paragraphs/0","offset":210,"length":12},"authors":[{"last":"Corpus"}],"year":"2006","references":["/references/9"]},{"style":0,"text":"Nie et al., 1999","origin":{"pointer":"/sections/2/paragraphs/0","offset":239,"length":16},"authors":[{"last":"Nie"},{"last":"al."}],"year":"1999","references":["/references/23"]},{"style":0,"text":"Shi et al., 2006","origin":{"pointer":"/sections/2/paragraphs/0","offset":257,"length":16},"authors":[{"last":"Shi"},{"last":"al."}],"year":"2006","references":["/references/29"]},{"style":0,"text":"Ishisaka et al., 2009","origin":{"pointer":"/sections/2/paragraphs/0","offset":275,"length":21},"authors":[{"last":"Ishisaka"},{"last":"al."}],"year":"2009","references":["/references/12"]},{"style":0,"text":"Jiang et al., 2009","origin":{"pointer":"/sections/2/paragraphs/0","offset":298,"length":18},"authors":[{"last":"Jiang"},{"last":"al."}],"year":"2009","references":["/references/13"]},{"style":0,"text":"Suchanek et al., 2007","origin":{"pointer":"/sections/2/paragraphs/0","offset":443,"length":21},"authors":[{"last":"Suchanek"},{"last":"al."}],"year":"2007","references":["/references/31"]},{"style":0,"text":"Zhu et al., 2009","origin":{"pointer":"/sections/2/paragraphs/0","offset":466,"length":16},"authors":[{"last":"Zhu"},{"last":"al."}],"year":"2009","references":["/references/34"]},{"style":0,"text":"Fader et al., 2011","origin":{"pointer":"/sections/2/paragraphs/0","offset":484,"length":18},"authors":[{"last":"Fader"},{"last":"al."}],"year":"2011","references":["/references/7"]},{"style":0,"text":"Nakashole et al., 2012","origin":{"pointer":"/sections/2/paragraphs/0","offset":504,"length":22},"authors":[{"last":"Nakashole"},{"last":"al."}],"year":"2012","references":["/references/22"]},{"style":0,"text":"Translator, 2009","origin":{"pointer":"/sections/2/paragraphs/0","offset":663,"length":16},"authors":[{"last":"Translator"}],"year":"2009","references":["/references/19"]},{"style":0,"text":"Translate, 2006","origin":{"pointer":"/sections/2/paragraphs/0","offset":688,"length":15},"authors":[{"last":"Translate"}],"year":"2006","references":["/references/10"]},{"style":0,"text":"Rarrick et al. (2011)","origin":{"pointer":"/sections/2/paragraphs/0","offset":785,"length":21},"authors":[{"last":"Rarrick"},{"last":"al."}],"year":"2011","references":["/references/27"]},{"style":0,"text":"Lopez, 2008","origin":{"pointer":"/sections/2/paragraphs/2","offset":146,"length":11},"authors":[{"last":"Lopez"}],"year":"2008","references":["/references/17"]},{"style":0,"text":"Rarrick et al., 2011","origin":{"pointer":"/sections/2/paragraphs/3","offset":151,"length":20},"authors":[{"last":"Rarrick"},{"last":"al."}],"year":"2011","references":["/references/27"]},{"style":0,"text":"Antonova and Misyurev (2011)","origin":{"pointer":"/sections/3/paragraphs/0","offset":110,"length":28},"authors":[{"last":"Antonova"},{"last":"Misyurev"}],"year":"2011","references":["/references/0"]},{"style":0,"text":"Papineni et al., 2002","origin":{"pointer":"/sections/3/paragraphs/0","offset":271,"length":21},"authors":[{"last":"Papineni"},{"last":"al."}],"year":"2002","references":["/references/24"]},{"style":0,"text":"Rarrick et al. (2011)","origin":{"pointer":"/sections/3/paragraphs/0","offset":419,"length":21},"authors":[{"last":"Rarrick"},{"last":"al."}],"year":"2011","references":["/references/27"]},{"style":0,"text":"Corston-Oliver et al., 2001","origin":{"pointer":"/sections/3/paragraphs/1","offset":226,"length":27},"authors":[{"last":"Corston-Oliver"},{"last":"al."}],"year":"2001","references":["/references/6"]},{"style":0,"text":"Gamon et al., 2005","origin":{"pointer":"/sections/3/paragraphs/1","offset":255,"length":18},"authors":[{"last":"Gamon"},{"last":"al."}],"year":"2005","references":["/references/8"]},{"style":0,"text":"Avramidis et al., 2011","origin":{"pointer":"/sections/3/paragraphs/1","offset":275,"length":22},"authors":[{"last":"Avramidis"},{"last":"al."}],"year":"2011","references":["/references/1"]},{"style":0,"text":"Parton et al., 2011","origin":{"pointer":"/sections/3/paragraphs/1","offset":324,"length":19},"authors":[{"last":"Parton"},{"last":"al."}],"year":"2011","references":["/references/25"]},{"style":0,"text":"Leacock et al., 2010","origin":{"pointer":"/sections/3/paragraphs/2","offset":111,"length":20},"authors":[{"last":"Leacock"},{"last":"al."}],"year":"2010","references":["/references/16"]},{"style":0,"text":"Baroni and Bernardini, 2005","origin":{"pointer":"/sections/3/paragraphs/2","offset":758,"length":27},"authors":[{"last":"Baroni"},{"last":"Bernardini"}],"year":"2005","references":["/references/3"]},{"style":0,"text":"Kurokawa et al., 2009","origin":{"pointer":"/sections/3/paragraphs/2","offset":787,"length":21},"authors":[{"last":"Kurokawa"},{"last":"al."}],"year":"2009","references":["/references/15"]},{"style":0,"text":"Ilisei et al., 2010","origin":{"pointer":"/sections/3/paragraphs/2","offset":810,"length":19},"authors":[{"last":"Ilisei"},{"last":"al."}],"year":"2010","references":["/references/11"]},{"style":0,"text":"Moore and Lewis (2010)","origin":{"pointer":"/sections/3/paragraphs/3","offset":54,"length":22},"authors":[{"last":"Moore"},{"last":"Lewis"}],"year":"2010","references":["/references/21"]},{"style":0,"text":"Bansal et al., 2011","origin":{"pointer":"/sections/4/paragraphs/4","offset":898,"length":19},"authors":[{"last":"Bansal"},{"last":"al."}],"year":"2011","references":["/references/2"]},{"style":0,"text":"Pei et al. (2001)","origin":{"pointer":"/sections/4/paragraphs/5","offset":877,"length":17},"authors":[{"last":"Pei"},{"last":"al."}],"year":"2001","references":["/references/26"]},{"style":0,"text":"Sebastiani, 2002","origin":{"pointer":"/sections/4/paragraphs/11","offset":68,"length":16},"authors":[{"last":"Sebastiani"}],"year":"2002","references":["/references/28"]},{"style":0,"text":"Vapnik, 1995","origin":{"pointer":"/sections/4/paragraphs/23","offset":154,"length":12},"authors":[{"last":"Vapnik"}],"year":"1995","references":["/references/33"]},{"style":0,"text":"Ma, 2006","origin":{"pointer":"/sections/5/paragraphs/4","offset":429,"length":8},"authors":[{"last":"Ma"}],"year":"2006","references":["/references/18"]},{"style":0,"text":"Chiang, 2005","origin":{"pointer":"/sections/5/paragraphs/5","offset":22,"length":12},"authors":[{"last":"Chiang"}],"year":"2005","references":["/references/5"]},{"style":0,"text":"Stolcke, 2002","origin":{"pointer":"/sections/5/paragraphs/7","offset":438,"length":13},"authors":[{"last":"Stolcke"}],"year":"2002","references":["/references/30"]},{"style":0,"text":"Kudo et al., 2004","origin":{"pointer":"/sections/5/paragraphs/7","offset":499,"length":17},"authors":[{"last":"Kudo"},{"last":"al."}],"year":"2004","references":["/references/14"]},{"style":0,"text":"Toutanova et al. (2003)","origin":{"pointer":"/sections/5/paragraphs/7","offset":561,"length":23},"authors":[{"last":"Toutanova"},{"last":"al."}],"year":"2003","references":["/references/32"]},{"style":0,"text":"Chang and Lin, 2011","origin":{"pointer":"/sections/5/paragraphs/11","offset":137,"length":19},"authors":[{"last":"Chang"},{"last":"Lin"}],"year":"2011","references":["/references/4"]},{"style":0,"text":"Moore and Lewis, 2010","origin":{"pointer":"/sections/5/paragraphs/14","offset":295,"length":21},"authors":[{"last":"Moore"},{"last":"Lewis"}],"year":"2010","references":["/references/21"]},{"style":0,"text":"Baroni and Bernardini, 2005","origin":{"pointer":"/sections/5/paragraphs/24","offset":257,"length":27},"authors":[{"last":"Baroni"},{"last":"Bernardini"}],"year":"2005","references":["/references/3"]},{"style":0,"text":"Rarrick et al. (2011)","origin":{"pointer":"/sections/5/paragraphs/24","offset":306,"length":21},"authors":[{"last":"Rarrick"},{"last":"al."}],"year":"2011","references":["/references/27"]},{"style":0,"text":"Rarrick et al. (2011)","origin":{"pointer":"/sections/6/paragraphs/13","offset":902,"length":21},"authors":[{"last":"Rarrick"},{"last":"al."}],"year":"2011","references":["/references/27"]},{"style":0,"text":"Lopez (2008)","origin":{"pointer":"/sections/7/paragraphs/1","offset":335,"length":12},"authors":[{"last":"Lopez"}],"year":"2008","references":["/references/17"]}]}
