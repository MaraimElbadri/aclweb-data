{"sections":[{"title":"","paragraphs":["Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 159–165, Sofia, Bulgaria, August 4-9 2013. c⃝2013 Association for Computational Linguistics"]},{"title":"Automatic Coupling of Answer Extraction and Information Retrieval Xuchen Yao and Benjamin Van Durme Johns Hopkins University Baltimore, MD, USA Peter Clark Vulcan Inc. Seattle, WA, USA Abstract","paragraphs":["Information Retrieval (IR) and Answer Extraction are often designed as isolated or loosely connected components in Question Answering (QA), with repeated over-engineering on IR, and not necessarily performance gain for QA. We propose to tightly integrate them by coupling automatically learned features for answer extraction to a shallow-structured IR model. Our method is very quick to implement, and significantly improves IR for QA (measured in Mean Average Precision and Mean Reciprocal Rank) by 10%-20% against an uncoupled retrieval baseline in both document and passage retrieval, which further leads to a downstream 20% improvement in QA F1."]},{"title":"1 Introduction","paragraphs":["The overall performance of a Question Answering system is bounded by its Information Retrieval (IR) front end, resulting in research specifically on Information Retrieval for Question Answering (IR4QA) (Greenwood, 2008; Sakai et al., 2010). Common approaches such as query expansion, structured retrieval, and translation models show patterns of complicated engineering on the IR side, or isolate the upstream passage retrieval from downstream answer extraction. We argue that: 1. an IR front end should deliver exactly what a QA1","back end needs; 2. many intuitions employed by QA should be and can be re-used in IR, rather than re-invented. We propose a coupled retrieval method with prior knowledge of its downstream QA component, that feeds QA with exactly the information needed.","1","After this point in the paper we use the term QA in a narrow sense: QA without the IR component, i.e., answer extraction.","As a motivating example, using the question When was Alaska purchased from the TREC 2002 QA track as the query to the Indri search engine, the top sentence retrieved from the accompanying AQUAINT corpus is:","Eventually Alaska Airlines will allow all travelers who have purchased electronic tickets through any means.","While this relates Alaska and purchased, it is not a useful passage for the given question.2","It is apparent that the question asks for a date. Prior work proposed predictive annotation (Prager et al., 2000; Prager et al., 2006): text is first annotated in a predictive manner (of what types of questions it might answer) with 20 answer types and then in-dexed. A question analysis component (consisting of 400 question templates) maps the desired answer type to one of the 20 existing answer types. Retrieval is then performed with both the question and predicated answer types in the query.","However, predictive annotation has the limita-tion of being labor intensive and assuming the underlying NLP pipeline to be accurate. We avoid these limitations by directly asking the downstream QA system for the information about which entities answer which questions, via two steps: 1. reusing the question analysis components from QA; 2. forming a query based on the most relevant answer features given a question from the learned QA model. There is no query-time overhead and no manual template creation. Moreover, this approach is more robust against, e.g., entity recognition errors, because answer typing knowledge is learned from how the data was actually labeled, not from how the data was assumed to be labeled (e.g., manual templates usually assume perfect labeling of named entities, but often it is not the case 2","Based on a non-optimized IR configuration, none of the top 1000 returned passages contained the correct answer: 1867. 159 in practice).","We use our statistically-trained QA system (Yao et al., 2013) that recognizes the association between question type and expected answer types through various features. The QA system employs a linear chain Conditional Random Field (CRF) (Lafferty et al., 2001) and tags each token as either an answer (ANS) or not (O). This will be our off-the-shelf QA system, which recognizes the association between question type and expected answer types through various features based on e.g., part-of-speech tagging (POS) and named entity recognition (NER).","With weights optimized by CRF training (Table 1), we can learn how answer features are correlated with question features. These features, whose weights are optimized by the CRF training, directly reflect what the most important answer types associated with each question type are. For instance, line 2 in Table 1 says that if there is a when question, and the current token’s NER label is DATE, then it is likely that this token is tagged as ANS. IR can easily make use of this knowledge: for a when question, IR retrieves sentences with tokens labeled as DATE by NER, or POS tagged as CD. The only extra processing is to pre-tag and index the text with POS and NER labels. The analyzing power of discriminative answer features for IR comes for free from a trained QA system. Unlike predictive annotation, statistical evidence determines the best answer features given the question, with no manual pattern or templates needed.","To compare again predictive annotation with our approach: predictive annotation works in a forward mode, downstream QA is tailored for upstream IR, i.e., QA works on whatever IR retrieves. Our method works in reverse (backward): downstream QA dictates upstream IR, i.e., IR retrieves what QA wants. Moreover, our approach extends easily beyond fixed answer types such as named entities: we are already using POS tags as a demonstration. We can potentially use any helpful answer features in retrieval. For instance, if the QA system learns that in order to is highly correlated with why question through lexicalized features, or some certain dependency relations are helpful in answering questions with specific structures, then it is natural and easy for the IR component to incorporate them.","There is also a distinction between our method and the technique of learning to rank applied in","feature label weight","qword=when|POS0=CD ANS 0.86","qword=when|NER0=DATE ANS 0.79","qword=when|POS0=CD O -0.74 Table 1: Learned weights for sampled features with respect to the label of current token (indexed by [0]) in a CRF. The larger the weight, the more “important” is this feature to help tag the current token with the corresponding label. For in-stance, line 1 says when answering a when question, and the POS of current token is CD (cardinal number), it is likely (large weight) that the token is tagged as ANS. QA (Bilotti et al., 2010; Agarwal et al., 2012). Our method is a QA-driven approach that provides supervision for IR from a learned QA model, while learning to rank is essentially an IR-driven approach: the supervision for IR comes from a labeled ranking list of retrieval results.","Overall, we make the following contributions: • Our proposed method tightly integrates QA","with IR and the reuse of analysis from QA does","not put extra overhead on the IR queries. This","QA-driven approach provides a holistic solution","to the task of IR4QA.","• We learn statistical evidence about what the form of answers to different questions look like, rather than using manually authored templates. This provides great flexibility in using answer features in IR queries. We give a full spectrum evaluation of all three","stages of IR+QA: document retrieval, passage re-","trieval and answer extraction, to examine thor-","oughly the effectiveness of the method.3","All of","our code and datasets are publicly available.4"]},{"title":"2 Background","paragraphs":["Besides Predictive Annotation, our work is closest to structured retrieval, which covers techniques of dependency path mapping (Lin and Pantel, 2001; Cui et al., 2005; Kaisser, 2012), graph matching with Semantic Role Labeling (Shen and Lapata, 2007) and answer type checking (Pinchak et al., 2009), etc. Specifically, Bilotti et al. (2007) proposed indexing text with their semantic roles and named entities. Queries then include constraints of semantic roles and named entities for the predicate and its arguments in the question. Improvements in recall of answer-bearing sentences were shown over the bag-of-words baseline. Zhao and 3 Rarely are all three aspects presented in concert (see §2). 4 http://code.google.com/p/jacana/ 160 Callan (2008) extended this work with approximate matching and smoothing. Most research uses parsing to assign deep structures. Compared to shallow (POS, NER) structured retrieval, deep structures need more processing power and smoothing, but might also be more precise. 5","Most of the above (except Kaisser (2012)) only reported on IR or QA, but not both, assuming that improvement in one naturally improves the other. Bilotti and Nyberg (2008) challenged this assump-tion and called for tighter coupling between IR and QA. This paper is aimed at that challenge."]},{"title":"3 Method","paragraphs":["Table 1 already shows some examples of features associating question types with answer types. We store the features and their learned weights from the trained model for IR usage.","We let the trained QA system guide the query formulation when performing coupled retrieval with Indri (Strohman et al., 2005), given a corpus already annotated with POS tags and NER labels. Then retrieval runs in four steps (Figure 1): 1. Question Analysis. The question analysis com-","ponent from QA is reused here. In this imple-","mentation, the only information we have cho-","sen to use from the question is the question","word (e.g., how, who) and the lexical answer","types (LAT) in case of what/which questions.","2. Answer Feature Selection. Given the question word, we select the 5 highest weighted features (e.g., POS[0]=CD for a when question).","3. Query Formulation. The original question is combined with the top features as the query. 4. Coupled Retrieval. Indri retrieves a ranked list","of documents or passages. As motivated in the introduction, this framework is aimed at providing the following benefits: Reuse of QA components on the IR side. IR reuses both code for question analysis and top weighted features from QA. Statistical selection of answer features. For in-stance, the NER tagger we used divides location into two categories: GPE (geo locations) and LOC","5","Ogilvie (2010) showed in chapter 4.3 that keyword and named entities based retrieval actually outperformed SRL-based structured retrieval in MAP for the answer-bearing sentence retrieval task in their setting. In this paper we do not intend to re-invent another parse-based structure matching algorithm, but only use shallow structures to show the idea of coupling QA with IR; in the future this might be extended to incorporate “deeper” structure. (non-GPE ). Both of them are learned to be important to where questions. Error tolerance along the NLP pipeline. IR and QA share the same processing pipeline. Systematic errors made by the processing tools are tolerated, in the sense that if the same pre-processing error is made on both the question and sentence, an answer may still be found. Take the previous where question, besides NER[0]=GPE and NER[0]=LOC, we also found oddly NER[0]=PERSON an important feature, due to that the NER tool sometimes mistakes PERSON for LOC. For instance, the volcano name Mauna Loa is labeled as a PERSON instead of a LOC. But since the importance of this feature is recognized by downstream QA, the upstream IR is still motivated to retrieve it.","Queries were lightly optimized using the following strategies: Query Weighting In practice query words are weighted:","#weight(1.0 When 1.0 was 1.0 Alaska 1.0 purchased α #max(#any:CD #any:DATE)) with a weight α for the answer types tuned via cross-validation.","Since NER and POS tags are not lexicalized they accumulate many more counts (i.e. term frequency) than individual words, thus we in general downweight by setting α < 1.0, giving the expected answer types “enough say” but not “too much say”: NER Types First We found NER labels better in-dicators of expected answer types than POS tags. The reasons are two-fold: 1. In general POS tags are too coarse-grained in answer types than NER labels. E.g., NNP can answer who and where questions, but is not as precise as PERSON and GPE. 2. POS tags accumulate even more counts than NER labels, thus they need separate downweighting. Learning the interplay of these weights in a joint IR/QA model, is an interesting path for future work. If the top-weighted features are based on NER, then we do not include POS tags for that question. Otherwise POS tags are useful, for in-stance, in answering how questions. Unigram QA Model The QA system uses up to trigram features (Table 1 shows examples of unigram and bigram features). Thus it is able to learn, for instance, that a POS sequence of IN CD NNS is likely an answer to a when question (such as: in 5 years). This requires that the IR queries 161 When was Alaska purchased? qword=when qword=when|POS[0]=CD → ANS: 0.86 qword=when|NER[0]=DATE → ANS: 0.79","... #combine(Alaska purchased #max(#any:CD #any:DATE))","1. Simple question analysis (reuse from QA) 2. Get top weighted features w.r.t qword (from trained QA model) 3. Query formulation 4. Coupled retrieval On <DATE>March 30, <CD> 1867 </CD> </DATE>, U.S. ... reached agreement ... to purchase ... Alaska ... The islands were sold to the United States in <CD>1867</CD> with the purchase of Alaska.","...","... Eventually Alaska Airlines will allow all travelers who have purchased electronic tickets ... 1 2 ... 50 Figure 1: Coupled retrieval with queries directly constructed from highest weighted features of downstream QA. The retrieved and ranked list of sentences is POS and NER tagged, but only query-relevant tags are shown due to space limit. A bag-of-words retrieval approach would have the sentence shown above at rank 50 at its top position instead. look for a consecutive IN CD NNS sequence. We drop this strict constraint (which may need further smoothing) and only use unigram features, not by simply extracting “good” unigram features from the trained model, but by re-training the model with only unigram features. In answer extraction, we still use up to trigram features. 6"]},{"title":"4 Experiments","paragraphs":["We want to measure and compare the performance of the following retrieval techniques: 1. uncoupled retrieval with an off-the-shelf IR en-","gine by using the question as query (baseline), 2. QA-driven coupled retrieval (proposed), and","3. answer-bearing retrieval by using both the question and known answer as query, only evaluated for answer extraction (upper bound),","at the three stages of question answering:","1. Document retrieval (for relevant docs from corpus), measured by Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR).","2. Passage retrieval (finding relevant sentences from the document), also by MAP and MRR. 3. Answer extraction, measured by F1.","6","This is because the weights of unigram to trigram features in a loglinear CRF model is a balanced consequence for maximization. A unigram feature might end up with lower weight because another trigram containing this unigram gets a higher weight. Then we would have missed this feature if we only used top unigram features. Thus we re-train the model with only unigram features to make sure weights are “assigned properly” among only unigram features. set","questions sentences","#all #pos. #all #pos. TRAIN 2205 1756 (80%) 22043 7637 (35%) TEST","gold 99 88 (89%) 990 368 (37%) Table 2: Statistics for AMT-collected data (total cost was around $800 for paying three Turkers per sentence). Positive questions are those with an answer found. Positive sentences are those bearing an answer. All coupled and uncoupled queries are performed with Indri v5.3 (Strohman et al., 2005). 4.1 Data Test Set for IR and QA The MIT109 test collection by Lin and Katz (2006) contains 109 questions from TREC 2002 and provides a nearexhaustive judgment of relevant documents for each question. We removed 10 questions that do not have an answer by matching the TREC answer patterns. Then we call this test set MIT99. Training Set for QA We used Amazon Mechanical Turk to collect training data for the QA system by issuing answer-bearing queries for TREC1999-2003 questions. For the top 10 retrieved sentences for each question, three Turkers judged whether each sentence contained the answer. The inter-coder agreement rate was 0.81 (Krippendorff, 2004; Artstein and Poesio, 2008).","The 99 questions of MIT99 were extracted from the Turk collection as our TESTgold with the remaining as TRAIN, with statistics shown in Table 2. Note that only 88 questions out of MIT99 have an answer from the top 10 query results.","Finally both the training and test data were sentence-segmented and word-tokenized by NLTK (Bird and Loper, 2004), dependency-parsed by the Stanford Parser (Klein and Manning, 2003), and NER-tagged by the Illinois Named Entity Tagger (Ratinov and Roth, 2009) with an 18-label type set. Corpus Preprocessing for IR The AQUAINT (LDC2002T31) corpus, on which the MIT99 questions are based, was processed in exactly the same manner as was the QA training set. But only sentence boundaries, POS tags and NER labels were kept as the annotation of the corpus. 4.2 Document and Passage Retrieval We issued uncoupled queries consisting of question words, and QA-driven coupled queries consisting of both the question and expected answer types, then retrieved the top 1000 documents, and 162 type","coupled uncoupled","MAP MRR MAP MRR","document 0.2524 0.4835 0.2110 0.4298","sentence 0.1375 0.2987 0.1200 0.2544","Table 3: Coupled vs. uncoupled document/sentence re-","trieval in MAP and MRR on MIT99. Significance level","(Smucker et al., 2007) for both MAP: p < 0.001 and for","both MRR: p < 0.05. finally computed MAP and MRR against the goldstandard MIT99 per-document judgment.","To find the best weighting α for coupled retrieval, we used 5-fold cross-validation and finalized at α = 0.1. Table 3 shows the results. Coupled retrieval outperforms (20% by MAP with p < 0.001 and 12% by MRR with p < 0.01) uncoupled retrieval significantly according to paired randomization test (Smucker et al., 2007).","For passage retrieval, we extracted relevant single sentences. Recall that MIT99 only contains document-level judgment. To generate a test set for sentence retrieval, we matched each sentence from relevant documents provided by MIT99 for each question against the TREC answer patterns.","We found no significant difference between retrieving sentences from the documents returned by document retrieval or directly from the corpus. Numbers of the latter are shown in Table 3. Still, coupled retrieval is significantly better by about 10% in MAP and 17% in MRR. 4.3 Answer Extraction Lastly we sent the sentences to the downstream QA engine (trained on TRAIN) and computed F1 per K for the top K retrieved sentences, 7","shown in Figure 2. The best F1 with coupled sentence retrieval is 0.231, 20% better than F1 of 0.192 with uncoupled retrieval, both at K = 1.","The two descending lines at the bottom reflect the fact that the majority-voting mechanism from the QA system was too simple: F1 drops as K in-creases. Thus we also computed F1’s assuming perfect voting: a voting oracle that always selects the correct answer as long as the QA system produces one, thus the two ascending lines in the center of Figure 2. Still, F1 with coupled retrieval is always better: reiterating the fact that coupled retrieval covers more answer-bearing sentences.","7","Lin (2007), Zhang et al. (2007), and Kaisser (2012) also evaluated on MIT109. However their QA engines used web-based search engines, thus leading to results that are neither reproducible nor directly comparable with ours.","Finally, to find the upper bound for QA, we drew the two upper lines, testing on TESTgold described in Table 2. The test sentences were obtained with answer-bearing queries. This is assuming almost perfect IR. The gap between the top two and other lines signals more room for improvements for IR in terms of better coverage and better rank for answer-bearing sentences.","1","2","3","5","10 15","20","50","100","200","500 1000 Top K Sentences Retrieved 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 F1 Coupled (0.231) Uncoupled (0.192) Gold Oracle (0.755) Gold (0.596) Coupled Oracle (0.609) Uncoupled Oracle (0.569) Figure 2: F1 values for answer extraction on MIT99. Best F1’s for each method are parenthesized in the legend. “Oracle” methods assumed perfect voting of answer candidates (a question is answered correctly if the system ever produced one correct answer for it). “Gold” was tested on TESTgold."]},{"title":"5 Conclusion","paragraphs":["We described a method to perform coupled information retrieval with a prior knowledge of the downstream QA system. Specifically, we coupled IR queries with automatically learned answer features from QA and observed significant improve-ments in document/passage retrieval and boosted F1 in answer extraction. This method has the mer-its of not requiring hand-built question and answer templates and being flexible in incorporating various answer features automatically learned and optimized from the downstream QA system."]},{"title":"Acknowledgement","paragraphs":["We thank Vulcan Inc. for funding this work. We also thank Paul Ogilvie, James Mayfield, Paul Mc-Namee, Jason Eisner and the three anonymous reviewers for insightful comments. 163"]},{"title":"References","paragraphs":["Arvind Agarwal, Hema Raghavan, Karthik Subbian, Prem Melville, Richard D. Lawrence, David C. Gondek, and James Fan. 2012. Learning to rank for robust question answering. In Proceedings of the 21st ACM international conference on Information and knowledge management, CIKM ’12, pages 833–842, New York, NY, USA. ACM.","Ron Artstein and Massimo Poesio. 2008. Inter-Coder Agreement for Computational Linguistics. Computational Linguistics, 34(4):555–596.","M.W. Bilotti and E. Nyberg. 2008. Improving text retrieval precision and answer accuracy in question answering systems. In Coling 2008: Proceedings of the 2nd workshop on Information Retrieval for Question Answering, pages 1–8.","M.W. Bilotti, P. Ogilvie, J. Callan, and E. Nyberg. 2007. Structured retrieval for question answering. In Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval, pages 351–358. ACM.","M.W. Bilotti, J. Elsas, J. Carbonell, and E. Nyberg. 2010. Rank learning for factoid question answering with linguistic and semantic constraints. In Proceedings of the 19th ACM international conference on Information and knowledge management, pages 459–468. ACM.","Steven Bird and Edward Loper. 2004. Nltk: The natural language toolkit. In The Companion Volume to the Proceedings of 42st Annual Meeting of the Association for Computational Linguistics, pages 214– 217, Barcelona, Spain, July.","Hang Cui, Renxu Sun, Keya Li, Min-Yen Kan, and Tat-Seng Chua. 2005. Question answering passage retrieval using dependency relations. In Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’05, pages 400–407, New York, NY, USA. ACM.","Mark A. Greenwood, editor. 2008. Coling 2008: Proceedings of the 2nd workshop on Information Retrieval for Question Answering. Coling 2008 Organizing Committee, Manchester, UK, August.","Michael Kaisser. 2012. Answer Sentence Retrieval by Matching Dependency Paths acquired from Question/Answer Sentence Pairs. In EACL, pages 88–98.","Dan Klein and Christopher D. Manning. 2003. Accurate Unlexicalized Parsing. In In Proc. the 41st Annual Meeting of the Association for Computational Linguistics.","Klaus H. Krippendorff. 2004. Content Analysis: An Introduction to Its Methodology. Sage Publications, Inc, 2nd edition.","John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the Eighteenth International Conference on Machine Learning, ICML ’01, pages 282–289, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.","J. Lin and B. Katz. 2006. Building a reusable test collection for question answering. Journal of the American Society for Information Science and Technology, 57(7):851–861.","D. Lin and P. Pantel. 2001. Discovery of inference rules for question-answering. Natural Language Engineering, 7(4):343–360.","Jimmy Lin. 2007. An exploration of the principles underlying redundancy-based factoid question answering. ACM Trans. Inf. Syst., 25(2), April.","P. Ogilvie. 2010. Retrieval using Document Structure and Annotations. Ph.D. thesis, Carnegie Mellon University.","Christopher Pinchak, Davood Rafiei, and Dekang Lin. 2009. Answer typing for information retrieval. In Proceedings of the 18th ACM conference on Information and knowledge management, CIKM ’09, pages 1955–1958, New York, NY, USA. ACM.","John Prager, Eric Brown, Anni Coden, and Dragomir Radev. 2000. Question-answering by predictive annotation. In Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’00, pages 184–191, New York, NY, USA. ACM.","J. Prager, J. Chu-Carroll, E. Brown, and K. Czuba. 2006. Question answering by predictive annotation. Advances in Open Domain Question Answering, pages 307–347.","L. Ratinov and D. Roth. 2009. Design challenges and misconceptions in named entity recognition. In CoNLL, 6.","Tetsuya Sakai, Hideki Shima, Noriko Kando, Ruihua Song, Chuan-Jie Lin, Teruko Mitamura, Miho Sugimito, and Cheng-Wei Lee. 2010. Overview of the ntcir-7 aclia ir4qa task. In Proceedings of NTCIR-8 Workshop Meeting, Tokyo, Japan.","D. Shen and M. Lapata. 2007. Using semantic roles to improve question answering. In Proceedings of EMNLP-CoNLL, pages 12–21.","M.D. Smucker, J. Allan, and B. Carterette. 2007. A comparison of statistical significance tests for information retrieval evaluation. In Proceedings of the sixteenth ACM conference on Conference on information and knowledge management, pages 623– 632. ACM. 164","T. Strohman, D. Metzler, H. Turtle, and W.B. Croft. 2005. Indri: A language model-based search engine for complex queries. In Proceedings of the International Conference on Intelligent Analysis, volume 2, pages 2–6. Citeseer.","Xuchen Yao, Benjamin Van Durme, Peter Clark, and Chris Callison-Burch. 2013. Answer Extraction as Sequence Tagging with Tree Edit Distance. In Proceedings of NAACL 2013.","Xian Zhang, Yu Hao, Xiaoyan Zhu, Ming Li, and David R. Cheriton. 2007. Information distance from a question to an answer. In Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’07, pages 874–883, New York, NY, USA. ACM.","L. Zhao and J. Callan. 2008. A generative retrieval model for structured documents. In Proceedings of the 17th ACM conference on Information and knowledge management, pages 1163–1172. ACM. 165"]}],"references":[{"authors":[{"first":"Arvind","last":"Agarwal"},{"first":"Hema","last":"Raghavan"},{"first":"Karthik","last":"Subbian"},{"first":"Prem","last":"Melville"},{"first":"Richard","middle":"D.","last":"Lawrence"},{"first":"David","middle":"C.","last":"Gondek"},{"first":"James","last":"Fan"}],"year":"2012","title":"Learning to rank for robust question answering","source":"Arvind Agarwal, Hema Raghavan, Karthik Subbian, Prem Melville, Richard D. Lawrence, David C. Gondek, and James Fan. 2012. Learning to rank for robust question answering. In Proceedings of the 21st ACM international conference on Information and knowledge management, CIKM ’12, pages 833–842, New York, NY, USA. ACM."},{"authors":[{"first":"Ron","last":"Artstein"},{"first":"Massimo","last":"Poesio"}],"year":"2008","title":"Inter-Coder Agreement for Computational Linguistics","source":"Ron Artstein and Massimo Poesio. 2008. Inter-Coder Agreement for Computational Linguistics. Computational Linguistics, 34(4):555–596."},{"authors":[{"first":"M.","middle":"W.","last":"Bilotti"},{"first":"E.","last":"Nyberg"}],"year":"2008","title":"Improving text retrieval precision and answer accuracy in question answering systems","source":"M.W. Bilotti and E. Nyberg. 2008. Improving text retrieval precision and answer accuracy in question answering systems. In Coling 2008: Proceedings of the 2nd workshop on Information Retrieval for Question Answering, pages 1–8."},{"authors":[{"first":"M.","middle":"W.","last":"Bilotti"},{"first":"P.","last":"Ogilvie"},{"first":"J.","last":"Callan"},{"first":"E.","last":"Nyberg"}],"year":"2007","title":"Structured retrieval for question answering","source":"M.W. Bilotti, P. Ogilvie, J. Callan, and E. Nyberg. 2007. Structured retrieval for question answering. In Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval, pages 351–358. ACM."},{"authors":[{"first":"M.","middle":"W.","last":"Bilotti"},{"first":"J.","last":"Elsas"},{"first":"J.","last":"Carbonell"},{"first":"E.","last":"Nyberg"}],"year":"2010","title":"Rank learning for factoid question answering with linguistic and semantic constraints","source":"M.W. Bilotti, J. Elsas, J. Carbonell, and E. Nyberg. 2010. Rank learning for factoid question answering with linguistic and semantic constraints. In Proceedings of the 19th ACM international conference on Information and knowledge management, pages 459–468. ACM."},{"authors":[{"first":"Steven","last":"Bird"},{"first":"Edward","last":"Loper"}],"year":"2004","title":"Nltk: The natural language toolkit","source":"Steven Bird and Edward Loper. 2004. Nltk: The natural language toolkit. In The Companion Volume to the Proceedings of 42st Annual Meeting of the Association for Computational Linguistics, pages 214– 217, Barcelona, Spain, July."},{"authors":[{"first":"Hang","last":"Cui"},{"first":"Renxu","last":"Sun"},{"first":"Keya","last":"Li"},{"first":"Min-Yen","last":"Kan"},{"first":"Tat-Seng","last":"Chua"}],"year":"2005","title":"Question answering passage retrieval using dependency relations","source":"Hang Cui, Renxu Sun, Keya Li, Min-Yen Kan, and Tat-Seng Chua. 2005. Question answering passage retrieval using dependency relations. In Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’05, pages 400–407, New York, NY, USA. ACM."},{"authors":[{"first":"Mark","middle":"A.","last":"Greenwood"},{"last":"editor"}],"year":"2008","title":"Coling 2008: Proceedings of the 2nd workshop on Information Retrieval for Question Answering","source":"Mark A. Greenwood, editor. 2008. Coling 2008: Proceedings of the 2nd workshop on Information Retrieval for Question Answering. Coling 2008 Organizing Committee, Manchester, UK, August."},{"authors":[{"first":"Michael","last":"Kaisser"}],"year":"2012","title":"Answer Sentence Retrieval by Matching Dependency Paths acquired from Question/Answer Sentence Pairs","source":"Michael Kaisser. 2012. Answer Sentence Retrieval by Matching Dependency Paths acquired from Question/Answer Sentence Pairs. In EACL, pages 88–98."},{"authors":[{"first":"Dan","last":"Klein"},{"first":"Christopher","middle":"D.","last":"Manning"}],"year":"2003","title":"Accurate Unlexicalized Parsing","source":"Dan Klein and Christopher D. Manning. 2003. Accurate Unlexicalized Parsing. In In Proc. the 41st Annual Meeting of the Association for Computational Linguistics."},{"authors":[{"first":"Klaus","middle":"H.","last":"Krippendorff"}],"year":"2004","title":"Content Analysis: An Introduction to Its Methodology","source":"Klaus H. Krippendorff. 2004. Content Analysis: An Introduction to Its Methodology. Sage Publications, Inc, 2nd edition."},{"authors":[{"first":"John","middle":"D.","last":"Lafferty"},{"first":"Andrew","last":"McCallum"},{"first":"Fernando","middle":"C. N.","last":"Pereira"}],"year":"2001","title":"Conditional random fields: Probabilistic models for segmenting and labeling sequence data","source":"John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the Eighteenth International Conference on Machine Learning, ICML ’01, pages 282–289, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc."},{"authors":[{"first":"J.","last":"Lin"},{"first":"B.","last":"Katz"}],"year":"2006","title":"Building a reusable test collection for question answering","source":"J. Lin and B. Katz. 2006. Building a reusable test collection for question answering. Journal of the American Society for Information Science and Technology, 57(7):851–861."},{"authors":[{"first":"D.","last":"Lin"},{"first":"P.","last":"Pantel"}],"year":"2001","title":"Discovery of inference rules for question-answering","source":"D. Lin and P. Pantel. 2001. Discovery of inference rules for question-answering. Natural Language Engineering, 7(4):343–360."},{"authors":[{"first":"Jimmy","last":"Lin"}],"year":"2007","title":"An exploration of the principles underlying redundancy-based factoid question answering","source":"Jimmy Lin. 2007. An exploration of the principles underlying redundancy-based factoid question answering. ACM Trans. Inf. Syst., 25(2), April."},{"authors":[{"first":"P.","last":"Ogilvie"}],"year":"2010","title":"Retrieval using Document Structure and Annotations","source":"P. Ogilvie. 2010. Retrieval using Document Structure and Annotations. Ph.D. thesis, Carnegie Mellon University."},{"authors":[{"first":"Christopher","last":"Pinchak"},{"first":"Davood","last":"Rafiei"},{"first":"Dekang","last":"Lin"}],"year":"2009","title":"Answer typing for information retrieval","source":"Christopher Pinchak, Davood Rafiei, and Dekang Lin. 2009. Answer typing for information retrieval. In Proceedings of the 18th ACM conference on Information and knowledge management, CIKM ’09, pages 1955–1958, New York, NY, USA. ACM."},{"authors":[{"first":"John","last":"Prager"},{"first":"Eric","last":"Brown"},{"first":"Anni","last":"Coden"},{"first":"Dragomir","last":"Radev"}],"year":"2000","title":"Question-answering by predictive annotation","source":"John Prager, Eric Brown, Anni Coden, and Dragomir Radev. 2000. Question-answering by predictive annotation. In Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’00, pages 184–191, New York, NY, USA. ACM."},{"authors":[{"first":"J.","last":"Prager"},{"first":"J.","last":"Chu-Carroll"},{"first":"E.","last":"Brown"},{"first":"K.","last":"Czuba"}],"year":"2006","title":"Question answering by predictive annotation","source":"J. Prager, J. Chu-Carroll, E. Brown, and K. Czuba. 2006. Question answering by predictive annotation. Advances in Open Domain Question Answering, pages 307–347."},{"authors":[{"first":"L.","last":"Ratinov"},{"first":"D.","last":"Roth"}],"year":"2009","title":"Design challenges and misconceptions in named entity recognition","source":"L. Ratinov and D. Roth. 2009. Design challenges and misconceptions in named entity recognition. In CoNLL, 6."},{"authors":[{"first":"Tetsuya","last":"Sakai"},{"first":"Hideki","last":"Shima"},{"first":"Noriko","last":"Kando"},{"first":"Ruihua","last":"Song"},{"first":"Chuan-Jie","last":"Lin"},{"first":"Teruko","last":"Mitamura"},{"first":"Miho","last":"Sugimito"},{"first":"Cheng-Wei","last":"Lee"}],"year":"2010","title":"Overview of the ntcir-7 aclia ir4qa task","source":"Tetsuya Sakai, Hideki Shima, Noriko Kando, Ruihua Song, Chuan-Jie Lin, Teruko Mitamura, Miho Sugimito, and Cheng-Wei Lee. 2010. Overview of the ntcir-7 aclia ir4qa task. In Proceedings of NTCIR-8 Workshop Meeting, Tokyo, Japan."},{"authors":[{"first":"D.","last":"Shen"},{"first":"M.","last":"Lapata"}],"year":"2007","title":"Using semantic roles to improve question answering","source":"D. Shen and M. Lapata. 2007. Using semantic roles to improve question answering. In Proceedings of EMNLP-CoNLL, pages 12–21."},{"authors":[{"first":"M.","middle":"D.","last":"Smucker"},{"first":"J.","last":"Allan"},{"first":"B.","last":"Carterette"}],"year":"2007","title":"A comparison of statistical significance tests for information retrieval evaluation","source":"M.D. Smucker, J. Allan, and B. Carterette. 2007. A comparison of statistical significance tests for information retrieval evaluation. In Proceedings of the sixteenth ACM conference on Conference on information and knowledge management, pages 623– 632. ACM. 164"},{"authors":[{"first":"T.","last":"Strohman"},{"first":"D.","last":"Metzler"},{"first":"H.","last":"Turtle"},{"first":"W.","middle":"B.","last":"Croft"}],"year":"2005","title":"Indri: A language model-based search engine for complex queries","source":"T. Strohman, D. Metzler, H. Turtle, and W.B. Croft. 2005. Indri: A language model-based search engine for complex queries. In Proceedings of the International Conference on Intelligent Analysis, volume 2, pages 2–6. Citeseer."},{"authors":[{"first":"Xuchen","last":"Yao"},{"first":"Benjamin","last":"Van Durme"},{"first":"Peter","last":"Clark"},{"first":"Chris","last":"Callison-Burch"}],"year":"2013","title":"Answer Extraction as Sequence Tagging with Tree Edit Distance","source":"Xuchen Yao, Benjamin Van Durme, Peter Clark, and Chris Callison-Burch. 2013. Answer Extraction as Sequence Tagging with Tree Edit Distance. In Proceedings of NAACL 2013."},{"authors":[{"first":"Xian","last":"Zhang"},{"first":"Yu","last":"Hao"},{"first":"Xiaoyan","last":"Zhu"},{"first":"Ming","last":"Li"},{"first":"David","middle":"R.","last":"Cheriton"}],"year":"2007","title":"Information distance from a question to an answer","source":"Xian Zhang, Yu Hao, Xiaoyan Zhu, Ming Li, and David R. Cheriton. 2007. Information distance from a question to an answer. In Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’07, pages 874–883, New York, NY, USA. ACM."},{"authors":[{"first":"L.","last":"Zhao"},{"first":"J.","last":"Callan"}],"year":"2008","title":"A generative retrieval model for structured documents","source":"L. Zhao and J. Callan. 2008. A generative retrieval model for structured documents. In Proceedings of the 17th ACM conference on Information and knowledge management, pages 1163–1172. ACM. 165"}],"cites":[{"style":0,"text":"Greenwood, 2008","origin":{"pointer":"/sections/2/paragraphs/0","offset":203,"length":15},"authors":[{"last":"Greenwood"}],"year":"2008","references":[]},{"style":0,"text":"Sakai et al., 2010","origin":{"pointer":"/sections/2/paragraphs/0","offset":220,"length":18},"authors":[{"last":"Sakai"},{"last":"al."}],"year":"2010","references":["/references/20"]},{"style":0,"text":"Prager et al., 2000","origin":{"pointer":"/sections/2/paragraphs/7","offset":93,"length":19},"authors":[{"last":"Prager"},{"last":"al."}],"year":"2000","references":["/references/17"]},{"style":0,"text":"Prager et al., 2006","origin":{"pointer":"/sections/2/paragraphs/7","offset":114,"length":19},"authors":[{"last":"Prager"},{"last":"al."}],"year":"2006","references":["/references/18"]},{"style":0,"text":"Yao et al., 2013","origin":{"pointer":"/sections/2/paragraphs/10","offset":44,"length":16},"authors":[{"last":"Yao"},{"last":"al."}],"year":"2013","references":["/references/24"]},{"style":0,"text":"Lafferty et al., 2001","origin":{"pointer":"/sections/2/paragraphs/10","offset":237,"length":21},"authors":[{"last":"Lafferty"},{"last":"al."}],"year":"2001","references":["/references/11"]},{"style":0,"text":"Bilotti et al., 2010","origin":{"pointer":"/sections/2/paragraphs/17","offset":441,"length":20},"authors":[{"last":"Bilotti"},{"last":"al."}],"year":"2010","references":["/references/4"]},{"style":0,"text":"Agarwal et al., 2012","origin":{"pointer":"/sections/2/paragraphs/17","offset":463,"length":20},"authors":[{"last":"Agarwal"},{"last":"al."}],"year":"2012","references":["/references/0"]},{"style":0,"text":"Lin and Pantel, 2001","origin":{"pointer":"/sections/3/paragraphs/0","offset":128,"length":20},"authors":[{"last":"Lin"},{"last":"Pantel"}],"year":"2001","references":["/references/13"]},{"style":0,"text":"Cui et al., 2005","origin":{"pointer":"/sections/3/paragraphs/0","offset":150,"length":16},"authors":[{"last":"Cui"},{"last":"al."}],"year":"2005","references":["/references/6"]},{"style":0,"text":"Kaisser, 2012","origin":{"pointer":"/sections/3/paragraphs/0","offset":168,"length":13},"authors":[{"last":"Kaisser"}],"year":"2012","references":["/references/8"]},{"style":0,"text":"Shen and Lapata, 2007","origin":{"pointer":"/sections/3/paragraphs/0","offset":228,"length":21},"authors":[{"last":"Shen"},{"last":"Lapata"}],"year":"2007","references":["/references/21"]},{"style":0,"text":"Pinchak et al., 2009","origin":{"pointer":"/sections/3/paragraphs/0","offset":277,"length":20},"authors":[{"last":"Pinchak"},{"last":"al."}],"year":"2009","references":["/references/16"]},{"style":0,"text":"Bilotti et al. (2007)","origin":{"pointer":"/sections/3/paragraphs/0","offset":319,"length":21},"authors":[{"last":"Bilotti"},{"last":"al."}],"year":"2007","references":["/references/3"]},{"style":0,"text":"Callan (2008)","origin":{"pointer":"/sections/3/paragraphs/0","offset":737,"length":13},"authors":[{"last":"Callan"}],"year":"2008","references":[]},{"style":0,"text":"Kaisser (2012)","origin":{"pointer":"/sections/3/paragraphs/1","offset":26,"length":14},"authors":[{"last":"Kaisser"}],"year":"2012","references":["/references/8"]},{"style":0,"text":"Bilotti and Nyberg (2008)","origin":{"pointer":"/sections/3/paragraphs/1","offset":146,"length":25},"authors":[{"last":"Bilotti"},{"last":"Nyberg"}],"year":"2008","references":["/references/2"]},{"style":0,"text":"Strohman et al., 2005","origin":{"pointer":"/sections/4/paragraphs/1","offset":103,"length":21},"authors":[{"last":"Strohman"},{"last":"al."}],"year":"2005","references":["/references/23"]},{"style":0,"text":"Ogilvie (2010)","origin":{"pointer":"/sections/4/paragraphs/11","offset":0,"length":14},"authors":[{"last":"Ogilvie"}],"year":"2010","references":["/references/15"]},{"style":0,"text":"Strohman et al., 2005","origin":{"pointer":"/sections/5/paragraphs/10","offset":306,"length":21},"authors":[{"last":"Strohman"},{"last":"al."}],"year":"2005","references":["/references/23"]},{"style":0,"text":"Lin and Katz (2006)","origin":{"pointer":"/sections/5/paragraphs/10","offset":392,"length":19},"authors":[{"last":"Lin"},{"last":"Katz"}],"year":"2006","references":["/references/12"]},{"style":0,"text":"Krippendorff, 2004","origin":{"pointer":"/sections/5/paragraphs/10","offset":970,"length":18},"authors":[{"last":"Krippendorff"}],"year":"2004","references":["/references/10"]},{"style":0,"text":"Artstein and Poesio, 2008","origin":{"pointer":"/sections/5/paragraphs/10","offset":990,"length":25},"authors":[{"last":"Artstein"},{"last":"Poesio"}],"year":"2008","references":["/references/1"]},{"style":0,"text":"Bird and Loper, 2004","origin":{"pointer":"/sections/5/paragraphs/12","offset":92,"length":20},"authors":[{"last":"Bird"},{"last":"Loper"}],"year":"2004","references":["/references/5"]},{"style":0,"text":"Klein and Manning, 2003","origin":{"pointer":"/sections/5/paragraphs/12","offset":157,"length":23},"authors":[{"last":"Klein"},{"last":"Manning"}],"year":"2003","references":["/references/9"]},{"style":0,"text":"Ratinov and Roth, 2009","origin":{"pointer":"/sections/5/paragraphs/12","offset":235,"length":22},"authors":[{"last":"Ratinov"},{"last":"Roth"}],"year":"2009","references":["/references/19"]},{"style":0,"text":"Smucker et al., 2007","origin":{"pointer":"/sections/5/paragraphs/19","offset":1,"length":20},"authors":[{"last":"Smucker"},{"last":"al."}],"year":"2007","references":["/references/22"]},{"style":0,"text":"Smucker et al., 2007","origin":{"pointer":"/sections/5/paragraphs/21","offset":298,"length":20},"authors":[{"last":"Smucker"},{"last":"al."}],"year":"2007","references":["/references/22"]},{"style":0,"text":"Lin (2007)","origin":{"pointer":"/sections/5/paragraphs/27","offset":0,"length":10},"authors":[{"last":"Lin"}],"year":"2007","references":["/references/14"]},{"style":0,"text":"Zhang et al. (2007)","origin":{"pointer":"/sections/5/paragraphs/27","offset":12,"length":19},"authors":[{"last":"Zhang"},{"last":"al."}],"year":"2007","references":["/references/25"]},{"style":0,"text":"Kaisser (2012)","origin":{"pointer":"/sections/5/paragraphs/27","offset":37,"length":14},"authors":[{"last":"Kaisser"}],"year":"2012","references":["/references/8"]}]}
