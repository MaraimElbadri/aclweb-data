{"sections":[{"title":"","paragraphs":["Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 623–627, Sofia, Bulgaria, August 4-9 2013. c⃝2013 Association for Computational Linguistics"]},{"title":"A Lattice-based Framework for Joint Chinese Word Segmentation, POS Tagging and Parsing Zhiguo Wang","paragraphs":["1"]},{"title":", Chengqing Zong","paragraphs":["1 "]},{"title":"and Nianwen Xue","paragraphs":["2  1"]},{"title":"National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China, 100190","paragraphs":["2"]},{"title":"Computer Science Department, Brandeis University, Waltham, MA 02452 {zgwang, cqzong}@nlpr.ia.ac.cn xuen@brandeis.edu  Abstract","paragraphs":["For the cascaded task of Chinese word segmentation, POS tagging and parsing, the pipeline approach suffers from error propagation while the joint learning approach suffers from inefficient decoding due to the large combined search space. In this paper, we present a novel lattice-based framework in which a Chinese sentence is first segmented into a word lattice, and then a lattice-based POS tagger and a lattice-based parser are used to process the lattice from two different viewpoints: sequential POS tagging and hierarchical tree building. A strategy is designed to exploit the complementary strengths of the tagger and parser, and encourage them to predict agreed structures. Experimental results on Chinese Treebank show that our lattice-based framework significantly improves the accuracy of the three sub-tasks."]},{"title":"1 Introduction","paragraphs":["Previous work on syntactic parsing generally assumes a processing pipeline where an input sentence is first tokenized, POS-tagged and then parsed (Collins, 1999; Charniak, 2000; Petrov and Klein, 2007). This approach works well for languages like English where automatic tokenization and POS tagging can be performed with high accuracy without the guidance of the high-level syntactic structure. Such an approach, however, is not optimal for languages like Chinese where there are no natural delimiters for word boundaries, and word segmentation (or tokeniza-tion) is a non-trivial research problem by itself. Errors in word segmentation would propagate to later processing stages such as POS tagging and syntactic parsing. More importantly, Chinese is a language that lacks the morphological clues that help determine the POS tag of a word. For example, 调查 (“investigate/investigation”) can either be a verb (“investigate”) or a noun (“investigation”), and there is no morphological varia-tion between its verbal form and nominal form. This contributes to the relatively low accuracy (95% or below) in Chinese POS tagging when evaluated as a stand-alone task (Sun and Uszkoreit, 2012), and the noun/verb ambiguity is a major source of error.","More recently, joint inference approaches have been proposed to address the shortcomings of the pipeline approach. Qian and Liu (2012) proposed a joint inference approach where syntactic parsing can provide feedback to word segmentation and POS tagging and showed that the joint inference approach leads to improvements in all three sub-tasks. However, a major challenge for joint inference approach is that the large combined search space makes efficient decoding and parameter estimation very hard.","In this paper, we present a novel lattice-based framework for Chinese. An input Chinese sentence is first segmented into a word lattice, which is a compact representation of a small set of high-quality word segmentations. Then, a lattice-based POS tagger and a lattice-based parser are used to process the word lattice from two different viewpoints. We next employ the dual decomposition method to exploit the complementary strengths of the tagger and parser, and encourage them to predict agreed structures. Experimental results show that our lattice-based framework significantly improves the accuracies of the three sub-tasks"]},{"title":"2 The Lattice-based Framework","paragraphs":["Figure 1 gives the organization of the framework. There are four types of linguistic structures: a Chinese sentence, the word lattice, tagged word sequence and parse tree of the Chinese sentence. An example for each structure is provided in Figure 2. We can see that the terminals and preterminals of a parse tree constitute a tagged word sequence. Therefore, we define a comparator between a tagged word sequence and a parse tree: if they contain the same word sequence and POS tags, they are equal, otherwise unequal. 623","Figure 1 also shows the workflow of the framework. First, the Chinese sentence is segmented into a word lattice using the word segmentation system. Then the word lattice is fed into the lattice-based POS tagger to produce a tagged word sequence and into the lattice-based parser to separately produce a parse tree . We then compare with to see whether they are equal. If they are equal, we output as the final result. Otherwise, the guidance generator generates some guidance orders based on the difference between and , and guides the tagger and the parser to process the lattice again. This procedure may iterate many times until the tagger and parser predict equal structures. ","","The motivation to design such a framework is as follows. First, state-of-the-art word segmentation systems can now perform with high accuracy. We can easily get an F1 score greater than 96%, and an oracle (upper bound) F1 score greater than 99% for the word lattice (Jiang et al., 2008). Therefore, a word lattice provides us a good enough search space to allow sufficient interaction among word segmentation, POS tagging and parsing systems. Second, both the lattice-based POS tagger and the lattice-based parser can select word segmentation from the word lattice and predict POS tags, but they do so from two different perspectives. The lattice-based POS tagger looks at a path in a word lattice as a sequence and performs sequence labeling based on linear local context, while the lattice-based parser builds the parse trees in a hierarchical manner. They have different strengths with regard to word segmentation and POS tagging. We hypothesize that exploring the complementary strengths of the tagger and parser would improve each of the sub-tasks.","We build a character-based model (Xue, 2003) for the word segmentation system, and treat segmentation as a sequence labeling task, where each Chinese character is labeled with a tag. We use the tag set provided in Wang et al. (2011) and use the same feature templates. We use the Maximum Entropy (ME) model to estimate the feature weights. To get a word lattice, we first generate N-best word segmentation results, and then compact the N-best lists into a word lattice by collapsing all the identical words into one edge. We also assign a probability to each edge, which is calculated by multiplying the tagging probabilities of each character in the word. The goal of the lattice-based POS tagger is to predict a tagged word sequence for an input word lattice :","=argmax","∈ ( ) ∙ ( ) where ( ) represents the set of all possible tagged word sequences derived from the word lattice . ( ) is used to map onto a global feature vector, and is the corresponding weight vector. We use the same non-local feature templates used in Jiang et al. (2008) and a similar decoding algorithm. We use the perceptron algorithm (Collins, 2002) for parameter estimation.","Goldberg and Elhadad (2011) proposed a lattice-based parser for Heberw based on the PCFG-LA model (Matsuzaki et al., 2005). We adopted their approach, but found the unweighted word lattice their parser takes as input to be ineffective for our Chinese experiments. Instead, we use a weighted lattice as input and weigh each edge in the lattice with the word probability. In our model, each syntactic category is split into multiple subcategories [ ] by labeling a latent annotation . Then, a parse tree 布朗一行于今晚离沪赴广州。","Brown’s group will leave Shanghai to Guangzhou tonight.","(a) Chinese Sentence  (b) Word Lattice 布朗 。广州赴沪离今晚于 NR NRVVNRPNTP PU 一行 NN","Brown .GuangzhougoShanghaileavetonightingroup"," (c) Tagged Word Sequence Brown . Guangzhou","go Shanghaileavetonight ingroup布朗 于 今晚 赴 广州 。 NR P NT NP PP VV NR NP VP PUNP IP VP 一行 NN NP NP 离 沪 VV NR NP VP VP ","(d) Parse Tree Figure 2: Linguistic structure examples. Chinese Sentence Word Segmentation Word Lattice Lattice-based Parser Lattice-based POS Tagger Guidance Generator","Parse Tree Tagged Word Sequence The Final Parse Tree No Yes Equal?"," Figure 1: The lattice-based framework. 624 is refined into [ ], where X is the latent annotation vector for all non-terminals in . The probability of [ ] is calculated as: ( [ ]) = ( [ ] → [ ] [ ]) × ( [ ] → ) × ( ) where the three terms are products of all syntactic rule probabilities, lexical rule probabilities and word probabilities in [ ] respectively."]},{"title":"3 Combined Optimization Between The Lattice-based POS Tagger and The Lattice-based Parser","paragraphs":["We first define some variables to make it easier to compare a tagged word sequence with a parse tree . We define as the set of all POS tags. For , we define ( , , )=1 if contains a POS tag ∈ spanning from the i-th character to the j-th character, otherwise ( , , ) = 0. We also define ( , ,#) = 1 if contains the word spanning from the i-th character to the j-th character, otherwise ( , ,#) = 0. Similarly, for , we define ( , , ) =1 if contains a POS tag ∈ spanning from the i-th character to the j-th character, otherwise ( , , ) = 0. We also define ( , ,#) = 1 if contains the word spanning from the i-th character to the j-th character, otherwise ( , ,#) = 0. Therefore, and are equal, only if ( , , ) = ( , , ) for all ∈ [0, ] , ∈ [ +1, ] and ∈ ∪ #, otherwise unequal.","Our framework expects the tagger and the parser to predict equal structures and we formulate it as a constraint optimization problem:",", =argmax",", ( ) + ( )","Such that for all ∈ [0, ] , ∈ [ +1, ] and ∈ ∪ #:","( , , ) = ( , , )"," where ( ) = ∙ ( ) is a scoring function from the viewpoint of the lattice-based POS tagger, and ( ) =log ( ) is a scoring function from the viewpoint of the lattice-based parser.","The dual decomposition (a special case of Lagrangian relaxation) method introduced in Komodakis et al. (2007) is suitable for this problem. Using this method, we solve the primal constraint optimization problem by optimizing the dual problem. First, we introduce a vector of Lagrange multipliers ( , , ) for each equality constraint. Then, the Lagrangian is formulated as: ( , , ) = ( ) + ( )","+ ( , , )( ( , , )− ( , , ))",", , ","By grouping the terms that depend on and , we rewrite the Lagrangian as","( , , ) = ( ) + ( , , ) ( , , ) , , ","+ ( ) − ( , , ) ( , , ) , , ","Then, the dual objective is ( ) =max",", ( , , )","=max ","( ) + ( , , ) ( , , ) , , + max ","( ) − ( , , ) ( , , ) , ,  The dual problem is to find min ( ).","We use the subgradient method (Boyd et al.,","2003) to minimize the dual. Following Rush et al.","(2010), we define the subgradient of ( ) as: ( , , ) = ( , , ) − ( , , ) for all ( , , ) Then, adjust ( , , ) as follows: ","( , , ) = ( , , ) − ( ( , , ) − ( , , ))","where >0 is a step size. ","Algorithm 1 presents the subgradient method to solve the dual problem. The algorithm initializes the Lagrange multiplier values with 0 (line 1) and then iterates many times. In each iteration, the algorithm finds the best ( )","and ( )","by running the lattice-based POS tagger (line 3) and the lattice-based parser (line 4). If ( )","and ( )"," share the same tagged word sequence (line 5), then the algorithm returns the solution (line 6). Otherwise, the algorithm adjusts the Lagrange multiplier values based on the differences between ( )","and ( )","(line 8). A crucial point is that the argmax problems in line 3 and line 4 can be solved efficiently using the original decoding algorithms, because the Lagrange multiplier can be regarded as adjustments for lexical rule probabilities and word probabilities."]},{"title":"4 Experiments","paragraphs":["We conduct experiments on the Chinese Treebank Version 5.0 and use the standard data split Algorithm 1: Combined Optimization 1: Set ( )","( , , )=0, for all ( , , ) 2: For k=1 to K 3: ( )","← argmax","( ) + ∑ ( ) ( , , ) ( , , ) , ,","4: ( ) ← argmax ( ) − ∑ ( )","( , , ) ( , , ) , ,","5: If ( )","( , , ) = ( )","( , , ) for all ( , , )","6: Return ( ( ) , ( )",")","7: Else","8: ( )","( , , ) =","( )","( , , ) − ( ( )","( , , ) − ( )","( , , ))  625 (Petrov and Klein, 2007). The traditional evalua-tion metrics for POS tagging and parsing are not suitable for the joint task. Following with Qian and Liu (2012), we redefine precision and recall by computing the span of a constituent based on character offsets rather than word offsets. 4.1 Performance of the Basic Sub-systems We train the word segmentation system with 100 iterations of the Maximum Entropy model using the OpenNLP toolkit. Table 1 shows the performance. It shows that our word segmentation system is comparable with the state-of-the-art systems and the upper bound F1 score of the word lattice exceeds 99.6%. This indicates that our word segmentation system can provide a good search space for the lattice-based POS tagger and the lattice-based parser. ","To train the lattice-based POS tagger, we generate the word lattice for each sentence in the training set using cross validation approach. We divide the entire training set into 18 folds on average (each fold contains 1,000 sentences). For each fold, we segment each sentence in the fold into a word lattice by compacting 20-best segmentation list produced with a model trained on the other 17 folds. Then, we train the lattice-based POS tagger with 20 iterations of the average perceptron algorithm. Table 2 presents the joint word segmentation and POS tagging performance and shows that our lattice-based POS tagger obtains results that are comparable with state-of-the-art systems. ","We implement the lattice-based parser by modifying the Berkeley Parser, and train it with 5 iterations of the split-merge-smooth strategy (Petrov et al., 2006). Table 3 shows the performance, where the “Pipeline Parser” represents the system taking one-best segmentation result from our word segmentation system as input and “Lattice-based Parser” represents the system taking the compacted word lattice as input. We find the lattice-based parser gets better performance than the pipeline system among all three sub-tasks.  4.2 Performance of the Framework For the lattice-based framework, we set the maximum iteration in Algorithm 1 as K = 20. The step size is tuned on the development set and empirically set to be 0.8. Table 4 shows the parsing performance on the test set. It shows that the lattice-based framework achieves improvement over the lattice-based parser alone among all three sub-tasks: 0.16 points for word segmentation, 1.19 points for POS tagging and 1.65 points for parsing. It also outperforms the lattice-based POS tagger by 0.65 points on POS tagging accuracy. Our lattice-based framework also improves over the best joint inference parsing system (Qian and Liu, 2012) by 0.57 points. "]},{"title":"5 Conclusion","paragraphs":["In this paper, we present a novel lattice-based framework for the cascaded task of Chinese word segmentation, POS tagging and parsing. We first segment a Chinese sentence into a word lattice, then process the lattice using a lattice-based POS tagger and a lattice-based parser. We also design a strategy to exploit the complementary strengths of the tagger and the parser and encourage them to predict agreed structures. Experimental results show that the lattice-based framework significantly improves the accuracies of the three tasks. The parsing accuracy of the framework also outperforms the best joint parsing system reported in the literature.","P R F","(Qian and Liu, 2012)  Seg. 97.56 98.36 97.96 POS 93.43 94.2 93.81 Parse 83.03 82.66 82.85 Lattice-based Framework Seg. 97.82 97.9 97.86 POS 94.36 94.44 94.40 Parse 83.34 83.5 83.42","Table 4: Lattice-based framework evaluation. P R F Pipeline Parser  Seg. 96.97 98.06 97.52 POS 92.01 93.04 92.52 Parse 80.86 81.47 81.17","","Lattice-based Parser Seg. 97.73 97.66 97.70 POS 93.24 93.18 93.21 Parse 81.83 81.71 81.77","Table 3: Parsing evaluation. ","P R F (Kruengkrai et al., 2009) 93.28 94.07 93.67 (Zhang and Clark, 2010) - - 93.67 (Qian and Liu, 2012) 93.1 93.96 93.53 (Sun, 2011) - - 94.02 Lattice-based POS tagger 93.64 93.87 93.75","Table 2: POS tagging evaluation. P R F (Kruengkrai et al., 2009) 97.46 98.29 97.87 (Zhang and Clark, 2010) - - 97.78 (Qian and Liu, 2012) 97.45 98.24 97.85 (Sun, 2011) - - 98.17 Our Word Seg. System 96.97 98.06 97.52 Word Lattice Upper Bound 99.55 99.75 99.65 Table 1: Word segmentation evaluation. 626"]},{"title":"Acknowledgments","paragraphs":["The research work has been funded by the Hi-Tech Research and Development Program (\"863\" Program) of China under Grant No. 2011AA01A207, 2012AA011101, and 2012AA011102 and also supported by the Key Project of Knowledge Innovation Program of Chinese Academy of Sciences under Grant No.KGZD-EW-501. This work is also supported in part by the DAPRA via contract HR0011-11-C-0145 entitled \"Linguistic Resources for Multilingual Processing\"."]},{"title":"References","paragraphs":["S. Boyd, L. Xiao and A. Mutapcic. 2003. Subgradient methods. Lecture notes of EE392o, Stanford University.","E. Charniak. 2000. A maximum–entropy–inspired parser. In NAACL ’00, page 132–139.","Michael Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania.","Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proc. of EMNLP2002, pages 1-8.","Yoav Goldberg and Michael Elhadad. 2011. Joint Hebrew segmentation and parsing using a PCFG-LA lattice parser. In Proc. of ACL2011.","Wenbin Jiang, Haitao Mi and Qun Liu. 2008. Word lattice reranking for Chinese word segmentation and part-of-speech tagging. In Proc. of Coling 2008, pages 385-392.","Komodakis, N., Paragios, N., and Tziritas, G. 2007. MRF optimization via dual decomposition: Message-passing revisited. In ICCV 2007.","C. Kruengkrai, K. Uchimoto, J. Kazama, Y. Wang, K. Torisawa and H. Isahara. 2009. An error-driven word-character hybrid model for joint Chinese word segmentation and POS tagging. In Proc. of ACL2009, pages 513-521.","Takuya Matsuzaki, Yusuke Miyao and Jun'ichi Tsujii. 2005. Probabilistic CFG with latent annotations. In Proc. of ACL2005, pages 75-82.","Slav Petrov, Leon Barrett, Romain Thibaux and Dan Klein. 2006. Learning accurate, compact, and in-terpretable tree annotation. In Proc. of ACL2006, pages 433-440.","Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In Proc. of NAACL2007, pages 404-411.","Xian Qian and Yang Liu. 2012. Joint Chinese Word segmentation, POS Tagging Parsing. In Proc. of EMNLP 2012, pages 501-511.","Alexander M. Rush, David Sontag, Michael Collins and Tommi Jaakkola. 2010. On dual decomposition and linear programming relaxations for natural language processing. In Proc. of EMNLP2010, pages 1-11.","Weiwei Sun. 2011. A stacked sub-word model for joint Chinese word segmentation and part-of-speech tagging. In Proc. of ACL2011, pages 1385-1394.","Weiwei Sun and Hans Uszkoreit. Capturing paradigmatic and syntagmatic lexical relations: Towards accurate Chinese part-of-speech tagging. In Proc. of ACL2012.","Yiou Wang, Jun'ichi Kazama, Yoshimasa Tsuruoka, Wenliang Chen, Yujie Zhang and Kentaro Torisawa. 2011. Improving Chinese word segmentation and POS tagging with semi-supervised methods using large auto-analyzed data. In Proc. of IJCNLP2011, pages 309-317.","Nianwen Xue. 2003. Chinese word segmentation as character tagging. Computational Linguistics and Chinese Language Processing, 8 (1). pages 29-48.","Yue Zhang and Stephen Clark. 2010. A fast decoder for joint word segmentation and POS-tagging using a single discriminative model. In Proc. of EMNLP2010, pages 843-852. 627"]}],"references":[{"authors":[{"first":"S.","last":"Boyd"},{"first":"L.","last":"Xiao"},{"first":"A.","last":"Mutapcic"}],"year":"2003","title":"Subgradient methods","source":"S. Boyd, L. Xiao and A. Mutapcic. 2003. Subgradient methods. Lecture notes of EE392o, Stanford University."},{"authors":[{"first":"E.","last":"Charniak"}],"year":"2000","title":"A maximum–entropy–inspired parser","source":"E. Charniak. 2000. A maximum–entropy–inspired parser. In NAACL ’00, page 132–139."},{"authors":[{"first":"Michael","last":"Collins"}],"year":"1999","title":"Head-Driven Statistical Models for Natural Language Parsing","source":"Michael Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania."},{"authors":[{"first":"Michael","last":"Collins"}],"year":"2002","title":"Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms","source":"Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proc. of EMNLP2002, pages 1-8."},{"authors":[{"first":"Yoav","last":"Goldberg"},{"first":"Michael","last":"Elhadad"}],"year":"2011","title":"Joint Hebrew segmentation and parsing using a PCFG-LA lattice parser","source":"Yoav Goldberg and Michael Elhadad. 2011. Joint Hebrew segmentation and parsing using a PCFG-LA lattice parser. In Proc. of ACL2011."},{"authors":[{"first":"Wenbin","last":"Jiang"},{"first":"Haitao","last":"Mi"},{"first":"Qun","last":"Liu"}],"year":"2008","title":"Word lattice reranking for Chinese word segmentation and part-of-speech tagging","source":"Wenbin Jiang, Haitao Mi and Qun Liu. 2008. Word lattice reranking for Chinese word segmentation and part-of-speech tagging. In Proc. of Coling 2008, pages 385-392."},{"authors":[{"first":"N.","last":"Komodakis"},{"first":"N.","last":"Paragios"},{"last":"Tziritas"},{"last":"G"}],"year":"2007","title":"MRF optimization via dual decomposition: Message-passing revisited","source":"Komodakis, N., Paragios, N., and Tziritas, G. 2007. MRF optimization via dual decomposition: Message-passing revisited. In ICCV 2007."},{"authors":[{"first":"C.","last":"Kruengkrai"},{"first":"K.","last":"Uchimoto"},{"first":"J.","last":"Kazama"},{"first":"Y.","last":"Wang"},{"first":"K.","last":"Torisawa"},{"first":"H.","last":"Isahara"}],"year":"2009","title":"An error-driven word-character hybrid model for joint Chinese word segmentation and POS tagging","source":"C. Kruengkrai, K. Uchimoto, J. Kazama, Y. Wang, K. Torisawa and H. Isahara. 2009. An error-driven word-character hybrid model for joint Chinese word segmentation and POS tagging. In Proc. of ACL2009, pages 513-521."},{"authors":[{"first":"Takuya","last":"Matsuzaki"},{"first":"Yusuke","last":"Miyao"},{"first":"Jun'ichi","last":"Tsujii"}],"year":"2005","title":"Probabilistic CFG with latent annotations","source":"Takuya Matsuzaki, Yusuke Miyao and Jun'ichi Tsujii. 2005. Probabilistic CFG with latent annotations. In Proc. of ACL2005, pages 75-82."},{"authors":[{"first":"Slav","last":"Petrov"},{"first":"Leon","last":"Barrett"},{"first":"Romain","last":"Thibaux"},{"first":"Dan","last":"Klein"}],"year":"2006","title":"Learning accurate, compact, and in-terpretable tree annotation","source":"Slav Petrov, Leon Barrett, Romain Thibaux and Dan Klein. 2006. Learning accurate, compact, and in-terpretable tree annotation. In Proc. of ACL2006, pages 433-440."},{"authors":[{"first":"Slav","last":"Petrov"},{"first":"Dan","last":"Klein"}],"year":"2007","title":"Improved inference for unlexicalized parsing","source":"Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In Proc. of NAACL2007, pages 404-411."},{"authors":[{"first":"Xian","last":"Qian"},{"first":"Yang","last":"Liu"}],"year":"2012","title":"Joint Chinese Word segmentation, POS Tagging Parsing","source":"Xian Qian and Yang Liu. 2012. Joint Chinese Word segmentation, POS Tagging Parsing. In Proc. of EMNLP 2012, pages 501-511."},{"authors":[{"first":"Alexander","middle":"M.","last":"Rush"},{"first":"David","last":"Sontag"},{"first":"Michael","last":"Collins"},{"first":"Tommi","last":"Jaakkola"}],"year":"2010","title":"On dual decomposition and linear programming relaxations for natural language processing","source":"Alexander M. Rush, David Sontag, Michael Collins and Tommi Jaakkola. 2010. On dual decomposition and linear programming relaxations for natural language processing. In Proc. of EMNLP2010, pages 1-11."},{"authors":[{"first":"Weiwei","last":"Sun"}],"year":"2011","title":"A stacked sub-word model for joint Chinese word segmentation and part-of-speech tagging","source":"Weiwei Sun. 2011. A stacked sub-word model for joint Chinese word segmentation and part-of-speech tagging. In Proc. of ACL2011, pages 1385-1394."},{"authors":[],"source":"Weiwei Sun and Hans Uszkoreit. Capturing paradigmatic and syntagmatic lexical relations: Towards accurate Chinese part-of-speech tagging. In Proc. of ACL2012."},{"authors":[{"first":"Yiou","last":"Wang"},{"first":"Jun'ichi","last":"Kazama"},{"first":"Yoshimasa","last":"Tsuruoka"},{"first":"Wenliang","last":"Chen"},{"first":"Yujie","last":"Zhang"},{"first":"Kentaro","last":"Torisawa"}],"year":"2011","title":"Improving Chinese word segmentation and POS tagging with semi-supervised methods using large auto-analyzed data","source":"Yiou Wang, Jun'ichi Kazama, Yoshimasa Tsuruoka, Wenliang Chen, Yujie Zhang and Kentaro Torisawa. 2011. Improving Chinese word segmentation and POS tagging with semi-supervised methods using large auto-analyzed data. In Proc. of IJCNLP2011, pages 309-317."},{"authors":[{"first":"Nianwen","last":"Xue"}],"year":"2003","title":"Chinese word segmentation as character tagging","source":"Nianwen Xue. 2003. Chinese word segmentation as character tagging. Computational Linguistics and Chinese Language Processing, 8 (1). pages 29-48."},{"authors":[{"first":"Yue","last":"Zhang"},{"first":"Stephen","last":"Clark"}],"year":"2010","title":"A fast decoder for joint word segmentation and POS-tagging using a single discriminative model","source":"Yue Zhang and Stephen Clark. 2010. A fast decoder for joint word segmentation and POS-tagging using a single discriminative model. In Proc. of EMNLP2010, pages 843-852. 627"}],"cites":[{"style":0,"text":"Collins, 1999","origin":{"pointer":"/sections/6/paragraphs/0","offset":147,"length":13},"authors":[{"last":"Collins"}],"year":"1999","references":["/references/2"]},{"style":0,"text":"Charniak, 2000","origin":{"pointer":"/sections/6/paragraphs/0","offset":162,"length":14},"authors":[{"last":"Charniak"}],"year":"2000","references":["/references/1"]},{"style":0,"text":"Petrov and Klein, 2007","origin":{"pointer":"/sections/6/paragraphs/0","offset":178,"length":22},"authors":[{"last":"Petrov"},{"last":"Klein"}],"year":"2007","references":["/references/10"]},{"style":0,"text":"Sun and Uszkoreit, 2012","origin":{"pointer":"/sections/6/paragraphs/0","offset":1161,"length":23},"authors":[{"last":"Sun"},{"last":"Uszkoreit"}],"year":"2012","references":[]},{"style":0,"text":"Qian and Liu (2012)","origin":{"pointer":"/sections/6/paragraphs/1","offset":115,"length":19},"authors":[{"last":"Qian"},{"last":"Liu"}],"year":"2012","references":["/references/11"]},{"style":0,"text":"Jiang et al., 2008","origin":{"pointer":"/sections/7/paragraphs/3","offset":267,"length":18},"authors":[{"last":"Jiang"},{"last":"al."}],"year":"2008","references":["/references/5"]},{"style":0,"text":"Xue, 2003","origin":{"pointer":"/sections/7/paragraphs/4","offset":34,"length":9},"authors":[{"last":"Xue"}],"year":"2003","references":["/references/16"]},{"style":0,"text":"Wang et al. (2011)","origin":{"pointer":"/sections/7/paragraphs/4","offset":214,"length":18},"authors":[{"last":"Wang"},{"last":"al."}],"year":"2011","references":["/references/15"]},{"style":0,"text":"Jiang et al. (2008)","origin":{"pointer":"/sections/7/paragraphs/6","offset":252,"length":19},"authors":[{"last":"Jiang"},{"last":"al."}],"year":"2008","references":["/references/5"]},{"style":0,"text":"Collins, 2002","origin":{"pointer":"/sections/7/paragraphs/6","offset":339,"length":13},"authors":[{"last":"Collins"}],"year":"2002","references":["/references/3"]},{"style":0,"text":"Goldberg and Elhadad (2011)","origin":{"pointer":"/sections/7/paragraphs/7","offset":0,"length":27},"authors":[{"last":"Goldberg"},{"last":"Elhadad"}],"year":"2011","references":["/references/4"]},{"style":0,"text":"Matsuzaki et al., 2005","origin":{"pointer":"/sections/7/paragraphs/7","offset":99,"length":22},"authors":[{"last":"Matsuzaki"},{"last":"al."}],"year":"2005","references":["/references/8"]},{"style":0,"text":"Komodakis et al. (2007)","origin":{"pointer":"/sections/8/paragraphs/7","offset":86,"length":23},"authors":[{"last":"Komodakis"},{"last":"al."}],"year":"2007","references":["/references/6"]},{"style":0,"text":"Petrov and Klein, 2007","origin":{"pointer":"/sections/9/paragraphs/17","offset":15,"length":22},"authors":[{"last":"Petrov"},{"last":"Klein"}],"year":"2007","references":["/references/10"]},{"style":0,"text":"Qian and Liu (2012)","origin":{"pointer":"/sections/9/paragraphs/17","offset":156,"length":19},"authors":[{"last":"Qian"},{"last":"Liu"}],"year":"2012","references":["/references/11"]},{"style":0,"text":"Petrov et al., 2006","origin":{"pointer":"/sections/9/paragraphs/19","offset":139,"length":19},"authors":[{"last":"Petrov"},{"last":"al."}],"year":"2006","references":["/references/9"]},{"style":0,"text":"Qian and Liu, 2012","origin":{"pointer":"/sections/9/paragraphs/19","offset":1172,"length":18},"authors":[{"last":"Qian"},{"last":"Liu"}],"year":"2012","references":["/references/11"]},{"style":0,"text":"Qian and Liu, 2012","origin":{"pointer":"/sections/10/paragraphs/2","offset":1,"length":18},"authors":[{"last":"Qian"},{"last":"Liu"}],"year":"2012","references":["/references/11"]},{"style":0,"text":"Kruengkrai et al., 2009","origin":{"pointer":"/sections/10/paragraphs/7","offset":7,"length":23},"authors":[{"last":"Kruengkrai"},{"last":"al."}],"year":"2009","references":["/references/7"]},{"style":0,"text":"Zhang and Clark, 2010","origin":{"pointer":"/sections/10/paragraphs/7","offset":51,"length":21},"authors":[{"last":"Zhang"},{"last":"Clark"}],"year":"2010","references":["/references/17"]},{"style":0,"text":"Qian and Liu, 2012","origin":{"pointer":"/sections/10/paragraphs/7","offset":85,"length":18},"authors":[{"last":"Qian"},{"last":"Liu"}],"year":"2012","references":["/references/11"]},{"style":0,"text":"Sun, 2011","origin":{"pointer":"/sections/10/paragraphs/7","offset":123,"length":9},"authors":[{"last":"Sun"}],"year":"2011","references":["/references/13"]},{"style":0,"text":"Kruengkrai et al., 2009","origin":{"pointer":"/sections/10/paragraphs/8","offset":40,"length":23},"authors":[{"last":"Kruengkrai"},{"last":"al."}],"year":"2009","references":["/references/7"]},{"style":0,"text":"Zhang and Clark, 2010","origin":{"pointer":"/sections/10/paragraphs/8","offset":84,"length":21},"authors":[{"last":"Zhang"},{"last":"Clark"}],"year":"2010","references":["/references/17"]},{"style":0,"text":"Qian and Liu, 2012","origin":{"pointer":"/sections/10/paragraphs/8","offset":118,"length":18},"authors":[{"last":"Qian"},{"last":"Liu"}],"year":"2012","references":["/references/11"]},{"style":0,"text":"Sun, 2011","origin":{"pointer":"/sections/10/paragraphs/8","offset":157,"length":9},"authors":[{"last":"Sun"}],"year":"2011","references":["/references/13"]}]}
