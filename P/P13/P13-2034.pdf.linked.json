{"sections":[{"title":"","paragraphs":["Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 190–195, Sofia, Bulgaria, August 4-9 2013. c⃝2013 Association for Computational Linguistics"]},{"title":"Broadcast News Story Segmentation Using Manifold Learning on Latent Topic Distributions Xiaoming Lu","paragraphs":["1,2"]},{"title":", Lei Xie","paragraphs":["1∗"]},{"title":", Cheung-Chi Leung","paragraphs":["2"]},{"title":", Bin Ma","paragraphs":["2"]},{"title":", Haizhou Li","paragraphs":["2 1"]},{"title":"School of Computer Science, Northwestern Polytechnical University, China","paragraphs":["2"]},{"title":"Institute for Infocomm Research, A","paragraphs":["⋆"]},{"title":"STAR, Singapore","paragraphs":["luxiaomingnpu@gmail.com, lxie@nwpu.edu.cn, {ccleung,mabin,hli}@i2r.a-star.edu.sg"]},{"title":"Abstract","paragraphs":["We present an efficient approach for broadcast news story segmentation using a manifold learning algorithm on latent topic distributions. The latent topic distribution estimated by Latent Dirichlet Allocation (LDA) is used to represent each text block. We employ Laplacian Eigenmaps (LE) to project the latent topic distributions into low-dimensional semantic representations while preserving the intrinsic local geometric structure. We evaluate two approaches employing LDA and probabilistic latent semantic analysis (PLSA) distributions respectively. The effects of different amounts of training data and different numbers of latent topics on the two approaches are studied. Experimental results show that our proposed LDA-based approach can outperform the corresponding PLSA-based approach. The proposed approach provides the best performance with the highest F1-measure of 0.7860."]},{"title":"1 Introduction","paragraphs":["Story segmentation refers to partitioning a multimedia stream into homogenous segments each embodying a main topic or coherent story (Allan, 2002). With the explosive growth of multimedia data, it becomes difficult to retrieve the most relevant components. For indexing broadcast news programs, it is desirable to divide each of them into a number of independent stories. Manual segmentation is accurate but labor-intensive and costly. Therefore, automatic story segmentation approaches are highly demanded.","Lexical-cohesion based approaches have been widely studied for automatic broadcast news story segmentation (Beeferman et al., 1997; Choi, 1999; Hearst, 1997; Rosenberg and Hirschberg, 2006;","∗","corresponding author Lo et al., 2009; Malioutov and Barzilay, 2006; Yamron et al., 1999; Tur et al., 2001). In this kind of approaches, the audio portion of the data stream is passed to an automatic speech recognition (ASR) system. Lexical cues are extracted from the ASR transcripts. Lexical cohesion is the phenomenon that different stories tend to employ different sets of terms. Term repetition is one of the most common appearances.","These rigid lexical-cohesion based approaches simply take term repetition into consideration, while term association in lexical cohesion is ignored. Moreover, polysemy and synonymy are not considered. To deal with these problems, some topic model techniques which provide conceptual level matching have been introduced to text and story segmentation task (Hearst, 1997). Probabilistic latent semantic analysis (PLSA) (Hofmann, 1999) is a typical instance and used widely. PLSA is the probabilistic variant of latent semantic analysis (LSA) (Choi et al., 2001), and offers a more solid statistical foundation. PLSA provides more significant improvement than LSA for story segmentation (Lu et al., 2011; Blei and Moreno, 2001).","Despite the success of PLSA, there are concerns that the number of parameters in PLSA grows linearly with the size of the corpus. This makes PLSA not desirable if there is a considerable amount of data available, and causes serious over-fitting problems (Blei, 2012). To deal with this issue, Latent Dirichlet Allocation (LDA) (Blei et al., 2003) has been proposed. LDA has been proved to be effective in many segmentation tasks (Arora and Ravindran, 2008; Hall et al., 2008; Sun et al., 2008; Riedl and Biemann, 2012; Chien and Chueh, 2012).","Recent studies have shown that intrinsic dimensionality of natural text corpus is significant-ly lower than its ambient Euclidean space (Belkin and Niyogi, 2002; Xie et al., 2012). Therefore, 190 Laplacian Eigenmaps (LE) was proposed to compute corresponding natural low-dimensional structure. LE is a geometrically motivated dimensionality reduction method. It projects data into a low-dimensional representation while preserving the intrinsic local geometric structure information (Belkin and Niyogi, 2002). The locality preserving property attempts to make the low-dimensional data representation more robust to the noise from ASR errors (Xie et al., 2012).","To further improve the segmentation performance, using latent topic distributions and LE in-stead of term frequencies to represent text blocks is studied in this paper. We study the effects of the size of training data and the number of latent topics on the LDA-based and the PLSA-based approaches. Another related work (Lu et al., 2013) is to use local geometric information to regularize the log-likelihood computation in PLSA."]},{"title":"2 Our Proposed Approach","paragraphs":["In this paper, we propose to apply LE on the LDA topic distributions, each of which is estimated from a text block. The low-dimensional vectors obtained by LE projection are used to detect story boundaries through dynamic programming. Moreover, as in (Xie et al., 2012), we incorporate the temporal distances between block pairs as a penalty factor in the weight matrix. 2.1 Latent Dirichlet Allocation Latent Dirichlet allocation (LDA) (Blei et al., 2003) is a generative probabilistic model of a corpus. It considers that documents are represented as random mixtures over latent topics, where each topic is characterized by a distribution over terms.","In LDA, given a corpus D = {d1, d2, . . . , dM } and a set of terms W = (w1, w2, . . . , wV ), the generative process can be summarized as follows:","1) For each document d, pick a multinomial distribution θ from a Dirichlet distribution parameter α, denoted as θ ∼ Dir(α).","2) For each term w in document d, select a topic z from the multinomial distribution θ, denoted as z ∼ M ultinomial(θ).","3) Select a term w from P (w|z, β), which is a multinomial probability conditioned on the topic.","An LDA model is characterized by two sets of prior parameters α and β. α = (α1, α2, . . . , αK ) represents the Dirichlet prior distributions for each K latent topics. β is a K ×V matrix, which defines the latent topic distributions over terms. 2.2 Construction of weight matrix in","Laplacian Eigenmaps Laplacian Eigenmaps (LE) is introduced to project high-dimensional data into a low-dimensional representation while preserving its locality property. Given the ASR transcripts of N text blocks, we apply LDA algorithm to compute the corresponding latent topic distributions X = [x1, x2, . . . , xN ] in RK",", where K is the number of latent topics, namely the dimensionality of LDA distributions.","We use G to denote an N-node (N is number of LDA distributions) graph which represents the relationship between all the text block pairs. If distribution vectors xi and xj come from the same story, we put an edge between nodes i and j. We define a weight matrix S of the graph G to denote the cohesive strength between the text block pairs. Each element of this weight matrix is defined as:","sij = cos(xi, xj)μ|i−j| , (1) where μ|i−j|","serves the penalty factor for the distance between i and j. μ is a constant lower than 1.0 that we tune from a set of development data. It makes the cohesive strength of two text blocks dramatically decrease when their distance is much larger than the normal length of a story. 2.3 Data projection in Laplacian Eigenmaps Given the weight matrix S, we define C as the diagonal matrix with its element: cij = ∑K i=1 sij. (2) Finally, we obtain the Laplacian matrix L, which is defined as: L = C − S. (3)","We use Y = [y1, y2, . . . , yN ] (yi is a column vector) to indicate the low-dimensional representation of the latent topic distributions X. The projection from the latent topic distribution space to the target space can be defined as: f : xi ⇒ yi. (4)","A reasonable criterion for computing an optimal mapping is to minimize the objective as follows: K ∑ i=1 K ∑ j=1","∥ yi − yj ∥2 sij. (5)","Under this constraint condition, we can preserve the local geometrical property in LDA distributions. The objective function can be transformed 191 as: K ∑ i=1 K ∑ j=1","(yi − yj)sij = tr(YT LY). (6)","Meanwhile, zero matrix and matrices with its rank less than K are meaningless solutions for our task. We impose YT","LY = I to prevent this situation, where I is an identity matrix. By the Reyleigh-Ritz theorem (Lutkepohl, 1997), the solution can obtained by the Q smallest eigenvalues of the generalized eigenmaps problem:","XLXT y = λXCXT","y. (7) With this formula, we calculate the mapping matrix Y, and its row vectors y′","1, y′","2, . . . , y′","Q are in the order of their eigenvalues λ1 ≤ λ2 ≤ . . . ≤ λQ. y′ i is a Q-dimensional (Q<K) eigenvectors. 2.4 Story boundary detection In story boundary detection, dynamic programming (DP) approach is adopted to obtain the global optimal solution. Given the low-dimensional semantic representation of the test data, an objective function can be defined as follows: I = Ns ∑ t=1 ( ∑","i,j∈Seg t","∥ yi − yj ∥2 ), (8) where yi and yj are the latent topic distributions of text blocks i and j respectively, and ∥ yi − yj ∥2 is the Euclidean distance between them. Segt in-dicates these text blocks assigned to a certain hypothesized story. Ns is the number of hypothesized stories.","The story boundaries which minimize the objective function I in Eq.(8) form the optimal result. Compared with classical local optimal approach, DP can more effectively capture the s-mooth story shifts, and achieve better segmentation performance."]},{"title":"3 Experimental setup","paragraphs":["Our experiments were evaluated on the ASR transcripts provided in TDT2 English Broadcast news corpus1",", which involved 1033 news programs. We separated this corpus into three non-overlapping sets: a training set of 500 programs for parameter estimation in topic modeling and LE, a development set of 133 programs for empirical tuning and a test set of 400 programs for performance evaluation.","In the training stage, ASR transcripts with manually labeled boundary tags were provided. Text","1","http://projects.ldc.upenn.edu/TDT2/ streams were broken into block units according to the given boundary tags, with each text block being a complete story. In the segmentation stage, we divided test data into text blocks using the time labels of pauses in the transcripts. If the pause duration between two blocks last for more than 1.0 sec, it was considered as a boundary candidate. To avoid the segmentation being suffered from ASR errors and the out-of-vocabulary issue, phoneme bigram was used as the basic term unit (Xie et al., 2012). Since the ASR transcripts were at word level, we performed word-to-phoneme conversion to obtain the phoneme bigram basic units. The following approaches, in which DP was used in story boundary detection, were evaluated in the experiments:","• PLSA-DP: PLSA topic distributions were","used to compute sentence cohesive strength.","• LDA-DP: LDA topic distributions were used","to compute sentence cohesive strength.","• PLSA-LE-DP: PLSA topic distributions fol-","lowed by LE projection were used to com-","pute sentence cohesive strength.","• LDA-LE-DP: LDA topic distributions fol-","lowed by LE projection were used to com-","pute sentence cohesion strength.","For LDA, we used the implementation from David M. Blei’s webpage2",". For PLSA, we used the Lemur Toolkit3",".","F1-measure was used as the evaluation criterion.We followed the evaluation rule: a detected boundary candidate is considered correct if it lies within a 15 sec tolerant window on each side of a reference boundary. A number of parameters were set through empirical tuning on the developent set. The penalty factor was set to 0.8. When evaluating the effects of different size of the training set, the number of latent topics in topic modeling process was set to 64. After the number of latent topics was fixed, the dimensionality after LE projection was set to 32. When evaluating the effects of different number of latent topics in topic modeling computation, we fixed the size of the training set to 500 news programs and changed the number of latent topics from 16 to 256."]},{"title":"4 Experimental results and analysis 4.1 Effect of the size of training dataset","paragraphs":["We used the training set from 100 programs to 500","programs (adding 100 programs in each step) to e-2 http://www.cs.princeton.edu/ blei/lda-c/ 3 http://www.lemurproject.org/ 192 valuate the effects of different size of training data in both PLSA-based and LDA-based approaches. Figure 1 shows the results on the development set and the test set.                                              Figure 1: Segmentation performance with different amounts of training data","LDA-LE-DP approach achieved the best result (0.7927 and 0.7860) on both the development and the test sets, when there were 500 programs in the training set. This demonstrates that LDA model and LE projection used in combination is excellent for the story segmentation task. The LE projection applied on the latent topic representations made relatively 9.88% and 10.93% improvement over the LDA-based approach and the PLSA-based approach, respectively on the test set. We can reveal that employing LE on PLSA and LDA topic distributions achieves much better performance than the corresponding approaches without using LE.","We have compared the performances between PLSA and LDA. We found that when the training data size was small, PLSA performed better than LDA. Both PLSA-based and LDA-based approaches got better with the increase in the size of the training data set. All the four approaches had similar performances on the development set and the test set.","With the increase in the size of the training data, the LDA-based approaches were improved dramatically. They even outperformed the PLSA-based approaches when the training data contained more than 300 programs. This may be attributed to the fact that LDA needs more training data to estimate the parameters. When the training data is not enough, its parameters estimated in the training stage is not stable for the development and the test data. Moreover, compared with PLSA, the parameters in LDA do not grow linearly with the size of the corpus. 4.2 Effect of the number of latent topics We evaluated the F1-measure of the four approaches with different number of latent topics prior to LE projection. Figure 2 shows the corresponding results.                            Figure 2: Segmentation performance with different numbers of latent topics","The best performances (0.7816-0.7847) were achieved at the number of latent topics between 64 and 96. When the number of latent topics was increased from 16 to 64, F1-measure increased. When the number of latent topics was larger than 96, F1-measure decreased gradually. We found that the best results were achieved when the number of topics was close to the real number of topics. There are 80 manually labeled main topics in the test set.","We observe that LE projection makes the topic model more stable with different numbers of latent topics. The best and the worst performances differed by relatively 9.12% in LDA-DP and 7.97% in PLSA-DP. However, the relative difference of 2.79% and 2.46% were observed in LDA-LE-DP and PLSA-LE-DP respectively."]},{"title":"5 Conclusions","paragraphs":["Our proposed approach achieves the best F1-measure of 0.7860. In the task of story segmentation, we believe that LDA can avoid data overfitting problem when there is a sufficient amount of training data. This is also applicable to LDA-LE-LP. Moreover, we find that when we apply LE projection to latent topic distributions, the segmentation performances become less sensitive to the predefined number of latent topics. 193"]},{"title":"Acknowledgments","paragraphs":["This work is supported by the National Natural Science Foundation of China (61175018), the Natural Science Basic Research Plan of Shaanx-i Province (2011JM8009) and the Fok Ying Tung Education Foundation (131059)."]},{"title":"References","paragraphs":["J. Allan. 2002. Topic Detection and Tracking: Event-Based Information Organization. Kluwer Academic Publisher, Norwell, MA.","Doug Beeferman, Adam Berger, and John Lafferty. 1997. A Model of Lexical Attraction and repulsion. In Proceedings of the 8th Conference on European Chapter of the Association for Computational Linguistics (EACL), pp.373-380.","Freddy Y. Y. Choi. 2000. Advances in Domain In-dependent Linear Text Segmentation. In Proceedings of the 1st North American Chapter of the Association for Computational Linguistics Conference (NAACL), pp.26-33.","Thomas Hofmann. 1999. Probabilistic Latent Semantic Indexing. In Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR), pp.20-57.","Mimi Lu, Cheung-Chi Leung, Lei Xie, Bin Ma, Haizhou Li. 2011. Probabilistic Latent Semantic Analysis for Broadcast New Story Segmentation. In Proceedings of the 11th Annual Conference of the International Speech Communication Association (INTERSPEECH), pp.1109-1112.","David M. Blei. 2012. Probabilistic topic models. Communication of the ACM, vol. 55, pp.77-84.","David M. Blei, Andrew Y. Ng, Michael I. Jordan. 2003. Latent Dirichlet Allocation. the Journal of Machine Learning Research, vol. 3, pp.993-1022.","Marti A. Hearst. 1997. TextTiling: Segmenting Text into Multiparagraph subtopic passages. Computational Liguistic, vol. 23, pp.33-64.","Gokhan Tur, Dilek Hakkani-Tur, Andreas Stolcke, Elizabeth Shriberg. 2001. Integrating Prosodic and Lexicial Cues for Automatic Topic Segmentation. Computational Liguistic, vol. 27, pp.31-57.","Andrew Rosenberg and Julia Hirschberg. 2006. Story Segmentation of Broadcast News in English, Mandarin and Aribic. In Proceedings of the 7th North American Chapter of the Association for Computational Linguistics Conference (NAACL), pp.125-128.","David M. Blei and Pedro J. Moreno. 2001. Topic Segmentation with An Aspect Hidden Markov Model. In Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrival (SIGIR), pp.343-348.","Wai-Kit Lo, Wenying Xiong, Helen Meng. 2009. Automatic Story Segmentation Using a Bayesian Decision Framwork for Statistical Models of Lexical Chain Feature. In Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics (ACL), pp.357-364.","Igor Malioutov and Regina Barzilay. 2006. Minimum Cut Model for Spoken Lecture Segmenation. In Proceedings of the 44th Annual Meeting of the Association for Computational Linguistics (ACL), pp.25-32.","Freddy Y. Y. Choi, Peter Wiemer-Hastings, Juhanna Moore. 2001. Latent Semantic Analysis for Text Segmentation. In Proceedings of the 2001 Conference on Empirical Methods on Natural Language Processing (EMNLP), pp.109-117.","Rachit Arora and Balaraman Ravindran. 2008. Latent Dirichlet Allocation Based Multi-document Summarization. In Proceedings of the 2nd Workshop on Analytics for Noisy Unstructured Text Data (AND), pp.91-97.","David Hall, Daniel Jurafsky, Christopher D. Manning. 2008. Latent Studying the History Ideas Using Topic Models. In Proceedings of the 2008 Conference on Empirical Methods on Natural Language Processing (EMNLP), pp.363-371.","Qi Sun, Runxin Li, Dingsheng Luo, Xihong Wu. 2008. Text Segmentation with LDA-based Fisher Kernel. In Proceedings of the 46th Annual Meeting of the Assocation for Computational Linguistics on Human Language Technologies (HLT-ACL), pp.269-272.","Mikhail Belkin and Partha Niyogi. 2002. Laplacian Eigenmaps for Dimensionality Reduction and Data Representation. Neural Computation, vol. 15, pp.1383-1396.","Lei Xie, Lilei Zheng, Zihan Liu and Yanning Zhang. 2012. Laplacian Eigenmaps for Automatic Story Segmentation of Broadcast News. IEEE Transaction on Audio, Speech and Language Processing, vol. 20, pp.264-277.","Deng Cai, Qiaozhu Mei, Jiawei Han, and Chengxiang Zhai. 2008. Modeling Hidden Topics on Document Manifold. In Proceedings of the 17th ACM Conference on Information and Knowledge Managemen-t (CIKM), pp.911-120.","Xiaoming Lu, Cheung-Chi Leung, Lei Xie, Bin Ma, and Haizhou Li. 2013. Broadcast News Story Segmentation Using Latent Topics on Data Manifold. In Proceedings of the 38th International Conference on Acoustics, Speech, and Signal Processing (ICASSP). 194","J. P. Yamron, I. Carp, L. Gillick, S. Lowe, and P. van Mulbregt. 1999. A Hidden Markov Model Approach to Text Segmenation and Event Tracking. In Proceedings of the 1999 International Conference on Acoustics, Speech, and Signal Processing (ICASSP), pp.333-336.","Martin Riedl and Chris Biemann. 2012. Text Segmentation with Topic Models. the Journal for Language Technology and Computational Linguistics, pp.47-69.","P. Fragkou , V. Petridis , Ath. Kehagias. 2002. A Dynamic Programming algorithm for Linear Text Story Segmentation. the Joural of Intelligent Information Systems, vol. 23, pp.179-197.","H. Lutkepohl. 1997. Handbook of Matrices. Wiley, Chichester, UK.","Jen-Tzung Chien and Chuang-Hua Chueh. 2012. Topic-Based Hieraachical Segmentation. IEEE Transaction on Audio, Speech and Language Processing, vol. 20, pp.55-66. 195"]}],"references":[{"authors":[{"first":"J.","last":"Allan"}],"year":"2002","title":"Topic Detection and Tracking: Event-Based Information Organization","source":"J. Allan. 2002. Topic Detection and Tracking: Event-Based Information Organization. Kluwer Academic Publisher, Norwell, MA."},{"authors":[{"first":"Doug","last":"Beeferman"},{"first":"Adam","last":"Berger"},{"first":"John","last":"Lafferty"}],"year":"1997","title":"A Model of Lexical Attraction and repulsion","source":"Doug Beeferman, Adam Berger, and John Lafferty. 1997. A Model of Lexical Attraction and repulsion. In Proceedings of the 8th Conference on European Chapter of the Association for Computational Linguistics (EACL), pp.373-380."},{"authors":[{"first":"Freddy","middle":"Y. Y.","last":"Choi"}],"year":"2000","title":"Advances in Domain In-dependent Linear Text Segmentation","source":"Freddy Y. Y. Choi. 2000. Advances in Domain In-dependent Linear Text Segmentation. In Proceedings of the 1st North American Chapter of the Association for Computational Linguistics Conference (NAACL), pp.26-33."},{"authors":[{"first":"Thomas","last":"Hofmann"}],"year":"1999","title":"Probabilistic Latent Semantic Indexing","source":"Thomas Hofmann. 1999. Probabilistic Latent Semantic Indexing. In Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR), pp.20-57."},{"authors":[{"first":"Mimi","last":"Lu"},{"first":"Cheung-Chi","last":"Leung"},{"first":"Lei","last":"Xie"},{"first":"Bin","last":"Ma"},{"first":"Haizhou","last":"Li"}],"year":"2011","title":"Probabilistic Latent Semantic Analysis for Broadcast New Story Segmentation","source":"Mimi Lu, Cheung-Chi Leung, Lei Xie, Bin Ma, Haizhou Li. 2011. Probabilistic Latent Semantic Analysis for Broadcast New Story Segmentation. In Proceedings of the 11th Annual Conference of the International Speech Communication Association (INTERSPEECH), pp.1109-1112."},{"authors":[{"first":"David","middle":"M.","last":"Blei"}],"year":"2012","title":"Probabilistic topic models","source":"David M. Blei. 2012. Probabilistic topic models. Communication of the ACM, vol. 55, pp.77-84."},{"authors":[{"first":"David","middle":"M.","last":"Blei"},{"first":"Andrew","middle":"Y.","last":"Ng"},{"first":"Michael I","middle":".","last":"Jordan"}],"year":"2003","title":"Latent Dirichlet Allocation","source":"David M. Blei, Andrew Y. Ng, Michael I. Jordan. 2003. Latent Dirichlet Allocation. the Journal of Machine Learning Research, vol. 3, pp.993-1022."},{"authors":[{"first":"Marti","middle":"A.","last":"Hearst"}],"year":"1997","title":"TextTiling: Segmenting Text into Multiparagraph subtopic passages","source":"Marti A. Hearst. 1997. TextTiling: Segmenting Text into Multiparagraph subtopic passages. Computational Liguistic, vol. 23, pp.33-64."},{"authors":[{"first":"Gokhan","last":"Tur"},{"first":"Dilek","last":"Hakkani-Tur"},{"first":"Andreas","last":"Stolcke"},{"first":"Elizabeth","last":"Shriberg"}],"year":"2001","title":"Integrating Prosodic and Lexicial Cues for Automatic Topic Segmentation","source":"Gokhan Tur, Dilek Hakkani-Tur, Andreas Stolcke, Elizabeth Shriberg. 2001. Integrating Prosodic and Lexicial Cues for Automatic Topic Segmentation. Computational Liguistic, vol. 27, pp.31-57."},{"authors":[{"first":"Andrew","last":"Rosenberg"},{"first":"Julia","last":"Hirschberg"}],"year":"2006","title":"Story Segmentation of Broadcast News in English, Mandarin and Aribic","source":"Andrew Rosenberg and Julia Hirschberg. 2006. Story Segmentation of Broadcast News in English, Mandarin and Aribic. In Proceedings of the 7th North American Chapter of the Association for Computational Linguistics Conference (NAACL), pp.125-128."},{"authors":[{"first":"David","middle":"M.","last":"Blei"},{"first":"Pedro","middle":"J.","last":"Moreno"}],"year":"2001","title":"Topic Segmentation with An Aspect Hidden Markov Model","source":"David M. Blei and Pedro J. Moreno. 2001. Topic Segmentation with An Aspect Hidden Markov Model. In Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrival (SIGIR), pp.343-348."},{"authors":[{"first":"Wai-Kit","last":"Lo"},{"first":"Wenying","last":"Xiong"},{"first":"Helen","last":"Meng"}],"year":"2009","title":"Automatic Story Segmentation Using a Bayesian Decision Framwork for Statistical Models of Lexical Chain Feature","source":"Wai-Kit Lo, Wenying Xiong, Helen Meng. 2009. Automatic Story Segmentation Using a Bayesian Decision Framwork for Statistical Models of Lexical Chain Feature. In Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics (ACL), pp.357-364."},{"authors":[{"first":"Igor","last":"Malioutov"},{"first":"Regina","last":"Barzilay"}],"year":"2006","title":"Minimum Cut Model for Spoken Lecture Segmenation","source":"Igor Malioutov and Regina Barzilay. 2006. Minimum Cut Model for Spoken Lecture Segmenation. In Proceedings of the 44th Annual Meeting of the Association for Computational Linguistics (ACL), pp.25-32."},{"authors":[{"first":"Freddy","middle":"Y. Y.","last":"Choi"},{"first":"Peter","last":"Wiemer-Hastings"},{"first":"Juhanna","last":"Moore"}],"year":"2001","title":"Latent Semantic Analysis for Text Segmentation","source":"Freddy Y. Y. Choi, Peter Wiemer-Hastings, Juhanna Moore. 2001. Latent Semantic Analysis for Text Segmentation. In Proceedings of the 2001 Conference on Empirical Methods on Natural Language Processing (EMNLP), pp.109-117."},{"authors":[{"first":"Rachit","last":"Arora"},{"first":"Balaraman","last":"Ravindran"}],"year":"2008","title":"Latent Dirichlet Allocation Based Multi-document Summarization","source":"Rachit Arora and Balaraman Ravindran. 2008. Latent Dirichlet Allocation Based Multi-document Summarization. In Proceedings of the 2nd Workshop on Analytics for Noisy Unstructured Text Data (AND), pp.91-97."},{"authors":[{"first":"David","last":"Hall"},{"first":"Daniel","last":"Jurafsky"},{"first":"Christopher","middle":"D.","last":"Manning"}],"year":"2008","title":"Latent Studying the History Ideas Using Topic Models","source":"David Hall, Daniel Jurafsky, Christopher D. Manning. 2008. Latent Studying the History Ideas Using Topic Models. In Proceedings of the 2008 Conference on Empirical Methods on Natural Language Processing (EMNLP), pp.363-371."},{"authors":[{"first":"Qi","last":"Sun"},{"first":"Runxin","last":"Li"},{"first":"Dingsheng","last":"Luo"},{"first":"Xihong","last":"Wu"}],"year":"2008","title":"Text Segmentation with LDA-based Fisher Kernel","source":"Qi Sun, Runxin Li, Dingsheng Luo, Xihong Wu. 2008. Text Segmentation with LDA-based Fisher Kernel. In Proceedings of the 46th Annual Meeting of the Assocation for Computational Linguistics on Human Language Technologies (HLT-ACL), pp.269-272."},{"authors":[{"first":"Mikhail","last":"Belkin"},{"first":"Partha","last":"Niyogi"}],"year":"2002","title":"Laplacian Eigenmaps for Dimensionality Reduction and Data Representation","source":"Mikhail Belkin and Partha Niyogi. 2002. Laplacian Eigenmaps for Dimensionality Reduction and Data Representation. Neural Computation, vol. 15, pp.1383-1396."},{"authors":[{"first":"Lei","last":"Xie"},{"first":"Lilei","last":"Zheng"},{"first":"Zihan","last":"Liu"},{"first":"Yanning","last":"Zhang"}],"year":"2012","title":"Laplacian Eigenmaps for Automatic Story Segmentation of Broadcast News","source":"Lei Xie, Lilei Zheng, Zihan Liu and Yanning Zhang. 2012. Laplacian Eigenmaps for Automatic Story Segmentation of Broadcast News. IEEE Transaction on Audio, Speech and Language Processing, vol. 20, pp.264-277."},{"authors":[{"first":"Deng","last":"Cai"},{"first":"Qiaozhu","last":"Mei"},{"first":"Jiawei","last":"Han"},{"first":"Chengxiang","last":"Zhai"}],"year":"2008","title":"Modeling Hidden Topics on Document Manifold","source":"Deng Cai, Qiaozhu Mei, Jiawei Han, and Chengxiang Zhai. 2008. Modeling Hidden Topics on Document Manifold. In Proceedings of the 17th ACM Conference on Information and Knowledge Managemen-t (CIKM), pp.911-120."},{"authors":[{"first":"Xiaoming","last":"Lu"},{"first":"Cheung-Chi","last":"Leung"},{"first":"Lei","last":"Xie"},{"first":"Bin","last":"Ma"},{"first":"Haizhou","last":"Li"}],"year":"2013","title":"Broadcast News Story Segmentation Using Latent Topics on Data Manifold","source":"Xiaoming Lu, Cheung-Chi Leung, Lei Xie, Bin Ma, and Haizhou Li. 2013. Broadcast News Story Segmentation Using Latent Topics on Data Manifold. In Proceedings of the 38th International Conference on Acoustics, Speech, and Signal Processing (ICASSP). 194"},{"authors":[{"first":"J.","middle":"P.","last":"Yamron"},{"first":"I.","last":"Carp"},{"first":"L.","last":"Gillick"},{"first":"S.","last":"Lowe"},{"first":"P.","last":"van Mulbregt"}],"year":"1999","title":"A Hidden Markov Model Approach to Text Segmenation and Event Tracking","source":"J. P. Yamron, I. Carp, L. Gillick, S. Lowe, and P. van Mulbregt. 1999. A Hidden Markov Model Approach to Text Segmenation and Event Tracking. In Proceedings of the 1999 International Conference on Acoustics, Speech, and Signal Processing (ICASSP), pp.333-336."},{"authors":[{"first":"Martin","last":"Riedl"},{"first":"Chris","last":"Biemann"}],"year":"2012","title":"Text Segmentation with Topic Models","source":"Martin Riedl and Chris Biemann. 2012. Text Segmentation with Topic Models. the Journal for Language Technology and Computational Linguistics, pp.47-69."},{"authors":[{"first":"P.","last":"Fragkou"},{"first":"V.","last":"Petridis"},{"first":"Ath.","last":"Kehagias"}],"year":"2002","title":"A Dynamic Programming algorithm for Linear Text Story Segmentation","source":"P. Fragkou , V. Petridis , Ath. Kehagias. 2002. A Dynamic Programming algorithm for Linear Text Story Segmentation. the Joural of Intelligent Information Systems, vol. 23, pp.179-197."},{"authors":[{"first":"H.","last":"Lutkepohl"}],"year":"1997","title":"Handbook of Matrices","source":"H. Lutkepohl. 1997. Handbook of Matrices. Wiley, Chichester, UK."},{"authors":[{"first":"Jen-Tzung","last":"Chien"},{"first":"Chuang-Hua","last":"Chueh"}],"year":"2012","title":"Topic-Based Hieraachical Segmentation","source":"Jen-Tzung Chien and Chuang-Hua Chueh. 2012. Topic-Based Hieraachical Segmentation. IEEE Transaction on Audio, Speech and Language Processing, vol. 20, pp.55-66. 195"}],"cites":[{"style":0,"text":"Allan, 2002","origin":{"pointer":"/sections/10/paragraphs/0","offset":134,"length":11},"authors":[{"last":"Allan"}],"year":"2002","references":["/references/0"]},{"style":0,"text":"Beeferman et al., 1997","origin":{"pointer":"/sections/10/paragraphs/1","offset":108,"length":22},"authors":[{"last":"Beeferman"},{"last":"al."}],"year":"1997","references":["/references/1"]},{"style":0,"text":"Choi, 1999","origin":{"pointer":"/sections/10/paragraphs/1","offset":132,"length":10},"authors":[{"last":"Choi"}],"year":"1999","references":[]},{"style":0,"text":"Hearst, 1997","origin":{"pointer":"/sections/10/paragraphs/1","offset":144,"length":12},"authors":[{"last":"Hearst"}],"year":"1997","references":["/references/7"]},{"style":0,"text":"Rosenberg and Hirschberg, 2006","origin":{"pointer":"/sections/10/paragraphs/1","offset":158,"length":30},"authors":[{"last":"Rosenberg"},{"last":"Hirschberg"}],"year":"2006","references":["/references/9"]},{"style":0,"text":"Lo et al., 2009","origin":{"pointer":"/sections/10/paragraphs/3","offset":21,"length":15},"authors":[{"last":"Lo"},{"last":"al."}],"year":"2009","references":["/references/11"]},{"style":0,"text":"Malioutov and Barzilay, 2006","origin":{"pointer":"/sections/10/paragraphs/3","offset":38,"length":28},"authors":[{"last":"Malioutov"},{"last":"Barzilay"}],"year":"2006","references":["/references/12"]},{"style":0,"text":"Yamron et al., 1999","origin":{"pointer":"/sections/10/paragraphs/3","offset":68,"length":19},"authors":[{"last":"Yamron"},{"last":"al."}],"year":"1999","references":["/references/21"]},{"style":0,"text":"Tur et al., 2001","origin":{"pointer":"/sections/10/paragraphs/3","offset":89,"length":16},"authors":[{"last":"Tur"},{"last":"al."}],"year":"2001","references":["/references/8"]},{"style":0,"text":"Hearst, 1997","origin":{"pointer":"/sections/10/paragraphs/4","offset":356,"length":12},"authors":[{"last":"Hearst"}],"year":"1997","references":["/references/7"]},{"style":0,"text":"Hofmann, 1999","origin":{"pointer":"/sections/10/paragraphs/4","offset":418,"length":13},"authors":[{"last":"Hofmann"}],"year":"1999","references":["/references/3"]},{"style":0,"text":"Choi et al., 2001","origin":{"pointer":"/sections/10/paragraphs/4","offset":541,"length":17},"authors":[{"last":"Choi"},{"last":"al."}],"year":"2001","references":["/references/13"]},{"style":0,"text":"Lu et al., 2011","origin":{"pointer":"/sections/10/paragraphs/4","offset":685,"length":15},"authors":[{"last":"Lu"},{"last":"al."}],"year":"2011","references":["/references/4"]},{"style":0,"text":"Blei and Moreno, 2001","origin":{"pointer":"/sections/10/paragraphs/4","offset":702,"length":21},"authors":[{"last":"Blei"},{"last":"Moreno"}],"year":"2001","references":["/references/10"]},{"style":0,"text":"Blei, 2012","origin":{"pointer":"/sections/10/paragraphs/5","offset":255,"length":10},"authors":[{"last":"Blei"}],"year":"2012","references":["/references/5"]},{"style":0,"text":"Blei et al., 2003","origin":{"pointer":"/sections/10/paragraphs/5","offset":328,"length":17},"authors":[{"last":"Blei"},{"last":"al."}],"year":"2003","references":["/references/6"]},{"style":0,"text":"Arora and Ravindran, 2008","origin":{"pointer":"/sections/10/paragraphs/5","offset":430,"length":25},"authors":[{"last":"Arora"},{"last":"Ravindran"}],"year":"2008","references":["/references/14"]},{"style":0,"text":"Hall et al., 2008","origin":{"pointer":"/sections/10/paragraphs/5","offset":457,"length":17},"authors":[{"last":"Hall"},{"last":"al."}],"year":"2008","references":["/references/15"]},{"style":0,"text":"Sun et al., 2008","origin":{"pointer":"/sections/10/paragraphs/5","offset":476,"length":16},"authors":[{"last":"Sun"},{"last":"al."}],"year":"2008","references":["/references/16"]},{"style":0,"text":"Riedl and Biemann, 2012","origin":{"pointer":"/sections/10/paragraphs/5","offset":494,"length":23},"authors":[{"last":"Riedl"},{"last":"Biemann"}],"year":"2012","references":["/references/22"]},{"style":0,"text":"Chien and Chueh, 2012","origin":{"pointer":"/sections/10/paragraphs/5","offset":519,"length":21},"authors":[{"last":"Chien"},{"last":"Chueh"}],"year":"2012","references":["/references/25"]},{"style":0,"text":"Belkin and Niyogi, 2002","origin":{"pointer":"/sections/10/paragraphs/6","offset":137,"length":23},"authors":[{"last":"Belkin"},{"last":"Niyogi"}],"year":"2002","references":["/references/17"]},{"style":0,"text":"Xie et al., 2012","origin":{"pointer":"/sections/10/paragraphs/6","offset":162,"length":16},"authors":[{"last":"Xie"},{"last":"al."}],"year":"2012","references":["/references/18"]},{"style":0,"text":"Belkin and Niyogi, 2002","origin":{"pointer":"/sections/10/paragraphs/6","offset":484,"length":23},"authors":[{"last":"Belkin"},{"last":"Niyogi"}],"year":"2002","references":["/references/17"]},{"style":0,"text":"Xie et al., 2012","origin":{"pointer":"/sections/10/paragraphs/6","offset":642,"length":16},"authors":[{"last":"Xie"},{"last":"al."}],"year":"2012","references":["/references/18"]},{"style":0,"text":"Lu et al., 2013","origin":{"pointer":"/sections/10/paragraphs/7","offset":321,"length":15},"authors":[{"last":"Lu"},{"last":"al."}],"year":"2013","references":["/references/20"]},{"style":0,"text":"Xie et al., 2012","origin":{"pointer":"/sections/11/paragraphs/0","offset":252,"length":16},"authors":[{"last":"Xie"},{"last":"al."}],"year":"2012","references":["/references/18"]},{"style":0,"text":"Blei et al., 2003","origin":{"pointer":"/sections/11/paragraphs/0","offset":438,"length":17},"authors":[{"last":"Blei"},{"last":"al."}],"year":"2003","references":["/references/6"]},{"style":0,"text":"Lutkepohl, 1997","origin":{"pointer":"/sections/11/paragraphs/17","offset":95,"length":15},"authors":[{"last":"Lutkepohl"}],"year":"1997","references":["/references/24"]},{"style":0,"text":"Xie et al., 2012","origin":{"pointer":"/sections/12/paragraphs/4","offset":523,"length":16},"authors":[{"last":"Xie"},{"last":"al."}],"year":"2012","references":["/references/18"]}]}
