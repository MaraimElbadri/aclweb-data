{"sections":[{"title":"","paragraphs":["Proceedings of the ACL 2010 Conference Short Papers, pages 275–280, Uppsala, Sweden, 11-16 July 2010. c⃝2010 Association for Computational Linguistics"]},{"title":"Jointly optimizing a two-step conditional random field model for machine transliteration and its fast decoding algorithm Dong Yang, Paul Dixon and Sadaoki Furui Department of Computer Science Tokyo Institute of Technology Tokyo 152-8552 Japan {raymond,dixonp,furui}@furui.cs.titech.ac.jp Abstract","paragraphs":["This paper presents a joint optimization method of a two-step conditional random field (CRF) model for machine transliteration and a fast decoding algorithm for the proposed method. Our method lies in the category of direct orthographical mapping (DOM) between two languages without using any intermediate phonemic mapping. In the two-step CRF model, the first CRF segments an input word into chunks and the second one converts each chunk into one unit in the target language. In this paper, we propose a method to jointly optimize the two-step CRFs and also a fast algorithm to realize it. Our experiments show that the proposed method outperforms the well-known joint source channel model (JSCM) and our proposed fast algorithm decreases the decoding time significantly. Furthermore, combination of the proposed method and the JSCM gives further improvement, which outperforms state-of-the-art results in terms of top-1 accuracy."]},{"title":"1 Introduction","paragraphs":["There are more than 6000 languages in the world and 10 languages of them have more than 100 million native speakers. With the information revolu-tion and globalization, systems that support multiple language processing and spoken language translation become urgent demands. The translation of named entities from alphabetic to syllabary language is usually performed through transliteration, which tries to preserve the pronunciation in the original language.","For example, in Chinese, foreign words are written with Chinese characters; in Japanese, foreign words are usually written with special char-G o o g l e English-to-Japanese G o o g l e English-to-Chinese Source Name Target Name Note gu ge Chinese Romanized writing guu gu ru Japanese Romanized writing Figure 1: Transliteration examples acters called Katakana; examples are given in Figure 1.","An intuitive transliteration method (Knight and Graehl, 1998; Oh et al., 2006) is to firstly convert a source word into phonemes, then find the corresponding phonemes in the target language, and finally convert them to the target language’s written system. There are two reasons why this method does not work well: first, the named entities have diverse origins and this makes the grapheme-to-phoneme conversion very difficult; second, the transliteration is usually not only determined by the pronunciation, but also affected by how they are written in the original language.","Direct orthographical mapping (DOM), which performs the transliteration between two languages directly without using any intermediate phonemic mapping, is recently gaining more attention in the transliteration research community, and it is also the “Standard Run” of the “NEWS 2009 Machine Transliteration Shared Task” (Li et al., 2009). In this paper, we try to make our system satisfy the standard evaluation condition, which requires that the system uses the provided parallel corpus (without pronunciation) only, and cannot use any other bilingual or monolingual resources.","The source channel and joint source channel models (JSCMs) (Li et al., 2004) have been proposed for DOM, which try to model P (T |S) and P (T, S) respectively, where T and S denote the words in the target and source languages. Ekbal et al. (2006) modified the JSCM to incorporate different context information into the model for 275 Indian languages. In the “NEWS 2009 Machine Transliteration Shared Task”, a new two-step CRF model for transliteration task has been proposed (Yang et al., 2009), in which the first step is to segment a word in the source language into character chunks and the second step is to perform a context-dependent mapping from each chunk into one written unit in the target language.","In this paper, we propose to jointly optimize a two-step CRF model. We also propose a fast decoding algorithm to speed up the joint search. The rest of this paper is organized as follows: Section 2 explains the two-step CRF method, followed by Section 3 which describes our joint optimization method and its fast decoding algorithm; Section 4 introduces a rapid implementation of a JSCM system in the weighted finite state transducer (WFST) framework; and the last section reports the experimental results and conclusions. Although our method is language independent, we use an English-to-Chinese transliteration task in all the explanations and experiments."]},{"title":"2 Two-step CRF method 2.1 CRF introduction","paragraphs":["A chain-CRF (Lafferty et al., 2001) is an undirected graphical model which assigns a probability to a label sequence L = l1l2 . . . lT , given an input sequence C = c1c2 . . . cT . CRF training is usually performed through the L-BFGS algorithm (Wallach, 2002) and decoding is performed by the Viterbi algorithm. We formalize machine transliteration as a CRF tagging problem, as shown in Figure 2. T i m o t h y T/B i/N m/B o/N t/B h/N y/N Ti/ mo/ thy/ Figure 2: An pictorial description of a CRF segmenter and a CRF converter 2.2 CRF segmenter In the CRF, a feature function describes a cooccurrence relation, and it is usually a binary function, taking the value 1 when both an observa-tion and a label transition are observed. Yang et al. (2009) used the following features in the segmentation tool: • Single unit features: C−2, C−1, C0, C1, C2 • Combination features: C−1C0, C0C1 Here, C0 is the current character, C−1 and C1 denote the previous and next characters, and C−2 and C2 are the characters located two positions to the left and right of C0.","One limitation of their work is that only top-1 segmentation is output to the following CRF converter. 2.3 CRF converter Similar to the CRF segmenter, the CRF converter has the format shown in Figure 2.","For this CRF, Yang et al. (2009) used the following features: • Single unit features: CK−1, CK0, CK1","• Combination features: CK−1CK0, CK0CK1 where CK represents the source language chunk, and the subscript notation is the same as the CRF segmenter."]},{"title":"3 Joint optimization and its fast decoding algorithm 3.1 Joint optimization","paragraphs":["We denote a word in the source language by S, a segmentation of S by A, and a word in the target langauge by T . Our goal is to find the best word T̂ in the target language which maximizes the probability P (T |S).","Yang et al. (2009) used only the best segmentation in the first CRF and the best output in the second CRF, which is equivalent to","Â = arg max A P (A|S)","T̂ = arg max T P (T |S, Â), (1) where P (A|S) and P (T |S, A) represent two CRFs respectively. This method considers the segmentation and the conversion as two independent steps. A major limitation is that, if the segmentation from the first step is wrong, the error propagates to the second step, and the error is very difficult to recover.","In this paper, we propose a new method to jointly optimize the two-step CRF, which can be 276 written as:","T̂ = arg max T P (T |S)","= arg max T ∑ A P (T, A|S)","= arg max T ∑ A P (A|S)P (T |S, A) (2)","The joint optimization considers all the segmentation possibilities and sums the probability over all the alternative segmentations which generate the same output. It considers the segmentation and conversion in a unified framework and is robust to segmentation errors. 3.2 N-best approximation In the process of finding the best output using Equation 2, a dynamic programming algorithm for joint decoding of the segmentation and conversion is possible, but the implementation becomes very complicated. Another direction is to divide the decoding into two steps of segmentation and conversion, which is this paper’s method. However, exact inference by listing all possible candidates explicitly and summing over all possible segmentations is intractable, because of the exponential computa-tion complexity with the source word’s increasing length.","In the segmentation step, the number of possible segmentations is 2N",", where N is the length of the source word and 2 is the size of the tagging set. In the conversion step, the number of possible candidates is M N′",", where N ′","is the number of chunks from the 1st step and M is the size of the tagging set. M is usually large, e.g., about 400 in Chinese and 50 in Japanese, and it is impossible to list all the candidates.","Our analysis shows that beyond the 10th candidate, almost all the probabilities of the candidates in both steps drop below 0.01. Therefore we decided to generate top-10 results for both steps to approximate the Equation 2. 3.3 Fast decoding algorithm As introduced in the previous subsection, in the whole decoding process we have to perform n-best CRF decoding in the segmentation step and 10 n-best CRF decoding in the second CRF. Is it really necessary to perform the second CRF for all the segmentations? The answer is “No” for candidates with low probabilities. Here we propose a no-loss fast decoding algorithm for deciding when to stop performing the second CRF decoding.","Suppose we have a list of segmentation candidates which are generated by the 1st CRF, ranked by probabilities P (A|S) in descending order A : A1, A2, ..., AN and we are performing the 2nd CRF decoding starting from A1. Up to Ak, we get a list of candidates T : T1, T2, ..., TL, ranked by probabilities in descending order. If we can guarantee that, even performing the 2nd CRF decoding for all the remaining segmentations Ak+1, Ak+2, ..., AN , the top 1 candidate does not change, then we can stop decoding.","We can show that the following formula is the stop condition: Pk(T1|S) − Pk(T2|S) > 1 − k ∑ j=1 P (Aj |S). (3)","The meaning of this formula is that the probability of all the remaining candidates is smaller than the probability difference between the best and the second best candidates; on the other hand, even if all the remaining probabilities are added to the second best candidate, it still cannot overturn the top candidate. The mathematical proof is provided in Appendix A.","The stop condition here has no approximation nor pre-defined assumption, and it is a no-loss fast decoding algorithm."]},{"title":"4 Rapid development of a JSCM system","paragraphs":["The JSCM represents how the source words and target names are generated simultaneously (Li et al., 2004): P (S, T ) = P (s1, s2, ..., sk, t1, t2, ..., tk) = P (< s, t >1, < s, t >2, ..., < s, t >k) = K ∏ k=1","P (< s, t >k | < s, t >k−1 1 ) (4) where S = (s1, s2, ..., sk) is a word in the source langauge and T = (t1, t2, ..., tk) is a word in the target language.","The training parallel data without alignment is first aligned by a Viterbi version EM algorithm (Li et al., 2004).","The decoding problem in JSCM can be written as:","T̂ = arg max T P (S, T ). (5) 277","After the alignments are generated, we use the MITLM toolkit (Hsu and Glass, 2008) to build a trigram model with modified Kneser-Ney smoothing. We then convert the n-gram to a WFST M (Sproat et al., 2000; Caseiro et al., 2002). To allow transliteration from a sequence of characters, a second WFST T is constructed. The input word is converted to an acceptor I, and it is then combined with T and M according to O = I ◦ T ◦ M where ◦ denotes the composition operator. The n–best paths are extracted by projecting the output, removing the epsilon labels and applying the n-shortest paths algorithm with determinization in the OpenFst Toolkit (Allauzen et al., 2007)."]},{"title":"5 Experiments","paragraphs":["We use several metrics from (Li et al., 2009) to measure the performance of our system.","1. Top-1 ACC: word accuracy of the top-1 candidate","2. Mean F-score: fuzziness in the top-1 candidate, how close the top-1 candidate is to the reference","3. MRR: mean reciprocal rank, 1/MRR tells approximately the average rank of the correct result 5.1 Comparison with the baseline and JSCM We use the training, development and test sets of NEWS 2009 data for English-to-Chinese in our experiments as detailed in Table 1. This is a parallel corpus without alignment. Training data Development data Test data 31961 2896 2896","Table 1: Corpus size (number of word pairs)","We compare the proposed decoding method with the baseline which uses only the best candidates in both CRF steps, and also with the well known JSCM. As we can see in Table 2, the proposed method improves the baseline top-1 ACC from 0.670 to 0.708, and it works as well as, or even better than the well known JSCM in all the three measurements.","Our experiments also show that the decoding time can be reduced significantly via using our fast decoding algorithm. As we have explained, without fast decoding, we need 11 CRF n-best decoding for each word; the number can be reduced to 3.53 (1 “the first CRF”+2.53 “the second CRF”) via the fast decoding algorithm.","We should notice that the decoding time is significantly shorter than the training time. While testing takes minutes on a normal PC, the training of the CRF converter takes up to 13 hours on an 8-core (8*3G Hz) server. Measure Top-1 Mean MRR","ACC F-score Baseline 0.670 0.869 0.750 Joint optimization 0.708 0.885 0.789 JSCM 0.706 0.882 0.789 Table 2: Comparison of the proposed decoding method with the previous method and the JSCM 5.2 Further improvement We tried to combine the two-step CRF model and the JSCM. From the two-step CRF model we get the conditional probability PCRF (T |S) and from the JSCM we get the joint probability P (S, T ). The conditional probability of PJSCM (T |S) can be calculuated as follows: PJSCM (T |S) = P (T, S) P (S) = P (T, S)","∑ T P (T, S) . (6) They are used in our combination method as: P (T |S) = λPCRF (T |S) + (1 − λ)PJSCM (T |S)","(7) where λ denotes the interpolation weight (λ is set by development data in this paper).","As we can see in Table 3, the linear combination of two sytems further improves the top-1 ACC to 0.720, and it has outperformed the best reported “Standard Run” (Li et al., 2009) result 0.717. (The reported best “Standard Run” result 0.731 used target language phoneme information, which requires a monolingual dictionary; as a result it is not a standard run.) Measure Top-1 Mean MRR","ACC F-score Baseline+JSCM 0.713 0.883 0.794 Joint optimization + JSCM 0.720 0.888 0.797 state-of-the-art 0.717 0.890 0.785 (Li et al., 2009) Table 3: Model combination results"]},{"title":"6 Conclusions and future work","paragraphs":["In this paper we have presented our new joint optimization method for a two-step CRF model and its fast decoding algorithm. The proposed 278 method improved the system significantly and outperformed the JSCM. Combining the proposed method with JSCM, the performance was further improved.","In future work we are planning to combine our system with multilingual systems. Also we want to make use of acoustic information in machine transliteration. We are currently investigating discriminative training as a method to further improve the JSCM. Another issue of our two-step CRF method is that the training complexity in-creases quadratically according to the size of the label set, and how to reduce the training time needs more research."]},{"title":"Appendix A. Proof of Equation 3","paragraphs":["The CRF segmentation provides a list of segmentations: A : A1, A2, ..., AN , with conditional probabilities P (A1|S), P (A2|S), ..., P (AN |S). N ∑ j=1 P (Aj |S) = 1.","The CRF conversion, given a segmentation Ai, provides a list of transliteration output T1, T2, ..., TM , with conditional probabilities P (T1|S, Ai), P (T2|S, Ai), ..., P (TM |S, Ai).","In our fast decoding algorithm, we start performing the CRF conversion from A1, then A2, and then A3, etc. Up to Ak, we get a list of candidates T : T1, T2, ..., TL, ranked by probabilities Pk(T |S) in descending order. The probability Pk(Tl|S)(l = 1, 2, ..., L) is accumulated probability of P (Tl|S) over A1, A2, ..., Ak, calculated by: Pk(Tl|S) = k ∑ j=1 P (Aj |S)P (Tl|S, Aj )","If we continue performing the CRF conversion to cover all N (N ≥ k) segmentations, eventually we will get: P (Tl|S) = N ∑ j=1 P (Aj |S)P (Tl|S, Aj ) ≥ k ∑ j=1 P (Aj |S)P (Tl|S, Aj ) = Pk(Tl|S) (8) If Equation 3 holds, then for ∀i ̸= 1, Pk(T1|S) > Pk(T2|S) + (1 − k ∑ j=1 P (Aj |S)) ≥ Pk(Ti|S) + (1 − k ∑ j=1 P (Aj |S)) = Pk(Ti|S) + N ∑ j=k+1 P (Aj |S) ≥ Pk(Ti|S) + N ∑ j=k+1 P (Aj |S)P (Ti|S, Aj ) = P (Ti|S) (9)","Therefore, P (T1|S) > P (Ti|S)(i ̸= 1), and T1 maximizes the probability P (T |S). 279"]},{"title":"References","paragraphs":["Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wojciech Skut and Mehryar Mohri 2007. OpenFst: A General and Efficient Weighted Finite-State Transducer Library. Proceedings of the Ninth International Conference on Implementation and Applica-tion of Automata, (CIAA), pages 11-23.","Diamantino Caseiro, Isabel Trancosoo, Luis Oliveira and Ceu Viana 2002. Grapheme-to-phone using finite state transducers. Proceedings IEEE Workshop on Speech Synthesis.","Asif Ekbal, Sudip Kumar Naskar and Sivaji Bandyopadhyay. 2006. A modified joint source-channel model for transliteration, Proceedings of the COL-ING/ACL, pages 191-198.","Bo-June Hsu and James Glass 2008. Iterative Language Model Estimation: Efficient Data Structure & Algorithms. Proceedings Interspeech, pages 841-844.","Kevin Knight and Jonathan Graehl. 1998. Machine Transliteration, Association for Computational Linguistics.","John Lafferty, Andrew McCallum, and Fernando Pereira 2001. Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data., Proceedings of International Conference on Machine Learning, pages 282-289.","Haizhou Li, Min Zhang and Jian Su. 2004. A joint source-channel model for machine transliteration, Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics.","Haizhou Li, A. Kumaran, Vladimir Pervouchine and Min Zhang 2009. Report of NEWS 2009 Machine Transliteration Shared Task, Proceedings of the 2009 Named Entities Workshop: Shared Task on Transliteration (NEWS 2009), pages 1-18","Jong-Hoon Oh, Key-Sun Choi and Hitoshi Isahara. 2006. A comparison of different machine transliteration models , Journal of Artificial Intelligence Research, 27, pages 119-151.","Richard Sproat 2000. Corpus-Based Methods and Hand-Built Methods. Proceedings of International Conference on Spoken Language Processing, pages 426-428.","Andrew J. Viterbi 1967. Error Bounds for Convolu-tional Codes and an Asymptotically Optimum De-coding Algorithm. IEEE Transactions on Informa-tion Theory, Volume IT-13, pages 260-269.","Hanna Wallach 2002. Efficient Training of Conditional Random Fields. M. Thesis, University of Edinburgh.","Dong Yang, Paul Dixon, Yi-Cheng Pan, Tasuku Oonishi, Masanobu Nakamura and Sadaoki Furui 2009. Combining a Two-step Conditional Random Field Model and a Joint Source Channel Model for Machine Transliteration, Proceedings of the 2009 Named Entities Workshop: Shared Task on Transliteration (NEWS 2009), pages 72-75 280"]}],"references":[{"authors":[{"first":"Cyril","last":"Allauzen"},{"first":"Michael","last":"Riley"},{"first":"Johan","last":"Schalkwyk"},{"first":"Wojciech","last":"Skut"},{"first":"Mehryar","last":"Mohri"}],"year":"2007","title":"OpenFst: A General and Efficient Weighted Finite-State Transducer Library","source":"Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wojciech Skut and Mehryar Mohri 2007. OpenFst: A General and Efficient Weighted Finite-State Transducer Library. Proceedings of the Ninth International Conference on Implementation and Applica-tion of Automata, (CIAA), pages 11-23."},{"authors":[{"first":"Diamantino","last":"Caseiro"},{"first":"Isabel","last":"Trancosoo"},{"first":"Luis","last":"Oliveira"},{"first":"Ceu","last":"Viana"}],"year":"2002","title":"Grapheme-to-phone using finite state transducers","source":"Diamantino Caseiro, Isabel Trancosoo, Luis Oliveira and Ceu Viana 2002. Grapheme-to-phone using finite state transducers. Proceedings IEEE Workshop on Speech Synthesis."},{"authors":[{"first":"Asif","last":"Ekbal"},{"first":"Sudip","middle":"Kumar","last":"Naskar"},{"first":"Sivaji","last":"Bandyopadhyay"}],"year":"2006","title":"A modified joint source-channel model for transliteration, Proceedings of the COL-ING/ACL, pages 191-198","source":"Asif Ekbal, Sudip Kumar Naskar and Sivaji Bandyopadhyay. 2006. A modified joint source-channel model for transliteration, Proceedings of the COL-ING/ACL, pages 191-198."},{"authors":[{"first":"Bo-June","last":"Hsu"},{"first":"James","last":"Glass"}],"year":"2008","title":"Iterative Language Model Estimation: Efficient Data Structure & Algorithms","source":"Bo-June Hsu and James Glass 2008. Iterative Language Model Estimation: Efficient Data Structure & Algorithms. Proceedings Interspeech, pages 841-844."},{"authors":[{"first":"Kevin","last":"Knight"},{"first":"Jonathan","last":"Graehl"}],"year":"1998","title":"Machine Transliteration, Association for Computational Linguistics","source":"Kevin Knight and Jonathan Graehl. 1998. Machine Transliteration, Association for Computational Linguistics."},{"authors":[{"first":"John","last":"Lafferty"},{"first":"Andrew","last":"McCallum"},{"first":"Fernando","last":"Pereira"}],"year":"2001","title":"Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data","source":"John Lafferty, Andrew McCallum, and Fernando Pereira 2001. Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data., Proceedings of International Conference on Machine Learning, pages 282-289."},{"authors":[{"first":"Haizhou","last":"Li"},{"first":"Min","last":"Zhang"},{"first":"Jian","last":"Su"}],"year":"2004","title":"A joint source-channel model for machine transliteration, Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics","source":"Haizhou Li, Min Zhang and Jian Su. 2004. A joint source-channel model for machine transliteration, Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics."},{"authors":[],"source":"Haizhou Li, A. Kumaran, Vladimir Pervouchine and Min Zhang 2009. Report of NEWS 2009 Machine Transliteration Shared Task, Proceedings of the 2009 Named Entities Workshop: Shared Task on Transliteration (NEWS 2009), pages 1-18"},{"authors":[{"first":"Jong-Hoon","last":"Oh"},{"first":"Key-Sun","last":"Choi"},{"first":"Hitoshi","last":"Isahara"}],"year":"2006","title":"A comparison of different machine transliteration models , Journal of Artificial Intelligence Research, 27, pages 119-151","source":"Jong-Hoon Oh, Key-Sun Choi and Hitoshi Isahara. 2006. A comparison of different machine transliteration models , Journal of Artificial Intelligence Research, 27, pages 119-151."},{"authors":[{"first":"Richard","last":"Sproat"}],"year":"2000","title":"Corpus-Based Methods and Hand-Built Methods","source":"Richard Sproat 2000. Corpus-Based Methods and Hand-Built Methods. Proceedings of International Conference on Spoken Language Processing, pages 426-428."},{"authors":[{"first":"Andrew","middle":"J.","last":"Viterbi"}],"year":"1967","title":"Error Bounds for Convolu-tional Codes and an Asymptotically Optimum De-coding Algorithm","source":"Andrew J. Viterbi 1967. Error Bounds for Convolu-tional Codes and an Asymptotically Optimum De-coding Algorithm. IEEE Transactions on Informa-tion Theory, Volume IT-13, pages 260-269."},{"authors":[{"first":"Hanna","last":"Wallach"}],"year":"2002","title":"Efficient Training of Conditional Random Fields","source":"Hanna Wallach 2002. Efficient Training of Conditional Random Fields. M. Thesis, University of Edinburgh."},{"authors":[],"source":"Dong Yang, Paul Dixon, Yi-Cheng Pan, Tasuku Oonishi, Masanobu Nakamura and Sadaoki Furui 2009. Combining a Two-step Conditional Random Field Model and a Joint Source Channel Model for Machine Transliteration, Proceedings of the 2009 Named Entities Workshop: Shared Task on Transliteration (NEWS 2009), pages 72-75 280"}],"cites":[{"style":0,"text":"Knight and Graehl, 1998","origin":{"pointer":"/sections/2/paragraphs/2","offset":37,"length":23},"authors":[{"last":"Knight"},{"last":"Graehl"}],"year":"1998","references":["/references/4"]},{"style":0,"text":"Oh et al., 2006","origin":{"pointer":"/sections/2/paragraphs/2","offset":62,"length":15},"authors":[{"last":"Oh"},{"last":"al."}],"year":"2006","references":["/references/8"]},{"style":0,"text":"Li et al., 2009","origin":{"pointer":"/sections/2/paragraphs/3","offset":320,"length":15},"authors":[{"last":"Li"},{"last":"al."}],"year":"2009","references":[]},{"style":0,"text":"Li et al., 2004","origin":{"pointer":"/sections/2/paragraphs/4","offset":60,"length":15},"authors":[{"last":"Li"},{"last":"al."}],"year":"2004","references":["/references/6"]},{"style":0,"text":"Ekbal et al. (2006)","origin":{"pointer":"/sections/2/paragraphs/4","offset":227,"length":19},"authors":[{"last":"Ekbal"},{"last":"al."}],"year":"2006","references":["/references/2"]},{"style":0,"text":"Yang et al., 2009","origin":{"pointer":"/sections/2/paragraphs/4","offset":476,"length":17},"authors":[{"last":"Yang"},{"last":"al."}],"year":"2009","references":[]},{"style":0,"text":"Lafferty et al., 2001","origin":{"pointer":"/sections/3/paragraphs/0","offset":13,"length":21},"authors":[{"last":"Lafferty"},{"last":"al."}],"year":"2001","references":["/references/5"]},{"style":0,"text":"Wallach, 2002","origin":{"pointer":"/sections/3/paragraphs/0","offset":245,"length":13},"authors":[{"last":"Wallach"}],"year":"2002","references":["/references/11"]},{"style":0,"text":"Yang et al. (2009)","origin":{"pointer":"/sections/3/paragraphs/0","offset":729,"length":18},"authors":[{"last":"Yang"},{"last":"al."}],"year":"2009","references":[]},{"style":0,"text":"Yang et al. (2009)","origin":{"pointer":"/sections/3/paragraphs/2","offset":14,"length":18},"authors":[{"last":"Yang"},{"last":"al."}],"year":"2009","references":[]},{"style":0,"text":"Yang et al. (2009)","origin":{"pointer":"/sections/4/paragraphs/1","offset":0,"length":18},"authors":[{"last":"Yang"},{"last":"al."}],"year":"2009","references":[]},{"style":0,"text":"Li et al., 2004","origin":{"pointer":"/sections/5/paragraphs/0","offset":88,"length":15},"authors":[{"last":"Li"},{"last":"al."}],"year":"2004","references":["/references/6"]},{"style":0,"text":"Li et al., 2004","origin":{"pointer":"/sections/5/paragraphs/2","offset":97,"length":15},"authors":[{"last":"Li"},{"last":"al."}],"year":"2004","references":["/references/6"]},{"style":0,"text":"Hsu and Glass, 2008","origin":{"pointer":"/sections/5/paragraphs/5","offset":62,"length":19},"authors":[{"last":"Hsu"},{"last":"Glass"}],"year":"2008","references":["/references/3"]},{"style":0,"text":"Sproat et al., 2000","origin":{"pointer":"/sections/5/paragraphs/5","offset":184,"length":19},"authors":[{"last":"Sproat"},{"last":"al."}],"year":"2000","references":[]},{"style":0,"text":"Caseiro et al., 2002","origin":{"pointer":"/sections/5/paragraphs/5","offset":205,"length":20},"authors":[{"last":"Caseiro"},{"last":"al."}],"year":"2002","references":["/references/1"]},{"style":0,"text":"Allauzen et al., 2007","origin":{"pointer":"/sections/5/paragraphs/5","offset":642,"length":21},"authors":[{"last":"Allauzen"},{"last":"al."}],"year":"2007","references":["/references/0"]},{"style":0,"text":"Li et al., 2009","origin":{"pointer":"/sections/6/paragraphs/0","offset":29,"length":15},"authors":[{"last":"Li"},{"last":"al."}],"year":"2009","references":[]},{"style":0,"text":"Li et al., 2009","origin":{"pointer":"/sections/6/paragraphs/11","offset":162,"length":15},"authors":[{"last":"Li"},{"last":"al."}],"year":"2009","references":[]},{"style":0,"text":"Li et al., 2009","origin":{"pointer":"/sections/6/paragraphs/12","offset":124,"length":15},"authors":[{"last":"Li"},{"last":"al."}],"year":"2009","references":[]}]}
