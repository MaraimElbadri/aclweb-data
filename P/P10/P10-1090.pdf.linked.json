{"sections":[{"title":"","paragraphs":["Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 875–885, Uppsala, Sweden, 11-16 July 2010. c⃝2010 Association for Computational Linguistics"]},{"title":"Convolution Kernel over Packed Parse Forest   Min Zhang Hui Zhang Haizhou Li Institute for Infocomm Research A-STAR, Singapore {mzhang,vishz,hli}@i2r.a-star.edu.sg    Abstract","paragraphs":["This paper proposes a convolution forest kernel to effectively explore rich structured features embedded in a packed parse forest. As opposed to the convolution tree kernel, the proposed forest kernel does not have to commit to a single best parse tree, is thus able to explore very large object spaces and much more structured features embedded in a forest. This makes the proposed kernel more robust against parsing errors and data sparseness issues than the convolution tree kernel. The paper presents the formal definition of convolution forest kernel and also illustrates the computing algorithm to fast compute the proposed convolution forest kernel. Experimental results on two NLP applications, relation extraction and semantic role labeling, show that the proposed forest kernel significantly outperforms the baseline of the convolution tree kernel."]},{"title":"1 Introduction","paragraphs":["Parse tree and packed forest of parse trees are two widely used data structures to represent the syntactic structure information of sentences in natural language processing (NLP). The structured features embedded in a parse tree have been well explored together with different machine learning algorithms and proven very useful in many NLP applications (Collins and Duffy, 2002; Moschitti, 2004; Zhang et al., 2007). A forest (Tomita, 1987) compactly encodes an exponential number of parse trees. In this paper, we study how to effectively explore structured features embedded in a forest using convolution kernel (Haussler, 1999).","As we know, feature-based machine learning methods are less effective in modeling highly structured objects (Vapnik, 1998), such as parse tree or semantic graph in NLP. This is due to the fact that it is usually very hard to represent structured objects using vectors of reasonable dimensions without losing too much information. For example, it is computationally infeasible to enumerate all subtree features (using subtree a feature) for a parse tree into a linear feature vector. Kernel-based machine learning method is a good way to overcome this problem. Kernel methods employ a kernel function, that must satisfy the properties of being symmetric and positive, to measure the similarity between two objects by computing implicitly the dot product of certain features of the input objects in high (or even in-finite) dimensional feature spaces without enumerating all the features (Vapnik, 1998).","Many learning algorithms, such as SVM (Vapnik, 1998), the Perceptron learning algorithm (Rosenblatt, 1962) and Voted Perceptron (Freund and Schapire, 1999), can work directly with kernels by replacing the dot product with a particular kernel function. This nice property of kernel methods, that implicitly calculates the dot product in a high-dimensional space over the original representations of objects, has made kernel methods an effective solution to modeling structured objects in NLP.","In the context of parse tree, convolution tree kernel (Collins and Duffy, 2002) defines a feature space consisting of all subtree types of parse trees and counts the number of common subtrees as the syntactic similarity between two parse trees. The tree kernel has shown much success in many NLP applications like parsing (Collins and Duffy, 2002), semantic role labeling (Moschitti, 2004; Zhang et al., 2007), relation extraction (Zhang et al., 2006), pronoun resolution (Yang et al., 2006), question classification (Zhang and Lee, 2003) and machine translation (Zhang and Li, 2009), where the tree kernel is used to compute the similarity between two NLP application instances that are usually represented by parse trees. However, in those studies, the tree kernel only covers the features derived from single 1-875 best parse tree. This may largely compromise the performance of tree kernel due to parsing errors and data sparseness.","To address the above issues, this paper constructs a forest-based convolution kernel to mine structured features directly from packed forest. A packet forest compactly encodes exponential number of n-best parse trees, and thus containing much more rich structured features than a single parse tree. This advantage enables the forest kernel not only to be more robust against parsing errors, but also to be able to learn more reliable feature values and help to solve the data sparseness issue that exists in the traditional tree kernel. We evaluate the proposed kernel in two real NLP applications, relation extraction and semantic role labeling. Experimental results on the benchmark data show that the forest kernel significantly outperforms the tree kernel.","The rest of the paper is organized as follows. Section 2 reviews the convolution tree kernel while section 3 discusses the proposed forest kernel in details. Experimental results are reported in section 4. Finally, we conclude the paper in section 5."]},{"title":"2 Convolution Kernel over Parse Tree","paragraphs":["Convolution kernel was proposed as a concept of kernels for discrete structures by Haussler (1999) and related but independently conceived ideas on string kernels first presented in (Watkins, 1999). The framework defines the kernel function between input objects as the convolution of “sub-kernels”, i.e. the kernels for the decompositions (parts) of the input objects.","The parse tree kernel (Collins and Duffy, 2002) is an instantiation of convolution kernel over syntactic parse trees. Given a parse tree, its features defined by a tree kernel are all of its subtree types and the value of a given feature is the number of the occurrences of the subtree in the parse tree. Fig. 1 illustrates a parse tree with all of its 11 subtree features covered by the convolution tree kernel. In the tree kernel, a parse tree"]},{"title":"T","paragraphs":["is represented by a vector of integer counts of each subtree type (i.e., subtree regardless of its ancestors, descendants and span covered):",""]},{"title":"( )T ","paragraphs":["(# subtreetype1(T), ..., # subtreetypen(T))  where # subtreetypei(T) is the occurrence number of the i","th","subtree type in T. The tree kernel counts the number of common subtrees as the syntactic similarity between two parse trees. Since the number of subtrees is exponential with the tree size, it is computationally infeasible to directly use the feature vector"]},{"title":"( )T","paragraphs":[". To solve this computational issue, Collins and Duffy (2002) proposed the following tree kernel to calculate the dot product between the above high dimensional vectors implicitly."," 1 1 2 2 1 1 2 2 1 2 1 2 12 12 12 ( , ) ( ), ( ) # ( ) # ( ) ( ) ( ) ( , ) ii ii i","subtree subtree i n N n N n N n N K T T T T subtreetype T subtreetype T I n I n nn      ","   ","    ","   ",""]},{"title":"        ","paragraphs":["where N1 and N2 are the sets of nodes in trees T1 and T2, respectively, and"]},{"title":"( )","paragraphs":["isubtree"]},{"title":"I n","paragraphs":["is a function that is 1 iff the subtreetypei occurs with root at node n and zero otherwise, and 1 2( , )n n is the number of the common subtrees rooted at n1 and n2, i.e.,  1 2 1 2"]},{"title":"( , ) ( ) ( )","paragraphs":["i isubtree subtreei"]},{"title":"n n I n I n   ","paragraphs":["1 2"]},{"title":"( , )n n","paragraphs":["can be computed by the following recursive rules: IN in the bankDT NNPP IN the bankDT NNPP IN in bankDT NNPP IN in theDT NNPP IN in DT NNPP IN theDTPP NN IN bankDT NNPP IN DT NNPP IN in the bankDT NN IN in the bankDT NNPP"," Figure 1. A parse tree and its 11 subtree features covered by convolution tree kernel 876 Rule 1: if the productions (CFG rules) at 1"]},{"title":"n","paragraphs":["and 2"]},{"title":"n","paragraphs":["are different, 1 2"]},{"title":"( , ) 0n n ","paragraphs":[";  Rule 2: else if both 1"]},{"title":"n","paragraphs":["and 2"]},{"title":"n","paragraphs":["are pre-terminals (POS tags), 1 2( , ) 1n n    ;  Rule 3: else,"," 1()","1 2 1 2 1( , ) (1 ( ( , ), ( , ))) nc n jn n ch n j ch n j     "]},{"title":"","paragraphs":[",  where 1"]},{"title":"( )nc n","paragraphs":["is the child number of 1"]},{"title":"n","paragraphs":[", ch(n,j) is the j th child of node n and"]},{"title":"","paragraphs":["(0<"]},{"title":"","paragraphs":["≤1) is the decay factor in order to make the kernel value less variable with respect to the subtree sizes (Collins and Duffy, 2002). The recursive Rule 3 holds because given two nodes with the same children, one can construct common subtrees using these children and common subtrees of further offspring. The time complexity for computing this kernel is 1 2"]},{"title":"(| | | |)O N N","paragraphs":[".","As discussed in previous section, when convolution tree kernel is applied to NLP applications, its performance is vulnerable to the errors from the single parse tree and data sparseness. In this paper, we present a convolution kernel over packed forest to address the above issues by exploring structured features embedded in a forest."]},{"title":"3 Convolution Kernel over Forest","paragraphs":["In this section, we first illustrate the concept of packed forest and then give a detailed discussion on the covered feature space, fractional count, feature value and the forest kernel function itself. 3.1 Packed forest of parse trees Informally, a packed parse forest, or (packed) forest in short, is a compact representation of all the derivations (i.e. parse trees) for a given sentence under context-free grammar (Tomita, 1987; Billot and Lang, 1989; Klein and Manning, 2001). It is the core data structure used in natural language parsing and other downstream NLP applications, such as syntax-based machine translation (Zhang et al., 2008; Zhang et al., 2009a). In parsing, a sentence corresponds to exponential number of parse trees with different tree probabilities, where a forest can compact all the parse trees by sharing their common subtrees in a bottom-up manner. Formally, a packed forest F can be described as a triple: F = < V, E, S >  where Vis the set of non-terminal nodes, E is the set of hyper-edges and S is a sentence  NNP[1,1] VV[2,2] NN[4,4] IN[5,5] John saw a man NP[3,4] in the bank DT[3,3] DT[6,6] NN[7,7] PP[5,7] VP[2,4] NP[3,7] VP[2,7] IP[1,7] NNP VV NN IN DT NN John saw a man in the bank DT VP NP VP IP PP NNP VV NN IN DT NN John saw a man in the bank DT NP NP VP IP PP IP[1,7] VP[2,7] NNP[1,1] a) A Forest f b) A Hyper-edge e c) A Parse Tree T1 d) A Parse Tree T2 "," Figure 2. An example of a packed forest, a hyper-edge and two parse trees covered by the packed forest  877 represented as an ordered word sequence. A hyper-edge e is a group of edges in a parse tree which connects a father node and its all child nodes, representing a CFG rule. A non-terminal node in a forest is represented as a “label [start, end]”, where the “label” is its syntax category and “[start, end]” is the span of words it covers. As shown in Fig. 2, these two parse trees (T1 and T2) can be represented as a single forest by sharing their common subtrees (such as NP[3,4] and PP[5,7]) and merging common non-terminal nodes covering the same span (such as VP[2,7], where there are two hyper-edges attach to it).","Given the definition of forest, we introduce the concepts of inside probability β . and outside probability α(. ) that are widely-used in parsing (Baker, 1979; Lari and Young, 1990) and are also to be used in our kernel calculation."," β v p, p = P(v → S[p])  β v p, q =   P e","e is a 𝑕yper −edge attac 𝑕ed to v ∙ β(ci [pi , qi]) ci pi,qi is a leaf","node of e   α root(f) = 1 α v p, q =   α root e ∙ P e e is a 𝑕yper − edge and v is its one leaf node","∙ β(ci [pi , qi])) ci pi,qi is a c𝑕ildren node of e except v  where v is a forest node, S[p] is the pt𝑕","word of input sentence S, P(v → S[p]) is the probability of the CFG rule v → S[p], root(. ) returns the root node of input structure, [pi , qi] is a sub-span of p, q , being covered by ci , and P e is the PCFG probability of e. From these definitions, we can see that the inside probability is total probability of generating words S p, q from non-terminal node v p, q while the outside probability is the total probability of generating node v p, q and words outside S[p, q] from the root of forest. The inside probability can be calculated using dynamic programming in a bottom-up fashion while the outside probability can be calculated using dynamic programming in a top-to-down way. 3.2 Convolution forest kernel In this subsection, we first define the feature space covered by forest kernel, and then define the forest kernel function.","3.2.1 Feature space, object space and feature value The forest kernel counts the number of common subtrees as the syntactic similarity between two forests. Therefore, in the same way as tree kernel, its feature space is also defined as all the possible subtree types that a CFG grammar allows. In a forest kernel, forest F is represented by a vector of fractional counts of each subtree type (subtree regardless of its ancestors, descendants and span covered): "]},{"title":"()F ","paragraphs":["(# subtreetype1(F), ..., # subtreetypen(F)) = (#subtreetype1(n-best parse trees), ..., (1) # subtreetypen(n-best parse trees)) ","where # subtreetypei(F) is the occurrence number","of the i","th","subtree type (subtreetypei) in forest F, i.e., a n-best parse tree lists with a huge n.","Although the feature spaces of the two kernels are the same, their object spaces (tree vs. forest) and feature values (integer counts vs. fractional counts) differ very much. A forest encodes exponential number of parse trees, and thus containing exponential times more subtrees than a single parse tree. This ensures forest kernel to learn more reliable feature values and is also able to help to address the data sparseness issues in a better way than tree kernel does. Forest kernel is also expected to yield more non-zero feature values than tree kernel. Furthermore, different parse tree in a forest represents different derivation and interpretation for a given sentence. Therefore, forest kernel should be more robust to parsing errors than tree kernel.","In tree kernel, one occurrence of a subtree contributes 1 to the value of its corresponding feature (subtree type), so the feature value is an integer count. However, the case turns out very complicated in forest kernel. In a forest, each of its parse trees, when enumerated, has its own 878 probability. So one subtree extracted from different parse trees should have different fractional count with regard to the probabilities of different parse trees. Following the previous work (Charniak and Johnson, 2005; Huang, 2008), we define the fractional count of the occurrence of a subtree in a parse tree ti as"," c subtree, ti = 0 if subtree ∉ ti P subtree, ti|f, s ot𝑕erwise ","= 0 if subtree ∉ ti P ti|f, s ot𝑕erwise "," where we have P subtree, ti|f, s = P ti|f, s if subtree ∈ ti. Then we define the fractional count of the occurrence of a subtree in a forest f as  c subtree, f = P subtree|f, s = P subtree, ti |f, s ti (2) = Isubtree ti ∙ P ti|f, s ti"," where Isubtree ti is a binary function that is 1 iif the subtree ∈ ti and zero otherwise. Obviously, it needs exponential time to compute the above fractional counts. However, due to the property of forest that compactly represents all the parse trees, the posterior probability of a subtree in a forest, P subtree|f, s , can be easily computed in an Inside-Outside fashion as the product of three parts: the outside probability of its root node, the probabilities of parse hyper-edges involved in the subtree, and the inside probabilities of its leaf nodes (Lari and Young, 1990; Mi and Huang, 2008).  c subtree, f = P subtree|f, s (3)  = αβ(subtree) αβ(root f )  where  αβ subtree = α root subtree (4) ∙ P e e∈subtree  ∙ β v v∈leaf subtree  and ","αβ root f = α root f ∙ β root f = β root f  where α . and β(. ) denote the outside and inside probabilities. They can be easily obtained using the equations introduced at section 3.1.","Given a subtree, we can easily compute its fractional count (i.e. its feature value) directly using eq. (3) and (4) without the need of enumerating each parse trees as shown at eq. (2) 1",". Nonetheless, it is still computationally infeasible to directly use the feature vector φ(F) (see eq. (1)) by explicitly enumerating all subtrees although its fractional count is easily calculated. In the next subsection, we present the forest kernel that implicitly calculates the dot-product between two φ(F)s in a polynomial time. 3.2.2 Convolution forest kernel The forest kernel counts the fractional numbers of common subtrees as the syntactic similarity between two forests. We define the forest kernel function Kf f1, f2 in the following way."," Kf f1, f2 =< φ f1 , φ f2 > (5)","= #subtreetypei(f1). #subtreetypei(f2) i ","= Ieq subtree1, subtree2 subtree 1∈f1 subtree 2∈f2 ∙ c subtree1, f1 ∙ c subtree2, f2","= Δ′ v1, v2 v2∈N2v1∈N1 ","where"," Ieq ∙,∙ is a binary function that is 1 iif the input two subtrees are identical (i.e. they have the same typology and node labels) and zero otherwise;"," c ∙,∙ is the fractional count defined at eq. (3);"," N1 and N2 are the sets of nodes in forests f1 and f2;"," Δ′ v1, v2 returns the accumulated value of products between each two fractional counts of the common subtrees rooted at v1 and v2, i.e.,  Δ′ v1, v2","= Ieq subtree1, subtree2 root subtree 1 =v1 root subtree 2 =v2","∙ c subtree1, f1","∙ c subtree2, f2  1 It has been proven in parsing literatures (Baker, 1979; Lari and Young, 1990) that eq. (3) defined by Inside-Outside probabilities is exactly to compute the sum of those parse tree probabilities that cover the subtree of being considered as defined at eq. (2). 879","We next show that Δ′ v1, v2 can be computed recursively in a polynomial time as illustrated at Algorithm 1. To facilitate discussion, we temporarily ignore all fractional counts in Algorithm 1. Indeed, Algorithm 1 can be viewed as a natural extension of convolution kernel from over tree to over forest. In forest","2",", a node can root multiple hyper-edges and each hyper-edge is independent to each other. Therefore, Algorithm 1 iterates each hyper-edge pairs with roots at v1 and v2 (line 3-4), and sums over (eq. (7) at line 9) each recursively-accumulated sub-kernel scores of subtree pairs extended from the hyper-edge pair e1, e2 (eq. (6) at line 8). Eq. (7) holds because the hyper-edges attached to the same node are independent to each other. Eq. (6) is very similar to the Rule 3 of tree kernel (see section 2) except its inputs are hyper-edges and its further expansion is based on forest nodes. Similar to tree kernel (Collins and Duffy, 2002), eq. (6) holds because a common subtree by extending from (e1, e2) can be formed by taking the hyper-edge (e1, e2), together with a choice at each of their leaf nodes of simply taking the non-terminal at the leaf node, or any one of the common subtrees with root at the leaf node. Thus there are 1 + Δ′","leaf e1, j , leaf e2, j possible choices at the j th leaf node. In total, there are Δ′′ e1, e2 (eq. (6)) common subtrees by extending from (e1, e2) and Δ′ v1, v2 (eq. (7)) common subtrees with root at v1, v2 .","Obviously Δ′ v1, v2 calculated by Algorithm 1 is a proper convolution kernel since it simply counts the number of common subtrees under the root v1, v2 . Therefore, Kf f1, f2 defined at eq. (5) and calculated through Δ′ v1, v2 is also a proper convolution kernel. From eq. (5) and Algorithm 1, we can see that each hyper-edge pair (e1, e2) is only visited at most one time in computing the forest kernel. Thus the time complexity for computing Kf f1, f2 is O(|E1| ∙ |E2|) , where E1 and E2 are the set of hyper-edges in forests f1 and f2 , respectively. Given a forest and the best parse trees, the number of hyper-edges is only several times (normally <=3 after pruning) than that of tree nodes in the parse tree 3 .  2 Tree can be viewed as a special case of forest with only one hyper-edge attached to each tree node. 3 Suppose there are K forest nodes in a forest, each node has M associated hyper-edges fan out and each hyper-edge has N children. Then the forest is capable of encoding M K−1 N−1","parse trees at most (Zhang et al., 2009b). Same as tree kernel, forest kernel is running more efficiently in practice since only two nodes with the same label needs to be further processed (line 2 of Algorithm 1).","Now let us see how to integrate fractional counts into forest kernel. According to Algorithm 1 (eq. (7)), we have (e1/e2 are attached to v1/v2, respectively)","","Δ′ v1, v2 = Δ′′ e1, e2 e1=e2  ","Recall eq. (4), a fractional count consists of outside, inside and subtree probabilities. It is more straightforward to incorporate the outside and subtree probabilities since all the subtrees with roots at v1, v2 share the same outside probability and each hyper-edge pair is only visited one time. Thus we can integrate the two probabilities into Δ′ v1, v2 as follows. ","Δ′ v1, v2 = λ ∙ α v1 ∙ α v2 ∙ P e1 ∙ P e2 ∙ Δ′′ e1, e2 e1=e2 (8)  where, following tree kernel, a decay factor λ(0 < λ ≤ 1) is also introduced in order to make the kernel value less variable with respect to the subtree sizes (Collins and Duffy, 2002). It functions like multiplying each feature value by λsize i",", where sizei is the number of hyper-edges in subtreei. Algorithm 1. Input: f1, f2: two packed forests v1, v2: any two nodes of f1 and f2 Notation:","Ieq ∙,∙ : defined at eq. (5)","nl e1 : number of leaf node of e1","leaf e1, j : the jth leaf node of e1 Output: Δ′ v1, v2"," 1. Δ′ v1, v2 = 0 2. if v1. label ≠ v2. label exit 3. for each hyper-edge e1 attached to v1 do 4. for each hyper-edge e2 attached to v2 do 5. if Ieq e1, e2 == 0 do 6. goto line 3 7. else do 8. Δ′′ e1, e2 = 1 +nl e1","j =1","Δ′ leaf e1, j , leaf e2, j (6) 9. Δ′ v1, v2 += Δ′′ e1, e2 (7) 10. end if 11. end for 12. end for  880","The inside probability is only involved when a node does not need to be further expanded. The integer 1 at eq. (6) represents such case. So the inside probability is integrated into eq. (6) by replacing the integer 1 as follows.","","Δ′′ e1, e2 = β leaf e1, j ∙ β leaf e2, j nl e1 j =1 + Δ′","leaf e1, j , leaf e2, j α leaf e1, j ∙ α leaf e2, j (9)  where in the last expression the two outside probabilities α leaf e1, j and α leaf e2, j are removed. This is because leaf e1, j and leaf e2, j are not roots of the subtrees of being explored (only outside probabilities of the root of a subtree should be counted in its fractional count), and Δ′","leaf e1, j , leaf e2, j already contains the two outside probabilities of leaf e1, j and leaf e2, j .","Referring to eq. (3), each fractional count needs to be normalized by αβ(root f ). Since αβ(root f ) is independent to each individual fractional count, we do the normalization outside the recursive function Δ′′ e1, e2 . Then we can re-formulize eq. (5) as"," Kf f1, f2 =< φ f1 , φ f2 > = Δ′ v1, v2 v2∈N2v1∈N1 αβ root f1 ∙ αβ root f2 (10)  Finally, since the size of input forests is not","constant, the forest kernel value is normalized","using the following equation.","  K f f1, f2 = Kf f1, f2 Kf f1, f1 ∙ Kf f2, f2 (11)","","From the above discussion, we can see that the proposed forest kernel is defined together by eqs. (11), (10), (9) and (8). Thanks to the compact representation of trees in forest and the recursive nature of the kernel function, the introduction of fractional counts and normalization do not change the convolution property and the time complexity of the forest kernel. Therefore, the forest kernel K f f1, f2 is still a proper convolution kernel with quadratic time complexity. 3.3 Comparison with previous work To the best of our knowledge, this is the first work to address convolution kernel over packed parse forest.","Convolution tree kernel is a special case of the proposed forest kernel. From feature exploration viewpoint, although theoretically they explore the same subtree feature spaces (defined recursively by CFG parsing rules), their feature values are different. Forest encodes exponential number of trees. So the number of subtree instances extracted from a forest is exponential number of times greater than that from its corresponding parse tree. The significant difference of the amount of subtree instances makes the parameters learned from forests more reliable and also can help to address the data sparseness issue. To some degree, forest kernel can be viewed as a tree kernel with very powerful back-off mechanism. In addition, forest kernel is much more robust against parsing errors than tree kernel.","Aiolli et al. (2006; 2007) propose using Direct Acyclic Graphs (DAG) as a compact representation of tree kernel-based models. This can largely reduce the computational burden and storage requirements by sharing the common structures and feature vectors in the kernel-based model. There are a few other previous works done by generalizing convolution tree kernels (Kashima and Koyanagi, 2003; Moschitti, 2006; Zhang et al., 2007). However, all of these works limit themselves to single tree structure from modeling viewpoint in nature.","From a broad viewpoint, as suggested by one reviewer of the paper, we can consider the forest kernel as an alternative solution proposed for the general problem of noisy inference pipelines (eg. speech translation by composition of FSTs, machine translation by translating over 'lattices' of segmentations (Dyer et al., 2008) or using parse tree info for downstream applications in our cases) . Following this line, Bunescu (2008) and Finkel et al. (2006) are two typical related works done in reducing cascading noisy. However, our works are not overlapped with each other as there are two totally different solutions for the same general problem. In addition, the main motivation of this paper is also different from theirs."]},{"title":"4 Experiments","paragraphs":["Forest kernel has a broad application potential in NLP. In this section, we verify the effectiveness of the forest kernel on two NLP applications, semantic role labeling (SRL) (Gildea, 2002) and relation extraction (RE) (ACE, 2002-2006).","In our experiments, SVM (Vapnik, 1998) is selected as our classifier and the one vs. others strategy is adopted to select the one with the 881 largest margin as the final answer. In our implementation, we use the binary SVMLight (Joachims, 1998) and borrow the framework of the Tree Kernel Tools (Moschitti, 2004) to integrate our forest kernel into the SVMLight. We modify Charniak parser (Charniak, 2001) to output a packed forest. Following previous forest-based studies (Charniak and Johnson, 2005), we use the marginal probabilities of hyper-edges (i.e., the Viterbi-style inside-outside probabilities and set the pruning threshold as 8) for forest pruning. 4.1 Semantic role labeling Given a sentence and each predicate (either a target verb or a noun), SRL recognizes and maps all the constituents in the sentence into their corresponding semantic arguments (roles, e.g., A0 for Agent, A1 for Patient ...) of the predicate or non-argument. We use the CoNLL-2005 shared task on Semantic Role Labeling (Carreras and Ma rquez, 2005) for the evaluation of our forest kernel method. To speed up the evaluation process, the same as Che et al. (2008), we use a subset of the entire training corpus (WSJ sections 02-05 of the entire sections 02-21) for training, section 24 for development and section 23 for test, where there are 35 roles including 7 Core (A0–A5, AA), 14 Adjunct (AM-) and 14 Reference (R-) arguments.","The state-of-the-art SRL methods (Carreras and Ma rquez, 2005) use constituents as the labeling units to form the labeled arguments. Due to the errors from automatic parsing, it is impossible for all arguments to find their matching constituents in the single 1-best parse trees. Statistics on the training data shows that 9.78% of arguments have no matching constituents using the Charniak parser (Charniak, 2001), and the number increases to 11.76% when using the Collins parser (Collins, 1999). In our method, we break the limitation of 1-best parse tree and regard each span rooted by a single forest node (i.e., a sub-forest with one or more roots) as a candidate argument. This largely reduces the unmatched arguments from 9.78% to 1.31% after forest pruning. However, it also results in a very large amount of argument candidates that is 5.6 times as many as that from 1-best tree. Fortunately, after the pre-processing stage of argument pruning (Xue and Palmer, 2004) 4 , although the  4 We extend (Xue and Palmer, 2004)’s argument pruning algorithm from tree-based to forest-based. The algorithm is very effective. It can prune out around 90% argument candidates in parse tree-based amount of unmatched argument increases a little bit to 3.1%, its generated total candidate amount decreases substantially to only 1.31 times of that from 1-best parse tree. This clearly shows the advantages of the forest-based method over tree-based in SRL.","The best-reported tree kernel method for SRL K𝑕ybrid = θ ∙ Kpat 𝑕 + (1 − θ) ∙ Kcs (0 ≤ θ ≤ 1), proposed by Che et al. (2006)","5",", is adopted as our baseline kernel. We implemented the K𝑕ybrid in tree case ( KT−𝑕ybrid , using tree kernel to compute Kpat 𝑕 and Kcs ) and in forest case (KF−𝑕ybrid , using tree kernel to compute Kpat 𝑕 and Kcs )."," Precision Recall F-Score KT−𝑕ybrid (Tree) 76.02 67.38 71.44 KF−𝑕ybrid (Forest) 79.06 69.12 73.76 Table 1: Performance comparison of SRL (%) ","Table 1 shows that the forest kernel significantly outperforms (χ2","test with p=0.01) the tree kernel with an absolute improvement of 2.32 (73.76-71.42) percentage in F-Score, representing a relative error rate reduction of 8.19% (2.32/(100-71.64)). This convincingly demonstrates the advantage of the forest kernel over the tree kernel. It suggests that the structured features represented by subtree are very useful to SRL. The performance improvement is mainly due to the fact that forest encodes much more such structured features and the forest kernel is able to more effectively capture such structured features than the tree kernel. Besides F-Score, both precision and recall also show significantly improvement (χ2","test with p=0.01). The reason for recall improvement is mainly due to the lower rate of unmatched argument (3.1% only) with only a little bit overhead (1.31 times) (see the previous discussion in this section). The precision improvement is mainly attributed to fact that we use sub-forest to represent argument instances, rather than subtree used in tree kernel, where the sub-tree is only one tree encoded in the sub-forest.  SRL and thus makes the amounts of positive and negative training instances (arguments) more balanced. We apply the same pruning strategies to forest plus our heuristic rules to prune out some of the arguments with span overlapped with each other and those arguments with very small inside probabilities, depend-ing on the numbers of candidates in the span. 5 Kpath and Kcs are two standard convolution tree kernels to describe predicate-argument path substructures and argument syntactic substructures, respectively. 882 4.2 Relation extraction As a subtask of information extraction, relation extraction is to extract various semantic relations between entity pairs from text. For example, the sentence “Bill Gates is chairman and chief software architect of Microsoft Corporation” conveys the semantic relation “EMPLOY-MENT.executive” between the entities “Bill Gates” (person) and “Microsoft Corporation” (company). We adopt the method reported in Zhang et al. (2006) as our baseline method as it reports the state-of-the-art performance using tree kernel-based composite kernel method for RE. We replace their tree kernels with our forest kernels and use the same experimental settings as theirs. We carry out the same five-fold cross validation experiment on the same subset of ACE 2004 data (LDC2005T09, ACE 2002-2004) as that in Zhang et al. (2006). The data contain 348 documents and 4400 relation instances.","In SRL, constituents are used as the labeling units to form the labeled arguments. However, previous work (Zhang et al., 2006) shows that if we use complete constituent (MCT) as done in SRL to represent relation instance, there is a large performance drop compared with using the path-enclosed tree (PT)","6",". By simulating PT, we use the minimal fragment of a forest covering the two entities and their internal words to represent a relation instance by only parsing the span covering the two entities and their internal words.   Precision Recall F-Score Zhang et al. (2006):Tree 68.6 59.3 6 63.6 Ours: Forest 70.3 60.0 64.7","","Table 2: Performance Comparison of RE (%) over 23 subtypes on the ACE 2004 data","","Table 2 compares the performance of the forest kernel and the tree kernel on relation extraction. We can see that the forest kernel significantly outperforms (χ2","test with p=0.05) the tree kernel by 1.1 point of F-score. This further verifies the effectiveness of the forest kernel method for  6 MCT is the minimal constituent rooted by the near-est common ancestor of the two entities under consideration while PT is the minimal portion of the parse tree (may not be a complete subtree) containing the two entities and their internal lexical words. Since in many cases, the two entities and their internal words cannot form a grammatical constituent, MCT may introduce too many noisy context features and thus lead to the performance drop. modeling NLP structured data. In summary, we further observe the high precision improvement that is consistent with the SRL experiments. However, the recall improvement is not as significant as observed in SRL. This is because unlike SRL, RE has no un-matching issues in generating relation instances. Moreover, we find that the performance improvement in RE is not as good as that in SRL. Although we know that performance is task-dependent, one of the possible reasons is that SRL tends to be long-distance grammatical structure-related while RE is local and semantic-related as observed from the two experimental benchmark data."]},{"title":"5 Conclusions and Future Work","paragraphs":["Many NLP applications have benefited from the success of convolution kernel over parse tree. Since a packed parse forest contains much richer structured features than a parse tree, we are motivated to develop a technology to measure the syntactic similarity between two forests.","To achieve this goal, in this paper, we design a convolution kernel over packed forest by generalizing the tree kernel. We analyze the object space of the forest kernel, the fractional count for feature value computing and design a dynamic programming algorithm to realize the forest kernel with quadratic time complexity. Compared with the tree kernel, the forest kernel is more robust against parsing errors and data sparseness issues. Among the broad potential NLP applications, the problems in SRL and RE provide two pointed scenarios to verify our forest kernel. Experimental results demonstrate the effectiveness of the proposed kernel in structured NLP data modeling and the advantages over tree kernel.","In the future, we would like to verify the forest kernel in more NLP applications. In addition, as suggested by one reviewer, we may consider rescaling the probabilities (exponentiating them by a constant value) that are used to compute the fractional counts. We can sharpen or flatten the distributions. This basically says \"how seriously do we want to take the very best derivation\" compared to the rest. However, the challenge is that we compute the fractional counts together with the forest kernel recursively by using the Inside-Outside probabilities. We cannot differentiate the individual parse tree’s contribution to a fractional count on the fly. One possible solution is to do the probability rescaling off-line before kernel calculation. This would be a very interest-ing research topic of our future work. 883"]},{"title":"References","paragraphs":["ACE (2002-2006). The Automatic Content Extraction Projects. http://www.ldc.upenn.edu/Projects/ACE/","Fabio Aiolli, Giovanni Da San Martino, Alessandro Sperduti and Alessandro Moschitti. 2006. Fast On-line Kernel Learning for Trees. ICDM-2006","Fabio Aiolli, Giovanni Da San Martino, Alessandro Sperduti and Alessandro Moschitti. 2007. Efficient Kernel-based Learning for Trees. IEEE Symposium on Computational Intelligence and Data Min-ing (CIDM-2007)","J. Baker. 1979. Trainable grammars for speech recognition. The 97th meeting of the Acoustical So-ciety of America","S. Billot and S. Lang. 1989. The structure of shared forest in ambiguous parsing. ACL-1989","Razvan Bunescu. 2008. Learning with Probabilistic Features for Improved Pipeline Models. EMNLP-2008","X. Carreras and Lluıs Ma rquez. 2005. Introduction to the CoNLL-2005 shared task: SRL. CoNLL-2005","E. Charniak. 2001. Immediate-head Parsing for Language Models. ACL-2001","E. Charniak and Mark Johnson. 2005. Corse-to-finegrained n-best parsing and discriminative reranking. ACL-2005","Wanxiang Che, Min Zhang, Ting Liu and Sheng Li. 2006. A hybrid convolution tree kernel for semantic role labeling. COLING-ACL-2006 (poster)","WanXiang Che, Min Zhang, Aiti Aw, Chew Lim Tan, Ting Liu and Sheng Li. 2008. Using a Hybrid Convolution Tree Kernel for Semantic Role Labeling. ACM Transaction on Asian Language Information Processing","M. Collins. 1999. Head-driven statistical models for natural language parsing. Ph.D. dissertation, Pennsylvania University","M. Collins and N. Duffy. 2002. Convolution Kernels for Natural Language. NIPS-2002","Christopher Dyer, Smaranda Muresan and Philip Resnik. 2008. Generalizing Word Lattice Translation. ACL-HLT-2008","Jenny Rose Finkel, Christopher D. Manning and Andrew Y. Ng. 2006. Solving the Problem of Cascad-ing Errors: Approximate Bayesian Inference for Linguistic Annotation Pipelines. EMNLP-2006","Y. Freund and R. E. Schapire. 1999. Large margin classification using the perceptron algorithm. Machine Learning, 37(3):277-296","D. Guldea. 2002. Probabilistic models of verb-argument structure. COLING-2002","D. Haussler. 1999. Convolution Kernels on Discrete Structures. Technical Report UCS-CRL-99-10, University of California, Santa Cruz","Liang Huang. 2008. Forest reranking: Discriminative parsing with non-local features. ACL-2008","Karim Lari and Steve J. Young. 1990. The estimation of stochastic context-free grammars using the inside-outside algorithm. Computer Speech and Language. 4(35–56)","H. Kashima and T. Koyanagi. 2003. Kernels for Semi-Structured Data. ICML-2003","Dan Klein and Christopher D. Manning. 2001. Parsing and Hypergraphs. IWPT-2001","T. Joachims. 1998. Text Categorization with Support Vecor Machine: learning with many relevant features. ECML-1998","Haitao Mi and Liang Huang. 2008. Forest-based Translation Rule Extraction. EMNLP-2008","Alessandro Moschitti. 2004. A Study on Convolution Kernels for Shallow Semantic Parsing. ACL-2004","Alessandro Moschitti. 2006. Syntactic kernels for natural language learning: the semantic role labeling case. HLT-NAACL-2006 (short paper)","Martha Palmer, Dan Gildea and Paul Kingsbury. 2005. The proposition bank: An annotated corpus of semantic roles. Computational Linguistics. 31(1)","F. Rosenblatt. 1962. Principles of Neurodynamics: Perceptrons and the theory of brain mechanisms. Spartan Books, Washington D.C.","Masaru Tomita. 1987. An Efficient Augmented-Context-Free Parsing Algorithm. Computational Linguistics 13(1-2): 31-46","Vladimir N. Vapnik. 1998. Statistical Learning Theory. Wiley","C. Watkins. 1999. Dynamic alignment kernels. In A. J. Smola, B. Schölkopf, P. Bartlett, and D. Schuurmans (Eds.), Advances in kernel methods. MIT Press","Nianwen Xue and Martha Palmer. 2004. Calibrating features for semantic role labeling. EMNLP-2004","Xiaofeng Yang, Jian Su and Chew Lim Tan. 2006. Kernel-Based Pronoun Resolution with Structured Syntactic Knowledge. COLING-ACL-2006","Dell Zhang and W. Lee. 2003. Question classification using support vector machines. SIGIR-2003","Hui Zhang, Min Zhang, Haizhou Li, Aiti Aw and Chew Lim Tan. 2009a. Forest-based Tree Sequence to String Translation Model. ACL-IJCNLP-2009","Hui Zhang, Min Zhang, Haizhou Li and Chew Lim Tan. 2009b. Fast Translation Rule Matching for 884 Syntax-based Statistical Machine Translation. EMNLP-2009","Min Zhang, Jie Zhang, Jian Su and GuoDong Zhou. 2006. A Composite Kernel to Extract Relations between Entities with Both Flat and Structured Features. COLING-ACL-2006","Min Zhang, W. Che, A. Aw, C. Tan, G. Zhou, T. Liu and S. Li. 2007. A Grammar-driven Convolution Tree Kernel for Semantic Role Classification. ACL-2007","Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li, Chew Lim Tan and Sheng Li. 2008. A Tree Sequence Alignment-based Tree-to-Tree Translation Model. ACL-2008","Min Zhang and Haizhou Li. 2009. Tree Kernel-based SVM with Structured Syntactic Knowledge for BTG-based Phrase Reordering. EMNLP-2009 885"]}],"references":[{"authors":[{"last":"ACE"}],"year":"2002-2006","title":"The Automatic Content Extraction Projects","source":"ACE (2002-2006). The Automatic Content Extraction Projects. http://www.ldc.upenn.edu/Projects/ACE/"},{"authors":[{"first":"Fabio","last":"Aiolli"},{"first":"Giovanni","middle":"Da San","last":"Martino"},{"first":"Alessandro","last":"Sperduti"},{"first":"Alessandro","last":"Moschitti"}],"year":"2006","title":"Fast On-line Kernel Learning for Trees","source":"Fabio Aiolli, Giovanni Da San Martino, Alessandro Sperduti and Alessandro Moschitti. 2006. Fast On-line Kernel Learning for Trees. ICDM-2006"},{"authors":[{"first":"Fabio","last":"Aiolli"},{"first":"Giovanni","middle":"Da San","last":"Martino"},{"first":"Alessandro","last":"Sperduti"},{"first":"Alessandro","last":"Moschitti"}],"year":"2007","title":"Efficient Kernel-based Learning for Trees","source":"Fabio Aiolli, Giovanni Da San Martino, Alessandro Sperduti and Alessandro Moschitti. 2007. Efficient Kernel-based Learning for Trees. IEEE Symposium on Computational Intelligence and Data Min-ing (CIDM-2007)"},{"authors":[{"first":"J.","last":"Baker"}],"year":"1979","title":"Trainable grammars for speech recognition","source":"J. Baker. 1979. Trainable grammars for speech recognition. The 97th meeting of the Acoustical So-ciety of America"},{"authors":[{"first":"S.","last":"Billot"},{"first":"S.","last":"Lang"}],"year":"1989","title":"The structure of shared forest in ambiguous parsing","source":"S. Billot and S. Lang. 1989. The structure of shared forest in ambiguous parsing. ACL-1989"},{"authors":[{"first":"Razvan","last":"Bunescu"}],"year":"2008","title":"Learning with Probabilistic Features for Improved Pipeline Models","source":"Razvan Bunescu. 2008. Learning with Probabilistic Features for Improved Pipeline Models. EMNLP-2008"},{"authors":[{"first":"X.","last":"Carreras"},{"first":"Lluıs","middle":"Ma","last":"rquez"}],"year":"2005","title":"Introduction to the CoNLL-2005 shared task: SRL","source":"X. Carreras and Lluıs Ma rquez. 2005. Introduction to the CoNLL-2005 shared task: SRL. CoNLL-2005"},{"authors":[{"first":"E.","last":"Charniak"}],"year":"2001","title":"Immediate-head Parsing for Language Models","source":"E. Charniak. 2001. Immediate-head Parsing for Language Models. ACL-2001"},{"authors":[{"first":"E.","last":"Charniak"},{"first":"Mark","last":"Johnson"}],"year":"2005","title":"Corse-to-finegrained n-best parsing and discriminative reranking","source":"E. Charniak and Mark Johnson. 2005. Corse-to-finegrained n-best parsing and discriminative reranking. ACL-2005"},{"authors":[{"first":"Wanxiang","last":"Che"},{"first":"Min","last":"Zhang"},{"first":"Ting","last":"Liu"},{"first":"Sheng","last":"Li"}],"year":"2006","title":"A hybrid convolution tree kernel for semantic role labeling","source":"Wanxiang Che, Min Zhang, Ting Liu and Sheng Li. 2006. A hybrid convolution tree kernel for semantic role labeling. COLING-ACL-2006 (poster)"},{"authors":[{"first":"WanXiang","last":"Che"},{"first":"Min","last":"Zhang"},{"first":"Aiti","last":"Aw"},{"first":"Chew","middle":"Lim","last":"Tan"},{"first":"Ting","last":"Liu"},{"first":"Sheng","last":"Li"}],"year":"2008","title":"Using a Hybrid Convolution Tree Kernel for Semantic Role Labeling","source":"WanXiang Che, Min Zhang, Aiti Aw, Chew Lim Tan, Ting Liu and Sheng Li. 2008. Using a Hybrid Convolution Tree Kernel for Semantic Role Labeling. ACM Transaction on Asian Language Information Processing"},{"authors":[{"first":"M.","last":"Collins"}],"year":"1999","title":"Head-driven statistical models for natural language parsing","source":"M. Collins. 1999. Head-driven statistical models for natural language parsing. Ph.D. dissertation, Pennsylvania University"},{"authors":[{"first":"M.","last":"Collins"},{"first":"N.","last":"Duffy"}],"year":"2002","title":"Convolution Kernels for Natural Language","source":"M. Collins and N. Duffy. 2002. Convolution Kernels for Natural Language. NIPS-2002"},{"authors":[{"first":"Christopher","last":"Dyer"},{"first":"Smaranda","last":"Muresan"},{"first":"Philip","last":"Resnik"}],"year":"2008","title":"Generalizing Word Lattice Translation","source":"Christopher Dyer, Smaranda Muresan and Philip Resnik. 2008. Generalizing Word Lattice Translation. ACL-HLT-2008"},{"authors":[{"first":"Jenny","middle":"Rose","last":"Finkel"},{"first":"Christopher","middle":"D.","last":"Manning"},{"first":"Andrew","middle":"Y.","last":"Ng"}],"year":"2006","title":"Solving the Problem of Cascad-ing Errors: Approximate Bayesian Inference for Linguistic Annotation Pipelines","source":"Jenny Rose Finkel, Christopher D. Manning and Andrew Y. Ng. 2006. Solving the Problem of Cascad-ing Errors: Approximate Bayesian Inference for Linguistic Annotation Pipelines. EMNLP-2006"},{"authors":[{"first":"Y.","last":"Freund"},{"first":"R.","middle":"E.","last":"Schapire"}],"year":"1999","title":"Large margin classification using the perceptron algorithm","source":"Y. Freund and R. E. Schapire. 1999. Large margin classification using the perceptron algorithm. Machine Learning, 37(3):277-296"},{"authors":[{"first":"D.","last":"Guldea"}],"year":"2002","title":"Probabilistic models of verb-argument structure","source":"D. Guldea. 2002. Probabilistic models of verb-argument structure. COLING-2002"},{"authors":[{"first":"D.","last":"Haussler"}],"year":"1999","title":"Convolution Kernels on Discrete Structures","source":"D. Haussler. 1999. Convolution Kernels on Discrete Structures. Technical Report UCS-CRL-99-10, University of California, Santa Cruz"},{"authors":[{"first":"Liang","last":"Huang"}],"year":"2008","title":"Forest reranking: Discriminative parsing with non-local features","source":"Liang Huang. 2008. Forest reranking: Discriminative parsing with non-local features. ACL-2008"},{"authors":[{"first":"Karim","last":"Lari"},{"first":"Steve","middle":"J.","last":"Young"}],"year":"1990","title":"The estimation of stochastic context-free grammars using the inside-outside algorithm","source":"Karim Lari and Steve J. Young. 1990. The estimation of stochastic context-free grammars using the inside-outside algorithm. Computer Speech and Language. 4(35–56)"},{"authors":[{"first":"H.","last":"Kashima"},{"first":"T.","last":"Koyanagi"}],"year":"2003","title":"Kernels for Semi-Structured Data","source":"H. Kashima and T. Koyanagi. 2003. Kernels for Semi-Structured Data. ICML-2003"},{"authors":[{"first":"Dan","last":"Klein"},{"first":"Christopher","middle":"D.","last":"Manning"}],"year":"2001","title":"Parsing and Hypergraphs","source":"Dan Klein and Christopher D. Manning. 2001. Parsing and Hypergraphs. IWPT-2001"},{"authors":[{"first":"T.","last":"Joachims"}],"year":"1998","title":"Text Categorization with Support Vecor Machine: learning with many relevant features","source":"T. Joachims. 1998. Text Categorization with Support Vecor Machine: learning with many relevant features. ECML-1998"},{"authors":[{"first":"Haitao","last":"Mi"},{"first":"Liang","last":"Huang"}],"year":"2008","title":"Forest-based Translation Rule Extraction","source":"Haitao Mi and Liang Huang. 2008. Forest-based Translation Rule Extraction. EMNLP-2008"},{"authors":[{"first":"Alessandro","last":"Moschitti"}],"year":"2004","title":"A Study on Convolution Kernels for Shallow Semantic Parsing","source":"Alessandro Moschitti. 2004. A Study on Convolution Kernels for Shallow Semantic Parsing. ACL-2004"},{"authors":[{"first":"Alessandro","last":"Moschitti"}],"year":"2006","title":"Syntactic kernels for natural language learning: the semantic role labeling case","source":"Alessandro Moschitti. 2006. Syntactic kernels for natural language learning: the semantic role labeling case. HLT-NAACL-2006 (short paper)"},{"authors":[{"first":"Martha","last":"Palmer"},{"first":"Dan","last":"Gildea"},{"first":"Paul","last":"Kingsbury"}],"year":"2005","title":"The proposition bank: An annotated corpus of semantic roles","source":"Martha Palmer, Dan Gildea and Paul Kingsbury. 2005. The proposition bank: An annotated corpus of semantic roles. Computational Linguistics. 31(1)"},{"authors":[{"first":"F.","last":"Rosenblatt"}],"year":"1962","title":"Principles of Neurodynamics: Perceptrons and the theory of brain mechanisms","source":"F. Rosenblatt. 1962. Principles of Neurodynamics: Perceptrons and the theory of brain mechanisms. Spartan Books, Washington D.C."},{"authors":[{"first":"Masaru","last":"Tomita"}],"year":"1987","title":"An Efficient Augmented-Context-Free Parsing Algorithm","source":"Masaru Tomita. 1987. An Efficient Augmented-Context-Free Parsing Algorithm. Computational Linguistics 13(1-2): 31-46"},{"authors":[{"first":"Vladimir","middle":"N.","last":"Vapnik"}],"year":"1998","title":"Statistical Learning Theory","source":"Vladimir N. Vapnik. 1998. Statistical Learning Theory. Wiley"},{"authors":[{"first":"C.","last":"Watkins"}],"year":"1999","title":"Dynamic alignment kernels","source":"C. Watkins. 1999. Dynamic alignment kernels. In A. J. Smola, B. Schölkopf, P. Bartlett, and D. Schuurmans (Eds.), Advances in kernel methods. MIT Press"},{"authors":[{"first":"Nianwen","last":"Xue"},{"first":"Martha","last":"Palmer"}],"year":"2004","title":"Calibrating features for semantic role labeling","source":"Nianwen Xue and Martha Palmer. 2004. Calibrating features for semantic role labeling. EMNLP-2004"},{"authors":[{"first":"Xiaofeng","last":"Yang"},{"first":"Jian","last":"Su"},{"first":"Chew","middle":"Lim","last":"Tan"}],"year":"2006","title":"Kernel-Based Pronoun Resolution with Structured Syntactic Knowledge","source":"Xiaofeng Yang, Jian Su and Chew Lim Tan. 2006. Kernel-Based Pronoun Resolution with Structured Syntactic Knowledge. COLING-ACL-2006"},{"authors":[{"first":"Dell","last":"Zhang"},{"first":"W.","last":"Lee"}],"year":"2003","title":"Question classification using support vector machines","source":"Dell Zhang and W. Lee. 2003. Question classification using support vector machines. SIGIR-2003"},{"authors":[{"first":"Hui","last":"Zhang"},{"first":"Min","last":"Zhang"},{"first":"Haizhou","last":"Li"},{"first":"Aiti","last":"Aw"},{"first":"Chew","middle":"Lim","last":"Tan"}],"year":"2009a","title":"Forest-based Tree Sequence to String Translation Model","source":"Hui Zhang, Min Zhang, Haizhou Li, Aiti Aw and Chew Lim Tan. 2009a. Forest-based Tree Sequence to String Translation Model. ACL-IJCNLP-2009"},{"authors":[{"first":"Hui","last":"Zhang"},{"first":"Min","last":"Zhang"},{"first":"Haizhou","last":"Li"},{"first":"Chew","middle":"Lim","last":"Tan"}],"year":"2009b","title":"Fast Translation Rule Matching for 884 Syntax-based Statistical Machine Translation","source":"Hui Zhang, Min Zhang, Haizhou Li and Chew Lim Tan. 2009b. Fast Translation Rule Matching for 884 Syntax-based Statistical Machine Translation. EMNLP-2009"},{"authors":[{"first":"Min","last":"Zhang"},{"first":"Jie","last":"Zhang"},{"first":"Jian","last":"Su"},{"first":"GuoDong","last":"Zhou"}],"year":"2006","title":"A Composite Kernel to Extract Relations between Entities with Both Flat and Structured Features","source":"Min Zhang, Jie Zhang, Jian Su and GuoDong Zhou. 2006. A Composite Kernel to Extract Relations between Entities with Both Flat and Structured Features. COLING-ACL-2006"},{"authors":[{"first":"Min","last":"Zhang"},{"first":"W.","last":"Che"},{"first":"A.","last":"Aw"},{"first":"C.","last":"Tan"},{"first":"G.","last":"Zhou"},{"first":"T.","last":"Liu"},{"first":"S.","last":"Li"}],"year":"2007","title":"A Grammar-driven Convolution Tree Kernel for Semantic Role Classification","source":"Min Zhang, W. Che, A. Aw, C. Tan, G. Zhou, T. Liu and S. Li. 2007. A Grammar-driven Convolution Tree Kernel for Semantic Role Classification. ACL-2007"},{"authors":[{"first":"Min","last":"Zhang"},{"first":"Hongfei","last":"Jiang"},{"first":"Aiti","last":"Aw"},{"first":"Haizhou","last":"Li"},{"first":"Chew","middle":"Lim","last":"Tan"},{"first":"Sheng","last":"Li"}],"year":"2008","title":"A Tree Sequence Alignment-based Tree-to-Tree Translation Model","source":"Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li, Chew Lim Tan and Sheng Li. 2008. A Tree Sequence Alignment-based Tree-to-Tree Translation Model. ACL-2008"},{"authors":[{"first":"Min","last":"Zhang"},{"first":"Haizhou","last":"Li"}],"year":"2009","title":"Tree Kernel-based SVM with Structured Syntactic Knowledge for BTG-based Phrase Reordering","source":"Min Zhang and Haizhou Li. 2009. Tree Kernel-based SVM with Structured Syntactic Knowledge for BTG-based Phrase Reordering. EMNLP-2009 885"}],"cites":[{"style":0,"text":"Collins and Duffy, 2002","origin":{"pointer":"/sections/2/paragraphs/0","offset":354,"length":23},"authors":[{"last":"Collins"},{"last":"Duffy"}],"year":"2002","references":["/references/12"]},{"style":0,"text":"Moschitti, 2004","origin":{"pointer":"/sections/2/paragraphs/0","offset":379,"length":15},"authors":[{"last":"Moschitti"}],"year":"2004","references":["/references/24"]},{"style":0,"text":"Zhang et al., 2007","origin":{"pointer":"/sections/2/paragraphs/0","offset":396,"length":18},"authors":[{"last":"Zhang"},{"last":"al."}],"year":"2007","references":["/references/37"]},{"style":0,"text":"Tomita, 1987","origin":{"pointer":"/sections/2/paragraphs/0","offset":427,"length":12},"authors":[{"last":"Tomita"}],"year":"1987","references":["/references/28"]},{"style":0,"text":"Haussler, 1999","origin":{"pointer":"/sections/2/paragraphs/0","offset":615,"length":14},"authors":[{"last":"Haussler"}],"year":"1999","references":["/references/17"]},{"style":0,"text":"Vapnik, 1998","origin":{"pointer":"/sections/2/paragraphs/1","offset":109,"length":12},"authors":[{"last":"Vapnik"}],"year":"1998","references":["/references/29"]},{"style":0,"text":"Vapnik, 1998","origin":{"pointer":"/sections/2/paragraphs/1","offset":887,"length":12},"authors":[{"last":"Vapnik"}],"year":"1998","references":["/references/29"]},{"style":0,"text":"Vapnik, 1998","origin":{"pointer":"/sections/2/paragraphs/2","offset":39,"length":12},"authors":[{"last":"Vapnik"}],"year":"1998","references":["/references/29"]},{"style":0,"text":"Rosenblatt, 1962","origin":{"pointer":"/sections/2/paragraphs/2","offset":89,"length":16},"authors":[{"last":"Rosenblatt"}],"year":"1962","references":["/references/27"]},{"style":0,"text":"Freund and Schapire, 1999","origin":{"pointer":"/sections/2/paragraphs/2","offset":129,"length":25},"authors":[{"last":"Freund"},{"last":"Schapire"}],"year":"1999","references":["/references/15"]},{"style":0,"text":"Collins and Duffy, 2002","origin":{"pointer":"/sections/2/paragraphs/3","offset":55,"length":23},"authors":[{"last":"Collins"},{"last":"Duffy"}],"year":"2002","references":["/references/12"]},{"style":0,"text":"Collins and Duffy, 2002","origin":{"pointer":"/sections/2/paragraphs/3","offset":323,"length":23},"authors":[{"last":"Collins"},{"last":"Duffy"}],"year":"2002","references":["/references/12"]},{"style":0,"text":"Moschitti, 2004","origin":{"pointer":"/sections/2/paragraphs/3","offset":373,"length":15},"authors":[{"last":"Moschitti"}],"year":"2004","references":["/references/24"]},{"style":0,"text":"Zhang et al., 2007","origin":{"pointer":"/sections/2/paragraphs/3","offset":390,"length":18},"authors":[{"last":"Zhang"},{"last":"al."}],"year":"2007","references":["/references/37"]},{"style":0,"text":"Zhang et al., 2006","origin":{"pointer":"/sections/2/paragraphs/3","offset":432,"length":18},"authors":[{"last":"Zhang"},{"last":"al."}],"year":"2006","references":["/references/36"]},{"style":0,"text":"Yang et al., 2006","origin":{"pointer":"/sections/2/paragraphs/3","offset":473,"length":17},"authors":[{"last":"Yang"},{"last":"al."}],"year":"2006","references":["/references/32"]},{"style":0,"text":"Zhang and Lee, 2003","origin":{"pointer":"/sections/2/paragraphs/3","offset":518,"length":19},"authors":[{"last":"Zhang"},{"last":"Lee"}],"year":"2003","references":["/references/33"]},{"style":0,"text":"Zhang and Li, 2009","origin":{"pointer":"/sections/2/paragraphs/3","offset":564,"length":18},"authors":[{"last":"Zhang"},{"last":"Li"}],"year":"2009","references":["/references/39"]},{"style":0,"text":"Haussler (1999)","origin":{"pointer":"/sections/3/paragraphs/0","offset":83,"length":15},"authors":[{"last":"Haussler"}],"year":"1999","references":["/references/17"]},{"style":0,"text":"Watkins, 1999","origin":{"pointer":"/sections/3/paragraphs/0","offset":183,"length":13},"authors":[{"last":"Watkins"}],"year":"1999","references":["/references/30"]},{"style":0,"text":"Collins and Duffy, 2002","origin":{"pointer":"/sections/3/paragraphs/1","offset":23,"length":23},"authors":[{"last":"Collins"},{"last":"Duffy"}],"year":"2002","references":["/references/12"]},{"style":0,"text":"Collins and Duffy (2002)","origin":{"pointer":"/sections/6/paragraphs/0","offset":37,"length":24},"authors":[{"last":"Collins"},{"last":"Duffy"}],"year":"2002","references":["/references/12"]},{"style":0,"text":"Collins and Duffy, 2002","origin":{"pointer":"/sections/22/paragraphs/0","offset":107,"length":23},"authors":[{"last":"Collins"},{"last":"Duffy"}],"year":"2002","references":["/references/12"]},{"style":0,"text":"Tomita, 1987","origin":{"pointer":"/sections/24/paragraphs/0","offset":419,"length":12},"authors":[{"last":"Tomita"}],"year":"1987","references":["/references/28"]},{"style":0,"text":"Billot and Lang, 1989","origin":{"pointer":"/sections/24/paragraphs/0","offset":433,"length":21},"authors":[{"last":"Billot"},{"last":"Lang"}],"year":"1989","references":["/references/4"]},{"style":0,"text":"Klein and Manning, 2001","origin":{"pointer":"/sections/24/paragraphs/0","offset":456,"length":23},"authors":[{"last":"Klein"},{"last":"Manning"}],"year":"2001","references":["/references/21"]},{"style":0,"text":"Zhang et al., 2008","origin":{"pointer":"/sections/24/paragraphs/0","offset":626,"length":18},"authors":[{"last":"Zhang"},{"last":"al."}],"year":"2008","references":["/references/38"]},{"style":0,"text":"Zhang et al., 2009a","origin":{"pointer":"/sections/24/paragraphs/0","offset":646,"length":19},"authors":[{"last":"Zhang"},{"last":"al."}],"year":"2009a","references":["/references/34"]},{"style":0,"text":"Baker, 1979","origin":{"pointer":"/sections/24/paragraphs/2","offset":147,"length":11},"authors":[{"last":"Baker"}],"year":"1979","references":["/references/3"]},{"style":0,"text":"Lari and Young, 1990","origin":{"pointer":"/sections/24/paragraphs/2","offset":160,"length":20},"authors":[{"last":"Lari"},{"last":"Young"}],"year":"1990","references":["/references/19"]},{"style":0,"text":"Charniak and Johnson, 2005","origin":{"pointer":"/sections/25/paragraphs/6","offset":484,"length":26},"authors":[{"last":"Charniak"},{"last":"Johnson"}],"year":"2005","references":["/references/8"]},{"style":0,"text":"Huang, 2008","origin":{"pointer":"/sections/25/paragraphs/6","offset":512,"length":11},"authors":[{"last":"Huang"}],"year":"2008","references":["/references/18"]},{"style":0,"text":"Lari and Young, 1990","origin":{"pointer":"/sections/25/paragraphs/10","offset":560,"length":20},"authors":[{"last":"Lari"},{"last":"Young"}],"year":"1990","references":["/references/19"]},{"style":0,"text":"Mi and Huang, 2008","origin":{"pointer":"/sections/25/paragraphs/10","offset":582,"length":18},"authors":[{"last":"Mi"},{"last":"Huang"}],"year":"2008","references":["/references/23"]},{"style":0,"text":"Baker, 1979","origin":{"pointer":"/sections/25/paragraphs/25","offset":63,"length":11},"authors":[{"last":"Baker"}],"year":"1979","references":["/references/3"]},{"style":0,"text":"Lari and Young, 1990","origin":{"pointer":"/sections/25/paragraphs/25","offset":76,"length":20},"authors":[{"last":"Lari"},{"last":"Young"}],"year":"1990","references":["/references/19"]},{"style":0,"text":"Collins and Duffy, 2002","origin":{"pointer":"/sections/25/paragraphs/28","offset":613,"length":23},"authors":[{"last":"Collins"},{"last":"Duffy"}],"year":"2002","references":["/references/12"]},{"style":0,"text":"Zhang et al., 2009b","origin":{"pointer":"/sections/25/paragraphs/31","offset":21,"length":19},"authors":[{"last":"Zhang"},{"last":"al."}],"year":"2009b","references":["/references/35"]},{"style":0,"text":"Collins and Duffy, 2002","origin":{"pointer":"/sections/25/paragraphs/36","offset":226,"length":23},"authors":[{"last":"Collins"},{"last":"Duffy"}],"year":"2002","references":["/references/12"]},{"style":0,"text":"Kashima and Koyanagi, 2003","origin":{"pointer":"/sections/25/paragraphs/57","offset":364,"length":26},"authors":[{"last":"Kashima"},{"last":"Koyanagi"}],"year":"2003","references":["/references/20"]},{"style":0,"text":"Moschitti, 2006","origin":{"pointer":"/sections/25/paragraphs/57","offset":392,"length":15},"authors":[{"last":"Moschitti"}],"year":"2006","references":["/references/25"]},{"style":0,"text":"Zhang et al., 2007","origin":{"pointer":"/sections/25/paragraphs/57","offset":409,"length":18},"authors":[{"last":"Zhang"},{"last":"al."}],"year":"2007","references":["/references/37"]},{"style":0,"text":"Dyer et al., 2008","origin":{"pointer":"/sections/25/paragraphs/58","offset":307,"length":17},"authors":[{"last":"Dyer"},{"last":"al."}],"year":"2008","references":["/references/13"]},{"style":0,"text":"Bunescu (2008)","origin":{"pointer":"/sections/25/paragraphs/58","offset":416,"length":14},"authors":[{"last":"Bunescu"}],"year":"2008","references":["/references/5"]},{"style":0,"text":"Finkel et al. (2006)","origin":{"pointer":"/sections/25/paragraphs/58","offset":435,"length":20},"authors":[{"last":"Finkel"},{"last":"al."}],"year":"2006","references":["/references/14"]},{"style":0,"text":"Gildea, 2002","origin":{"pointer":"/sections/26/paragraphs/0","offset":177,"length":12},"authors":[{"last":"Gildea"}],"year":"2002","references":[]},{"style":0,"text":"ACE, 2002-2006","origin":{"pointer":"/sections/26/paragraphs/0","offset":221,"length":14},"authors":[{"last":"ACE"}],"year":"2002-2006","references":["/references/0"]},{"style":0,"text":"Vapnik, 1998","origin":{"pointer":"/sections/26/paragraphs/1","offset":25,"length":12},"authors":[{"last":"Vapnik"}],"year":"1998","references":["/references/29"]},{"style":0,"text":"Joachims, 1998","origin":{"pointer":"/sections/26/paragraphs/1","offset":230,"length":14},"authors":[{"last":"Joachims"}],"year":"1998","references":["/references/22"]},{"style":0,"text":"Moschitti, 2004","origin":{"pointer":"/sections/26/paragraphs/1","offset":297,"length":15},"authors":[{"last":"Moschitti"}],"year":"2004","references":["/references/24"]},{"style":0,"text":"Charniak, 2001","origin":{"pointer":"/sections/26/paragraphs/1","offset":391,"length":14},"authors":[{"last":"Charniak"}],"year":"2001","references":["/references/7"]},{"style":0,"text":"Charniak and Johnson, 2005","origin":{"pointer":"/sections/26/paragraphs/1","offset":475,"length":26},"authors":[{"last":"Charniak"},{"last":"Johnson"}],"year":"2005","references":["/references/8"]},{"style":0,"text":"Che et al. (2008)","origin":{"pointer":"/sections/26/paragraphs/1","offset":1133,"length":17},"authors":[{"last":"Che"},{"last":"al."}],"year":"2008","references":["/references/10"]},{"style":0,"text":"Charniak, 2001","origin":{"pointer":"/sections/26/paragraphs/2","offset":399,"length":14},"authors":[{"last":"Charniak"}],"year":"2001","references":["/references/7"]},{"style":0,"text":"Collins, 1999","origin":{"pointer":"/sections/26/paragraphs/2","offset":482,"length":13},"authors":[{"last":"Collins"}],"year":"1999","references":["/references/11"]},{"style":0,"text":"Xue and Palmer, 2004","origin":{"pointer":"/sections/26/paragraphs/2","offset":954,"length":20},"authors":[{"last":"Xue"},{"last":"Palmer"}],"year":"2004","references":["/references/31"]},{"style":0,"text":"Xue and Palmer, 2004","origin":{"pointer":"/sections/26/paragraphs/2","offset":1007,"length":20},"authors":[{"last":"Xue"},{"last":"Palmer"}],"year":"2004","references":["/references/31"]},{"style":0,"text":"Che et al. (2006)","origin":{"pointer":"/sections/26/paragraphs/3","offset":109,"length":17},"authors":[{"last":"Che"},{"last":"al."}],"year":"2006","references":["/references/9"]},{"style":0,"text":"Zhang et al. (2006)","origin":{"pointer":"/sections/26/paragraphs/9","offset":1378,"length":19},"authors":[{"last":"Zhang"},{"last":"al."}],"year":"2006","references":["/references/36"]},{"style":0,"text":"Zhang et al. (2006)","origin":{"pointer":"/sections/26/paragraphs/9","offset":1763,"length":19},"authors":[{"last":"Zhang"},{"last":"al."}],"year":"2006","references":["/references/36"]},{"style":0,"text":"Zhang et al., 2006","origin":{"pointer":"/sections/26/paragraphs/10","offset":107,"length":18},"authors":[{"last":"Zhang"},{"last":"al."}],"year":"2006","references":["/references/36"]},{"style":0,"text":"Zhang et al. (2006)","origin":{"pointer":"/sections/26/paragraphs/12","offset":248,"length":19},"authors":[{"last":"Zhang"},{"last":"al."}],"year":"2006","references":["/references/36"]}]}
