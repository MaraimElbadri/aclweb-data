{"sections":[{"title":"","paragraphs":["Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 834–843, Uppsala, Sweden, 11-16 July 2010. c⃝2010 Association for Computational Linguistics"]},{"title":"Bilingual Sense Similarity for Statistical Machine Translation   Boxing Chen, George Foster and Roland Kuhn","paragraphs":["National Research Council Canada"]},{"title":"283 Alexandre-Taché Boulevard, Gatineau (Québec), Canada J8X 3X7 {Boxing.Chen, George.Foster, Roland.Kuhn}@nrc.ca    Abstract ","paragraphs":["This paper proposes new algorithms to compute the sense similarity between two units (words, phrases, rules, etc.) from parallel corpora. The sense similarity scores are computed by using the vector space model. We then apply the algorithms to statistical machine translation by computing the sense similarity between the source and target side of translation rule pairs. Similarity scores are used as additional features of the translation model to improve translation performance. Significant improvements are obtained over a state-of-the-art hierarchical phrase-based machine translation system."]},{"title":"1 Introduction","paragraphs":["The sense of a term can generally be inferred from its context. The underlying idea is that a term is characterized by the contexts it co-occurs with. This is also well known as the Distributional Hypothesis (Harris, 1954): terms occurring in similar contexts tend to have similar meanings. There has been a lot of work to compute the sense similarity between terms based on their distribution in a corpus, such as (Hindle, 1990; Lund and Burgess, 1996; Landauer and Dumais, 1997; Lin, 1998; Turney, 2001; Pantel and Lin, 2002; Pado and Lapata, 2007).","In the work just cited, a common procedure is followed. Given two terms to be compared, one first extracts various features for each term from their contexts in a corpus and forms a vector space model (VSM); then, one computes their similarity by using similarity functions. The features include words within a surface window of a fixed size (Lund and Burgess, 1996), grammatical dependencies (Lin, 1998; Pantel and Lin 2002; Pado and Lapata, 2007), etc. The similarity function which has been most widely used is cosine distance (Salton and McGill, 1983); other similarity functions include Euclidean distance, City Block distance (Bullinaria and Levy; 2007), and Dice and Jaccard coefficients (Frakes and Baeza-Yates, 1992), etc. Measures of monolingual sense similarity have been widely used in many applications, such as synonym recognizing (Landauer and Dumais, 1997), word clustering (Pantel and Lin 2002), word sense disambiguation (Yuret and Yatbaz 2009), etc.","Use of the vector space model to compute sense similarity has also been adapted to the multilingual condition, based on the assumption that two terms with similar meanings often occur in comparable contexts across languages. Fung (1998) and Rapp (1999) adopted VSM for the application of extracting translation pairs from comparable or even unrelated corpora. The vectors in different languages are first mapped to a common space using an initial bilingual dictionary, and then compared.","However, there is no previous work that uses the VSM to compute sense similarity for terms from parallel corpora. The sense similarities, i.e. the translation probabilities in a translation model, for units from parallel corpora are mainly based on the co-occurrence counts of the two units. Therefore, questions emerge: how good is the sense similarity computed via VSM for two units from parallel corpora? Is it useful for multilingual applications, such as statistical machine translation (SMT)?","In this paper, we try to answer these questions, focusing on sense similarity applied to the SMT task. For this task, translation rules are heuristically extracted from automatically word-aligned sentence pairs. Due to noise in the training corpus or wrong word alignment, the source and target sides of some rules are not semantically equivalent, as can be seen from the following 834 real examples which are taken from the rule table built on our training data (Section 5.1): 世界 上 X 之一 ||| one of X (*) 世界 上 X 之一 ||| one of X in the world 许多 市民 ||| many citizens 许多 市民 ||| many hong kong residents (*) The source and target sides of the rules with (*) at the end are not semantically equivalent; it seems likely that measuring the semantic similarity from their context between the source and target sides of rules might be helpful to machine translation.","In this work, we first propose new algorithms to compute the sense similarity between two units (unit here includes word, phrase, rule, etc.) in different languages by using their contexts. Second, we use the sense similarities between the source and target sides of a translation rule to improve statistical machine translation performance.","This work attempts to measure directly the sense similarity for units from different languages by comparing their contexts","1",". Our contribution includes proposing new bilingual sense similarity algorithms and applying them to machine translation.","We chose a hierarchical phrase-based SMT system as our baseline; thus, the units involved in computation of sense similarities are hierarchical rules."]},{"title":"2 Hierarchical phrase-based MT system","paragraphs":["The hierarchical phrase-based translation method (Chiang, 2005; Chiang, 2007) is a formal syntax-based translation modeling method; its translation model is a weighted synchronous context free grammar (SCFG). No explicit linguistic syntactic information appears in the model. An SCFG rule has the following form:"]},{"title":"~,,γα→X","paragraphs":["where X is a non-terminal symbol shared by all the rules; each rule has at most two non-terminals."]},{"title":"α","paragraphs":["("]},{"title":"γ","paragraphs":[") is a source (target) string consisting of terminal and non-terminal symbols."]},{"title":"~","paragraphs":["defines a one-to-one correspondence between non-terminals in"]},{"title":"α","paragraphs":["and"]},{"title":"γ","paragraphs":[". 1 There has been a lot of work (more details in Section 7) on applying word sense disambiguation (WSD) techniques in SMT for translation selection. However, WSD techniques for SMT do so indirectly, using source-side context to help select a particular translation for a source rule.","source target Ini. phr. 他 出席 了 会议 he attended the meeting Rule 1 Context 1","他 出席 了 X1 会议 he attended X1 the, meeting Rule 2 Context 2","会议 他, 出席, 了 the meeting he, attended Rule 3 Context 3 他 X1 会议 出席, 了","he X1 the meeting attended Rule 4 Context 4 出席 了 他,会议","attended he, the, meeting  Figure 1: example of hierarchical rule pairs and their context features.","","Rule frequencies are counted during rule extraction over word-aligned sentence pairs, and they are normalized to estimate features on rules. Following (Chiang, 2005; Chiang, 2007), 4 features are computed for each rule:","• )|( αγP and )|( γαP are direct and in-","verse rule-based conditional probabilities;","• )|( αγwP and )|( γαwP are direct and in-verse lexical weights (Koehn et al., 2003).","Empirically, this method has yielded better performance on language pairs such as Chinese-English than the phrase-based method because it permits phrases with gaps; it generalizes the normal phrase-based models in a way that allows long-distance reordering (Chiang, 2005; Chiang, 2007). We use the Joshua implementation of the method for decoding (Li et al., 2009)."]},{"title":"3 Bag-of-Words Vector Space Model","paragraphs":["To compute the sense similarity via VSM, we follow the previous work (Lin, 1998) and represent the source and target side of a rule by feature vectors. In our work, each feature corresponds to a context word which co-occurs with the translation rule. 3.1 Context Features In the hierarchical phrase-based translation method, the translation rules are extracted by abstracting some words from an initial phrase pair (Chiang, 2005). Consider a rule with non-terminals on the source and target side; for a given instance of the rule (a particular phrase pair in the training corpus), the context will be the words instantiating the non-terminals. In turn, the context for the sub-phrases that instantiate the non-terminals will be the words in the remainder of the phrase pair. For example in Figure 1, if we 835 have an initial phrase pair 他 出席 了 会议 ||| he attended the meeting, and we extract four rules from this initial phrase: 他 出席 了 X1 ||| he attended X1, 会议 ||| the meeting, 他 X1 会议 ||| he X1 the meeting, and 出席 了 ||| attended. There-fore, the and meeting are context features of target pattern he attended X1; he and attended are the context features of the meeting; attended is the context feature of he X1 the meeting; also he, the and meeting are the context feature of attended (in each case, there are also source-side context features). 3.2 Bag-of-Words Model For each side of a translation rule pair, its context words are all collected from the training data, and two “bags-of-words” which consist of collections of source and target context words co-occurring with the rule’s source and target sides are created."]},{"title":"},...,,{ },...,,{","paragraphs":["21 21 Je If"]},{"title":"eeeB fffB = = ","paragraphs":["(1) where"]},{"title":")1( Iif","paragraphs":["i"]},{"title":"≤≤","paragraphs":["are source context words which co-occur with the source side of rule"]},{"title":"α","paragraphs":[", and"]},{"title":")1( Jje","paragraphs":["j"]},{"title":"≤≤","paragraphs":["are target context words which co-occur with the target side of rule"]},{"title":"γ","paragraphs":[". Therefore, we can represent source and target","sides of the rule by vectors f"]},{"title":"vv ","paragraphs":["and e"]},{"title":"vv ","paragraphs":["as in Equation (2):"]},{"title":"},...,,{ },...,,{","paragraphs":["21 21 J I eeee ffff"]},{"title":"wwwv wwwv = = v v ","paragraphs":["(2) where if"]},{"title":"w","paragraphs":["and je"]},{"title":"w","paragraphs":["are values for each source and target context feature; normally, these values are based on the counts of the words in the corresponding bags. 3.3 Feature Weighting Schemes We use pointwise mutual information (Church et al., 1990) to compute the feature values. Let c ( f"]},{"title":"Bc ∈","paragraphs":["or e"]},{"title":"Bc ∈","paragraphs":[") be a context word and"]},{"title":"),( crF","paragraphs":["be the frequency count of a rule r ("]},{"title":"α","paragraphs":["or"]},{"title":"γ","paragraphs":[") co-occurring with the context word c. The pointwise mutual information"]},{"title":"),( crMI","paragraphs":["is defined as: NcF NrF N crF crMIcrw )( log )( log ),( log ),(),( ×== (3) where N is the total frequency counts of all rules and their context words. Since we are using this value as a weight, following (Turney, 2001), we drop log, N and"]},{"title":")(rF","paragraphs":[". Thus (3) simplifies to:"]},{"title":")( ),( ),( cF crFcrw =","paragraphs":["(4) It can be seen as an estimate of"]},{"title":")|( crP","paragraphs":[", the em-","pirical probability of observing r given c. A problem with"]},{"title":")|( crP","paragraphs":["is that it is biased towards infrequent words/features. We therefore smooth"]},{"title":"),( crw","paragraphs":["with add-k smoothing:"]},{"title":"kRcF kcrF kcrF kcrF crw","paragraphs":["R i i"]},{"title":"+ + = + + = ∑","paragraphs":["="]},{"title":")( ),( )),(( ),( ),(","paragraphs":["1 (5) where k is a tunable global smoothing constant, and R is the number of rules."]},{"title":"4 Similarity Functions","paragraphs":["There are many possibilities for calculating similarities between bags-of-words in different languages. We consider IBM model 1 probabilities and cosine distance similarity functions. 4.1 IBM Model 1 Probabilities For the IBM model 1 similarity function, we take the geometric mean of symmetrized conditional IBM model 1 (Brown et al., 1993) bag probabilities, as in Equation (6).","))|()|((),( feef BBPBBPsqrtsim ⋅=γα (6) To compute"]},{"title":")|(","paragraphs":["ef"]},{"title":"BBP","paragraphs":[", IBM model 1 as-sumes that all source words are conditionally independent, so that: "]},{"title":"∏","paragraphs":["="]},{"title":"=","paragraphs":["I i eief"]},{"title":"BfpBBP","paragraphs":["1"]},{"title":")|()|(","paragraphs":["(7)","To compute, we use a “Noisy-OR” combination which has shown better performance than standard IBM model 1 probability, as described in (Zens and Ney, 2004):"]},{"title":")|(1)|(","paragraphs":["eiei"]},{"title":"BfpBfp −=","paragraphs":["(8)"]},{"title":"∏","paragraphs":["="]},{"title":"−−≈","paragraphs":["J j jiei"]},{"title":"efpBfp","paragraphs":["1"]},{"title":"))|(1(1)|(","paragraphs":["(9) where"]},{"title":")|(","paragraphs":["ei"]},{"title":"Bfp","paragraphs":["is the probability that i"]},{"title":"f","paragraphs":["is not in the translation of e"]},{"title":"B","paragraphs":[", and is the IBM model 1 probability. 4.2 Vector Space Mapping A common way to calculate semantic similarity is by vector space cosine distance; we will also 836 use this similarity function in our algorithm. However, the two vectors in Equation (2) cannot be directly compared because the axes of their spaces represent different words in different languages, and also their dimensions I and J are not assured to be the same. Therefore, we need to first map a vector into the space of the other vector, so that the similarity can be calculated. Fung (1998) and Rapp (1999) map the vector one-dimension-to-one-dimension (a context word is a dimension in each vector space) from one language to another language via an initial bilingual dictionary. We follow (Zhao et al., 2004) to do vector space mapping.","Our goal is – given a source pattern – to distinguish between the senses of its associated target patterns. Therefore, we map all vectors in target language into the vector space in the source language. What we want is a representa-tion avv in the source language space of the target vector evv . To get avv",", we can let if a"]},{"title":"w","paragraphs":[", the weight of the i th","source feature, be a linear combination over target features. That is to say, given a source feature weight for fi, each target feature weight is linked to it with some probability. So that we can calculate a transformed vector from the target vectors by calculating weights if","aw using a translation lexicon:"]},{"title":"∑","paragraphs":["="]},{"title":"=","paragraphs":["J j eji f a ji"]},{"title":"wefw","paragraphs":["1"]},{"title":")|Pr(","paragraphs":["(10) where"]},{"title":")|(","paragraphs":["ji"]},{"title":"efp","paragraphs":["is a lexical probability (we use IBM model 1 probability). Now the source vector and the mapped vector a"]},{"title":"vv ","paragraphs":["have the same dimensions as shown in (11):"]},{"title":"},...,,{ },...,,{","paragraphs":["21 21 I I f a f a f aa ffff"]},{"title":"wwwv wwwv = = v v ","paragraphs":["(11) 4.3 Naïve Cosine Distance Similarity The standard cosine distance is defined as the inner product of the two vectors f"]},{"title":"vv ","paragraphs":["and a"]},{"title":"vv ","paragraphs":["normalized by their norms. Based on Equation (10) and (11), it is easy to derive the similarity as follows: )()( )|Pr( |||| ),cos(),( 1 2 1 2 1 1"]},{"title":"∑∑ ∑∑","paragraphs":["== = = = ⋅⋅ == I i f a I I f I i J j ejif af af af i i ji wsqrtwsqrt wefw","vv vv vvsim vv vv vv γα (12) where I and J are the number of the words in source and target bag-of-words; if"]},{"title":"w","paragraphs":["and je"]},{"title":"w","paragraphs":["are","values of source and target features; if a"]},{"title":"w","paragraphs":["is the transformed weight mapped from all target features to the source dimension at word fi. 4.4 Improved Similarity Function To incorporate more information than the original similarity functions – IBM model 1 probabilities in Equation (6) and naïve cosine distance similarity function in Equation (12) – we refine the similarity function and propose a new algorithm.","As shown in Figure 2, suppose that we have a rule pair"]},{"title":"),( γα","paragraphs":[". full f"]},{"title":"C","paragraphs":["and full e"]},{"title":"C","paragraphs":["are the contexts extracted according to the definition in section 3 from the full training data for"]},{"title":"α","paragraphs":["and for"]},{"title":"γ","paragraphs":[", respectively. cooc f"]},{"title":"C","paragraphs":["and cooc e"]},{"title":"C","paragraphs":["are the contexts for"]},{"title":"α","paragraphs":["and"]},{"title":"γ","paragraphs":["when"]},{"title":"α","paragraphs":["and"]},{"title":"γ","paragraphs":["co-occur. Obviously, they satisfy the constraints: full f cooc f"]},{"title":"CC ⊆","paragraphs":["and full e cooc e"]},{"title":"CC ⊆","paragraphs":[". Therefore, the original similarity","functions are to compare the two context vectors","built on full training data directly, as shown in","Equation (13).","),(),( full","e","full","f CCsimsim =γα (13)","Then, we propose a new similarity function as","follows:","3","2","1","),(),(),( ),( λλλ γα cooc e full e cooc e cooc f cooc f full f CCsimCCsimCCsim sim ⋅⋅ = (14)","where the parameters iλ (i=1,2,3) can be tuned","via minimal error rate training (MERT) (Och,","2003).         "," Figure 2: contexts for rule"]},{"title":"α","paragraphs":["and"]},{"title":"γ","paragraphs":[".","","A unit’s sense is defined by all its contexts in the whole training data; it may have a lot of different senses in the whole training data. However, when it is linked with another unit in the other language, its sense pool is constrained and is just"]},{"title":"α γ","paragraphs":["full f"]},{"title":"C","paragraphs":["cooc f"]},{"title":"C ","paragraphs":["full e"]},{"title":"C","paragraphs":["cooc e"]},{"title":"C","paragraphs":["837 a subset of the whole sense set."]},{"title":"),(","paragraphs":["cooc f full f"]},{"title":"CCsim","paragraphs":["is the metric which evaluates the similarity between the whole sense pool of"]},{"title":"α","paragraphs":["and the sense pool when"]},{"title":"α","paragraphs":["co-occurs with"]},{"title":"γ","paragraphs":[";"]},{"title":"),(","paragraphs":["cooc e full e"]},{"title":"CCsim","paragraphs":["is the analogous similarity metric for"]},{"title":"γ","paragraphs":[". They range from 0 to 1. These two metrics both evaluate the similarity for two vectors in the same language, so using cosine distance to compute the similarity is straightforward. And we can set a relatively large size for the vector, since it is not necessary to do vector mapping as the vectors are in the same language.","),( cooc","e cooc f CCsim computes the similarity between the context vectors when"]},{"title":"α","paragraphs":["and"]},{"title":"γ","paragraphs":["co-occur. We","may compute ),( cooc e cooc f CCsim by using IBM model 1 probability and cosine distance similarity functions as Equation (6) and (12). Therefore, on top of the degree of bilingual semantic similarity between a source and a target translation unit, we have also incorporated the monolingual semantic similarity between all occurrences of a source or target unit, and that unit’s occurrence as part of the given rule, into the sense similarity measure."]},{"title":"5 Experiments","paragraphs":["We evaluate the algorithm of bilingual sense similarity via machine translation. The sense similarity scores are used as feature functions in the translation model. 5.1 Data We evaluated with different language pairs: Chinese-to-English, and German-to-English. For Chinese-to-English tasks, we carried out the experiments in two data conditions. The first one is the large data condition, based on training data for the NIST 2","2009 evaluation Chinese-to-English track. In particular, all the allowed bilingual corpora except the UN corpus and Hong Kong Hansard corpus have been used for estimating the translation model. The second one is the small data condition where only the FBIS","3"," corpus is used to train the translation model. We trained two language models: the first one is a 4-gram LM which is estimated on the target side of the texts used in the large data condition. The second LM is a 5-gram LM trained on the so-2 http://www.nist.gov/speech/tests/mt 3 LDC2003E14 called English Gigaword corpus. Both language models are used for both tasks.","We carried out experiments for translating Chinese to English. We use the same development and test sets for the two data conditions. We first created a development set which used mainly data from the NIST 2005 test set, and also some balanced-genre web-text from the NIST training material. Evaluation was performed on the NIST 2006 and 2008 test sets. Table 1 gives figures for training, development and test corpora; |S| is the number of the sentences, and |W| is the number of running words. Four references are provided for all dev and test sets."," Chi Eng ","Parallel Train Large Data |S| 3,322K |W| 64.2M 62.6M Small Data","|S| 245K","|W| 9.0M 10.5M","Dev |S| 1,506 1,506× 4","Test NIST06 |S| 1,664 1,664× 4","NIST08 |S| 1,357 1,357× 4","Gigaword |S| - 11.7M  Table 1: Statistics of training, dev, and test sets for Chinese-to-English task.","","For German-to-English tasks, we used WMT 2006 4","data sets. The parallel training data contains 21 million target words; both the dev set and test set contain 2000 sentences; one reference is provided for each source input sentence. Only the target-language half of the parallel training data are used to train the language model in this task. 5.2 Results For the baseline, we train the translation model by following (Chiang, 2005; Chiang, 2007) and our decoder is Joshua 5",", an open-source hierarchical phrase-based machine translation system written in Java. Our evaluation metric is IBM BLEU (Papineni et al., 2002), which performs case-insensitive matching of n-grams up to n = 4. Following (Koehn, 2004), we use the bootstrapresampling test to do significance testing.","By observing the results on dev set in the additional experiments, we first set the smoothing constant k in Equation (5) to 0.5.","Then, we need to set the sizes of the vectors to balance the computing time and translation accu-4 http://www.statmt.org/wmt06/ 5 http://www.cs.jhu.edu/~ccb/joshua/index.html 838 racy, i.e., we keep only the top N context words with the highest feature value for each side of a rule 6",". In the following, we use “Alg1” to represent the original similarity functions which compare the two context vectors built on full training data, as in Equation (13); while we use “Alg2” to represent the improved similarity as in Equation (14). “IBM” represents IBM model 1 probabilities, and “COS” represents cosine distance similarity function.","After carrying out a series of additional experiments on the small data condition and observing the results on the dev set, we set the size of the vector to 500 for Alg1; while for Alg2, we set the sizes of full","fC and full","eC N1 to 1000, and the sizes of cooc","fC and cooc","eC N2 to 100.","The sizes of the vectors in Alg2 are set in the following process: first, we set N2 to 500 and let N1 range from 500 to 3,000, we observed that the dev set got best performance when N1 was 1000; then we set N1 to 1000 and let N1 range from 50 to 1000, we got best performance when N1 =100. We use this setting as the default setting in all remaining experiments."," Algorithm NIST’06 NIST’08 Baseline 27.4 21.2 Alg1 IBM 27.8* 21.5 Alg1 COS 27.8* 21.5 Alg2 IBM 27.9* 21.6* Alg2 COS 28.1** 21.7*  Table 2: Results (BLEU%) of small data Chinese-to-English NIST task. Alg1 represents the original similarity functions as in Equation (13); while Alg2 represents the improved similarity as in Equation (14). IBM represents IBM model 1 probability, and COS represents cosine distance similarity function. * or ** means result is significantly better than the baseline (p < 0.05 or p < 0.01, respectively).","","Ch-En De-En Algorithm NIST’06 NIST’08 Test’06 Baseline 31.0 23.8 26.9 Alg2 IBM 31.5* 24.5** 27.2* Alg2 COS 31.6** 24.5** 27.3*  Table 3: Results (BLEU%) of large data Chinese-to-English NIST task and German-to-English WMT task. 6 We have also conducted additional experiments by remov-ing the stop words from the context vectors; however, we did not observe any consistent improvement. So we filter the context vectors by only considering the feature values.","Table 2 compares the performance of Alg1 and Alg2 on the Chinese-to-English small data condition. Both Alg1 and Alg2 improved the performance over the baseline, and Alg2 obtained slight and consistent improvements over Alg1. The improved similarity function Alg2 makes it possible to incorporate monolingual semantic similarity on top of the bilingual semantic similarity, thus it may improve the accuracy of the similarity estimate. Alg2 significantly improved the performance over the baseline. The Alg2 cosine similarity function got 0.7 BLEU-score (p<0.01) improvement over the baseline for NIST 2006 test set, and a 0.5 BLEU-score (p<0.05) for NIST 2008 test set.","Table 3 reports the performance of Alg2 on Chinese-to-English NIST large data condition and German-to-English WMT task. We can see that IBM model 1 and cosine distance similarity function both obtained significant improvement on all test sets of the two tasks. The two similarity functions obtained comparable results."]},{"title":"6 Analysis and Discussion 6.1 Effect of Single Features","paragraphs":["In Alg2, the similarity score consists of three parts as in Equation (14): ),( cooc","f full f CCsim ,","),( cooc e full e CCsim , and ),( cooc","e cooc f CCsim ; where","),( cooc e cooc f CCsim could be computed by IBM mod-","el 1 probabilities ),( cooc","e cooc fIBM CCsim or cosine dis-","tance similarity function ),( cooc","e","cooc","fCOS CCsim . Therefore, our first study is to determine which one of the above four features has the most impact on the result. Table 4 shows the results obtained by using each of the 4 features. First, we can see that ),( cooc","e cooc fIBM CCsim always gives a","better improvement than ),( cooc","e","cooc","fCOS CCsim . This","is because ),( cooc","e","cooc","fIBM CCsim scores are more diverse than the latter when the number of context features is small (there are many rules that have only a few contexts.) For an extreme example, suppose that there is only one context word in each vector of source and target context features, and the translation probability of the two context words is not 0. In this case,","),( cooc","e cooc fIBM CCsim reflects the translation proba-","bility of the context word pair, while","),( cooc","e cooc fCOS CCsim is always 1.","Second, ),( cooc","f full f CCsim and ),( cooc","e full e CCsim also give some improvements even when used 839 independently. For a possible explanation, consider the following example. The Chinese word “ 红 ” can translate to “red”, “communist”, or “hong” (the transliteration of 红, when it is used in a person’s name). Since these translations are likely to be associated with very different source contexts, each will have a low ),( cooc","f full f CCsim score. Another Chinese word 小溪 may translate into synonymous words, such as “brook”, “stream”, and “rivulet”, each of which will have a high ),( cooc","f full f CCsim score. Clearly, 红 is a more “dangerous” word than 小溪, since choos-ing the wrong translation for it would be a bad mistake. But if the two words have similar translation distributions, the system cannot distinguish between them. The monolingual similarity scores give it the ability to avoid “dangerous” words, and choose alternatives (such as larger phrase translations) when available.","Third, the similarity function of Alg2 consistently achieved further improvement by incorporating the monolingual similarities computed for the source and target side. This confirms the effectiveness of our algorithm.  CE_LD CE_SD","testset (NIST) ’06 ’08 ’06 ’08","Baseline 31.0 23.8 27.4 21.2","),( cooc","f","full","f CCsim 31.1 24.3 27.5 21.3","),( cooc","e","full","e CCsim 31.1 23.9 27.9 21.5","),( cooc","e","cooc","fIBM CCsim 31.4 24.3 27.9 21.5","),( cooc","e cooc fCOS CCsim 31.2 23.9 27.7 21.4 Alg2 IBM 31.5 24.5 27.9 21.6 Alg2 COS 31.6 24.5 28.1 21.7  Table 4: Results (BLEU%) of Chinese-to-English large data (CE_LD) and small data (CE_SD) NIST task by applying one feature.","6.2 Effect of Combining the Two Similarities We then combine the two similarity scores by using both of them as features to see if we could obtain further improvement. In practice, we use the four features in Table 4 together.","Table 5 reports the results on the small data condition. We observed further improvement on dev set, but failed to get the same improvements on test sets or even lost performance. Since the IBM+COS configuration has one extra feature, it is possible that it overfits the dev set.  Algorithm Dev NIST’06 NIST’08 Baseline 20.2 27.4 21.2 Alg2 IBM 20.5 27.9 21.6 Alg2 COS 20.6 28.1 21.7","Alg2 IBM+COS 20.8 27.9 21.5  Table 5: Results (BLEU%) for combination of two similarity scores. Further improvement was only obtained on dev set but not on test sets.","6.3 Comparison with Simple Contextual Features Now, we try to answer the question: can the similarity features computed by the function in Equation (14) be replaced with some other simple features? We did additional experiments on small data Chinese-to-English task to test the following features: (15) and (16) represent the sum of the counts of the context words in Cfull",", while (17) represents the proportion of words in the context of"]},{"title":"α","paragraphs":["that appeared in the context of the rule ("]},{"title":"γα ,","paragraphs":["); similarly, (18) is related to the properties of the words in the context of"]},{"title":"γ","paragraphs":["."]},{"title":"∑","paragraphs":["∈"]},{"title":"=","paragraphs":["full fi Cf if"]},{"title":"fFN ),()( αα","paragraphs":["(15)"]},{"title":"∑","paragraphs":["∈"]},{"title":"=","paragraphs":["full ej Ce je"]},{"title":"eFN ),()( γγ","paragraphs":["(16)"]},{"title":")( ),( ),( α α γα","paragraphs":["f Cf i f"]},{"title":"N fF E","paragraphs":["cooc fi"]},{"title":"∑","paragraphs":["∈"]},{"title":"=","paragraphs":["(17)"]},{"title":")( ),( ),( γ γ γα","paragraphs":["e Ce j e"]},{"title":"N eF E","paragraphs":["cooc ej"]},{"title":"∑","paragraphs":["∈"]},{"title":"=","paragraphs":["(18) where"]},{"title":"),(","paragraphs":["i"]},{"title":"fF α","paragraphs":["and"]},{"title":"),(","paragraphs":["j"]},{"title":"eF γ","paragraphs":["are the frequency counts of rule"]},{"title":"α","paragraphs":["or"]},{"title":"γ","paragraphs":["co-occurring with the context word i"]},{"title":"f","paragraphs":["or j"]},{"title":"e","paragraphs":["respectively.  Feature Dev NIST’06 NIST’08","Baseline 20.2 27.4 21.2 +Nf 20.5 27.6 21.4 +Ne 20.5 27.5 21.3 +Ef 20.4 27.5 21.2 +Ee 20.4 27.3 21.2","+Nf+Ne 20.5 27.5 21.3  Table 6: Results (BLEU%) of using simple features based on context on small data NIST task. Some improvements are obtained on dev set, but there was no significant effect on the test sets. ","Table 6 shows results obtained by adding the above features to the system for the small data 840 condition. Although all these features have obtained some improvements on dev set, there was no significant effect on the test sets. This means simple features based on context, such as the sum of the counts of the context features, are not as helpful as the sense similarity computed by Equation (14). 6.4 Null Context Feature There are two cases where no context word can be extracted according to the definition of context in Section 3.1. The first case is when a rule pair is always a full sentence-pair in the training data. The second case is when for some rule pairs, either their source or target contexts are out of the span limit of the initial phrase, so that we cannot extract contexts for those rule-pairs. For Chinese-to-English NIST task, there are about 1% of the rules that do not have contexts; for German-to-English task, this number is about 0.4%. We assign a uniform number as their bilingual sense similarity score, and this number is tuned through MERT. We call it the null context feature. It is included in all the results reported from Table 2 to Table 6. In Table 7, we show the weight of the null context feature tuned by running MERT in the experiments reported in Section 5.2. We can learn that penalties always discourage using those rules which have no context to be extracted. "," Alg.","Task","CE_SD CE_LD DE","Alg2 IBM -0.09 -0.37 -0.15","Alg2 COS -0.59 -0.42 -0.36  Table 7: Weight learned for employing the null context feature. CE_SD, CE_LD and DE are Chinese-to-English small data task, large data task and German-to-English task respectively. 6.5 Discussion Our aim in this paper is to characterize the semantic similarity of bilingual hierarchical rules. We can make several observations concerning our features:","1) Rules that are largely syntactic in nature, such as 的 X ||| the X of, will have very diffuse “meanings” and therefore lower similarity scores. It could be that the gains we obtained come simply from biasing the system against such rules. However, the results in table 6 show that this is unlikely to be the case: features that just count context words help very little.","2) In addition to bilingual similarity, Alg2 relies on the degree of monolingual similarity between the sense of a source or target unit within a rule, and the sense of the unit in general. This has a bias in favor of less ambiguous rules, i.e. rules involving only units with closely related meanings. Although this bias is helpful on its own, possibly due to the mechanism we outline in section 6.1, it appears to have a synergistic effect when used along with the bilingual similarity feature.","3) Finally, we note that many of the features we use for capturing similarity, such as the context “the, of” for instantiations of X in the unit the X of, are arguably more syntactic than semantic. Thus, like other “semantic” approaches, ours can be seen as blending syntactic and semantic information."]},{"title":"7 Related Work","paragraphs":["There has been extensive work on incorporating semantics into SMT. Key papers by Carpuat and Wu (2007) and Chan et al (2007) showed that word-sense disambiguation (WSD) techniques relying on source-language context can be effective in selecting translations in phrase-based and hierarchical SMT. More recent work has aimed at incorporating richer disambiguating features into the SMT log-linear model (Gimpel and Smith, 2008; Chiang et al, 2009); predicting coherent sets of target words rather than individual phrase translations (Bangalore et al, 2009; Mauser et al, 2009); and selecting applicable rules in hierarchical (He et al, 2008) and syntactic (Liu et al, 2008) translation, relying on source as well as target context. Work by Wu and Fung (2009) breaks new ground in attempting to match semantic roles derived from a semantic parser across source and target languages.","Our work is different from all the above approaches in that we attempt to discriminate among hierarchical rules based on: 1) the degree of bilingual semantic similarity between source and target translation units; and 2) the monolingual semantic similarity between occurrences of source or target units as part of the given rule, and in general. In another words, WSD explicitly tries to choose a translation given the current source context, while our work rates rule pairs independent of the current context."]},{"title":"8 Conclusions and Future Work","paragraphs":["In this paper, we have proposed an approach that uses the vector space model to compute the sense 841 similarity for terms from parallel corpora and applied it to statistical machine translation. We saw that the bilingual sense similarity computed by our algorithm led to significant improvements. Therefore, we can answer the questions proposed in Section 1. We have shown that the sense similarity computed between units from parallel corpora by means of our algorithm is helpful for at least one multilingual application: statistical machine translation.","Finally, although we described and evaluated bilingual sense similarity algorithms applied to a hierarchical phrase-based system, this method is also suitable for syntax-based MT systems and phrase-based MT systems. The only difference is the definition of the context. For a syntax-based system, the context of a rule could be defined similarly to the way it was defined in the work described above. For a phrase-based system, the context of a phrase could be defined as its surrounding words in a given size window. In our future work, we may try this algorithm on syntax-based MT systems and phrase-based MT systems with different context features. It would also be possible to use this technique during training of an SMT system – for instance, to improve the bilingual word alignment or reduce the training data noise."]},{"title":"References","paragraphs":["S. Bangalore, S. Kanthak, and P. Haffner. 2009. Statistical Machine Translation through Global Lexical Selection and Sentence Reconstruction. In: Goutte et al (ed.), Learning Machine Translation. MIT Press.","P. F. Brown, V. J. Della Pietra, S. A. Della Pietra & R. L. Mercer. 1993. The Mathematics of Statistical Machine Translation: Parameter Estimation. Computational Linguistics, 19(2) 263-312.","J. Bullinaria and J. Levy. 2007. Extracting semantic representations from word co-occurrence statistics: A computational study. Behavior Research Methods, 39 (3), 510–526.","M. Carpuat and D. Wu. 2007. Improving Statistical Machine Translation using Word Sense Disambiguation. In: Proceedings of EMNLP, Prague.","M. Carpuat. 2009. One Translation per Discourse. In: Proceedings of NAACL HLT Workshop on Semantic Evaluations, Boulder, CO.","Y. Chan, H. Ng and D. Chiang. 2007. Word Sense Disambiguation Improves Statistical Machine Translation. In: Proceedings of ACL, Prague.","D. Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In: Proceedings of ACL, pp. 263–270.","D. Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics. 33(2):201–228.","D. Chiang, W. Wang and K. Knight. 2009. 11,001 new features for statistical machine translation. In: Proc. NAACL HLT, pp. 218–226.","K. W. Church and P. Hanks. 1990. Word association norms, mutual information, and lexicography. Computational Linguistics, 16(1):22–29.","W. B. Frakes and R. Baeza-Yates, editors. 1992. Information Retrieval, Data Structure and Algorithms. Prentice Hall.","P. Fung. 1998. A statistical view on bilingual lexicon extraction: From parallel corpora to non-parallel corpora. In: Proceedings of AMTA, pp. 1–17. Oct. Langhorne, PA, USA.","J. Gimenez and L. Marquez. 2009. Discriminative Phrase Selection for SMT. In: Goutte et al (ed.), Learning Machine Translation. MIT Press.","K. Gimpel and N. A. Smith. 2008. Rich Source-Side Context for Statistical Machine Translation. In: Proceedings of WMT, Columbus, OH.","Z. Harris. 1954. Distributional structure. Word, 10(23): 146-162.","Z. He, Q. Liu, and S. Lin. 2008. Improving Statistical Machine Translation using Lexicalized Rule Selection. In: Proceedings of COLING, Manchester, UK.","D. Hindle. 1990. Noun classification from predicateargument structures. In: Proceedings of ACL. pp. 268-275. Pittsburgh, PA.","P. Koehn, F. Och, D. Marcu. 2003. Statistical Phrase-Based Translation. In: Proceedings of HLT-NAACL. pp. 127-133, Edmonton, Canada","P. Koehn. 2004. Statistical significance tests for machine translation evaluation. In: Proceedings of EMNLP, pp. 388–395. July, Barcelona, Spain.","T. Landauer and S. T. Dumais. 1997. A solution to Plato’s problem: The Latent Semantic Analysis theory of the acquisition, induction, and representa-tion of knowledge. Psychological Review. 104:211-240.","Z. Li, C. Callison-Burch, C. Dyer, J. Ganitkevitch, S. Khudanpur, L. Schwartz, W. Thornton, J. Weese and O. Zaidan, 2009. Joshua: An Open Source Toolkit for Parsing-based Machine Translation. In: Proceedings of the WMT. March. Athens, Greece.","D. Lin. 1998. Automatic retrieval and clustering of similar words. In: Proceedings of COLING/ACL-98. pp. 768-774. Montreal, Canada. 842","Q. Liu, Z. He, Y. Liu and S. Lin. 2008. Maximum Entropy based Rule Selection Model for Syntax-based Statistical Machine Translation. In: Proceedings of EMNLP, Honolulu, Hawaii.","K. Lund, and C. Burgess. 1996. Producing highdimensional semantic spaces from lexical co-occurrence. Behavior Research Methods, Instruments, and Computers, 28 (2), 203–208.","A. Mauser, S. Hasan and H. Ney. 2009. Extending Statistical Machine Translation with Discriminative and Trigger-Based Lexicon Models. In: Proceedings of EMNLP, Singapore.","F. Och. 2003. Minimum error rate training in statistical machine translation. In: Proceedings of ACL. Sapporo, Japan.","S. Pado and M. Lapata. 2007. Dependency-based construction of semantic space models. Computational Linguistics, 33 (2), 161–199.","P. Pantel and D. Lin. 2002. Discovering word senses from text. In: Proceedings of ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 613–619. Edmonton, Canada.","K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of ACL, pp. 311– 318. July. Philadelphia, PA, USA.","R. Rapp. 1999. Automatic Identification of Word Translations from Unrelated English and German Corpora. In: Proceedings of ACL, pp. 519–526. June. Maryland.","G. Salton and M. J. McGill. 1983. Introduction to Modern Information Retrieval. McGraw-Hill, New York.","P. Turney. 2001. Mining the Web for synonyms: PMI-IR versus LSA on TOEFL. In: Proceedings of the Twelfth European Conference on Machine Learning, pp. 491–502, Berlin, Germany.","D. Wu and P. Fung. 2009. Semantic Roles for SMT: A Hybrid Two-Pass Model. In: Proceedings of NAACL/HLT, Boulder, CO.","D. Yuret and M. A. Yatbaz. 2009. The Noisy Channel Model for Unsupervised Word Sense Disambiguation. In: Computational Linguistics. Vol. 1(1) 1-18.","R. Zens and H. Ney. 2004. Improvements in phrase-based statistical machine translation. In: Proceedings of NAACL-HLT. Boston, MA.","B. Zhao, S. Vogel, M. Eck, and A. Waibel. 2004. Phrase pair rescoring with term weighting for statistical machine translation. In Proceedings of EMNLP, pp. 206–213. July. Barcelona, Spain.  843"]}],"references":[{"authors":[{"first":"S.","last":"Bangalore"},{"first":"S.","last":"Kanthak"},{"first":"P.","last":"Haffner"}],"year":"2009","title":"Statistical Machine Translation through Global Lexical Selection and Sentence Reconstruction","source":"S. Bangalore, S. Kanthak, and P. Haffner. 2009. Statistical Machine Translation through Global Lexical Selection and Sentence Reconstruction. In: Goutte et al (ed.), Learning Machine Translation. MIT Press."},{"authors":[{"first":"P.","middle":"F.","last":"Brown"},{"first":"V.","middle":"J. Della","last":"Pietra"},{"first":"S.","middle":"A. Della","last":"Pietra"},{"first":"R.","middle":"L.","last":"Mercer"}],"year":"1993","title":"The Mathematics of Statistical Machine Translation: Parameter Estimation","source":"P. F. Brown, V. J. Della Pietra, S. A. Della Pietra & R. L. Mercer. 1993. The Mathematics of Statistical Machine Translation: Parameter Estimation. Computational Linguistics, 19(2) 263-312."},{"authors":[{"first":"J.","last":"Bullinaria"},{"first":"J.","last":"Levy"}],"year":"2007","title":"Extracting semantic representations from word co-occurrence statistics: A computational study","source":"J. Bullinaria and J. Levy. 2007. Extracting semantic representations from word co-occurrence statistics: A computational study. Behavior Research Methods, 39 (3), 510–526."},{"authors":[{"first":"M.","last":"Carpuat"},{"first":"D.","last":"Wu"}],"year":"2007","title":"Improving Statistical Machine Translation using Word Sense Disambiguation","source":"M. Carpuat and D. Wu. 2007. Improving Statistical Machine Translation using Word Sense Disambiguation. In: Proceedings of EMNLP, Prague."},{"authors":[{"first":"M.","last":"Carpuat"}],"year":"2009","title":"One Translation per Discourse","source":"M. Carpuat. 2009. One Translation per Discourse. In: Proceedings of NAACL HLT Workshop on Semantic Evaluations, Boulder, CO."},{"authors":[{"first":"Y.","last":"Chan"},{"first":"H.","last":"Ng"},{"first":"D.","last":"Chiang"}],"year":"2007","title":"Word Sense Disambiguation Improves Statistical Machine Translation","source":"Y. Chan, H. Ng and D. Chiang. 2007. Word Sense Disambiguation Improves Statistical Machine Translation. In: Proceedings of ACL, Prague."},{"authors":[{"first":"D.","last":"Chiang"}],"year":"2005","title":"A hierarchical phrase-based model for statistical machine translation","source":"D. Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In: Proceedings of ACL, pp. 263–270."},{"authors":[{"first":"D.","last":"Chiang"}],"year":"2007","title":"Hierarchical phrase-based translation","source":"D. Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics. 33(2):201–228."},{"authors":[{"first":"D.","last":"Chiang"},{"first":"W.","last":"Wang"},{"first":"K.","last":"Knight"}],"year":"2009","title":"11,001 new features for statistical machine translation","source":"D. Chiang, W. Wang and K. Knight. 2009. 11,001 new features for statistical machine translation. In: Proc. NAACL HLT, pp. 218–226."},{"authors":[{"first":"K.","middle":"W.","last":"Church"},{"first":"P.","last":"Hanks"}],"year":"1990","title":"Word association norms, mutual information, and lexicography","source":"K. W. Church and P. Hanks. 1990. Word association norms, mutual information, and lexicography. Computational Linguistics, 16(1):22–29."},{"authors":[{"first":"W.","middle":"B.","last":"Frakes"},{"first":"R.","last":"Baeza-Yates"},{"last":"editors"}],"year":"1992","title":"Information Retrieval, Data Structure and Algorithms","source":"W. B. Frakes and R. Baeza-Yates, editors. 1992. Information Retrieval, Data Structure and Algorithms. Prentice Hall."},{"authors":[{"first":"P.","last":"Fung"}],"year":"1998","title":"A statistical view on bilingual lexicon extraction: From parallel corpora to non-parallel corpora","source":"P. Fung. 1998. A statistical view on bilingual lexicon extraction: From parallel corpora to non-parallel corpora. In: Proceedings of AMTA, pp. 1–17. Oct. Langhorne, PA, USA."},{"authors":[{"first":"J.","last":"Gimenez"},{"first":"L.","last":"Marquez"}],"year":"2009","title":"Discriminative Phrase Selection for SMT","source":"J. Gimenez and L. Marquez. 2009. Discriminative Phrase Selection for SMT. In: Goutte et al (ed.), Learning Machine Translation. MIT Press."},{"authors":[{"first":"K.","last":"Gimpel"},{"first":"N.","middle":"A.","last":"Smith"}],"year":"2008","title":"Rich Source-Side Context for Statistical Machine Translation","source":"K. Gimpel and N. A. Smith. 2008. Rich Source-Side Context for Statistical Machine Translation. In: Proceedings of WMT, Columbus, OH."},{"authors":[{"first":"Z.","last":"Harris"}],"year":"1954","title":"Distributional structure","source":"Z. Harris. 1954. Distributional structure. Word, 10(23): 146-162."},{"authors":[{"first":"Z.","last":"He"},{"first":"Q.","last":"Liu"},{"first":"S.","last":"Lin"}],"year":"2008","title":"Improving Statistical Machine Translation using Lexicalized Rule Selection","source":"Z. He, Q. Liu, and S. Lin. 2008. Improving Statistical Machine Translation using Lexicalized Rule Selection. In: Proceedings of COLING, Manchester, UK."},{"authors":[{"first":"D.","last":"Hindle"}],"year":"1990","title":"Noun classification from predicateargument structures","source":"D. Hindle. 1990. Noun classification from predicateargument structures. In: Proceedings of ACL. pp. 268-275. Pittsburgh, PA."},{"authors":[{"first":"P.","last":"Koehn"},{"first":"F.","last":"Och"},{"first":"D.","last":"Marcu"}],"year":"2003","title":"Statistical Phrase-Based Translation","source":"P. Koehn, F. Och, D. Marcu. 2003. Statistical Phrase-Based Translation. In: Proceedings of HLT-NAACL. pp. 127-133, Edmonton, Canada"},{"authors":[{"first":"P.","last":"Koehn"}],"year":"2004","title":"Statistical significance tests for machine translation evaluation","source":"P. Koehn. 2004. Statistical significance tests for machine translation evaluation. In: Proceedings of EMNLP, pp. 388–395. July, Barcelona, Spain."},{"authors":[{"first":"T.","last":"Landauer"},{"first":"S.","middle":"T.","last":"Dumais"}],"year":"1997","title":"A solution to Plato’s problem: The Latent Semantic Analysis theory of the acquisition, induction, and representa-tion of knowledge","source":"T. Landauer and S. T. Dumais. 1997. A solution to Plato’s problem: The Latent Semantic Analysis theory of the acquisition, induction, and representa-tion of knowledge. Psychological Review. 104:211-240."},{"authors":[{"first":"Z.","last":"Li"},{"first":"C.","last":"Callison-Burch"},{"first":"C.","last":"Dyer"},{"first":"J.","last":"Ganitkevitch"},{"first":"S.","last":"Khudanpur"},{"first":"L.","last":"Schwartz"},{"first":"W.","last":"Thornton"},{"first":"J.","last":"Weese"},{"first":"O.","last":"Zaidan"}],"year":"2009","title":"Joshua: An Open Source Toolkit for Parsing-based Machine Translation","source":"Z. Li, C. Callison-Burch, C. Dyer, J. Ganitkevitch, S. Khudanpur, L. Schwartz, W. Thornton, J. Weese and O. Zaidan, 2009. Joshua: An Open Source Toolkit for Parsing-based Machine Translation. In: Proceedings of the WMT. March. Athens, Greece."},{"authors":[{"first":"D.","last":"Lin"}],"year":"1998","title":"Automatic retrieval and clustering of similar words","source":"D. Lin. 1998. Automatic retrieval and clustering of similar words. In: Proceedings of COLING/ACL-98. pp. 768-774. Montreal, Canada. 842"},{"authors":[{"first":"Q.","last":"Liu"},{"first":"Z.","last":"He"},{"first":"Y.","last":"Liu"},{"first":"S.","last":"Lin"}],"year":"2008","title":"Maximum Entropy based Rule Selection Model for Syntax-based Statistical Machine Translation","source":"Q. Liu, Z. He, Y. Liu and S. Lin. 2008. Maximum Entropy based Rule Selection Model for Syntax-based Statistical Machine Translation. In: Proceedings of EMNLP, Honolulu, Hawaii."},{"authors":[{"first":"K.","last":"Lund"},{"first":"C.","last":"Burgess"}],"year":"1996","title":"Producing highdimensional semantic spaces from lexical co-occurrence","source":"K. Lund, and C. Burgess. 1996. Producing highdimensional semantic spaces from lexical co-occurrence. Behavior Research Methods, Instruments, and Computers, 28 (2), 203–208."},{"authors":[{"first":"A.","last":"Mauser"},{"first":"S.","last":"Hasan"},{"first":"H.","last":"Ney"}],"year":"2009","title":"Extending Statistical Machine Translation with Discriminative and Trigger-Based Lexicon Models","source":"A. Mauser, S. Hasan and H. Ney. 2009. Extending Statistical Machine Translation with Discriminative and Trigger-Based Lexicon Models. In: Proceedings of EMNLP, Singapore."},{"authors":[{"first":"F.","last":"Och"}],"year":"2003","title":"Minimum error rate training in statistical machine translation","source":"F. Och. 2003. Minimum error rate training in statistical machine translation. In: Proceedings of ACL. Sapporo, Japan."},{"authors":[{"first":"S.","last":"Pado"},{"first":"M.","last":"Lapata"}],"year":"2007","title":"Dependency-based construction of semantic space models","source":"S. Pado and M. Lapata. 2007. Dependency-based construction of semantic space models. Computational Linguistics, 33 (2), 161–199."},{"authors":[{"first":"P.","last":"Pantel"},{"first":"D.","last":"Lin"}],"year":"2002","title":"Discovering word senses from text","source":"P. Pantel and D. Lin. 2002. Discovering word senses from text. In: Proceedings of ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 613–619. Edmonton, Canada."},{"authors":[{"first":"K.","last":"Papineni"},{"first":"S.","last":"Roukos"},{"first":"T.","last":"Ward"},{"first":"W.","last":"Zhu"}],"year":"2002","title":"Bleu: a method for automatic evaluation of machine translation","source":"K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of ACL, pp. 311– 318. July. Philadelphia, PA, USA."},{"authors":[{"first":"R.","last":"Rapp"}],"year":"1999","title":"Automatic Identification of Word Translations from Unrelated English and German Corpora","source":"R. Rapp. 1999. Automatic Identification of Word Translations from Unrelated English and German Corpora. In: Proceedings of ACL, pp. 519–526. June. Maryland."},{"authors":[{"first":"G.","last":"Salton"},{"first":"M.","middle":"J.","last":"McGill"}],"year":"1983","title":"Introduction to Modern Information Retrieval","source":"G. Salton and M. J. McGill. 1983. Introduction to Modern Information Retrieval. McGraw-Hill, New York."},{"authors":[{"first":"P.","last":"Turney"}],"year":"2001","title":"Mining the Web for synonyms: PMI-IR versus LSA on TOEFL","source":"P. Turney. 2001. Mining the Web for synonyms: PMI-IR versus LSA on TOEFL. In: Proceedings of the Twelfth European Conference on Machine Learning, pp. 491–502, Berlin, Germany."},{"authors":[{"first":"D.","last":"Wu"},{"first":"P.","last":"Fung"}],"year":"2009","title":"Semantic Roles for SMT: A Hybrid Two-Pass Model","source":"D. Wu and P. Fung. 2009. Semantic Roles for SMT: A Hybrid Two-Pass Model. In: Proceedings of NAACL/HLT, Boulder, CO."},{"authors":[{"first":"D.","last":"Yuret"},{"first":"M.","middle":"A.","last":"Yatbaz"}],"year":"2009","title":"The Noisy Channel Model for Unsupervised Word Sense Disambiguation","source":"D. Yuret and M. A. Yatbaz. 2009. The Noisy Channel Model for Unsupervised Word Sense Disambiguation. In: Computational Linguistics. Vol. 1(1) 1-18."},{"authors":[{"first":"R.","last":"Zens"},{"first":"H.","last":"Ney"}],"year":"2004","title":"Improvements in phrase-based statistical machine translation","source":"R. Zens and H. Ney. 2004. Improvements in phrase-based statistical machine translation. In: Proceedings of NAACL-HLT. Boston, MA."},{"authors":[{"first":"B.","last":"Zhao"},{"first":"S.","last":"Vogel"},{"first":"M.","last":"Eck"},{"first":"A.","last":"Waibel"}],"year":"2004","title":"Phrase pair rescoring with term weighting for statistical machine translation","source":"B. Zhao, S. Vogel, M. Eck, and A. Waibel. 2004. Phrase pair rescoring with term weighting for statistical machine translation. In Proceedings of EMNLP, pp. 206–213. July. Barcelona, Spain.  843"}],"cites":[{"style":0,"text":"Harris, 1954","origin":{"pointer":"/sections/3/paragraphs/0","offset":209,"length":12},"authors":[{"last":"Harris"}],"year":"1954","references":["/references/14"]},{"style":0,"text":"Hindle, 1990","origin":{"pointer":"/sections/3/paragraphs/0","offset":416,"length":12},"authors":[{"last":"Hindle"}],"year":"1990","references":["/references/16"]},{"style":0,"text":"Lund and Burgess, 1996","origin":{"pointer":"/sections/3/paragraphs/0","offset":430,"length":22},"authors":[{"last":"Lund"},{"last":"Burgess"}],"year":"1996","references":["/references/23"]},{"style":0,"text":"Landauer and Dumais, 1997","origin":{"pointer":"/sections/3/paragraphs/0","offset":454,"length":25},"authors":[{"last":"Landauer"},{"last":"Dumais"}],"year":"1997","references":["/references/19"]},{"style":0,"text":"Lin, 1998","origin":{"pointer":"/sections/3/paragraphs/0","offset":481,"length":9},"authors":[{"last":"Lin"}],"year":"1998","references":["/references/21"]},{"style":0,"text":"Turney, 2001","origin":{"pointer":"/sections/3/paragraphs/0","offset":492,"length":12},"authors":[{"last":"Turney"}],"year":"2001","references":["/references/31"]},{"style":0,"text":"Pantel and Lin, 2002","origin":{"pointer":"/sections/3/paragraphs/0","offset":506,"length":20},"authors":[{"last":"Pantel"},{"last":"Lin"}],"year":"2002","references":["/references/27"]},{"style":0,"text":"Pado and Lapata, 2007","origin":{"pointer":"/sections/3/paragraphs/0","offset":528,"length":21},"authors":[{"last":"Pado"},{"last":"Lapata"}],"year":"2007","references":["/references/26"]},{"style":0,"text":"Lund and Burgess, 1996","origin":{"pointer":"/sections/3/paragraphs/1","offset":343,"length":22},"authors":[{"last":"Lund"},{"last":"Burgess"}],"year":"1996","references":["/references/23"]},{"style":0,"text":"Lin, 1998","origin":{"pointer":"/sections/3/paragraphs/1","offset":394,"length":9},"authors":[{"last":"Lin"}],"year":"1998","references":["/references/21"]},{"style":0,"text":"Pado and Lapata, 2007","origin":{"pointer":"/sections/3/paragraphs/1","offset":426,"length":21},"authors":[{"last":"Pado"},{"last":"Lapata"}],"year":"2007","references":["/references/26"]},{"style":0,"text":"Salton and McGill, 1983","origin":{"pointer":"/sections/3/paragraphs/1","offset":531,"length":23},"authors":[{"last":"Salton"},{"last":"McGill"}],"year":"1983","references":["/references/30"]},{"style":0,"text":"Frakes and Baeza-Yates, 1992","origin":{"pointer":"/sections/3/paragraphs/1","offset":696,"length":28},"authors":[{"last":"Frakes"},{"last":"Baeza-Yates"}],"year":"1992","references":[]},{"style":0,"text":"Landauer and Dumais, 1997","origin":{"pointer":"/sections/3/paragraphs/1","offset":846,"length":25},"authors":[{"last":"Landauer"},{"last":"Dumais"}],"year":"1997","references":["/references/19"]},{"style":0,"text":"Fung (1998)","origin":{"pointer":"/sections/3/paragraphs/2","offset":225,"length":11},"authors":[{"last":"Fung"}],"year":"1998","references":["/references/11"]},{"style":0,"text":"Rapp (1999)","origin":{"pointer":"/sections/3/paragraphs/2","offset":241,"length":11},"authors":[{"last":"Rapp"}],"year":"1999","references":["/references/29"]},{"style":0,"text":"Chiang, 2005","origin":{"pointer":"/sections/4/paragraphs/0","offset":50,"length":12},"authors":[{"last":"Chiang"}],"year":"2005","references":["/references/6"]},{"style":0,"text":"Chiang, 2007","origin":{"pointer":"/sections/4/paragraphs/0","offset":64,"length":12},"authors":[{"last":"Chiang"}],"year":"2007","references":["/references/7"]},{"style":0,"text":"Chiang, 2005","origin":{"pointer":"/sections/10/paragraphs/7","offset":152,"length":12},"authors":[{"last":"Chiang"}],"year":"2005","references":["/references/6"]},{"style":0,"text":"Chiang, 2007","origin":{"pointer":"/sections/10/paragraphs/7","offset":166,"length":12},"authors":[{"last":"Chiang"}],"year":"2007","references":["/references/7"]},{"style":0,"text":"Koehn et al., 2003","origin":{"pointer":"/sections/10/paragraphs/10","offset":65,"length":18},"authors":[{"last":"Koehn"},{"last":"al."}],"year":"2003","references":["/references/17"]},{"style":0,"text":"Chiang, 2005","origin":{"pointer":"/sections/10/paragraphs/11","offset":258,"length":12},"authors":[{"last":"Chiang"}],"year":"2005","references":["/references/6"]},{"style":0,"text":"Chiang, 2007","origin":{"pointer":"/sections/10/paragraphs/11","offset":272,"length":12},"authors":[{"last":"Chiang"}],"year":"2007","references":["/references/7"]},{"style":0,"text":"Li et al., 2009","origin":{"pointer":"/sections/10/paragraphs/11","offset":348,"length":15},"authors":[{"last":"Li"},{"last":"al."}],"year":"2009","references":["/references/20"]},{"style":0,"text":"Lin, 1998","origin":{"pointer":"/sections/11/paragraphs/0","offset":70,"length":9},"authors":[{"last":"Lin"}],"year":"1998","references":["/references/21"]},{"style":0,"text":"Chiang, 2005","origin":{"pointer":"/sections/11/paragraphs/0","offset":416,"length":12},"authors":[{"last":"Chiang"}],"year":"2005","references":["/references/6"]},{"style":0,"text":"Church et al., 1990","origin":{"pointer":"/sections/25/paragraphs/0","offset":209,"length":19},"authors":[{"last":"Church"},{"last":"al."}],"year":"1990","references":[]},{"style":0,"text":"Turney, 2001","origin":{"pointer":"/sections/31/paragraphs/0","offset":204,"length":12},"authors":[{"last":"Turney"}],"year":"2001","references":["/references/31"]},{"style":0,"text":"Brown et al., 1993","origin":{"pointer":"/sections/40/paragraphs/0","offset":322,"length":18},"authors":[{"last":"Brown"},{"last":"al."}],"year":"1993","references":["/references/1"]},{"style":0,"text":"Zens and Ney, 2004","origin":{"pointer":"/sections/46/paragraphs/1","offset":135,"length":18},"authors":[{"last":"Zens"},{"last":"Ney"}],"year":"2004","references":["/references/34"]},{"style":0,"text":"Fung (1998)","origin":{"pointer":"/sections/56/paragraphs/0","offset":546,"length":11},"authors":[{"last":"Fung"}],"year":"1998","references":["/references/11"]},{"style":0,"text":"Rapp (1999)","origin":{"pointer":"/sections/56/paragraphs/0","offset":562,"length":11},"authors":[{"last":"Rapp"}],"year":"1999","references":["/references/29"]},{"style":0,"text":"Zhao et al., 2004","origin":{"pointer":"/sections/56/paragraphs/0","offset":759,"length":17},"authors":[{"last":"Zhao"},{"last":"al."}],"year":"2004","references":["/references/35"]},{"style":0,"text":"Chiang, 2005","origin":{"pointer":"/sections/103/paragraphs/15","offset":370,"length":12},"authors":[{"last":"Chiang"}],"year":"2005","references":["/references/6"]},{"style":0,"text":"Chiang, 2007","origin":{"pointer":"/sections/103/paragraphs/15","offset":384,"length":12},"authors":[{"last":"Chiang"}],"year":"2007","references":["/references/7"]},{"style":0,"text":"Papineni et al., 2002","origin":{"pointer":"/sections/103/paragraphs/16","offset":122,"length":21},"authors":[{"last":"Papineni"},{"last":"al."}],"year":"2002","references":["/references/28"]},{"style":0,"text":"Koehn, 2004","origin":{"pointer":"/sections/103/paragraphs/16","offset":222,"length":11},"authors":[{"last":"Koehn"}],"year":"2004","references":["/references/18"]},{"style":0,"text":"Carpuat and Wu (2007)","origin":{"pointer":"/sections/130/paragraphs/0","offset":81,"length":21},"authors":[{"last":"Carpuat"},{"last":"Wu"}],"year":"2007","references":["/references/3"]},{"style":0,"text":"Gimpel and Smith, 2008","origin":{"pointer":"/sections/130/paragraphs/0","offset":402,"length":22},"authors":[{"last":"Gimpel"},{"last":"Smith"}],"year":"2008","references":["/references/13"]},{"style":0,"text":"Wu and Fung (2009)","origin":{"pointer":"/sections/130/paragraphs/0","offset":738,"length":18},"authors":[{"last":"Wu"},{"last":"Fung"}],"year":"2009","references":["/references/32"]}]}
