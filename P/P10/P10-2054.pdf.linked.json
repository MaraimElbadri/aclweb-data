{"sections":[{"title":"","paragraphs":["Proceedings of the ACL 2010 Conference Short Papers, pages 291–295, Uppsala, Sweden, 11-16 July 2010. c⃝2010 Association for Computational Linguistics"]},{"title":"An Entity-Level Approach to Information Extraction Aria Haghighi UC Berkeley, CS Division aria42@cs.berkeley.edu Dan Klein UC Berkeley, CS Division klein@cs.berkeley.edu Abstract","paragraphs":["We present a generative model of template-filling in which coreference resolution and role assignment are jointly determined. Underlying template roles first generate abstract entities, which in turn generate concrete textual mentions. On the standard corporate acquisitions dataset, joint resolution in our entity-level model reduces error over a mention-level discriminative approach by up to 20%."]},{"title":"1 Introduction","paragraphs":["Template-filling information extraction (IE) systems must merge information across multiple sentences to identify all role fillers of interest. For instance, in the MUC4 terrorism event extraction task, the entity filling the individual perpetrator role often occurs multiple times, variously as proper, nominal, or pronominal mentions. However, most template-filling systems (Freitag and McCallum, 2000; Patwardhan and Riloff, 2007) assign roles to individual textual mentions using only local context as evidence, leaving aggrega-tion for post-processing. While prior work has acknowledged that coreference resolution and discourse analysis are integral to accurate role identification, to our knowledge no model has been proposed which jointly models these phenomena.","In this work, we describe an entity-centered approach to template-filling IE problems. Our model jointly merges surface mentions into underlying entities (coreference resolution) and assigns roles to those discovered entities. In the generative process proposed here, document entities are generated for each template role, along with a set of non-template entities. These entities then generate mentions in a process sensitive to both lexical and structural properties of the mention. Our model outperforms a discriminative mention-level baseline. Moreover, since our model is generative, it [S CSR] has said that [S it] has sold [S its] [B oil interests] held in [A Delhi Fund]. [P Esso Inc.] did not disclose how much [P they] paid for [A Dehli]."]},{"title":"(a) (b)","paragraphs":["Document Esso Inc. PURCHASERACQUIRED Delhi FundOil and Gas BUSINESS CSR Limited SELLER Template Figure 1: Example of the corporate acquisitions role-filling task. In (a), an example template specifying the entities play-ing each domain role. In (b), an example document with coreferent mentions sharing the same role label. Note that pronoun mentions provide direct clues to entity roles. can naturally incorporate unannotated data, which further increases accuracy."]},{"title":"2 Problem Setting","paragraphs":["Figure 1(a) shows an example template-filling task from the corporate acquisitions domain (Freitag, 1998).1","We have a template of K roles (PURCHASER, AMOUNT, etc.) and we must identify which entity (if any) fills each role (CSR Limited, etc.). Often such problems are modeled at the mention level, directly labeling individual mentions as in Figure 1(b). Indeed, in this data set, the mention-level perspective is evident in the gold annotations, which ignore pronominal references. However, roles in this domain appear in several locations throughout the document, with pronominal mentions often carrying the critical information for template filling. Therefore, Section 3 presents a model in which entities are explicitly modeled, naturally merging information across all mention types and explicitly representing latent structure very much like the entity-level template structure from Figure 1(a).","1","In Freitag (1998), some of these fields are split in two to distinguish a full versus abbreviated name, but we ignore this distinction. Also we ignore the status field as it doesn’t apply to entities and its meaning is not consistent. 291 R1 R2 RK Z1 Z2 Zn M1 M2 Mn........... Document Role Entity Parameters Mentions φ Role Priors E1 E2 M3 Z3 ........... EK Other Entities .... .... Other Entity Parameters .... .... Entity Indicators 1 [1: 0.02, 0:0.015, 2: 0.01,...] MOD-APPOS [company: 0.02, firm:0.015, group: 0.01,...] [1: 0.19, 2:0.14, 0: 0.08,...] HEAD-NAM [Inc.: 0.02, Corp.:0.015, Ltd.: 0.01,...] [2: 0.18, 3:0.12, 1: 0.09,...] GOV-NSUBJ","frθrr [bought: 0.02, obtained:0.015, acquired: 0.01,...] Purchaser Role Role Entites CaliforniaMOD-PREP MOD-NN search, giant companyHEAD-NOM","HEAD-NAM Lrr Google, GOOG Purchaser Entity GOV-NSUBJ boughtHEAD-NAM Googlewr Purchaser Mention Figure 2: Graphical model depiction of our generative model described in Section 3. Sample values are illustrated for key parameters and latent variables."]},{"title":"3 Model","paragraphs":["We describe our generative model for a document, which has many similarities to the coreference-only model of Haghighi and Klein (2010), but which integrally models template role-fillers. We briefly describe the key abstractions of our model.","Mentions: A mention is an observed textual reference to a latent real-world entity. Mentions are associated with nodes in a parse tree and are typically realized as NPs. There are three basic forms of mentions: proper (NAM), nominal (NOM), and pronominal (PRO). Each mention M is represented as collection of key-value pairs. The keys are called properties and the values are words. The set of properties utilized here, de-noted R, are the same as in Haghighi and Klein (2010) and consist of the mention head, its dependencies, and its governor. See Figure 2 for a concrete example. Mention types are trivially determined from mention head POS tag. All mention properties and their values are observed.","Entities: An entity is a specific individual or object in the world. Entities are always latent in text. Where a mention has a single word for each property, an entity has a list of signature words. Formally, entities are mappings from properties r ∈ R to lists Lr of “canonical” words which that entity uses for that property.","Roles: The elements we have described so far are standard in many coreference systems. Our model performs role-filling by assuming that each entity is drawn from an underlying role. These roles include the K template roles as well as ‘junk’ roles to represent entities which do not fill a template role (see Section 5.2). Each role R is represented as a mapping between properties r and pairs of multinomials (θr, fr). θr is a unigram distribution of words for property r that are semantically licensed for the role (e.g., being the subject of “acquired” for the ACQUIRED role). fr is a “fertility” distribution over the integers that characterizes entity list lengths. Together, these distributions control the lists Lr for entities which in-stantiate the role.","We first present a broad sketch of our model’s components and then detail each in a subsequent section. We temporarily assume that all mentions belong to a template role-filling entity; we lift this restriction in Section 5.2. First, a semantic component generates a sequence of entities E = (E1, . . . , EK ), where each Ei is generated from a corresponding role Ri. We use R = (R1, . . . , RK ) to denote the vector of template role parameters. Note that this work assumes that there is a one-to-one mapping between entities and roles; in particular, at most one entity can fill each role. This assumption is appropriate for the domain considered here.","Once entities have been generated, a discourse component generates which entities will be evoked in each of the n mention positions. We represent these choices using entity indicators de-noted by Z = (Z1, . . . , Zn). This component utilizes a learned global prior φ over roles. The Zi in-292 dicators take values in 1, . . . , K indicating the entity number (and thereby the role) underlying the ith mention position. Finally, a mention genera-tion component renders each mention conditioned on the underlying entity and role. Formally: P (E, Z, M|R, φ) =","( K ∏ i=1 P (Ei|Ri) ) [Semantic, Sec. 3.1]   n ∏ j=1 P (Zj|Z<j, φ)   [Discourse, Sec. 3.2]   n ∏ j=1 P (Mj|EZj , RZj )   [Mention, Sec. 3.3] 3.1 Semantic Component Each role R generates an entity E as follows: for each mention property r, a word list, Lr, is drawn by first generating a list length from the corresponding fr distribution in R.2","This list is then populated by an independent draw from R’s unigram distribution θr. Formally, for each r ∈ R, an entity word list is drawn according to,3 P (Lr|R) = P (len(Lr)|fr) ∏ w∈Lr P (w|θr) 3.2 Discourse Component The discourse component draws the entity indicator Zj for the jth mention according to, P (Zj|Z<j, φ) = { P (Zj|φ), if non-pronominal ∑ j′ 1[Zj = Zj′]P (j′","|j), o.w. When the jth mention is non-pronominal, we draw Zj from φ, a global prior over the K roles. When Mj is a pronoun, we first draw an antecedent mention position j′",", such that j′","< j, and then we set Zj = Zj′. The antecedent position is selected according to the distribution,","P (j′","|j) ∝ exp{−γTREEDIST(j′",", j)} where TREEDIST(j′",",j) represents the tree distance between the parse nodes for Mj and Mj′.4","Mass is 2","There is one exception: the sizes of the proper and nominal head property lists are jointly generated, but their word lists are still independently populated. 3","While, in principle, this process can yield word lists with duplicate words, we constrain the model during inference to not allow that to occur. 4","Sentence parse trees are merged into a right-branching document parse tree. This allows us to extend tree distance to inter-sentence nodes. restricted to antecedent mention positions j′","which occur earlier in the same sentence or in the previous sentence.5 3.3 Mention Generation Once the entity indicator has been drawn, we generate words associated with mention conditioned on the underlying entity E and role R. For each mention property r associated with the mention, a word w is drawn utilizing E’s word list Lr as well as the multinomials (fr, θr) from role R. The word w is drawn according to, P (w|E, R)=(1 − αr) 1 [w ∈ Lr] len(Lr) + αrP (w|θr) For each property r, there is a hyper-parameter αr which interpolates between selecting a word uniformly from the entity list Lr and drawing from the underlying role distribution θr. Intuitively, a small αr indicates that an entity prefers to re-use a small number of words for property r. This is typically the case for proper and nominal heads as well as modifiers. At the other extreme, setting αr to 1 indicates the property isn’t particular to the entity itself, but rather always drawn from the underlying role distribution. We set αr to 1 for pronoun heads as well as for the governor properties."]},{"title":"4 Learning and Inference","paragraphs":["Since we will make use of unannotated data (see Section 5), we utilize a variational EM algorithm to learn parameters R and φ. The E-Step requires the posterior P (E, Z|R, M, φ), which is intractable to compute exactly. We approximate it using a surrogate variational distribution of the following factored form: Q(E, Z) = ( K ∏ i=1 qi(Ei) )   n ∏ j=1 rj(Zj)   Each rj(Zj) is a distribution over the entity indicator for mention Mj, which approximates the true posterior of Zj. Similarly, qi(Ei) approximates the posterior over entity Ei which is associated with role Ri. As is standard, we iteratively update each component distribution to minimize KL-divergence, fixing all other distributions:","qi ← argmin qi KL(Q(E, Z)|P (E, Z|M, R, φ) ∝ exp{EQ/qi ln P (E, Z|M, R, φ))} 5 The sole parameter γ is fixed at 0.1. 293","Ment Acc. Ent. Acc. INDEP 60.0 43.7 JOINT 64.6 54.2 JOINT+PRO 68.2 57.8 Table 1: Results on corporate acquisition tasks with given role mention boundaries. We report mention role accuracy and entity role accuracy (correctly labeling all entity mentions).","For example, the update for a non-pronominal entity indicator component rj(·) is given by:6 ln rj(z) ∝ EQ/rj ln P (E, Z, M|R, φ) ∝ Eqz ln (P (z|φ)P (Mj|Ez, Rz)) = ln P (z|φ) + Eqz ln P (Mj|Ez, Rz) A similar update is performed on pronominal entity indicator distributions, which we omit here for space. The update for variational entity distribution is given by: ln qi(ei) ∝ EQ/qi ln P (E, Z, M|R, φ) ∝ E{rj} ln  P (ei|Ri) ∏ j:Zj=i P (Mj|ei, Ri)   = ln P (ei|Ri) + ∑ j rj(i) ln P (Mj|ei, Ri) It is intractable to enumerate all possible entities ei (each consisting of several sets of words). We instead limit the support of qi(ei) to several sampled entities. We obtain entity samples by sampling mention entity indicators according to rj. For a given sample, we assume that Ei consists of the non-pronominal head words and modifiers of mentions such that Zj has sampled value i.","During the E-Step, we perform 5 iterations of updating each variational factor, which results in an approximate posterior distribution. Using expectations from this approximate posterior, our M-Step is relatively straightforward. The role parameters Ri are computed from the qi(ei) and rj(z) distributions, and the global role prior φ from the non-pronominal components of rj(z)."]},{"title":"5 Experiments","paragraphs":["We present results on the corporate acquisitions task, which consists of 600 annotated documents split into a 300/300 train/test split. We use 50 training documents as a development set. In all","6","For simplicity of exposition, we omit terms where Mj is an antecedent to a pronoun. documents, proper and (usually) nominal mentions are annotated with roles, while pronouns are not. We preprocess each document identically to Haghighi and Klein (2010): we sentence-segment using the OpenNLP toolkit, parse sentences with the Berkeley Parser (Petrov et al., 2006), and extract mention properties from parse trees and the Stanford Dependency Extractor (de Marneffe et al., 2006). 5.1 Gold Role Boundaries We first consider the simplified task where role mention boundaries are given. We map each labeled token span in training and test data to a parse tree node that shares the same head. In this setting, the role-filling task is a collective classification problem, since we know each mention is filling some role.","As our baseline, INDEP, we built a maximum entropy model which independently classifies each mention’s role. It uses features as similar as possible to the generative model (and more), in-cluding the head word, typed dependencies of the head, various tree features, governing word, and several conjunctions of these features as well as coarser versions of lexicalized features. This system yields 60.0 mention labeling accuracy (see Table 1). The primary difficulty in classification is the disambiguation amongst the acquired, seller, and purchaser roles, which have similar internal structure, and differ primarily in their semantic contexts. Our entity-centered model, JOINT in Table 1, has no latent variables at training time in this setting, since each role maps to a unique entity. This model yields 64.6, outperforming INDEP.7","During development, we noted that often the most direct evidence of the role of an entity was associated with pronoun usage (see the first “it” in Figure 1). Training our model with pronominal mentions, whose roles are latent variables at training time, improves accuracy to 68.2.8 5.2 Full Task We now consider the more difficult setting where role mention boundaries are not provided at test time. In this setting, we automatically extract mentions from a parse tree using a heuristic ap-7 We use the mode of the variational posteriors rj(Zj) to","make predictions (see Section 4). 8 While this approach incorrectly assumes that all pro-","nouns have antecedents amongst our given mentions, this did","not appear to degrade performance. 294","ROLE ID OVERALL","P R F1 P R F1 INDEP 79.0 65.5 71.6 48.6 40.3 44.0 JOINT+PRO 80.3 69.2 74.3 53.4 46.4 49.7 BEST 80.1 70.1 74.8 57.3 49.2 52.9 Table 2: Results on corporate acquisitions data where mention boundaries are not provided. Systems must determine which mentions are template role-fillers as well as label them. ROLE ID only evaluates the binary decision of whether a mention is a template role-filler or not. OVERALL includes correctly labeling mentions. Our BEST system, see Section 5, adds extra unannotated data to our JOINT+PRO system. proach. Our mention extraction procedure yields 95% recall over annotated role mentions and 45% precision.9","Using extracted mentions as input, our task is to label some subset of the mentions with template roles. Since systems can label mentions as non-role bearing, only recall is critical to mention extraction. To adapt INDEP to this setting, we first use a binary classifier trained to distinguish role-bearing mentions. The baseline then classifies mentions which pass this first phase as before. We add ‘junk’ roles to our model to flexibly model entities that do not correspond to annotated template roles. During training, extracted mentions which are not matched in the labeled data have posteriors which are constrained to be amongst the ‘junk’ roles.","We first evaluate role identification (ROLE ID in Table 2), the task of identifying mentions which play some role in the template. The binary classifier for INDEP yields 71.6 F1. Our JOINT+PRO system yields 74.3. On the task of identifying and correctly labeling role mentions, our model outperforms INDEP as well (OVERALL in Table 2). As our model is generative, it is straightforward to utilize totally unannotated data. We added 700 fully unannotated documents from the mergers and acquisitions portion of the Reuters 21857 corpus. Training JOINT+PRO on this data as well as our original training data yields the best performance (BEST in Table 2).10","To our knowledge, the best previously published results on this dataset are from Siefkes (2008), who report 45.9 weighted F1. Our BEST system evaluated in their slightly stricter way yields 51.1. 9","Following Patwardhan and Riloff (2009), we match extracted mentions to labeled spans if the head of the mention matches the labeled span. 10","We scaled expected counts from the unlabeled data so that they did not overwhelm those from our (partially) labeled data."]},{"title":"6 Conclusion","paragraphs":["We have presented a joint generative model of coreference resolution and role-filling information extraction. This model makes role decisions at the entity, rather than at the mention level. This approach naturally aggregates information across multiple mentions, incorporates unannotated data, and yields strong performance. Acknowledgements: This project is funded in part by the Office of Naval Research under MURI Grant No. N000140911081."]},{"title":"References","paragraphs":["M. C. de Marneffe, B. Maccartney, and C. D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In LREC.","Dayne Freitag and Andrew McCallum. 2000. Information extraction with hmm structures learned by stochastic optimization. In Association for the Advancement of Artificial Intelligence (AAAI).","Dayne Freitag. 1998. Machine learning for information extraction in informal domains.","A. Haghighi and D. Klein. 2010. Coreference resolution in a modular, entity-centered model. In North American Association of Computational Linguistics (NAACL).","P. Liang and D. Klein. 2007. Structured Bayesian nonparametric models with variational inference (tutorial). In Association for Computational Linguistics (ACL).","S. Patwardhan and E. Riloff. 2007. Effective information extraction with semantic affinity patterns and relevant regions. In Joint Conference on Empirical Methods in Natural Language Processing.","S. Patwardhan and E Riloff. 2009. A unified model of phrasal and sentential evidence for information extraction. In Empirical Methods in Natural Language Processing (EMNLP).","Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 433–440, Sydney, Australia, July. Association for Computational Linguistics.","Christian Siefkes. 2008. An Incrementally Trainable Statistical Approach to Information Extraction: Based on Token Classification and Rich Context Model. VDM Verlag, Saarbrücken, Germany, Germany. 295"]}],"references":[{"authors":[{"first":"M.","middle":"C.","last":"de Marneffe"},{"first":"B.","last":"Maccartney"},{"first":"C.","middle":"D.","last":"Manning"}],"year":"2006","title":"Generating typed dependency parses from phrase structure parses","source":"M. C. de Marneffe, B. Maccartney, and C. D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In LREC."},{"authors":[{"first":"Dayne","last":"Freitag"},{"first":"Andrew","last":"McCallum"}],"year":"2000","title":"Information extraction with hmm structures learned by stochastic optimization","source":"Dayne Freitag and Andrew McCallum. 2000. Information extraction with hmm structures learned by stochastic optimization. In Association for the Advancement of Artificial Intelligence (AAAI)."},{"authors":[{"first":"Dayne","last":"Freitag"}],"year":"1998","title":"Machine learning for information extraction in informal domains","source":"Dayne Freitag. 1998. Machine learning for information extraction in informal domains."},{"authors":[{"first":"A.","last":"Haghighi"},{"first":"D.","last":"Klein"}],"year":"2010","title":"Coreference resolution in a modular, entity-centered model","source":"A. Haghighi and D. Klein. 2010. Coreference resolution in a modular, entity-centered model. In North American Association of Computational Linguistics (NAACL)."},{"authors":[{"first":"P.","last":"Liang"},{"first":"D.","last":"Klein"}],"year":"2007","title":"Structured Bayesian nonparametric models with variational inference (tutorial)","source":"P. Liang and D. Klein. 2007. Structured Bayesian nonparametric models with variational inference (tutorial). In Association for Computational Linguistics (ACL)."},{"authors":[{"first":"S.","last":"Patwardhan"},{"first":"E.","last":"Riloff"}],"year":"2007","title":"Effective information extraction with semantic affinity patterns and relevant regions","source":"S. Patwardhan and E. Riloff. 2007. Effective information extraction with semantic affinity patterns and relevant regions. In Joint Conference on Empirical Methods in Natural Language Processing."},{"authors":[{"first":"S.","last":"Patwardhan"},{"first":"E","last":"Riloff"}],"year":"2009","title":"A unified model of phrasal and sentential evidence for information extraction","source":"S. Patwardhan and E Riloff. 2009. A unified model of phrasal and sentential evidence for information extraction. In Empirical Methods in Natural Language Processing (EMNLP)."},{"authors":[{"first":"Slav","last":"Petrov"},{"first":"Leon","last":"Barrett"},{"first":"Romain","last":"Thibaux"},{"first":"Dan","last":"Klein"}],"year":"2006","title":"Learning accurate, compact, and interpretable tree annotation","source":"Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 433–440, Sydney, Australia, July. Association for Computational Linguistics."},{"authors":[{"first":"Christian","last":"Siefkes"}],"year":"2008","title":"An Incrementally Trainable Statistical Approach to Information Extraction: Based on Token Classification and Rich Context Model","source":"Christian Siefkes. 2008. An Incrementally Trainable Statistical Approach to Information Extraction: Based on Token Classification and Rich Context Model. VDM Verlag, Saarbrücken, Germany, Germany. 295"}],"cites":[{"style":0,"text":"Freitag and McCallum, 2000","origin":{"pointer":"/sections/2/paragraphs/0","offset":377,"length":26},"authors":[{"last":"Freitag"},{"last":"McCallum"}],"year":"2000","references":["/references/1"]},{"style":0,"text":"Patwardhan and Riloff, 2007","origin":{"pointer":"/sections/2/paragraphs/0","offset":405,"length":27},"authors":[{"last":"Patwardhan"},{"last":"Riloff"}],"year":"2007","references":["/references/5"]},{"style":0,"text":"Freitag, 1998","origin":{"pointer":"/sections/4/paragraphs/0","offset":91,"length":13},"authors":[{"last":"Freitag"}],"year":"1998","references":["/references/2"]},{"style":0,"text":"Freitag (1998)","origin":{"pointer":"/sections/4/paragraphs/3","offset":3,"length":14},"authors":[{"last":"Freitag"}],"year":"1998","references":["/references/2"]},{"style":0,"text":"Haghighi and Klein (2010)","origin":{"pointer":"/sections/5/paragraphs/0","offset":110,"length":25},"authors":[{"last":"Haghighi"},{"last":"Klein"}],"year":"2010","references":["/references/3"]},{"style":0,"text":"Haghighi and Klein (2010)","origin":{"pointer":"/sections/5/paragraphs/1","offset":451,"length":25},"authors":[{"last":"Haghighi"},{"last":"Klein"}],"year":"2010","references":["/references/3"]},{"style":0,"text":"Haghighi and Klein (2010)","origin":{"pointer":"/sections/7/paragraphs/2","offset":226,"length":25},"authors":[{"last":"Haghighi"},{"last":"Klein"}],"year":"2010","references":["/references/3"]},{"style":0,"text":"Petrov et al., 2006","origin":{"pointer":"/sections/7/paragraphs/2","offset":342,"length":19},"authors":[{"last":"Petrov"},{"last":"al."}],"year":"2006","references":["/references/7"]},{"style":0,"text":"Marneffe et al., 2006","origin":{"pointer":"/sections/7/paragraphs/2","offset":454,"length":21},"authors":[{"last":"Marneffe"},{"last":"al."}],"year":"2006","references":[]},{"style":0,"text":"Siefkes (2008)","origin":{"pointer":"/sections/7/paragraphs/12","offset":81,"length":14},"authors":[{"last":"Siefkes"}],"year":"2008","references":["/references/8"]},{"style":0,"text":"Patwardhan and Riloff (2009)","origin":{"pointer":"/sections/7/paragraphs/13","offset":10,"length":28},"authors":[{"last":"Patwardhan"},{"last":"Riloff"}],"year":"2009","references":["/references/6"]}]}
