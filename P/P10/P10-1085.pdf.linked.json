{"sections":[{"title":"","paragraphs":["Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 825–833, Uppsala, Sweden, 11-16 July 2010. c⃝2010 Association for Computational Linguistics"]},{"title":"Improving Statistical Machine Translation with Monolingual Collocation  Zhanyi Liu1 , Haifeng Wang 2 , Hua Wu2 , Sheng Li 1 ","paragraphs":["1"]},{"title":"Harbin Institute of Technology, Harbin, China","paragraphs":["2"]},{"title":"Baidu.com Inc., Beijing, China zhanyiliu@gmail.com {wanghaifeng, wu_hua}@baidu.com lisheng@hit.edu.cn  Abstract","paragraphs":["  This paper proposes to use monolingual collocations to improve Statistical Machine Translation (SMT). We make use of the collocation probabilities, which are estimated from monolingual corpora, in two aspects, namely improving word alignment for various kinds of SMT systems and improving phrase table for phrase-based SMT. The experimental results show that our method improves the performance of both word alignment and translation quality significantly. As compared to baseline systems, we achieve absolute improvements of 2.40 BLEU score on a phrase-based SMT system and 1.76 BLEU score on a parsing-based SMT system."]},{"title":"1 Introduction","paragraphs":["Statistical bilingual word alignment (Brown et al. 1993) is the base of most SMT systems. As compared to single-word alignment, multi-word alignment is more difficult to be identified. Although many methods were proposed to improve the quality of word alignments (Wu, 1997; Och and Ney, 2000; Marcu and Wong, 2002; Cherry and Lin, 2003; Liu et al., 2005; Huang, 2009), the correlation of the words in multi-word alignments is not fully considered.","In phrase-based SMT (Koehn et al., 2003), the phrase boundary is usually determined based on the bi-directional word alignments. But as far as we know, few previous studies exploit the collocation relations of the words in a phrase. Some  This work was partially done at Toshiba (China) Research and Development Center. researches used soft syntactic constraints to predict whether source phrase can be translated together (Marton and Resnik, 2008; Xiong et al., 2009). However, the constraints were learned from the parsed corpus, which is not available for many languages.","In this paper, we propose to use monolingual collocations to improve SMT. We first identify potentially collocated words and estimate collocation probabilities from monolingual corpora using a Monolingual Word Alignment (MWA) method (Liu et al., 2009), which does not need any additional resource or linguistic preprocessing, and which outperforms previous methods on the same experimental data. Then the collocation information is employed to improve Bilingual Word Alignment (BWA) for various kinds of SMT systems and to improve phrase table for phrase-based SMT.","To improve BWA, we re-estimate the alignment probabilities by using the collocation probabilities of words in the same cept. A cept is the set of source words that are connected to the same target word (Brown et al., 1993). An alignment between a source multi-word cept and a target word is a many-to-one multi-word alignment.","To improve phrase table, we calculate phrase collocation probabilities based on word collocation probabilities. Then the phrase collocation probabilities are used as additional features in phrase-based SMT systems.","The evaluation results show that the proposed method in this paper significantly improves multi-word alignment, achieving an absolute error rate reduction of 29%. The alignment improvement results in an improvement of 2.16 BLEU score on phrase-based SMT system and an improvement of 1.76 BLEU score on parsing-based SMT system. If we use phrase collocation probabilities as additional features, the phrase-based 825 SMT performance is further improved by 0.24 BLEU score.","The paper is organized as follows: In section 2, we introduce the collocation model based on the MWA method. In section 3 and 4, we show how to improve the BWA method and the phrase table using collocation models respectively. We describe the experimental results in section 5, 6 and 7. Lastly, we conclude in section 8."]},{"title":"2 Collocation Model","paragraphs":["Collocation is generally defined as a group of words that occur together more often than by chance (McKeown and Radev, 2000). A collocation is composed of two words occurring as either a consecutive word sequence or an interrupted word sequence in sentences, such as \"by accident\" or \"take ... advice\". In this paper, we use the MWA method (Liu et al., 2009) for collocation extraction. This method adapts the bilingual word alignment algorithm to monolingual scenario to extract collocations only from monolingual corpora. And the experimental results in (Liu et al., 2009) showed that this method achieved higher precision and recall than previous methods on the same experimental data. 2.1 Monolingual word alignment The monolingual corpus is first replicated to generate a parallel corpus, where each sentence pair consists of two identical sentences in the same language. Then the monolingual word alignment algorithm is employed to align the potentially collocated words in the monolingual sentences.","According to Liu et al. (2009), we employ the MWA Model 3 (corresponding to IBM Model 3) to calculate the probability of the monolingual word alignment sequence, as shown in Eq. (1).      l j jaj l i ii lajdwwt wnSASp","j 1 1 3 ModelMWA ),|()|( )|()|,(  (1)","Where l","wS 1 is a monolingual sentence, i denotes the number of words that are aligned with iw . Since a word never collocates with itself, the alignment set is denoted as","}&],1[|),{( ialiaiA ii  . Three kinds of probabilities are involved in this model: word collocation probability )|( jaj wwt , position collocation probability ),|( lajd j and fertility probability )|( ii wn  .","In the MWA method, the similar algorithm to bilingual word alignment is used to estimate the parameters of the models, except that a word cannot be aligned to itself.","Figure 1 shows an example of the potentially collocated word pairs aligned by the MWA method.  Figure 1. MWA Example 2.2 Collocation probability Given the monolingual word aligned corpus, we calculate the frequency of two words aligned in the corpus, denoted as ),( ji wwfreq . We filtered the aligned words occurring only once. Then the probability for each aligned word pair is estimated as follows:    w j ji ji wwfreq wwfreq wwp ),(","),( )|( (2)    w i ji ij wwfreq wwfreq wwp ),(","),( )|( (3)","In this paper, the words of collocation are symmetric and we do not determine which word is the head and which word is the modifier. Thus, the collocation probability of two words is defined as the average of both probabilities, as in Eq. (4). 2 )|()|( ),( ijji ji wwpwwp wwr","  (4)","If we have multiple monolingual corpora to estimate the collocation probabilities, we interpolate the probabilities as shown in Eq. (5).","),(),( ji k kkji wwrwwr   (5)","k denotes the interpolation coefficient for the probabilities estimated on the k th corpus."]},{"title":"3 Improving Statistical Bilingual Word Alignment","paragraphs":["We use the collocation information to improve both one-directional and bi-directional bilingual word alignments. The alignment probabilities are re-estimated by using the collocation probabilities of words in the same cept. The team leader plays a key role in the project undertaking. The team leader plays a key role in the project undertaking.  826","3.1 Improving one-directional bilingual word alignment","According to the BWA method, given a bilingual","sentence pair l eE 1 and m","fF 1 , the optimal","alignment sequence A between E and F can be","obtained as in Eq. (6).",")|,(maxarg*","EAFpA","A (6)","The method is implemented in a series of five models (IBM Models). IBM Model 1 only employs the word translation model to calculate the probabilities of alignments. In IBM Model 2, both the word translation model and position distribution model are used. IBM Model 3, 4 and 5 consider the fertility model in addition to the word translation model and position distribution model. And these three models are similar, except for the word distortion models.","One-to-one and many-to-one alignments could be produced by using IBM models. Although the fertility model is used to restrict the number of source words in a cept and the position distortion model is used to describe the correlation of the positions of the source words, the quality of many-to-one alignments is lower than that of one-to-one alignments.","Intuitively, the probability of the source words aligned to a target word is not only related to the fertility ability and their relative positions, but also related to lexical tokens of words, such as common phrase or idiom. In this paper, we use the collocation probability of the source words in a cept to measure their correlation strength. Given source words }|{ iaf jj  aligned to ie , their collocation probability is calculated as in Eq. (7). )1(* ),(2 })|({ 1 1 1 ][][        ii k kg giki jj i i ffr iafr    (7) Here, kif ][ and gi"]},{"title":"f","paragraphs":["][ denote the th","k word and","th","g word in }|{ iaf jj  ;"]},{"title":"),(","paragraphs":["][][ giki"]},{"title":"ffr","paragraphs":["denotes the collocation probability of kif ][ and gi"]},{"title":"f","paragraphs":["][ , as shown in Eq. (4).","Thus, the collocation probability of the alignment sequence of a sentence pair can be calculated according to Eq. (8).    l i jj iafrEAFr 1 })|({)|,( (8)","Based on maximum entropy framework, we combine the collocation model and the BWA model to calculate the word alignment probability of a sentence pair, as shown in Eq. (9).      ' )),,(exp( )),,(exp( )|,( A i ii i ii r AEFh AEFh EAFp   (9) Here, ),,( AEFhi and i denote features and feature weights, respectively. We use two features in this paper, namely alignment probabilities and collocation probabilities. Thus, we obtain the decision rule:","}),,({maxarg*"]},{"title":"","paragraphs":[" i ii A AEFhA  (10) Based on the GIZA++ package 1",", we implemented a tool for the improved BWA method. We first train IBM Model 4 and collocation model on bilingual corpus and monolingual corpus respectively. Then we employ the hillclimbing algorithm (Al-Onaizan et al., 1999) to search for the optimal alignment sequence of a given sentence pair, where the score of an alignment sequence is calculated as in Eq. (10).","We note that Eq. (8) only deals with many-to-one alignments, but the alignment sequence of a sentence pair also includes one-to-one alignments. To calculate the collocation probability of the alignment sequence, we should also consider the collocation probabilities of such one-to-one alignments. To solve this problem, we use the collocation probability of the whole source sentence, )(Fr , as the collocation probability of one-word cept.","3.2 Improving bi-directional bilingual word alignments In word alignment models implemented in GI-ZA++, only one-to-one and many-to-one word alignment links can be found. Thus, some multi-word units cannot be correctly aligned. The symmetrization method is used to effectively overcome this deficiency (Och and Ney, 2003). Bi-directional alignments are generally obtained from source-to-target alignments tsA 2 and target-to-source alignments stA 2 , using some heuristic rules (Koehn et al., 2005). This method ignores the correlation of the words in the same alignment unit, so an alignment may include many unrelated words 2",", which influences the performances of SMT systems. 1 http://www.fjoch.com/GIZA++.html 2 In our experiments, a multi-word unit may include up to 40 words. 827","In order to solve the above problem, we incorporate the collocation probabilities into the bi-directional word alignment process.","Given alignment sets tsA 2 and stA 2 . We can obtain the union sttsts AAA 22  . The source sentence m","f1 can be segmented into m cepts m f  1 . The target sentence l","e1 can also be seg-","mented into l cepts l","e ","1 . The words in the same","cept can be a consecutive word sequence or an","interrupted word sequence. Finally, the optimal alignments A between m f  1 and l","e ","1 can be obtained from tsA  using the","following decision rule. })()(),({maxarg ),,(","3 21 ),( *' 1 ' 1"," "]},{"title":"","paragraphs":[" "," Afe jiji AA ml jits frerfep Afe (11)","Here, )( jfr and )( ier denote the collocation probabilities of the words in the source language and target language respectively, which are calculated by using Eq. (7). ),( ji fep denotes the word translation probability that is calculated according to Eq. (12). i denotes the weights of these probabilities. ||*|| 2/))|()|(( ),( ji ee ff ji fe efpfep fep","i j      (12)",")|( fep and )|( efp are the source-to-target and target-to-source translation probabilities trained from the word aligned bilingual corpus."]},{"title":"4 Improving Phrase Table","paragraphs":["Phrase-based SMT system automatically extracts bilingual phrase pairs from the word aligned bilingual corpus. In such a system, an idiomatic expression may be split into several fragments, and the phrases may include irrelevant words. In this paper, we use the collocation probability to measure the possibility of words composing a phrase.","For each bilingual phrase pair automatically extracted from word aligned corpus, we calculate the collocation probabilities of source phrase and target phrase respectively, according to Eq. (13). )1(* ),(2 )( 1 1 1 1        nn wwr wr n i n ij ji","n","(13)","Here, n","w1 denotes a phrase with n words; ),( ji wwr denotes the collocation probability of a Corpora Chinese words","English","words","Bilingual corpus 6.3M 8.5M","Additional monolingual corpora 312M 203M Table 1. Statistics of training data word pair calculated according to Eq. (4). For the phrase only including one word, we set a fixed collocation probability that is the average of the collocation probabilities of the sentences on a development set. These collocation probabilities are incorporated into the phrase-based SMT system as features."]},{"title":"5 Experiments on Word Alignment 5.1 Experimental settings","paragraphs":["We use a bilingual corpus, FBIS (LDC2003E14), to train the IBM models. To train the collocation models, besides the monolingual parts of FBIS, we also employ some other larger Chinese and English monolingual corpora, namely, Chinese Gigaword (LDC2007T38), English Gigaword (LDC2007T07), UN corpus (LDC2004E12), Sinorama corpus (LDC2005T10), as shown in Table 1.","Using these corpora, we got three kinds of collocation models: CM-1: the training data is the additional mo-","nolingual corpora; CM-2: the training data is either side of the bi-","lingual corpus; CM-3: the interpolation of CM-1 and CM-2.","To investigate the quality of the generated word alignments, we randomly selected a subset from the bilingual corpus as test set, including 500 sentence pairs. Then word alignments in the subset were manually labeled, referring to the guideline of the Chinese-to-English alignment (LDC2006E93), but we made some modifications for the guideline. For example, if a preposi-tion appears after a verb as a phrase aligned to one single word in the corresponding sentence, then they are glued together.","There are several different evaluation metrics for word alignment (Ahrenberg et al., 2000). We use precision (P), recall (R) and alignment error ratio (AER), which are similar to those in Och and Ney (2000), except that we consider each alignment as a sure link. 828 Experiments Single word alignments Multi-word alignments","P R AER P R AER","Baseline 0.77 0.45 0.43 0.23 0.71 0.65 Improved BWA methods CM-1 0.70 0.50 0.42 0.35 0.86 0.50 CM-2 0.73 0.48 0.42 0.36 0.89 0.49 CM-3 0.73 0.48 0.41 0.39 0.78 0.47 Table 2. English-to-Chinese word alignment results  Figure 2. Example of the English-to-Chinese word alignments generated by the BWA method and","the improved BWA method using CM-3. \" \" denotes the alignments of our method; \" \" denotes","the alignments of the baseline method. || || g rg S SS P","  (14) || || r rg S SS R","  (15) |||| ||*2 1 rg rg SS SS AER    (16)","Where, gS and rS denote the automatically generated alignments and the reference alignments.","In order to tune the interpolation coefficients in Eq. (5) and the weights of the probabilities in Eq. (11), we also manually labeled a development set including 100 sentence pairs, in the same manner as the test set. By minimizing the AER on the development set, the interpolation coefficients of the collocation probabilities on CM-1 and CM-2 were set to 0.1 and 0.9. And the weights of probabilities were set as 6.01  ,","2.02  and 2.03  . 5.2 Evaluation results One-directional alignment results","To train a Chinese-to-English SMT system, we need to perform both Chinese-to-English and English-to-Chinese word alignment. We only evaluate the English-to-Chinese word alignment here. GIZA++ with the default settings is used as the baseline method. The evaluation results in Table 2 indicate that the performances of our methods on single word alignments are close to that of the baseline method. For multi-word alignments, our methods significantly outperform the baseline method in terms of both precision and recall, achieving up to 18% absolute error rate reduction.","Although the size of the bilingual corpus is much smaller than that of additional monolingual corpora, our methods using CM-1 and CM-2 achieve comparable performances. It is because CM-2 and the BWA model are derived from the same resource. By interpolating CM1 and CM2, i.e. CM-3, the error rate of multi-word alignment results is further reduced.","Figure 2 shows an example of word alignment results generated by the baseline method and the improved method using CM-3. In this example, our method successfully identifies many-to-one alignments such as \"the people of the world 世人\". In our collocation model, the collocation probability of \"the people of the world\" is much higher than that of \"people world\". And our method is also effective to prevent the unrelated 中国 的 科学技术 研究 取得 了 许多 令 世人 瞩目 的 成就 。 China's science and technology research has made achievements which have gained the attention of the people of the world . 中国 的 科学技术 研究 取得 了 许多 令 世人 瞩目 的 成就 。 zhong-guo de ke-xue-ji-shu yan-jiu qu-de le xu-duo ling shi-ren zhu-mu de cheng-jiu . china DE science and research obtain LE many let common attract DE achievement . technology people attention 829 Experiments Single word alignments Multi-word alignments All alignments","P R AER P R AER P R AER","Baseline 0.84 0.43 0.42 0.18 0.74 0.70 0.52 0.45 0.51 Our methods WA-1 0.80 0.51 0.37 0.30 0.89 0.55 0.58 0.51 0.45 WA-2 0.81 0.50 0.37 0.33 0.81 0.52 0.62 0.50 0.44 WA-3 0.78 0.56 0.34 0.44 0.88 0.41 0.63 0.54 0.40 Table 3. Bi-directional word alignment results words from being aligned. For example, in the baseline alignment \"has made ... have 取得\", \"have\" and \"has\" are unrelated to the target word, while our method only generated \"made 取 得\", this is because that the collocation probabilities of \"has/have\" and \"made\" are much lower than that of the whole source sentence. Bi-directional alignment results","We build a bi-directional alignment baseline in two steps: (1) GIZA++ is used to obtain the source-to-target and target-to-source alignments; (2) the bi-directional alignments are generated by using \"grow-diag-final\". We use the methods proposed in section 3 to replace the corresponding steps in the baseline method. We evaluate three methods: WA-1: one-directional alignment method proposed in section 3.1 and grow-diag-final; WA-2: GIZA++ and the bi-directional bilingual word alignments method proposed in section 3.2; WA-3: both methods proposed in section 3.","Here, CM-3 is used in our methods. The results are shown in Table 3.","We can see that WA-1 achieves lower alignment error rate as compared to the baseline method, since the performance of the improved one-directional alignment method is better than that of GIZA++. This result indicates that improving one-directional word alignment results in bi-directional word alignment improvement.","The results also show that the AER of WA-2 is lower than that of the baseline. This is because the proposed bi-directional alignment method can effectively recognize the correct alignments from the alignment union, by leveraging collocation probabilities of the words in the same cept.","Our method using both methods proposed in section 3 produces the best alignment performance, achieving 11% absolute error rate reduction.","Experiments BLEU (%) Baseline 29.62 Our methods WA-1 CM-1 30.85 CM-2 31.28 CM-3 31.48 WA-2 CM-1 31.00 CM-2 31.33 CM-3 31.51 WA-3 CM-1 31.43 CM-2 31.62 CM-3 31.78 Table 4. Performances of Moses using the different bi-directional word alignments (Significantly better than baseline with p < 0.01)"]},{"title":"6 Experiments on Phrase-Based SMT 6.1 Experimental settings","paragraphs":["We use FBIS corpus to train the Chinese-to-English SMT systems. Moses (Koehn et al., 2007) is used as the baseline phrase-based SMT system. We use SRI language modeling toolkit (Stolcke, 2002) to train a 5-gram language model on the English sentences of FBIS corpus. We used the NIST MT-2002 set as the development set and the NIST MT-2004 test set as the test set. And Koehn's implementation of minimum error rate training (Och, 2003) is used to tune the feature weights on the development set.","We use BLEU (Papineni et al., 2002) as evaluation metrics. We also calculate the statistical significance differences between our methods and the baseline method by using paired bootstrap re-sample method (Koehn, 2004).","6.2 Effect of improved word alignment on phrase-based SMT We investigate the effectiveness of the improved word alignments on the phrase-based SMT system. The bi-directional alignments are obtained 830","","Figure 3. Example of the translations generated by the baseline system and the system where the phrase collocation probabilities are added","Experiments BLEU (%) Moses 29.62 + Phrase collocation probability 30.47 + Improved word alignments + Phrase collocation probability 32.02 Table 5. Performances of Moses employing","our proposed methods (Significantly better than","baseline with p < 0.01) using the same methods as those shown in Table 3. Here, we investigate three different collocation models for translation quality improvement. The results are shown in Table 4.","From the results of Table 4, it can be seen that the systems using the improved bi-directional alignments achieve higher quality of translation than the baseline system. If the same alignment method is used, the systems using CM-3 got the highest BLEU scores. And if the same collocation model is used, the systems using WA-3 achieved the higher scores. These results are consistent with the evaluations of word alignments as shown in Tables 2 and 3.","6.3 Effect of phrase collocation probabilities To investigate the effectiveness of the method proposed in section 4, we only use the collocation model CM-3 as described in section 5.1. The results are shown in Table 5. When the phrase collocation probabilities are incorporated into the SMT system, the translation quality is improved, achieving an absolute improvement of 0.85 BLEU score. This result indicates that the collocation probabilities of phrases are useful in de-termining the boundary of phrase and predicting whether phrases should be translated together, which helps to improve the phrase-based SMT performance.","Figure 3 shows an example: T1 is generated by the system where the phrase collocation probabilities are used and T2 is generated by the baseline system. In this example, since the collocation probability of \"出 问题\" is much higher than that of \"问题 。\", our method tends to split \"出 问题 。\" into \"(出 问题) (。)\", rather than \"(出) (问题 。)\". For the phrase \"才能 避免\" in the source sentence, the collocation probability of the translation \"in order to avoid\" is higher than that of the translation \"can we avoid\". Thus, our method selects the former as the translation. Although the phrase \"我们 必须 采取 有效 措 施\" in the source sentence has the same translation \"We must adopt effective measures\", our method splits this phrase into two parts \"我们 必 须\" and \"采取 有效 措施\", because two parts have higher collocation probabilities than the whole phrase.","We also investigate the performance of the system employing both the word alignment improvement and phrase table improvement methods. From the results in Table 5, it can be seen that the quality of translation is future improved. As compared with the baseline system, an absolute improvement of 2.40 BLEU score is achieved. And this result is also better than the results shown in Table 4."]},{"title":"7 Experiments on Parsing-Based SMT","paragraphs":["We also investigate the effectiveness of the improved word alignments on the parsing-based SMT system, Joshua (Li et al., 2009). In this system, the Hiero-style SCFG model is used (Chiang, 2007), without syntactic information. The rules are extracted only based on the FBIS corpus, where words are aligned by \"MW-3 & CM-3\". And the language model is the same as that in Moses. The feature weights are tuned on the development set using the minimum error 我们 必须 采取 有效 措施 才能 避免 出 问题 。 wo-men bi-xu cai-qu you-xiao cuo-shi cai-neng bi-mian chu wen-ti . we must use effective measure can avoid out problem . We must adopt effective measures in order to avoid problems .   We must adopt effective measures can we avoid out of the question . T1: T2: 831 Experiments BLEU (%)","Joshua 30.05","+ Improved word alignments 31.81 Table 6. Performances of Joshua using the dif-","ferent word alignments (Significantly better than","baseline with p < 0.01) rate training method. We use the same evaluation measure as described in section 6.1.","The translation results on Joshua are shown in Table 6. The system using the improved word alignments achieves an absolute improvement of 1.76 BLEU score, which indicates that the improvements of word alignments are also effective to improve the performance of the parsing-based SMT systems."]},{"title":"8 Conclusion","paragraphs":["We presented a novel method to use monolingual collocations to improve SMT. We first used the MWA method to identify potentially collocated words and estimate collocation probabilities only from monolingual corpora, no additional resource or linguistic preprocessing is needed. Then the collocation information was employed to improve BWA for various kinds of SMT systems and to improve phrase table for phrase-based SMT.","To improve BWA, we re-estimate the alignment probabilities by using the collocation probabilities of words in the same cept. To improve phrase table, we calculate phrase collocation probabilities based on word collocation probabilities. Then the phrase collocation probabilities are used as additional features in phrase-based SMT systems.","The evaluation results showed that the proposed method significantly improved word alignment, achieving an absolute error rate reduction of 29% on multi-word alignment. The improved word alignment results in an improvement of 2.16 BLEU score on a phrase-based SMT system and an improvement of 1.76 BLEU score on a parsing-based SMT system. When we also used phrase collocation probabilities as additional features, the phrase-based SMT performance is finally improved by 2.40 BLEU score as compared with the baseline system."]},{"title":"Reference","paragraphs":["Lars Ahrenberg, Magnus Merkel, Anna Sagvall Hein, and Jorg Tiedemann. 2000. Evaluation of Word Alignment Systems. In Proceedings of the Second International Conference on Language Resources and Evaluation, pp. 1255-1261.","Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin Knight, John Lafferty, Dan Melamed, Franz-Josef Och, David Purdy, Noah A. Smith, and David Yarowsky. 1999. Statistical Machine Translation. Final Report. In Johns Hopkins University Workshop.","Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert. L. Mercer. 1993. The Mathematics of Statistical Machine Translation: Parameter estimation. Computational Linguistics, 19(2): 263-311.","Colin Cherry and Dekang Lin. 2003. A Probability Model to Improve Word Alignment. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pp. 88-95.","David Chiang. 2007. Hierarchical Phrase-Based Translation. Computational Linguistics, 33(2): 201-228.","Fei Huang. 2009. Confidence Measure for Word Alignment. In Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP, pp. 932-940.","Philipp Koehn. 2004. Statistical Significance Tests for Machine Translation Evaluation. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pp. 388-395.","Philipp Koehn, Amittai Axelrod, Alexandra Birch Mayne, Chris Callison-Burch, Miles Osborne, and David Talbot. 2005. Edinburgh System Description for the 2005 IWSLT Speech Translation Evaluation. In Processings of the International Workshop on Spoken Language Translation 2005.","Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. Statistical Phrase-based Translation. In Proceedings of the Human Language Technology Conference and the North American Association for Computational Linguistics, pp. 127-133.","Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In Proceedings of the 45th Annual Meeting of the ACL, Poster and Demonstration Sessions, pp. 177-180.","Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren Thornton, Jonathan Weese, and Omar Zaidan. 2009. Demonstration of Joshua: An Open Source Toolkit for Parsing-based Machine Translation. In Proceedings of the 47th Annual Meeting of the As-832 sociation for Computational Linguistics, Software Demonstrations, pp. 25-28.","Yang Liu, Qun Liu, and Shouxun Lin. Log-linear Models for Word Alignment. 2005. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pp. 459-466.","Zhanyi Liu, Haifeng Wang, Hua Wu, and Sheng Li. 2009. Collocation Extraction Using Monolingual Word Alignment Method. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pp. 487-495.","Daniel Marcu and William Wong. 2002. A Phrase-Based, Joint Probability Model for Statistical Machine Translation. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing, pp. 133-139.","Yuval Marton and Philip Resnik. 2008. Soft Syntactic Constraints for Hierarchical Phrase-Based Translation. In Proceedings of the 46st Annual Meeting of the Association for Computational Linguistics, pp. 1003-1011.","Kathleen R. McKeown and Dragomir R. Radev. 2000. Collocations. In Robert Dale, Hermann Moisl, and Harold Somers (Ed.), A Handbook of Natural Language Processing, pp. 507-523.","Franz Josef Och and Hermann Ney. 2000. Improved Statistical Alignment Models. In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics, pp. 440-447.","Franz Josef Och. 2003. Minimum Error Rate Training in Statistical Machine Translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pp. 160-167.","Franz Josef Och and Hermann Ney. 2003. A Systematic Comparison of Various Statistical Alignment Models. Computational Linguistics, 29(1): 19-52.","Kishore Papineni, Salim Roukos, Todd Ward, and Weijing Zhu. 2002. BLEU: A Method for Automatic Evaluation of Machine Translation. In Proceedings of 40th annual meeting of the Association for Computational Linguistics, pp. 311-318.","Andreas Stolcke. 2002. SRILM - An Extensible Language Modeling Toolkit. In Proceedings for the International Conference on Spoken Language Processing, pp. 901-904.","Dekai Wu. 1997. Stochastic Inversion Transduction Grammars and Bilingual Parsing of Parallel Corpora. Computational Linguistics, 23(3): 377-403.","Deyi Xiong, Min Zhang, Aiti Aw, and Haizhou Li. 2009. A Syntax-Driven Bracketing Model for Phrase-Based Translation. In Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP, pp. 315-323.  833"]}],"references":[{"authors":[{"first":"Lars","last":"Ahrenberg"},{"first":"Magnus","last":"Merkel"},{"first":"Anna","middle":"Sagvall","last":"Hein"},{"first":"Jorg","last":"Tiedemann"}],"year":"2000","title":"Evaluation of Word Alignment Systems","source":"Lars Ahrenberg, Magnus Merkel, Anna Sagvall Hein, and Jorg Tiedemann. 2000. Evaluation of Word Alignment Systems. In Proceedings of the Second International Conference on Language Resources and Evaluation, pp. 1255-1261."},{"authors":[{"first":"Yaser","last":"Al-Onaizan"},{"first":"Jan","last":"Curin"},{"first":"Michael","last":"Jahr"},{"first":"Kevin","last":"Knight"},{"first":"John","last":"Lafferty"},{"first":"Dan","last":"Melamed"},{"first":"Franz-Josef","last":"Och"},{"first":"David","last":"Purdy"},{"first":"Noah","middle":"A.","last":"Smith"},{"first":"David","last":"Yarowsky"}],"year":"1999","title":"Statistical Machine Translation","source":"Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin Knight, John Lafferty, Dan Melamed, Franz-Josef Och, David Purdy, Noah A. Smith, and David Yarowsky. 1999. Statistical Machine Translation. Final Report. In Johns Hopkins University Workshop."},{"authors":[{"first":"Peter","middle":"F.","last":"Brown"},{"first":"Stephen","middle":"A. Della","last":"Pietra"},{"first":"Vincent","middle":"J. Della","last":"Pietra"},{"first":"Robert.","middle":"L.","last":"Mercer"}],"year":"1993","title":"The Mathematics of Statistical Machine Translation: Parameter estimation","source":"Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert. L. Mercer. 1993. The Mathematics of Statistical Machine Translation: Parameter estimation. Computational Linguistics, 19(2): 263-311."},{"authors":[{"first":"Colin","last":"Cherry"},{"first":"Dekang","last":"Lin"}],"year":"2003","title":"A Probability Model to Improve Word Alignment","source":"Colin Cherry and Dekang Lin. 2003. A Probability Model to Improve Word Alignment. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pp. 88-95."},{"authors":[{"first":"David","last":"Chiang"}],"year":"2007","title":"Hierarchical Phrase-Based Translation","source":"David Chiang. 2007. Hierarchical Phrase-Based Translation. Computational Linguistics, 33(2): 201-228."},{"authors":[{"first":"Fei","last":"Huang"}],"year":"2009","title":"Confidence Measure for Word Alignment","source":"Fei Huang. 2009. Confidence Measure for Word Alignment. In Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP, pp. 932-940."},{"authors":[{"first":"Philipp","last":"Koehn"}],"year":"2004","title":"Statistical Significance Tests for Machine Translation Evaluation","source":"Philipp Koehn. 2004. Statistical Significance Tests for Machine Translation Evaluation. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pp. 388-395."},{"authors":[{"first":"Philipp","last":"Koehn"},{"first":"Amittai","last":"Axelrod"},{"first":"Alexandra","middle":"Birch","last":"Mayne"},{"first":"Chris","last":"Callison-Burch"},{"first":"Miles","last":"Osborne"},{"first":"David","last":"Talbot"}],"year":"2005","title":"Edinburgh System Description for the 2005 IWSLT Speech Translation Evaluation","source":"Philipp Koehn, Amittai Axelrod, Alexandra Birch Mayne, Chris Callison-Burch, Miles Osborne, and David Talbot. 2005. Edinburgh System Description for the 2005 IWSLT Speech Translation Evaluation. In Processings of the International Workshop on Spoken Language Translation 2005."},{"authors":[{"first":"Philipp","last":"Koehn"},{"first":"Franz","middle":"J.","last":"Och"},{"first":"Daniel","last":"Marcu"}],"year":"2003","title":"Statistical Phrase-based Translation","source":"Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. Statistical Phrase-based Translation. In Proceedings of the Human Language Technology Conference and the North American Association for Computational Linguistics, pp. 127-133."},{"authors":[{"first":"Philipp","last":"Koehn"},{"first":"Hieu","last":"Hoang"},{"first":"Alexandra","last":"Birch"},{"first":"Chris","last":"Callison-Burch"},{"first":"Marcello","last":"Federico"},{"first":"Nicola","last":"Bertoldi"},{"first":"Brooke","last":"Cowan"},{"first":"Wade","last":"Shen"},{"first":"Christine","middle":"Moran Richard","last":"Zens"},{"first":"Chris","last":"Dyer"},{"first":"Ondrej","last":"Bojar"},{"first":"Alexandra","last":"Constantin"},{"first":"Evan","last":"Herbst"}],"year":"2007","title":"Moses: Open Source Toolkit for Statistical Machine Translation","source":"Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In Proceedings of the 45th Annual Meeting of the ACL, Poster and Demonstration Sessions, pp. 177-180."},{"authors":[{"first":"Zhifei","last":"Li"},{"first":"Chris","last":"Callison-Burch"},{"first":"Chris","last":"Dyer"},{"first":"Juri","last":"Ganitkevitch"},{"first":"Sanjeev","last":"Khudanpur"},{"first":"Lane","last":"Schwartz"},{"first":"Wren","last":"Thornton"},{"first":"Jonathan","last":"Weese"},{"first":"Omar","last":"Zaidan"}],"year":"2009","title":"Demonstration of Joshua: An Open Source Toolkit for Parsing-based Machine Translation","source":"Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren Thornton, Jonathan Weese, and Omar Zaidan. 2009. Demonstration of Joshua: An Open Source Toolkit for Parsing-based Machine Translation. In Proceedings of the 47th Annual Meeting of the As-832 sociation for Computational Linguistics, Software Demonstrations, pp. 25-28."},{"authors":[{"first":"Yang","last":"Liu"},{"first":"Qun","last":"Liu"},{"first":"Shouxun","middle":"Lin. Log-linear Models for Word","last":"Alignment"}],"year":"2005","title":"In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pp","source":"Yang Liu, Qun Liu, and Shouxun Lin. Log-linear Models for Word Alignment. 2005. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pp. 459-466."},{"authors":[{"first":"Zhanyi","last":"Liu"},{"first":"Haifeng","last":"Wang"},{"first":"Hua","last":"Wu"},{"first":"Sheng","last":"Li"}],"year":"2009","title":"Collocation Extraction Using Monolingual Word Alignment Method","source":"Zhanyi Liu, Haifeng Wang, Hua Wu, and Sheng Li. 2009. Collocation Extraction Using Monolingual Word Alignment Method. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pp. 487-495."},{"authors":[{"first":"Daniel","last":"Marcu"},{"first":"William","last":"Wong"}],"year":"2002","title":"A Phrase-Based, Joint Probability Model for Statistical Machine Translation","source":"Daniel Marcu and William Wong. 2002. A Phrase-Based, Joint Probability Model for Statistical Machine Translation. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing, pp. 133-139."},{"authors":[{"first":"Yuval","last":"Marton"},{"first":"Philip","last":"Resnik"}],"year":"2008","title":"Soft Syntactic Constraints for Hierarchical Phrase-Based Translation","source":"Yuval Marton and Philip Resnik. 2008. Soft Syntactic Constraints for Hierarchical Phrase-Based Translation. In Proceedings of the 46st Annual Meeting of the Association for Computational Linguistics, pp. 1003-1011."},{"authors":[{"first":"Kathleen","middle":"R.","last":"McKeown"},{"first":"Dragomir","middle":"R.","last":"Radev"}],"year":"2000","title":"Collocations","source":"Kathleen R. McKeown and Dragomir R. Radev. 2000. Collocations. In Robert Dale, Hermann Moisl, and Harold Somers (Ed.), A Handbook of Natural Language Processing, pp. 507-523."},{"authors":[{"first":"Franz","middle":"Josef","last":"Och"},{"first":"Hermann","last":"Ney"}],"year":"2000","title":"Improved Statistical Alignment Models","source":"Franz Josef Och and Hermann Ney. 2000. Improved Statistical Alignment Models. In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics, pp. 440-447."},{"authors":[{"first":"Franz","middle":"Josef","last":"Och"}],"year":"2003","title":"Minimum Error Rate Training in Statistical Machine Translation","source":"Franz Josef Och. 2003. Minimum Error Rate Training in Statistical Machine Translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pp. 160-167."},{"authors":[{"first":"Franz","middle":"Josef","last":"Och"},{"first":"Hermann","last":"Ney"}],"year":"2003","title":"A Systematic Comparison of Various Statistical Alignment Models","source":"Franz Josef Och and Hermann Ney. 2003. A Systematic Comparison of Various Statistical Alignment Models. Computational Linguistics, 29(1): 19-52."},{"authors":[{"first":"Kishore","last":"Papineni"},{"first":"Salim","last":"Roukos"},{"first":"Todd","last":"Ward"},{"first":"Weijing","last":"Zhu"}],"year":"2002","title":"BLEU: A Method for Automatic Evaluation of Machine Translation","source":"Kishore Papineni, Salim Roukos, Todd Ward, and Weijing Zhu. 2002. BLEU: A Method for Automatic Evaluation of Machine Translation. In Proceedings of 40th annual meeting of the Association for Computational Linguistics, pp. 311-318."},{"authors":[{"first":"Andreas","last":"Stolcke"}],"year":"2002","title":"SRILM - An Extensible Language Modeling Toolkit","source":"Andreas Stolcke. 2002. SRILM - An Extensible Language Modeling Toolkit. In Proceedings for the International Conference on Spoken Language Processing, pp. 901-904."},{"authors":[{"first":"Dekai","last":"Wu"}],"year":"1997","title":"Stochastic Inversion Transduction Grammars and Bilingual Parsing of Parallel Corpora","source":"Dekai Wu. 1997. Stochastic Inversion Transduction Grammars and Bilingual Parsing of Parallel Corpora. Computational Linguistics, 23(3): 377-403."},{"authors":[{"first":"Deyi","last":"Xiong"},{"first":"Min","last":"Zhang"},{"first":"Aiti","last":"Aw"},{"first":"Haizhou","last":"Li"}],"year":"2009","title":"A Syntax-Driven Bracketing Model for Phrase-Based Translation","source":"Deyi Xiong, Min Zhang, Aiti Aw, and Haizhou Li. 2009. A Syntax-Driven Bracketing Model for Phrase-Based Translation. In Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP, pp. 315-323.  833"}],"cites":[{"style":0,"text":"Wu, 1997","origin":{"pointer":"/sections/4/paragraphs/0","offset":264,"length":8},"authors":[{"last":"Wu"}],"year":"1997","references":["/references/21"]},{"style":0,"text":"Och and Ney, 2000","origin":{"pointer":"/sections/4/paragraphs/0","offset":274,"length":17},"authors":[{"last":"Och"},{"last":"Ney"}],"year":"2000","references":["/references/16"]},{"style":0,"text":"Marcu and Wong, 2002","origin":{"pointer":"/sections/4/paragraphs/0","offset":293,"length":20},"authors":[{"last":"Marcu"},{"last":"Wong"}],"year":"2002","references":["/references/13"]},{"style":0,"text":"Cherry and Lin, 2003","origin":{"pointer":"/sections/4/paragraphs/0","offset":315,"length":20},"authors":[{"last":"Cherry"},{"last":"Lin"}],"year":"2003","references":["/references/3"]},{"style":0,"text":"Liu et al., 2005","origin":{"pointer":"/sections/4/paragraphs/0","offset":337,"length":16},"authors":[{"last":"Liu"},{"last":"al."}],"year":"2005","references":["/references/11"]},{"style":0,"text":"Huang, 2009","origin":{"pointer":"/sections/4/paragraphs/0","offset":355,"length":11},"authors":[{"last":"Huang"}],"year":"2009","references":["/references/5"]},{"style":0,"text":"Koehn et al., 2003","origin":{"pointer":"/sections/4/paragraphs/1","offset":21,"length":18},"authors":[{"last":"Koehn"},{"last":"al."}],"year":"2003","references":["/references/8"]},{"style":0,"text":"Marton and Resnik, 2008","origin":{"pointer":"/sections/4/paragraphs/1","offset":424,"length":23},"authors":[{"last":"Marton"},{"last":"Resnik"}],"year":"2008","references":["/references/14"]},{"style":0,"text":"Xiong et al., 2009","origin":{"pointer":"/sections/4/paragraphs/1","offset":449,"length":18},"authors":[{"last":"Xiong"},{"last":"al."}],"year":"2009","references":["/references/22"]},{"style":0,"text":"Liu et al., 2009","origin":{"pointer":"/sections/4/paragraphs/2","offset":234,"length":16},"authors":[{"last":"Liu"},{"last":"al."}],"year":"2009","references":["/references/12"]},{"style":0,"text":"Brown et al., 1993","origin":{"pointer":"/sections/4/paragraphs/3","offset":203,"length":18},"authors":[{"last":"Brown"},{"last":"al."}],"year":"1993","references":["/references/2"]},{"style":0,"text":"McKeown and Radev, 2000","origin":{"pointer":"/sections/5/paragraphs/0","offset":100,"length":23},"authors":[{"last":"McKeown"},{"last":"Radev"}],"year":"2000","references":["/references/15"]},{"style":0,"text":"Liu et al., 2009","origin":{"pointer":"/sections/5/paragraphs/0","offset":341,"length":16},"authors":[{"last":"Liu"},{"last":"al."}],"year":"2009","references":["/references/12"]},{"style":0,"text":"Liu et al., 2009","origin":{"pointer":"/sections/5/paragraphs/0","offset":557,"length":16},"authors":[{"last":"Liu"},{"last":"al."}],"year":"2009","references":["/references/12"]},{"style":0,"text":"Liu et al. (2009)","origin":{"pointer":"/sections/5/paragraphs/1","offset":13,"length":17},"authors":[{"last":"Liu"},{"last":"al."}],"year":"2009","references":["/references/12"]},{"style":0,"text":"Al-Onaizan et al., 1999","origin":{"pointer":"/sections/11/paragraphs/1","offset":202,"length":23},"authors":[{"last":"Al-Onaizan"},{"last":"al."}],"year":"1999","references":["/references/1"]},{"style":0,"text":"Och and Ney, 2003","origin":{"pointer":"/sections/11/paragraphs/3","offset":303,"length":17},"authors":[{"last":"Och"},{"last":"Ney"}],"year":"2003","references":["/references/18"]},{"style":0,"text":"Koehn et al., 2005","origin":{"pointer":"/sections/11/paragraphs/3","offset":479,"length":18},"authors":[{"last":"Koehn"},{"last":"al."}],"year":"2005","references":["/references/7"]},{"style":0,"text":"Ahrenberg et al., 2000","origin":{"pointer":"/sections/14/paragraphs/5","offset":67,"length":22},"authors":[{"last":"Ahrenberg"},{"last":"al."}],"year":"2000","references":["/references/0"]},{"style":0,"text":"Och and Ney (2000)","origin":{"pointer":"/sections/14/paragraphs/5","offset":188,"length":18},"authors":[{"last":"Och"},{"last":"Ney"}],"year":"2000","references":["/references/16"]},{"style":0,"text":"Koehn et al., 2007","origin":{"pointer":"/sections/15/paragraphs/0","offset":71,"length":18},"authors":[{"last":"Koehn"},{"last":"al."}],"year":"2007","references":["/references/9"]},{"style":0,"text":"Stolcke, 2002","origin":{"pointer":"/sections/15/paragraphs/0","offset":178,"length":13},"authors":[{"last":"Stolcke"}],"year":"2002","references":["/references/20"]},{"style":0,"text":"Och, 2003","origin":{"pointer":"/sections/15/paragraphs/0","offset":425,"length":9},"authors":[{"last":"Och"}],"year":"2003","references":["/references/17"]},{"style":0,"text":"Papineni et al., 2002","origin":{"pointer":"/sections/15/paragraphs/1","offset":13,"length":21},"authors":[{"last":"Papineni"},{"last":"al."}],"year":"2002","references":["/references/19"]},{"style":0,"text":"Koehn, 2004","origin":{"pointer":"/sections/15/paragraphs/1","offset":206,"length":11},"authors":[{"last":"Koehn"}],"year":"2004","references":["/references/6"]},{"style":0,"text":"Li et al., 2009","origin":{"pointer":"/sections/16/paragraphs/0","offset":111,"length":15},"authors":[{"last":"Li"},{"last":"al."}],"year":"2009","references":["/references/10"]},{"style":0,"text":"Chiang, 2007","origin":{"pointer":"/sections/16/paragraphs/0","offset":181,"length":12},"authors":[{"last":"Chiang"}],"year":"2007","references":["/references/4"]}]}
