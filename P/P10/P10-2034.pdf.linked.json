{"sections":[{"title":"","paragraphs":["Proceedings of the ACL 2010 Conference Short Papers, pages 184–188, Uppsala, Sweden, 11-16 July 2010. c⃝2010 Association for Computational Linguistics"]},{"title":"Learning Common Grammar from Multilingual Corpus Tomoharu Iwata Daichi Mochihashi NTT Communication Science Laboratories 2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto, Japan {iwata,daichi,sawada}@cslab.kecl.ntt.co.jp Hiroshi Sawada Abstract","paragraphs":["We propose a corpus-based probabilistic framework to extract hidden common syntax across languages from non-parallel multilingual corpora in an unsupervised fashion. For this purpose, we assume a generative model for multilingual corpora, where each sentence is generated from a language dependent probabilistic context-free grammar (PCFG), and these PCFGs are generated from a prior grammar that is common across languages. We also develop a variational method for efficient inference. Experiments on a non-parallel multilingual corpus of eleven languages demonstrate the feasibility of the proposed method."]},{"title":"1 Introduction","paragraphs":["Languages share certain common properties (Pinker, 1994). For example, the word order in most European languages is subject-verb-object (SVO), and some words with similar forms are used with similar meanings in different languages. The reasons for these common properties can be attributed to: 1) a common ancestor language, 2) borrowing from nearby languages, and 3) the innate abilities of humans (Chomsky, 1965).","We assume hidden commonalities in syntax across languages, and try to extract a common grammar from non-parallel multilingual corpora. For this purpose, we propose a generative model for multilingual grammars that is learned in an unsupervised fashion. There are some computational models for capturing commonalities at the phoneme and word level (Oakes, 2000; Bouchard-Côté et al., 2008), but, as far as we know, no at-tempt has been made to extract commonalities in syntax level from non-parallel and non-annotated multilingual corpora.","In our scenario, we use probabilistic context-free grammars (PCFGs) as our monolingual grammar model. We assume that a PCFG for each language is generated from a general model that are common across languages, and each sentence in multilingual corpora is generated from the language dependent PCFG. The inference of the general model as well as the multilingual PCFGs can be performed by using a variational method for efficiency. Our approach is based on a Bayesian multitask learning framework (Yu et al., 2005; Daumé III, 2009). Hierarchical Bayesian model-ing provides a natural way of obtaining a joint regularization for individual models by assuming that the model parameters are drawn from a common prior distribution (Yu et al., 2005)."]},{"title":"2 Related work","paragraphs":["The unsupervised grammar induction task has been extensively studied (Carroll and Charniak, 1992; Stolcke and Omohundro, 1994; Klein and Manning, 2002; Klein and Manning, 2004; Liang et al., 2007). Recently, models have been proposed that outperform PCFG in the grammar induction task (Klein and Manning, 2002; Klein and Manning, 2004). We used PCFG as a first step for capturing commonalities in syntax across languages because of its simplicity. The proposed framework can be used for probabilistic grammar models other than PCFG.","Grammar induction using bilingual parallel corpora has been studied mainly in machine translation research (Wu, 1997; Melamed, 2003; Eisner, 2003; Chiang, 2005; Blunsom et al., 2009; Snyder et al., 2009). These methods require sentence-aligned parallel data, which can be costly to obtain and difficult to scale to many languages. On the other hand, our model does not require sentences to be aligned. Moreover, since the complexity of our model increases linearly with the number of languages, our model is easily applicable to cor-184 pora of more than two languages, as we will show in the experiments. To our knowledge, the only grammar induction work on non-parallel corpora is (Cohen and Smith, 2009), but their method does not model a common grammar, and requires prior information such as part-of-speech tags. In contrast, our method does not require any such prior information."]},{"title":"3 Proposed Method 3.1 Model","paragraphs":["Let X = {Xl}l∈L be a non-parallel and non-annotated multilingual corpus, where Xl is a set of sentences in language l, and L is a set of languages. The task is to learn multilingual PCFGs G = {Gl}l∈L and a common grammar that generates these PCFGs. Here, Gl = (K, W l, Φl) represents a PCFG of language l, where K is a set of nonterminals, W l is a set of terminals, and Φl is a set of rule probabilities. Note that a set of nonterminals K is shared among languages, but a set of terminals W l and rule probabilities Φl are specific to the language. For simplicity, we consider Chomsky normal form grammars, which have two types of rules: emissions rewrite a nonterminal as a terminal A → w, and binary productions rewrite a nonterminal as two nonterminals A → BC, where A, B, C ∈ K and w ∈ W l.","The rule probabilities for each nonterminal A of PCFG Gl in language l consist of: 1) θAl = {θlAt}t∈{0,1}, where θlA0 and θlA1 represent probabilities of choosing the emission rule and the binary production rule, respectively, 2) φlA = {φlABC }B,C∈K , where φlABC represents the probability of nonterminal production A → BC, and 3) ψlA = {ψlAw}w∈W l",", where ψlAw represents the probability of terminal emission A → w. Note that θlA0 + θlA1 = 1, θlAt ≥ 0,∑ B,C φlABC = 1, φlABC ≥ 0,","∑","w ψlAw = 1, and ψlAw ≥ 0. In the proposed model, multinomial parameters θlA and φlA are generated from Dirichlet distributions that are common across languages: θlA ∼ Dir(αθ","A) and φlA ∼ Dir(αφ","A), since we assume that languages share a common syntax structure. αθ","A and αφ","A represent the parameters of a common grammar. We use the Dirichlet prior because it is the conjugate prior for the multinomial distribution. In summary, the proposed model assumes the following generative process for a multilingual corpus, 1. For each nonterminal A ∈ K: a af Aa,b a,b |L| q Aa lA lA |K| f f q y lA |L| z 1 z 2 z 3 x2 x3 f y q q Figure 1: Graphical model.","(a) For each rule type t ∈ {0, 1}:","i. Draw common rule type parameters αθ At ∼ Gam(aθ",", bθ",")","(b) For each nonterminal pair (B, C):","i. Draw common production parameters αφ ABC ∼ Gam(aφ",", bφ",") 2. For each language l ∈ L:","(a) For each nonterminal A ∈ K:","i. Draw rule type parameters θlA ∼ Dir(αθ","A)","ii. Draw binary production parameters φlA ∼ Dir(αφ","A)","iii. Draw emission parameters ψlA ∼ Dir(αψ",")","(b) For each node i in the parse tree:","i. Choose rule type tli ∼ Mult(θlz","i)","ii. If tli = 0: A. Emit terminal","xli ∼ Mult(ψlz","i)","iii. Otherwise: A. Generate children nonterminals","(zlL(i), zlR(i)) ∼ Mult(φlz","i), where L(i) and R(i) represent the left and right children of node i. Figure 1 shows a graphical model representation of the proposed model, where the shaded and unshaded nodes indicate observed and latent variables, respectively. 3.2 Inference The inference of the proposed model can be efficiently computed using a variational Bayesian method. We extend the variational method to the monolingual PCFG learning of Kurihara and Sato (2004) for multilingual corpora. The goal is to estimate posterior p(Z, Φ, α|X), where Z is a set of parse trees, Φ = {Φl}l∈L is a set of language dependent parameters, Φl = {θlA, φlA, ψlA}A∈K , and α = {αθ","A, αφ","A}A∈K is a set of common parameters. In the variational method, posterior p(Z, Φ, α|X) is approximated by a tractable variational distribution q(Z, Φ, α). 185 We use the following variational distribution, q(Z, Φ, α) = ∏ A","q(αθ A)q(αφ","A) ∏ l,d q(zld) × ∏ l,A q(θlA)q(φlA)q(ψlA), (1)","where we assume that hyperparameters q(αθ A) and q(αφ","A) are degenerated, or q(α) = δα∗ (α), and infer them by point estimation instead of distribution estimation. We find an approximate posterior distribution that minimizes the Kullback-Leibler divergence from the true posterior. The variational distribution of the parse tree of the dth sentence in language l is obtained as follows, q(zld) ∝ ∏ A→BC ( πθ lA1π φ lABC ) C(A→BC;zld,l,d) × ∏ A→w ( πθ lA0π ψ lAw ) C(A→w;zld,l,d) , (2) where C(r; z, l, d) is the count of rule r that occurs in the dth sentence of language l with parse tree z. The multinomial weights are calculated as follows, πθ lAt = exp ( Eq(θ lA) [ log θlAt ]) , (3) π φ lABC = exp ( Eq(φ lA) [ log φlABC ]) , (4) π ψ lAw = exp ( Eq(ψ lA) [ log ψlAw ]) . (5) The variational Dirichlet parameters for q(θlA) = Dir(γθ","lA), q(φlA) = Dir(γ φ lA), and q(ψlA) = Dir(γ ψ lA), are obtained as follows, γθ lAt = αθ","At + ∑ d,zld q(zld)C(A, t; zld, l, d), (6) γ φ lABC = α φ ABC + ∑ d,zld q(zld)C(A → BC; zld, l, d), (7) γ ψ lAw = αψ","+ ∑ d,zld q(zld)C(A → w; zld, l, d),","(8) where C(A, t; z, l, d) is the count of rule type t that is selected in nonterminal A in the dth sentence of language l with parse tree z.","The common rule type parameter αθ","At that minimizes the KL divergence between the true posterior and the approximate posterior can be obtained by using the fixed-point iteration method described in (Minka, 2000). The update rule is as follows, α θ(new) At ←","aθ −1+αθ","AtL (","Ψ(∑ t′ αθ At′)−Ψ(αθ","At) )","bθ +","∑ l (","Ψ(∑ t′ γθ lAt′) − Ψ(γθ","lAt)) ,","(9) where L is the number of languages, and Ψ(x) = ∂ log Γ(x)","∂x is the digamma function. Similarly, the common production parameter α φ ABC can be updated as follows, α φ(new) ABC ←","aφ − 1 + αφ","ABC LJABC","bφ +","∑ l J ′","lABC , (10)","where JABC = Ψ(∑ B′",",C′ α φ AB′","C′) − Ψ(α φ ABC ),","and J ′ lABC = Ψ(","∑ B′",",C′ γ φ lAB′","C′) − Ψ(γ","φ","lABC ).","Since factored variational distributions depend on each other, an optimal approximated posterior can be obtained by updating parameters by (2) - (10) alternatively until convergence. The updating of language dependent distributions by (2) - (8) is also described in (Kurihara and Sato, 2004; Liang et al., 2007) while the updating of common grammar parameters by (9) and (10) is new. The inference can be carried out efficiently using the inside-outside algorithm based on dynamic programming (Lari and Young, 1990).","After the inference, the probability of a common grammar rule A → BC is calculated by φ̂A→BC = θ̂1 φ̂ABC , where θ̂1 = αθ","1/(αθ","0 + αθ","1)","and φ̂ABC = αφ ABC /","∑ B′",",C′ α φ AB′","C′ represent","the mean values of θl0 and φlABC , respectively."]},{"title":"4 Experimental results","paragraphs":["We evaluated our method by employing the EuroParl corpus (Koehn, 2005). The corpus consists of the proceedings of the European Parliament in eleven western European languages: Dan-ish (da), German (de), Greek (el), English (en), Spanish (es), Finnish (fi), French (fr), Italian (it), Dutch (nl), Portuguese (pt), and Swedish (sv), and it contains roughly 1,500,000 sentences in each language. We set the number of nonterminals at |K| = 20, and omitted sentences with more than ten words for tractability. We randomly sampled 100,000 sentences for each language, and analyzed them using our method. It should be noted that our random samples are not sentence-aligned.","Figure 2 shows the most probable terminals of emission for each language and nonterminal with a high probability of selecting the emission rule. 186 2: verb and auxiliary verb (V) 5: noun (N) 7: subject (SBJ) 9: preposition (PR) 11: punctuation (.) 13: determiner (DT) Figure 2: Probable terminals of emission for each language and nonterminal. 0 → 16 11 (R → S . ) 0.11 16 → 7 6 (S → SBJ VP) 0.06 6 → 2 12 (VP → V NP) 0.04 12 → 13 5 (NP → DT N) 0.19 15 → 17 19 (NP → NP N) 0.07 17 → 5 9 (NP → N PR) 0.07 15 → 13 5 (NP → DT N) 0.06 Figure 3: Examples of inferred common grammar rules in eleven languages, and their probabilities. Hand-provided annotations have the following meanings, R: root, S: sentence, NP: noun phrase, VP: verb phrase, and others appear in Figure 2. We named nonterminals by using grammatical categories after the inference. We can see that words in the same grammatical category clustered across languages as well as within a language. Figure 3 shows examples of inferred common grammar rules with high probabilities. Grammar rules that seem to be common to European languages have been extracted."]},{"title":"5 Discussion","paragraphs":["We have proposed a Bayesian hierarchical PCFG model for capturing commonalities at the syntax level for non-parallel multilingual corpora. Although our results have been encouraging, a number of directions remain in which we must extend our approach. First, we need to evaluate our model quantitatively using corpora with a greater diversity of languages. Measurement examples include the perplexity, and machine translation score. Second, we need to improve our model. For example, we can infer the number of nonterminals with a nonparametric Bayesian model (Liang et al., 2007), infer the model more robustly based on a Markov chain Monte Carlo inference (Johnson et al., 2007), and use probabilistic grammar models other than PCFGs. In our model, all the multilingual grammars are generated from a general model. We can extend it hierarchically using the coalescent (Kingman, 1982). That model may help to infer an evolutionary tree of languages in terms of grammatical structure without the etymological information that is generally used (Gray and Atkinson, 2003). Finally, the proposed approach may help to indicate the presence of a universal grammar (Chomsky, 1965), or to find it. 187"]},{"title":"References","paragraphs":["Phil Blunsom, Trevor Cohn, and Miles Osborne. 2009. Bayesian synchronous grammar induction. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors, Advances in Neural Information Processing Systems 21, pages 161–168.","Alexandre Bouchard-C ôté, Percy Liang, Thomas Griffiths, and Dan Klein. 2008. A probabilistic approach to language change. In J.C. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems 20, pages 169–176, Cambridge, MA. MIT Press.","Glenn Carroll and Eugene Charniak. 1992. Two experiments on learning probabilistic dependency grammars from corpora. In Working Notes of the Workshop Statistically-Based NLP Techniques, pages 1–13. AAAI.","David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In ACL ’05: Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 263–270, Morristown, NJ, USA. Association for Computational Linguistics.","Norm Chomsky. 1965. Aspects of the Theory of Syntax. MIT Press.","Shay B. Cohen and Noah A. Smith. 2009. Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction. In NAACL ’09: Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 74–82, Morristown, NJ, USA. Association for Computational Linguistics.","Hal Daumé III. 2009. Bayesian multitask learning with latent hierarchies. In Proceedings of the Twenty-Fifth Annual Conference on Uncertainty in Artificial Intelligence (UAI-09), pages 135–142, Corvallis, Oregon. AUAI Press.","Jason Eisner. 2003. Learning non-isomorphic tree mappings for machine translation. In ACL ’03: Proceedings of the 41st Annual Meeting on Association for Computational Linguistics, pages 205–208, Morristown, NJ, USA. Association for Computational Linguistics.","Russell D. Gray and Quentin D. Atkinson. 2003. Language-tree divergence times support the Anatolian theory of Indo-European origin. Nature, 426(6965):435–439, November.","Mark Johnson, Thomas Griffiths, and Sharon Goldwater. 2007. Bayesian inference for PCFGs via Markov chain Monte Carlo. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 139–146, Rochester, New York, April. Association for Computational Linguistics.","J. F. C. Kingman. 1982. The coalescent. Stochastic Processes and their Applications, 13:235–248.","Dan Klein and Christopher D. Manning. 2002. A generative constituent-context model for improved grammar induction. In ACL ’02: Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 128–135, Morristown, NJ, USA. Association for Computational Linguistics.","Dan Klein and Christopher D. Manning. 2004. Corpus-based induction of syntactic structure: models of dependency and constituency. In ACL ’04: Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, page 478, Morristown, NJ, USA. Association for Computational Linguistics.","Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In Proceedings of the 10th Machine Translation Summit, pages 79–86.","Kenichi Kurihara and Taisuke Sato. 2004. An applica-tion of the variational Bayesian approach to probabilistic context-free grammars. In International Joint Conference on Natural Language Processing Workshop Beyond Shallow Analysis.","K. Lari and S.J. Young. 1990. The estimation of stochastic context-free grammars using the inside-outside algorithm. Computer Speech and Language, 4:35–56.","Percy Liang, Slav Petrov, Michael I. Jordan, and Dan Klein. 2007. The infinite PCFG using hierarchical dirichlet processes. In EMNLP ’07: Proceedings of the Empirical Methods on Natural Language Processing, pages 688– 697.","I. Dan Melamed. 2003. Multitext grammars and synchronous parsers. In NAACL ’03: Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, pages 79–86, Morristown, NJ, USA. Association for Computational Linguistics.","Thomas Minka. 2000. Estimating a Dirichlet distribution. Technical report, M.I.T.","Michael P. Oakes. 2000. Computer estimation of vocabulary in a protolanguage from word lists in four daughter languages. Journal of Quantitative Linguistics, 7(3):233– 243.","Steven Pinker. 1994. The Language Instinct: How the Mind Creates Language. HarperCollins, New York.","Benjamin Snyder, Tahira Naseem, and Regina Barzilay. 2009. Unsupervised multilingual grammar induction. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 73–81, Suntec, Singapore, August. Association for Computational Linguistics.","Andreas Stolcke and Stephen M. Omohundro. 1994. In-ducing probabilistic grammars by Bayesian model merg-ing. In ICGI ’94: Proceedings of the Second International Colloquium on Grammatical Inference and Applications, pages 106–118, London, UK. Springer-Verlag.","Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Comput. Linguist., 23(3):377–403.","Kai Yu, Volker Tresp, and Anton Schwaighofer. 2005. Learning gaussian processes from multiple tasks. In ICML ’05: Proceedings of the 22nd International Conference on Machine Learning, pages 1012–1019, New York, NY, USA. ACM. 188"]}],"references":[{"authors":[{"first":"Phil","last":"Blunsom"},{"first":"Trevor","last":"Cohn"},{"first":"Miles","last":"Osborne"}],"year":"2009","title":"Bayesian synchronous grammar induction","source":"Phil Blunsom, Trevor Cohn, and Miles Osborne. 2009. Bayesian synchronous grammar induction. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors, Advances in Neural Information Processing Systems 21, pages 161–168."},{"authors":[{"first":"Alexandre","middle":"Bouchard-C","last":"ôté"},{"first":"Percy","last":"Liang"},{"first":"Thomas","last":"Griffiths"},{"first":"Dan","last":"Klein"}],"year":"2008","title":"A probabilistic approach to language change","source":"Alexandre Bouchard-C ôté, Percy Liang, Thomas Griffiths, and Dan Klein. 2008. A probabilistic approach to language change. In J.C. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems 20, pages 169–176, Cambridge, MA. MIT Press."},{"authors":[{"first":"Glenn","last":"Carroll"},{"first":"Eugene","last":"Charniak"}],"year":"1992","title":"Two experiments on learning probabilistic dependency grammars from corpora","source":"Glenn Carroll and Eugene Charniak. 1992. Two experiments on learning probabilistic dependency grammars from corpora. In Working Notes of the Workshop Statistically-Based NLP Techniques, pages 1–13. AAAI."},{"authors":[{"first":"David","last":"Chiang"}],"year":"2005","title":"A hierarchical phrase-based model for statistical machine translation","source":"David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In ACL ’05: Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 263–270, Morristown, NJ, USA. Association for Computational Linguistics."},{"authors":[{"first":"Norm","last":"Chomsky"}],"year":"1965","title":"Aspects of the Theory of Syntax","source":"Norm Chomsky. 1965. Aspects of the Theory of Syntax. MIT Press."},{"authors":[{"first":"Shay","middle":"B.","last":"Cohen"},{"first":"Noah","middle":"A.","last":"Smith"}],"year":"2009","title":"Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction","source":"Shay B. Cohen and Noah A. Smith. 2009. Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction. In NAACL ’09: Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 74–82, Morristown, NJ, USA. Association for Computational Linguistics."},{"authors":[{"first":"Hal","last":"Daumé III"}],"year":"2009","title":"Bayesian multitask learning with latent hierarchies","source":"Hal Daumé III. 2009. Bayesian multitask learning with latent hierarchies. In Proceedings of the Twenty-Fifth Annual Conference on Uncertainty in Artificial Intelligence (UAI-09), pages 135–142, Corvallis, Oregon. AUAI Press."},{"authors":[{"first":"Jason","last":"Eisner"}],"year":"2003","title":"Learning non-isomorphic tree mappings for machine translation","source":"Jason Eisner. 2003. Learning non-isomorphic tree mappings for machine translation. In ACL ’03: Proceedings of the 41st Annual Meeting on Association for Computational Linguistics, pages 205–208, Morristown, NJ, USA. Association for Computational Linguistics."},{"authors":[{"first":"Russell","middle":"D.","last":"Gray"},{"first":"Quentin","middle":"D.","last":"Atkinson"}],"year":"2003","title":"Language-tree divergence times support the Anatolian theory of Indo-European origin","source":"Russell D. Gray and Quentin D. Atkinson. 2003. Language-tree divergence times support the Anatolian theory of Indo-European origin. Nature, 426(6965):435–439, November."},{"authors":[{"first":"Mark","last":"Johnson"},{"first":"Thomas","last":"Griffiths"},{"first":"Sharon","last":"Goldwater"}],"year":"2007","title":"Bayesian inference for PCFGs via Markov chain Monte Carlo","source":"Mark Johnson, Thomas Griffiths, and Sharon Goldwater. 2007. Bayesian inference for PCFGs via Markov chain Monte Carlo. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 139–146, Rochester, New York, April. Association for Computational Linguistics."},{"authors":[{"first":"J.","middle":"F. C.","last":"Kingman"}],"year":"1982","title":"The coalescent","source":"J. F. C. Kingman. 1982. The coalescent. Stochastic Processes and their Applications, 13:235–248."},{"authors":[{"first":"Dan","last":"Klein"},{"first":"Christopher","middle":"D.","last":"Manning"}],"year":"2002","title":"A generative constituent-context model for improved grammar induction","source":"Dan Klein and Christopher D. Manning. 2002. A generative constituent-context model for improved grammar induction. In ACL ’02: Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 128–135, Morristown, NJ, USA. Association for Computational Linguistics."},{"authors":[{"first":"Dan","last":"Klein"},{"first":"Christopher","middle":"D.","last":"Manning"}],"year":"2004","title":"Corpus-based induction of syntactic structure: models of dependency and constituency","source":"Dan Klein and Christopher D. Manning. 2004. Corpus-based induction of syntactic structure: models of dependency and constituency. In ACL ’04: Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, page 478, Morristown, NJ, USA. Association for Computational Linguistics."},{"authors":[{"first":"Philipp","last":"Koehn"}],"year":"2005","title":"Europarl: A parallel corpus for statistical machine translation","source":"Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In Proceedings of the 10th Machine Translation Summit, pages 79–86."},{"authors":[{"first":"Kenichi","last":"Kurihara"},{"first":"Taisuke","last":"Sato"}],"year":"2004","title":"An applica-tion of the variational Bayesian approach to probabilistic context-free grammars","source":"Kenichi Kurihara and Taisuke Sato. 2004. An applica-tion of the variational Bayesian approach to probabilistic context-free grammars. In International Joint Conference on Natural Language Processing Workshop Beyond Shallow Analysis."},{"authors":[{"first":"K.","last":"Lari"},{"first":"S.","middle":"J.","last":"Young"}],"year":"1990","title":"The estimation of stochastic context-free grammars using the inside-outside algorithm","source":"K. Lari and S.J. Young. 1990. The estimation of stochastic context-free grammars using the inside-outside algorithm. Computer Speech and Language, 4:35–56."},{"authors":[{"first":"Percy","last":"Liang"},{"first":"Slav","last":"Petrov"},{"first":"Michael I","middle":".","last":"Jordan"},{"first":"Dan","last":"Klein"}],"year":"2007","title":"The infinite PCFG using hierarchical dirichlet processes","source":"Percy Liang, Slav Petrov, Michael I. Jordan, and Dan Klein. 2007. The infinite PCFG using hierarchical dirichlet processes. In EMNLP ’07: Proceedings of the Empirical Methods on Natural Language Processing, pages 688– 697."},{"authors":[{"first":"I.","middle":"Dan","last":"Melamed"}],"year":"2003","title":"Multitext grammars and synchronous parsers","source":"I. Dan Melamed. 2003. Multitext grammars and synchronous parsers. In NAACL ’03: Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, pages 79–86, Morristown, NJ, USA. Association for Computational Linguistics."},{"authors":[{"first":"Thomas","last":"Minka"}],"year":"2000","title":"Estimating a Dirichlet distribution","source":"Thomas Minka. 2000. Estimating a Dirichlet distribution. Technical report, M.I.T."},{"authors":[{"first":"Michael","middle":"P.","last":"Oakes"}],"year":"2000","title":"Computer estimation of vocabulary in a protolanguage from word lists in four daughter languages","source":"Michael P. Oakes. 2000. Computer estimation of vocabulary in a protolanguage from word lists in four daughter languages. Journal of Quantitative Linguistics, 7(3):233– 243."},{"authors":[{"first":"Steven","last":"Pinker"}],"year":"1994","title":"The Language Instinct: How the Mind Creates Language","source":"Steven Pinker. 1994. The Language Instinct: How the Mind Creates Language. HarperCollins, New York."},{"authors":[{"first":"Benjamin","last":"Snyder"},{"first":"Tahira","last":"Naseem"},{"first":"Regina","last":"Barzilay"}],"year":"2009","title":"Unsupervised multilingual grammar induction","source":"Benjamin Snyder, Tahira Naseem, and Regina Barzilay. 2009. Unsupervised multilingual grammar induction. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 73–81, Suntec, Singapore, August. Association for Computational Linguistics."},{"authors":[{"first":"Andreas","last":"Stolcke"},{"first":"Stephen","middle":"M.","last":"Omohundro"}],"year":"1994","title":"In-ducing probabilistic grammars by Bayesian model merg-ing","source":"Andreas Stolcke and Stephen M. Omohundro. 1994. In-ducing probabilistic grammars by Bayesian model merg-ing. In ICGI ’94: Proceedings of the Second International Colloquium on Grammatical Inference and Applications, pages 106–118, London, UK. Springer-Verlag."},{"authors":[{"first":"Dekai","last":"Wu"}],"year":"1997","title":"Stochastic inversion transduction grammars and bilingual parsing of parallel corpora","source":"Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Comput. Linguist., 23(3):377–403."},{"authors":[{"first":"Kai","last":"Yu"},{"first":"Volker","last":"Tresp"},{"first":"Anton","last":"Schwaighofer"}],"year":"2005","title":"Learning gaussian processes from multiple tasks","source":"Kai Yu, Volker Tresp, and Anton Schwaighofer. 2005. Learning gaussian processes from multiple tasks. In ICML ’05: Proceedings of the 22nd International Conference on Machine Learning, pages 1012–1019, New York, NY, USA. ACM. 188"}],"cites":[{"style":0,"text":"Pinker, 1994","origin":{"pointer":"/sections/2/paragraphs/0","offset":43,"length":12},"authors":[{"last":"Pinker"}],"year":"1994","references":["/references/20"]},{"style":0,"text":"Chomsky, 1965","origin":{"pointer":"/sections/2/paragraphs/0","offset":400,"length":13},"authors":[{"last":"Chomsky"}],"year":"1965","references":["/references/4"]},{"style":0,"text":"Oakes, 2000","origin":{"pointer":"/sections/2/paragraphs/1","offset":348,"length":11},"authors":[{"last":"Oakes"}],"year":"2000","references":["/references/19"]},{"style":0,"text":"Bouchard-Côté et al., 2008","origin":{"pointer":"/sections/2/paragraphs/1","offset":361,"length":26},"authors":[{"last":"Bouchard-Côté"},{"last":"al."}],"year":"2008","references":[]},{"style":0,"text":"Yu et al., 2005","origin":{"pointer":"/sections/2/paragraphs/2","offset":497,"length":15},"authors":[{"last":"Yu"},{"last":"al."}],"year":"2005","references":["/references/24"]},{"style":0,"text":"Daumé III, 2009","origin":{"pointer":"/sections/2/paragraphs/2","offset":514,"length":15},"authors":[{"last":"Daumé III"}],"year":"2009","references":["/references/6"]},{"style":0,"text":"Yu et al., 2005","origin":{"pointer":"/sections/2/paragraphs/2","offset":727,"length":15},"authors":[{"last":"Yu"},{"last":"al."}],"year":"2005","references":["/references/24"]},{"style":0,"text":"Carroll and Charniak, 1992","origin":{"pointer":"/sections/3/paragraphs/0","offset":70,"length":26},"authors":[{"last":"Carroll"},{"last":"Charniak"}],"year":"1992","references":["/references/2"]},{"style":0,"text":"Stolcke and Omohundro, 1994","origin":{"pointer":"/sections/3/paragraphs/0","offset":98,"length":27},"authors":[{"last":"Stolcke"},{"last":"Omohundro"}],"year":"1994","references":["/references/22"]},{"style":0,"text":"Klein and Manning, 2002","origin":{"pointer":"/sections/3/paragraphs/0","offset":127,"length":23},"authors":[{"last":"Klein"},{"last":"Manning"}],"year":"2002","references":["/references/11"]},{"style":0,"text":"Klein and Manning, 2004","origin":{"pointer":"/sections/3/paragraphs/0","offset":152,"length":23},"authors":[{"last":"Klein"},{"last":"Manning"}],"year":"2004","references":["/references/12"]},{"style":0,"text":"Liang et al., 2007","origin":{"pointer":"/sections/3/paragraphs/0","offset":177,"length":18},"authors":[{"last":"Liang"},{"last":"al."}],"year":"2007","references":["/references/16"]},{"style":0,"text":"Klein and Manning, 2002","origin":{"pointer":"/sections/3/paragraphs/0","offset":286,"length":23},"authors":[{"last":"Klein"},{"last":"Manning"}],"year":"2002","references":["/references/11"]},{"style":0,"text":"Klein and Manning, 2004","origin":{"pointer":"/sections/3/paragraphs/0","offset":311,"length":23},"authors":[{"last":"Klein"},{"last":"Manning"}],"year":"2004","references":["/references/12"]},{"style":0,"text":"Wu, 1997","origin":{"pointer":"/sections/3/paragraphs/1","offset":108,"length":8},"authors":[{"last":"Wu"}],"year":"1997","references":["/references/23"]},{"style":0,"text":"Melamed, 2003","origin":{"pointer":"/sections/3/paragraphs/1","offset":118,"length":13},"authors":[{"last":"Melamed"}],"year":"2003","references":["/references/17"]},{"style":0,"text":"Eisner, 2003","origin":{"pointer":"/sections/3/paragraphs/1","offset":133,"length":12},"authors":[{"last":"Eisner"}],"year":"2003","references":["/references/7"]},{"style":0,"text":"Chiang, 2005","origin":{"pointer":"/sections/3/paragraphs/1","offset":147,"length":12},"authors":[{"last":"Chiang"}],"year":"2005","references":["/references/3"]},{"style":0,"text":"Blunsom et al., 2009","origin":{"pointer":"/sections/3/paragraphs/1","offset":161,"length":20},"authors":[{"last":"Blunsom"},{"last":"al."}],"year":"2009","references":["/references/0"]},{"style":0,"text":"Snyder et al., 2009","origin":{"pointer":"/sections/3/paragraphs/1","offset":183,"length":19},"authors":[{"last":"Snyder"},{"last":"al."}],"year":"2009","references":["/references/21"]},{"style":0,"text":"Cohen and Smith, 2009","origin":{"pointer":"/sections/3/paragraphs/1","offset":684,"length":21},"authors":[{"last":"Cohen"},{"last":"Smith"}],"year":"2009","references":["/references/5"]},{"style":0,"text":"Kurihara and Sato (2004)","origin":{"pointer":"/sections/4/paragraphs/32","offset":418,"length":24},"authors":[{"last":"Kurihara"},{"last":"Sato"}],"year":"2004","references":["/references/14"]},{"style":0,"text":"Minka, 2000","origin":{"pointer":"/sections/4/paragraphs/44","offset":165,"length":11},"authors":[{"last":"Minka"}],"year":"2000","references":["/references/18"]},{"style":0,"text":"Kurihara and Sato, 2004","origin":{"pointer":"/sections/4/paragraphs/69","offset":267,"length":23},"authors":[{"last":"Kurihara"},{"last":"Sato"}],"year":"2004","references":["/references/14"]},{"style":0,"text":"Liang et al., 2007","origin":{"pointer":"/sections/4/paragraphs/69","offset":292,"length":18},"authors":[{"last":"Liang"},{"last":"al."}],"year":"2007","references":["/references/16"]},{"style":0,"text":"Lari and Young, 1990","origin":{"pointer":"/sections/4/paragraphs/69","offset":494,"length":20},"authors":[{"last":"Lari"},{"last":"Young"}],"year":"1990","references":["/references/15"]},{"style":0,"text":"Koehn, 2005","origin":{"pointer":"/sections/5/paragraphs/0","offset":58,"length":11},"authors":[{"last":"Koehn"}],"year":"2005","references":["/references/13"]},{"style":0,"text":"Liang et al., 2007","origin":{"pointer":"/sections/6/paragraphs/0","offset":560,"length":18},"authors":[{"last":"Liang"},{"last":"al."}],"year":"2007","references":["/references/16"]},{"style":0,"text":"Johnson et al., 2007","origin":{"pointer":"/sections/6/paragraphs/0","offset":658,"length":20},"authors":[{"last":"Johnson"},{"last":"al."}],"year":"2007","references":["/references/9"]},{"style":0,"text":"Kingman, 1982","origin":{"pointer":"/sections/6/paragraphs/0","offset":870,"length":13},"authors":[{"last":"Kingman"}],"year":"1982","references":["/references/10"]},{"style":0,"text":"Gray and Atkinson, 2003","origin":{"pointer":"/sections/6/paragraphs/0","offset":1044,"length":23},"authors":[{"last":"Gray"},{"last":"Atkinson"}],"year":"2003","references":["/references/8"]},{"style":0,"text":"Chomsky, 1965","origin":{"pointer":"/sections/6/paragraphs/0","offset":1159,"length":13},"authors":[{"last":"Chomsky"}],"year":"1965","references":["/references/4"]}]}
