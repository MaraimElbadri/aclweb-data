{"sections":[{"title":"","paragraphs":["Proceedings of the ACL 2010 Conference Short Papers, pages 269–274, Uppsala, Sweden, 11-16 July 2010. c⃝2010 Association for Computational Linguistics"]},{"title":"Hierarchical Sequential Learning for Extracting Opinions and their Attributes Yejin Choi and Claire Cardie Department of Computer Science Cornell University Ithaca, NY 14853 {ychoi,cardie}@cs.cornell.edu Abstract","paragraphs":["Automatic opinion recognition involves a number of related tasks, such as identifying the boundaries of opinion expression, determining their polarity, and determining their intensity. Although much progress has been made in this area, existing research typically treats each of the above tasks in isolation. In this paper, we apply a hierarchical parameter sharing technique using Conditional Random Fields for fine-grained opinion analysis, jointly detecting the boundaries of opinion expressions as well as determining two of their key attributes — polarity and intensity. Our experimental results show that our proposed approach improves the performance over a baseline that does not exploit hierarchical structure among the classes. In addition, we find that the joint approach outperforms a baseline that is based on cascading two separate components."]},{"title":"1 Introduction","paragraphs":["Automatic opinion recognition involves a number of related tasks, such as identifying expressions of opinion (e.g. Kim and Hovy (2005), Popescu and Etzioni (2005), Breck et al. (2007)), determining their polarity (e.g. Hu and Liu (2004), Kim and Hovy (2004), Wilson et al. (2005)), and determining their strength, or intensity (e.g. Popescu and Etzioni (2005), Wilson et al. (2006)). Most previous work treats each subtask in isolation: opinion expression extraction (i.e. detecting the boundaries of opinion expressions) and opinion attribute classification (e.g. determining values for polarity and intensity) are tackled as separate steps in opinion recognition systems. Unfortunately, errors from individual components will propagate in systems with cascaded component architectures, causing performance degradation in the end-to-end system (e.g. Finkel et al. (2006)) — in our case, in the end-to-end opinion recognition system.","In this paper, we apply a hierarchical parameter sharing technique (e.g., Cai and Hofmann (2004), Zhao et al. (2008)) using Conditional Random Fields (CRFs) (Lafferty et al., 2001) to fine-grained opinion analysis. In particular, we aim to jointly identify the boundaries of opinion expressions as well as to determine two of their key attributes — polarity and intensity.","Experimental results show that our proposed approach improves the performance over the baseline that does not exploit the hierarchical structure among the classes. In addition, we find that the joint approach outperforms a baseline that is based on cascading two separate systems."]},{"title":"2 Hierarchical Sequential Learning","paragraphs":["We define the problem of joint extraction of opinion expressions and their attributes as a sequence tagging task as follows. Given a sequence of to-kens, x = x1 ... xn, we predict a sequence of labels, y = y1 ... yn, where yi ∈ {0, ..., 9} are defined as conjunctive values of polarity labels and intensity labels, as shown in Table 1. Then the conditional probability p(y|x) for linear-chain CRFs is given as (Lafferty et al., 2001) P (y|x) = 1 Zx exp ∑ i (","λ f(yi, x, i)+λ′ f′","(y i−1, yi, x, i) ) where Zx is the normalization factor.","In order to apply a hierarchical parameter sharing technique (e.g., Cai and Hofmann (2004), Zhao et al. (2008)), we extend parameters as follows. 269    ","","","     ","    ","","","        ","","","         ","","    ","","    ","","    ","   ","","    ","    ","","     ","","    ","","     ","   ","","     ","","","","","","","","","","","    ","","","","","","","","","","","","","","","","","","","","","","","","","","","","       ","",""," Figure 1: The hierarchical structure of classes for opinion expressions with polarity (positive, neutral, negative) and intensity (high, medium, low)","LABEL 0 1 2 3 4 5 6 7 8 9 POLARITY none positive positive positive neutral neutral neutral negative negative negative INTENSITY none high medium low high medium low high medium low Table 1: Labels for Opinion Extraction with Polarity and Intensity","λ f(yi, x, i) = λα gO(α, x, i) (1) + λβ gP(β, x, i) + λγ gS(γ, x, i)","λ′ f′","(y","i−1, yi, x, i) = λ′ α,α̂ g′","O(α, α̂, x, i)","+ λ′ β,ˆ β g′","P(β, β̂, x, i)","+ λ′ γ,γ̂ g′","S(γ, γ̂, x, i) where gO and g′","O are feature vectors defined for Opinion extraction, gP and g′","P are feature vectors defined for Polarity extraction, and gS and g′","S are feature vectors defined for Strength extraction, and α, α̂ ∈ {OPINION, NO-OPINION} β, β̂ ∈ {POSITIVE, NEGATIVE, NEUTRAL, NO-POLARITY} γ, γ̂ ∈ {HIGH, MEDIUM, LOW, NO-INTENSITY} For instance, if yi = 1, then","λ f(1, x, i) = λOPINION gO(OPINION, x, i) + λPOSITIVE gP(POSITVE, x, i) + λHIGH gS(HIGH, x, i) If yi−1 = 0, yi = 4, then","λ′ f′ (0, 4, x, i)","= λ′ NO-OPINION,OPINION g′","O(NO-OPINION, OPINION, x, i)","+ λ′ NO-POLARITY, NEUTRAL g′","P(NO-POLARITY, NEUTRAL, x, i)","+ λ′ NO-INTENSITY, HIGH g′","S(NO-INTENSITY, HIGH, x, i) This hierarchical construction of feature and weight vectors allows similar labels to share the same subcomponents of feature and weight vectors. For instance, all λ f (yi, x, i) such that yi ∈ {1, 2, 3} will share the same component λPOSITIVE gP(POSITVE, x, i). Note that there can be other variations of hierarchical construction. For instance, one can add λδ gI(δ, x, i) and λ′","δ,δ̂ g′","I (δ, δ̂, x, i) to Equation (1) for δ ∈ {0, 1, ..., 9}, in order to allow more individualized learning for each label.","Notice also that the number of sets of parameters constructed by Equation (1) is significantly smaller than the number of sets of parameters that are needed without the hierarchy. The former requires (2 + 4 + 4) + (2 × 2 + 4 × 4 + 4 × 4) = 46 sets of parameters, but the latter requires (10) + (10 × 10) = 110 sets of parameters. Because a combination of a polarity component and an intensity component can distinguish each label, it is not necessary to define a separate set of parameters for each label."]},{"title":"3 Features","paragraphs":["We first introduce definitions of key terms that will","be used to describe features.","• PRIOR-POLARITY & PRIOR-INTENSITY: We obtain these prior-attributes from the polarity lexicon populated by Wilson et al. (2005).","• EXP-POLARITY, EXP-INTENSITY & EXP-SPAN: Words in a given opinion expression often do not share the same prior-attributes. Such discontinuous distribution of features can make it harder to learn the desired opinion expression boundaries. Therefore, we try to obtain expression-level attributes (EXP-POLARITY and EXP-INTENSITY) using simple heuristics. In order to derive EXP-POLARITY, we perform simple 270 voting. If there is a word with a negation effect, such as “never”, “not”, “hardly”, “against”, then we flip the polarity. For EXP-INTENSITY, we use the highest PRIOR-INTENSITY in the span. The text span with the same expression-level attributes are referred to as EXP-SPAN. 3.1 Per-Token Features Per-token features are defined in the form of gO(α, x, i), gP (β, x, i) and gS(γ, x, i). The do-mains of α, β, γ are as given in Section 3. Common Per-Token Features Following features are common for all class labels. The notation ⊗ indicates conjunctive operation of two values. • PART-OF-SPEECH(xi):","based on GATE (Cunningham et al., 2002). • WORD(xi), WORD(xi−1), WORD(xi+1) • WORDNET-HYPERNYM(xi):","based on WordNet (Miller, 1995). • OPINION-LEXICON(xi):","based on opinion lexicon (Wiebe et al., 2002). • SHALLOW-PARSER(xi):","based on CASS partial parser (Abney, 1996). • PRIOR-POLARITY(xi) ⊗ PRIOR-INTENSITY(xi) • EXP-POLARITY(xi) ⊗ EXP-INTENSITY(xi) • EXP-POLARITY(xi) ⊗ EXP-INTENSITY(xi) ⊗","STEM(xi) • EXP-SPAN(xi):","boolean to indicate whether xi is in an EXP-SPAN. • DISTANCE-TO-EXP-SPAN(xi): 0, 1, 2, 3+. • EXP-POLARITY(xi) ⊗ EXP-INTENSITY(xi) ⊗","EXP-SPAN(xi) Polarity Per-Token Features These features are included only for gO(α, x, i) and gP(β, x, i), which are the feature functions corresponding to the polarity-based classes. • PRIOR-POLARITY(xi), EXP-POLARITY((xi) • STEM(xi) ⊗ EXP-POLARITY(xi) • COUNT-OF-P olarity:","where P olarity ∈ {positive, neutral, negative}.","This feature encodes the number of positive,","neutral, and negative EXP-POLARITY words re-","spectively, in the current sentence. • STEM(xi) ⊗ COUNT-OF-P olarity • EXP-POLARITY(xi) ⊗ COUNT-OF-P olarity • EXP-SPAN(xi) and EXP-POLARITY(xi) • DISTANCE-TO-EXP-SPAN(xi) ⊗ EXP-POLARITY(xp) Intensity Per-Token Features These features are included only for gO(α, x, i) and gS(γ, x, i), which are the feature functions corresponding to the intensity-based classes. • PRIOR-INTENSITY(xi), EXP-INTENSITY(xi) • STEM(xi) ⊗ EXP-INTENSITY(xi) • COUNT-OF-STRONG, COUNT-OF-WEAK:","the number of strong and weak EXP-INTENSITY","words in the current sentence. • INTENSIFIER(xi): whether xi is an intensifier,","such as “extremely”, “highly”, “really”. • STRONGMODAL(xi): whether xi is a strong modal","verb, such as “must”, “can”, “will”. • WEAKMODAL(xi): whether xi is a weak modal","verb, such as “may”, “could”, “would”. • DIMINISHER(xi): whether xi is a diminisher, such","as “little”, “somewhat”, “less”. • PRECEDED-BY-τ (xi),","PRECEDED-BY-τ (xi) ⊗ EXP-INTENSITY(xi):","where τ ∈ { INTENSIFIER, STRONGMODAL, WEAK-","MODAL, DIMINISHER} • τ (xi) ⊗ EXP-INTENSITY(xi),","τ (xi) ⊗ EXP-INTENSITY(xi−1),","τ (xi−1) ⊗ EXP-INTENSITY(xi+1) • EXP-SPAN(xi) ⊗ EXP-INTENSITY(xi) • DISTANCE-TO-EXP-SPAN(xi) ⊗ EXP-INTENSITY(xp) 3.2 Transition Features Transition features are employed to help with boundary extraction as follows: Polarity Transition Features Polarity transition features are features that are used only for g′","O(α, α̂, x, i) and g′","P(β, β̂, x, i). • PART-OF-SPEECH(xi) ⊗ PART-OF-SPEECH(xi+1) ⊗","EXP-POLARITY(xi) • EXP-POLARITY(xi) ⊗ EXP-POLARITY(xi+1) Intensity Transition Features Intensity transition features are features that are used only for g′","O(α, α̂, x, i) and g′","S(γ, γ̂, x, i). • PART-OF-SPEECH(xi) ⊗ PART-OF-SPEECH(xi+1) ⊗","EXP-INTENSITY(xi) • EXP-INTENSITY(xi) ⊗ EXP-INTENSITY(xi+1)"]},{"title":"4 Evaluation","paragraphs":["We evaluate our system using the Multi-Perspective Question Answering (MPQA) corpus1",". Our gold standard opinion expressions cor-","1","The MPQA corpus can be obtained at http://nrrc.mitre.org/NRRC/publications.htm. 271","Positive Neutral Negative Method Description r(%) p(%) f(%) r(%) p(%) f(%) r(%) p(%) f(%) Polarity-Only ∩ Intensity-Only (BASELINE1) 29.6 65.7 40.8 26.5 69.1 38.3 35.5 77.0 48.6 Joint without Hierarchy (BASELINE2) 30.7 65.7 41.9 29.9 66.5 41.2 37.3 77.1 50.3 Joint with Hierarchy 31.8 67.1 43.1 31.9 66.6 43.1 40.4 76.2 52.8 Table 2: Performance of Opinion Extraction with Correct Polarity Attribute","High Medium Low Method Description r(%) p(%) f(%) r(%) p(%) f(%) r(%) p(%) f(%) Polarity-Only ∩ Intensity-Only (BASELINE1) 26.4 58.3 36.3 29.7 59.0 39.6 15.4 60.3 24.5 Joint without Hierarchy (BASELINE2) 29.7 54.2 38.4 28.0 57.4 37.6 18.8 55.0 28.0 Joint with Hierarchy 27.1 55.2 36.3 32.0 56.5 40.9 21.1 56.3 30.7 Table 3: Performance of Opinion Extraction with Correct Intensity Attribute Method Description r(%) p(%) f(%) Polar-Only ∩ Intensity-Only 43.3 92.0 58.9 Joint without Hierarchy 46.0 88.4 60.5 Joint with Hierarchy 48.0 87.8 62.0 Table 4: Performance of Opinion Extraction respond to direct subjective expression and expressive subjective element (Wiebe et al., 2005).2","Our implementation of hierarchical sequential learning is based on the Mallet (McCallum, 2002) code for CRFs. In all experiments, we use a Gaussian prior of 1.0 for regularization. We use 135 documents for development, and test on a different set of 400 documents using 10-fold crossvalidation. We investigate three options for jointly extracting opinion expressions with their attributes as follows: [Baseline-1] Polarity-Only ∩ Intensity-Only: For this baseline, we train two separate sequence tagging CRFs: one that extracts opinion expressions only with the polarity attribute (using common features and polarity extraction features in Section 3), and another that extracts opinion expressions only with the intensity attribute (using common features and intensity extraction features in Section 3). We then combine the results from two separate CRFs by collecting all opinion entities extracted by both sequence taggers.3","This","2","Only 1.5% of the polarity annotations correspond to both; hence, we merge both into the neutral. Similarly, for gold standard intensity, we merge extremely high into high.","3","We collect all entities whose portions of text spans are extracted by both models. baseline effectively represents a cascaded component approach. [Baseline-2] Joint without Hierarchy: Here we use simple linear-chain CRFs without exploiting the class hierarchy for the opinion recognition task. We use the tags shown in Table 1. Joint with Hierarchy: Finally, we test the hierarchical sequential learning approach elaborated in Section 3. 4.1 Evaluation Results We evaluate all experiments at the opinion entity level, i.e. at the level of each opinion expression rather than at the token level. We use three evaluation metrics: recall, precision, and F-measure with equally weighted recall and precision.","Table 4 shows the performance of opinion extraction without matching any attribute. That is, an extracted opinion entity is counted as correct if it overlaps4","with a gold standard opinion expression, without checking the correctness of its attributes. Table 2 and 3 show the performance of opinion extraction with the correct polarity and intensity respectively.","From all of these evaluation criteria, JOINT WITH","4","Overlap matching is a reasonable choice as the annotator agreement study is also based on overlap matching (Wiebe et al., 2005). One might wonder whether the overlap matching scheme could allow a degenerative case where extracting the entire test dataset as one giant opinion expression would yield 100% recall and precision. Because each sentence corresponds to a different test instance in our model, and because some sentences do not contain any opinion expression in the dataset, such degenerative case is not possible in our experiments. 272 HIERARCHY performs the best, and the least effective one is BASELINE-1, which cascades two separately trained models. It is interesting that the simple sequential tagging approach even without exploiting the hierarchy (BASELINE-2) performs better than the cascaded approach (BASELINE-1).","When evaluating with respect to the polarity attribute, the performance of the negative class is substantially higher than the that of other classes. This is not surprising as there is approximately twice as much data for the negative class. When evaluating with respect to the intensity attribute, the performance of the LOW class is substantially lower than that of other classes. This result reflects the fact that it is inherently harder to distinguish an opinion expression with low intensity from no opinion. In general, we observe that determining correct intensity attributes is a much harder task than determining correct polarity attributes.","In order to have a sense of upper bound, we also report the individual performance of two separately trained models used for BASELINE-1: for the Polarity-Only model that extracts opinion boundaries only with polarity attribute, the F-scores with respect to the positive, neutral, negative classes are 46.7, 47.5, 57.0, respectively. For the Intensity-Only model, the F-scores with respect to the high, medium, low classes are 37.1, 40.8, 26.6, respectively. Remind that neither of these models alone fully solve the joint task of extracting boundaries as well as determining two attributions simultaneously. As a result, when conjoining the results from the two models (BASELINE-1), the final performance drops substantially.","We conclude from our experiments that the simple joint sequential tagging approach even without exploiting the hierarchy brings a better performance than combining two separately developed systems. In addition, our hierarchical joint sequential learning approach brings a further performance gain over the simple joint sequential tagging method."]},{"title":"5 Related Work","paragraphs":["Although there have been much research for fine-grained opinion analysis (e.g., Hu and Liu (2004), Wilson et al. (2005), Wilson et al. (2006), Choi and Claire (2008), Wilson et al. (2009)),5","none is 5","For instance, the results of Wilson et al. (2005) is not comparable even for our Polarity-Only model used inside BASELINE-1, because Wilson et al. (2005) does not operate directly comparable to our results; much of previous work studies only a subset of what we tackle in this paper. However, as shown in Section 4.1, when we train the learning models only for a subset of the tasks, we can achieve a better performance instantly by making the problem simpler. Our work differs from most of previous work in that we investigate how solving multiple related tasks affects performance on sub-tasks.","The hierarchical parameter sharing technique used in this paper has been previously used by Zhao et al. (2008) for opinion analysis. However, Zhao et al. (2008) employs this technique only to classify sentence-level attributes (polarity and intensity), without involving a much harder task of detecting boundaries of sub-sentential entities."]},{"title":"6 Conclusion","paragraphs":["We applied a hierarchical parameter sharing technique using Conditional Random Fields for fine-grained opinion analysis. Our proposed approach jointly extract opinion expressions from unstructured text and determine their attributes — polarity and intensity. Empirical results indicate that the simple joint sequential tagging approach even without exploiting the hierarchy brings a better performance than combining two separately developed systems. In addition, we found that the hierarchical joint sequential learning approach improves the performance over the simple joint sequential tagging method."]},{"title":"Acknowledgments","paragraphs":["This work was supported in part by National Science Foundation Grants BCS-0904822, BCS-0624277, IIS-0535099 and by the Department of Homeland Security under ONR Grant N0014-07-1-0152. We thank the reviewers and Ainur Yessenalina for many helpful comments."]},{"title":"References","paragraphs":["S. Abney. 1996. Partial parsing via finite-state cascades. In Journal of Natural Language Engineering, 2(4).","E. Breck, Y. Choi and C. Cardie. 2007. Identifying Expressions of Opinion in Context. In IJCAI. on the entire corpus as unstructured input. Instead, Wilson et al. (2005) evaluate only on known words that are in their opinion lexicon. Furthermore, Wilson et al. (2005) simplifies the problem by combining neutral opinions and no opinions into the same class, while our system distinguishes the two. 273","L. Cai and T. Hofmann. 2004. Hierarchical document categorization with support vector machines. In CIKM.","Y. Choi and C. Cardie. 2008. Learning with Compositional Semantics as Structural Inference for Subsentential Sentiment Analysis. In EMNLP.","H. Cunningham, D. Maynard, K. Bontcheva and V. Tablan. 2002. GATE: A Framework and Graphical Development Environment for Robust NLP Tools and Applications. In ACL.","J. R. Finkel, C. D. Manning and A. Y. Ng. 2006. Solving the Problem of Cascading Errors: Approximate Bayesian Inference for Linguistic Annotation Pipelines. In EMNLP.","M. Hu and B. Liu. 2004. Mining and Summarizing Customer Reviews. In KDD.","S. Kim and E. Hovy. 2004. Determining the sentiment of opinions. In COLING.","S. Kim and E. Hovy. 2005. Automatic Detection of Opinion Bearing Words and Sentences. In Companion Volume to the Proceedings of the Second In-ternational Joint Conference on Natural Language Processing (IJCNLP-05).","J. Lafferty, A. McCallum and F. Pereira. 2001. Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. In ICML.","A. McCallum. 2002. MALLET: A Machine Learning for Language Toolkit. http://mallet.cs.umass.edu.","G. A. Miller. 1995. WordNet: a lexical database for English. In Communications of the ACM, 38(11).","Ana-Maria Popescu and O. Etzioni. 2005. Extracting Product Features and Opinions from Reviews. In HLT-EMNLP.","J. Wiebe, E. Breck, C. Buckley, C. Cardie, P. Davis, B. Fraser, D. Litman, D. Pierce, E. Riloff and T. Wilson. 2002. Summer Workshop on Multiple-Perspective Question Answering: Final Report. In NRRC.","J. Wiebe and T. Wilson and C. Cardie 2005. Annotat-ing Expressions of Opinions and Emotions in Language. In Language Resources and Evaluation, volume 39, issue 2-3.","T. Wilson, J. Wiebe and P. Hoffmann. 2005. Recognizing Contextual Polarity in Phrase-Level Sentiment Analysis. In HLT-EMNLP.","T. Wilson, J. Wiebe and R. Hwa. 2006. Recognizing strong and weak opinion clauses. In Computational Intelligence. 22 (2): 73-99.","T. Wilson, J. Wiebe and P. Hoffmann. 2009. Recognizing Contextual Polarity: an exploration of features for phrase-level sentiment analysis. Computational Linguistics 35(3).","J. Zhao, K. Liu and G. Wang. 2008. Adding Redundant Features for CRFs-based Sentence Sentiment Classification. In EMNLP. 274"]}],"references":[{"authors":[{"first":"S.","last":"Abney"}],"year":"1996","title":"Partial parsing via finite-state cascades","source":"S. Abney. 1996. Partial parsing via finite-state cascades. In Journal of Natural Language Engineering, 2(4)."},{"authors":[{"first":"E.","last":"Breck"},{"first":"Y.","last":"Choi"},{"first":"C.","last":"Cardie"}],"year":"2007","title":"Identifying Expressions of Opinion in Context","source":"E. Breck, Y. Choi and C. Cardie. 2007. Identifying Expressions of Opinion in Context. In IJCAI. on the entire corpus as unstructured input. Instead, Wilson et al. (2005) evaluate only on known words that are in their opinion lexicon. Furthermore, Wilson et al. (2005) simplifies the problem by combining neutral opinions and no opinions into the same class, while our system distinguishes the two. 273"},{"authors":[{"first":"L.","last":"Cai"},{"first":"T.","last":"Hofmann"}],"year":"2004","title":"Hierarchical document categorization with support vector machines","source":"L. Cai and T. Hofmann. 2004. Hierarchical document categorization with support vector machines. In CIKM."},{"authors":[{"first":"Y.","last":"Choi"},{"first":"C.","last":"Cardie"}],"year":"2008","title":"Learning with Compositional Semantics as Structural Inference for Subsentential Sentiment Analysis","source":"Y. Choi and C. Cardie. 2008. Learning with Compositional Semantics as Structural Inference for Subsentential Sentiment Analysis. In EMNLP."},{"authors":[{"first":"H.","last":"Cunningham"},{"first":"D.","last":"Maynard"},{"first":"K.","last":"Bontcheva"},{"first":"V.","last":"Tablan"}],"year":"2002","title":"GATE: A Framework and Graphical Development Environment for Robust NLP Tools and Applications","source":"H. Cunningham, D. Maynard, K. Bontcheva and V. Tablan. 2002. GATE: A Framework and Graphical Development Environment for Robust NLP Tools and Applications. In ACL."},{"authors":[{"first":"J.","middle":"R.","last":"Finkel"},{"first":"C.","middle":"D.","last":"Manning"},{"first":"A.","middle":"Y.","last":"Ng"}],"year":"2006","title":"Solving the Problem of Cascading Errors: Approximate Bayesian Inference for Linguistic Annotation Pipelines","source":"J. R. Finkel, C. D. Manning and A. Y. Ng. 2006. Solving the Problem of Cascading Errors: Approximate Bayesian Inference for Linguistic Annotation Pipelines. In EMNLP."},{"authors":[{"first":"M.","last":"Hu"},{"first":"B.","last":"Liu"}],"year":"2004","title":"Mining and Summarizing Customer Reviews","source":"M. Hu and B. Liu. 2004. Mining and Summarizing Customer Reviews. In KDD."},{"authors":[{"first":"S.","last":"Kim"},{"first":"E.","last":"Hovy"}],"year":"2004","title":"Determining the sentiment of opinions","source":"S. Kim and E. Hovy. 2004. Determining the sentiment of opinions. In COLING."},{"authors":[{"first":"S.","last":"Kim"},{"first":"E.","last":"Hovy"}],"year":"2005","title":"Automatic Detection of Opinion Bearing Words and Sentences","source":"S. Kim and E. Hovy. 2005. Automatic Detection of Opinion Bearing Words and Sentences. In Companion Volume to the Proceedings of the Second In-ternational Joint Conference on Natural Language Processing (IJCNLP-05)."},{"authors":[{"first":"J.","last":"Lafferty"},{"first":"A.","last":"McCallum"},{"first":"F.","last":"Pereira"}],"year":"2001","title":"Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data","source":"J. Lafferty, A. McCallum and F. Pereira. 2001. Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. In ICML."},{"authors":[{"first":"A.","last":"McCallum"}],"year":"2002","title":"MALLET: A Machine Learning for Language Toolkit","source":"A. McCallum. 2002. MALLET: A Machine Learning for Language Toolkit. http://mallet.cs.umass.edu."},{"authors":[{"first":"G.","middle":"A.","last":"Miller"}],"year":"1995","title":"WordNet: a lexical database for English","source":"G. A. Miller. 1995. WordNet: a lexical database for English. In Communications of the ACM, 38(11)."},{"authors":[{"first":"Ana-Maria","last":"Popescu"},{"first":"O.","last":"Etzioni"}],"year":"2005","title":"Extracting Product Features and Opinions from Reviews","source":"Ana-Maria Popescu and O. Etzioni. 2005. Extracting Product Features and Opinions from Reviews. In HLT-EMNLP."},{"authors":[{"first":"J.","last":"Wiebe"},{"first":"E.","last":"Breck"},{"first":"C.","last":"Buckley"},{"first":"C.","last":"Cardie"},{"first":"P.","last":"Davis"},{"first":"B.","last":"Fraser"},{"first":"D.","last":"Litman"},{"first":"D.","last":"Pierce"},{"first":"E.","last":"Riloff"},{"first":"T.","last":"Wilson"}],"year":"2002","title":"Summer Workshop on Multiple-Perspective Question Answering: Final Report","source":"J. Wiebe, E. Breck, C. Buckley, C. Cardie, P. Davis, B. Fraser, D. Litman, D. Pierce, E. Riloff and T. Wilson. 2002. Summer Workshop on Multiple-Perspective Question Answering: Final Report. In NRRC."},{"authors":[{"first":"J.","last":"Wiebe"},{"first":"T.","last":"Wilson"},{"first":"C.","last":"Cardie"}],"year":"2005","title":"Annotat-ing Expressions of Opinions and Emotions in Language","source":"J. Wiebe and T. Wilson and C. Cardie 2005. Annotat-ing Expressions of Opinions and Emotions in Language. In Language Resources and Evaluation, volume 39, issue 2-3."},{"authors":[{"first":"T.","last":"Wilson"},{"first":"J.","last":"Wiebe"},{"first":"P.","last":"Hoffmann"}],"year":"2005","title":"Recognizing Contextual Polarity in Phrase-Level Sentiment Analysis","source":"T. Wilson, J. Wiebe and P. Hoffmann. 2005. Recognizing Contextual Polarity in Phrase-Level Sentiment Analysis. In HLT-EMNLP."},{"authors":[{"first":"T.","last":"Wilson"},{"first":"J.","last":"Wiebe"},{"first":"R.","last":"Hwa"}],"year":"2006","title":"Recognizing strong and weak opinion clauses","source":"T. Wilson, J. Wiebe and R. Hwa. 2006. Recognizing strong and weak opinion clauses. In Computational Intelligence. 22 (2): 73-99."},{"authors":[{"first":"T.","last":"Wilson"},{"first":"J.","last":"Wiebe"},{"first":"P.","last":"Hoffmann"}],"year":"2009","title":"Recognizing Contextual Polarity: an exploration of features for phrase-level sentiment analysis","source":"T. Wilson, J. Wiebe and P. Hoffmann. 2009. Recognizing Contextual Polarity: an exploration of features for phrase-level sentiment analysis. Computational Linguistics 35(3)."},{"authors":[{"first":"J.","last":"Zhao"},{"first":"K.","last":"Liu"},{"first":"G.","last":"Wang"}],"year":"2008","title":"Adding Redundant Features for CRFs-based Sentence Sentiment Classification","source":"J. Zhao, K. Liu and G. Wang. 2008. Adding Redundant Features for CRFs-based Sentence Sentiment Classification. In EMNLP. 274"}],"cites":[{"style":0,"text":"Kim and Hovy (2005)","origin":{"pointer":"/sections/2/paragraphs/0","offset":115,"length":19},"authors":[{"last":"Kim"},{"last":"Hovy"}],"year":"2005","references":["/references/8"]},{"style":0,"text":"Popescu and Etzioni (2005)","origin":{"pointer":"/sections/2/paragraphs/0","offset":136,"length":26},"authors":[{"last":"Popescu"},{"last":"Etzioni"}],"year":"2005","references":["/references/12"]},{"style":0,"text":"Breck et al. (2007)","origin":{"pointer":"/sections/2/paragraphs/0","offset":164,"length":19},"authors":[{"last":"Breck"},{"last":"al."}],"year":"2007","references":["/references/1"]},{"style":0,"text":"Hu and Liu (2004)","origin":{"pointer":"/sections/2/paragraphs/0","offset":219,"length":17},"authors":[{"last":"Hu"},{"last":"Liu"}],"year":"2004","references":["/references/6"]},{"style":0,"text":"Kim and Hovy (2004)","origin":{"pointer":"/sections/2/paragraphs/0","offset":238,"length":19},"authors":[{"last":"Kim"},{"last":"Hovy"}],"year":"2004","references":["/references/7"]},{"style":0,"text":"Wilson et al. (2005)","origin":{"pointer":"/sections/2/paragraphs/0","offset":259,"length":20},"authors":[{"last":"Wilson"},{"last":"al."}],"year":"2005","references":["/references/15"]},{"style":0,"text":"Popescu and Etzioni (2005)","origin":{"pointer":"/sections/2/paragraphs/0","offset":333,"length":26},"authors":[{"last":"Popescu"},{"last":"Etzioni"}],"year":"2005","references":["/references/12"]},{"style":0,"text":"Wilson et al. (2006)","origin":{"pointer":"/sections/2/paragraphs/0","offset":361,"length":20},"authors":[{"last":"Wilson"},{"last":"al."}],"year":"2006","references":["/references/16"]},{"style":0,"text":"Finkel et al. (2006)","origin":{"pointer":"/sections/2/paragraphs/0","offset":851,"length":20},"authors":[{"last":"Finkel"},{"last":"al."}],"year":"2006","references":["/references/5"]},{"style":0,"text":"Cai and Hofmann (2004)","origin":{"pointer":"/sections/2/paragraphs/1","offset":74,"length":22},"authors":[{"last":"Cai"},{"last":"Hofmann"}],"year":"2004","references":["/references/2"]},{"style":0,"text":"Zhao et al. (2008)","origin":{"pointer":"/sections/2/paragraphs/1","offset":98,"length":18},"authors":[{"last":"Zhao"},{"last":"al."}],"year":"2008","references":["/references/18"]},{"style":0,"text":"Lafferty et al., 2001","origin":{"pointer":"/sections/2/paragraphs/1","offset":158,"length":21},"authors":[{"last":"Lafferty"},{"last":"al."}],"year":"2001","references":["/references/9"]},{"style":0,"text":"Lafferty et al., 2001","origin":{"pointer":"/sections/3/paragraphs/0","offset":411,"length":21},"authors":[{"last":"Lafferty"},{"last":"al."}],"year":"2001","references":["/references/9"]},{"style":0,"text":"Cai and Hofmann (2004)","origin":{"pointer":"/sections/3/paragraphs/3","offset":68,"length":22},"authors":[{"last":"Cai"},{"last":"Hofmann"}],"year":"2004","references":["/references/2"]},{"style":0,"text":"Zhao et al. (2008)","origin":{"pointer":"/sections/3/paragraphs/3","offset":92,"length":18},"authors":[{"last":"Zhao"},{"last":"al."}],"year":"2008","references":["/references/18"]},{"style":0,"text":"Wilson et al. (2005)","origin":{"pointer":"/sections/4/paragraphs/2","offset":108,"length":20},"authors":[{"last":"Wilson"},{"last":"al."}],"year":"2005","references":["/references/15"]},{"style":0,"text":"Cunningham et al., 2002","origin":{"pointer":"/sections/4/paragraphs/4","offset":15,"length":23},"authors":[{"last":"Cunningham"},{"last":"al."}],"year":"2002","references":["/references/4"]},{"style":0,"text":"Miller, 1995","origin":{"pointer":"/sections/4/paragraphs/5","offset":18,"length":12},"authors":[{"last":"Miller"}],"year":"1995","references":["/references/11"]},{"style":0,"text":"Wiebe et al., 2002","origin":{"pointer":"/sections/4/paragraphs/6","offset":26,"length":18},"authors":[{"last":"Wiebe"},{"last":"al."}],"year":"2002","references":["/references/13"]},{"style":0,"text":"Abney, 1996","origin":{"pointer":"/sections/4/paragraphs/7","offset":30,"length":11},"authors":[{"last":"Abney"}],"year":"1996","references":["/references/0"]},{"style":0,"text":"Wiebe et al., 2005","origin":{"pointer":"/sections/5/paragraphs/5","offset":661,"length":18},"authors":[{"last":"Wiebe"},{"last":"al."}],"year":"2005","references":["/references/14"]},{"style":0,"text":"McCallum, 2002","origin":{"pointer":"/sections/5/paragraphs/6","offset":79,"length":14},"authors":[{"last":"McCallum"}],"year":"2002","references":["/references/10"]},{"style":0,"text":"Wiebe et al., 2005","origin":{"pointer":"/sections/5/paragraphs/16","offset":108,"length":18},"authors":[{"last":"Wiebe"},{"last":"al."}],"year":"2005","references":["/references/14"]},{"style":0,"text":"Hu and Liu (2004)","origin":{"pointer":"/sections/6/paragraphs/0","offset":80,"length":17},"authors":[{"last":"Hu"},{"last":"Liu"}],"year":"2004","references":["/references/6"]},{"style":0,"text":"Wilson et al. (2005)","origin":{"pointer":"/sections/6/paragraphs/0","offset":99,"length":20},"authors":[{"last":"Wilson"},{"last":"al."}],"year":"2005","references":["/references/15"]},{"style":0,"text":"Wilson et al. (2006)","origin":{"pointer":"/sections/6/paragraphs/0","offset":121,"length":20},"authors":[{"last":"Wilson"},{"last":"al."}],"year":"2006","references":["/references/16"]},{"style":0,"text":"Choi and Claire (2008)","origin":{"pointer":"/sections/6/paragraphs/0","offset":143,"length":22},"authors":[{"last":"Choi"},{"last":"Claire"}],"year":"2008","references":[]},{"style":0,"text":"Wilson et al. (2009)","origin":{"pointer":"/sections/6/paragraphs/0","offset":167,"length":20},"authors":[{"last":"Wilson"},{"last":"al."}],"year":"2009","references":["/references/17"]},{"style":0,"text":"Wilson et al. (2005)","origin":{"pointer":"/sections/6/paragraphs/2","offset":29,"length":20},"authors":[{"last":"Wilson"},{"last":"al."}],"year":"2005","references":["/references/15"]},{"style":0,"text":"Wilson et al. (2005)","origin":{"pointer":"/sections/6/paragraphs/2","offset":133,"length":20},"authors":[{"last":"Wilson"},{"last":"al."}],"year":"2005","references":["/references/15"]},{"style":0,"text":"Zhao et al. (2008)","origin":{"pointer":"/sections/6/paragraphs/3","offset":92,"length":18},"authors":[{"last":"Zhao"},{"last":"al."}],"year":"2008","references":["/references/18"]},{"style":0,"text":"Zhao et al. (2008)","origin":{"pointer":"/sections/6/paragraphs/3","offset":142,"length":18},"authors":[{"last":"Zhao"},{"last":"al."}],"year":"2008","references":["/references/18"]}]}
