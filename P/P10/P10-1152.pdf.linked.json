{"sections":[{"title":"","paragraphs":["Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1502–1511, Uppsala, Sweden, 11-16 July 2010. c⃝2010 Association for Computational Linguistics"]},{"title":"Viterbi Training for PCFGs: Hardness Results and Competitiveness of Uniform Initialization Shay B. Cohen and Noah A. Smith School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213, USA {scohen,nasmith}@cs.cmu.edu Abstract","paragraphs":["We consider the search for a maximum likelihood assignment of hidden derivations and grammar weights for a probabilistic context-free grammar, the problem approximately solved by “Viterbi training.” We show that solving and even approximating Viterbi training for PCFGs is NP-hard. We motivate the use of uniform-at-random initialization for Viterbi EM as an optimal initializer in absence of further information about the correct model parameters, providing an approximate bound on the log-likelihood."]},{"title":"1 Introduction","paragraphs":["Probabilistic context-free grammars are an essential ingredient in many natural language processing models (Charniak, 1997; Collins, 2003; Johnson et al., 2006; Cohen and Smith, 2009, inter alia). Various algorithms for training such models have been proposed, including unsupervised methods. Many of these are based on the expectationmaximization (EM) algorithm.","There are alternatives to EM, and one such alternative is Viterbi EM, also called “hard” EM or “sparse” EM (Neal and Hinton, 1998). Instead of using the parameters (which are maintained in the algorithm’s current state) to find the true posterior over the derivations, Viterbi EM algorithm uses a posterior focused on the Viterbi parse of those parameters. Viterbi EM and variants have been used in various settings in natural language processing (Yejin and Cardie, 2007; Wang et al., 2007; Goldwater and Johnson, 2005; DeNero and Klein, 2008; Spitkovsky et al., 2010).","Viterbi EM can be understood as a coordinate ascent procedure that locally optimizes a function; we call this optimization goal “Viterbi training.”","In this paper, we explore Viterbi training for probabilistic context-free grammars. We first show that under the assumption that P ̸= NP, solving and even approximating the Viterbi training problem is hard. This result holds even for hidden Markov models. We extend the main hardness result to the EM algorithm (giving an alternative proof to this known result), as well as the problem of conditional Viterbi training. We then describe a “competitiveness” result for uniform initialization of Viterbi EM: we show that initialization of the trees in an E-step which uses uniform distributions over the trees is optimal with respect to a certain approximate bound.","The rest of this paper is organized as follows. §2 gives background on PCFGs and introduces some notation. §3 explains Viterbi training, the declarative form of Viterbi EM. §4 describes a hardness result for Viterbi training. §5 extends this result to a hardness result of approximation and §6 further extends these results for other cases. §7 describes the advantages in using uniform-at-random initialization for Viterbi training. We relate these results to work on the k-means problem in §8."]},{"title":"2 Background and Notation","paragraphs":["We assume familiarity with probabilistic context-free grammars (PCFGs). A PCFG G consists of: • A finite set of nonterminal symbolsN; • A finite set of terminal symbolsΣ;","• For each A ∈ N, a set of rewrite rules R(A) of the form A → α, where α ∈ (N ∪ Σ)∗",", and R = ∪A∈NR(A);","• For each rule A → α, a probability θA→α. The collection of probabilities is denoted θ, and they are constrained such that: ∀(A → α) ∈ R(A), θA→α ≥ 0 ∀A ∈ N, ∑ α:(A→α)∈R(A) θA→α = 1 That is, θ is grouped into |N| multinomial distributions. 1502 Under the PCFG, the joint probability of a string x ∈ Σ∗","and a grammatical derivation z is1 p(x, z | θ) = ∏ (A→α)∈R (θA→α)fA→α(z) (1) = exp ∑ (A→α)∈R fA→α(z) log θA→α where fA→α(z) is a function that “counts” the number of times the rule A → α appears in the derivation z. fA(z) will similarly denote the number of times that nonterminal A appears in z. Given a sample of derivations z = ⟨z1, . . . , zn⟩, let: FA→α(z) = n ∑ i=1 fA→α(zi) (2) FA(z) = n ∑ i=1 fA(zi) (3) We use the following notation for G:","• L(G) is the set of all strings (sentences) x that can be generated using the grammar G (the “language of G”).","• D(G) is the set of all possible derivations z that can be generated using the grammar G.","• D(G, x) is the set of all possible derivations z that can be generated using the grammar G and have the yield x."]},{"title":"3 Viterbi Training","paragraphs":["Viterbi EM, or “hard” EM, is an unsupervised learning algorithm, used in NLP in various settings (Yejin and Cardie, 2007; Wang et al., 2007; Goldwater and Johnson, 2005; DeNero and Klein, 2008; Spitkovsky et al., 2010). In the context of PCFGs, it aims to select parameters θ and phrase-structure trees z jointly. It does so by iteratively updating a state consisting of (θ, z). The state is initialized with some value, then the algorithm alternates between (i) a “hard” E-step, where the strings x1, . . . , xn are parsed according to a current, fixed θ, giving new values for z, and (ii) an M-step, where the θ are selected to maximize likelihood, with z fixed.","With PCFGs, the E-step requires running an algorithm such as (probabilistic) CKY or Earley’s","1","Note that x = yield(z); if the derivation is known, the string is also known. On the other hand, there may be many derivations with the same yield, perhaps even infinitely many. algorithm, while the M-step normalizes frequency counts FA→α(z) to obtain the maximum likelihood estimate’s closed-form solution.","We can understand Viterbi EM as a coordinate ascent procedure that approximates the solution to the following declarative problem: Problem 1. ViterbiTrain Input: G context-free grammar, x1, . . . , xn training instances from L(G) Output: θ and z1, . . . , zn such that","(θ, z1, . . . , zn) = argmax θ,z n ∏ i=1 p(xi, zi | θ) (4)","The optimization problem in Eq. 4 is nonconvex and, as we will show in §4, hard to optimize. Therefore it is necessary to resort to approximate algorithms like Viterbi EM.","Neal and Hinton (1998) use the term “sparse EM” to refer to a version of the EM algorithm where the E-step finds the modes of hidden variables (rather than marginals as in standard EM). Viterbi EM is a variant of this, where the E-step finds the mode for each xi’s derivation, argmaxz∈D(G,xi) p(xi, z | θ).","We will refer to L(θ, z) = n ∏ i=1 p(xi, zi | θ) (5) as “the objective function of ViterbiTrain.”","Viterbi training and Viterbi EM are closely related to self-training, an important concept in semi-supervised NLP (Charniak, 1997; McClosky et al., 2006a; McClosky et al., 2006b). With self-training, the model is learned with some seed an-notated data, and then iterates by labeling new, unannotated data and adding it to the original an-notated training set. McClosky et al. consider self-training to be “one round of Viterbi EM” with supervised initialization using labeled seed data. We refer the reader to Abney (2007) for more details."]},{"title":"4 Hardness of Viterbi Training","paragraphs":["We now describe hardness results for Problem 1. We first note that the following problem is known to be NP-hard, and in fact, NP-complete (Sipser, 2006): Problem 2. 3-SAT Input: A formula φ = ∧m","i=1 (ai ∨ bi ∨ ci) in con-","junctive normal form, such that each clause has 3 1503 Sφ2",""," Sφ1","A1"," A2","","","UY1,0","","UY2,1","","UY4,0","","UY1,0","","UY2,1","","UY3,1","","","V Ȳ1 VY1 VY2 V Ȳ2 V Ȳ4 VY4 V Ȳ1 VY1 VY2 V Ȳ2 VY3 V Ȳ3 1 0 1 0 1 0 1 0 1 0 1 0 Figure 1: An example of a Viterbi parse tree which represents a satisfying assignment for φ = (Y1 ∨ Y2 ∨ Ȳ4) ∧ ( Ȳ1 ∨ Ȳ2 ∨ Y3). In θφ, all rules appearing in the parse tree have probability 1. The extracted assignment would be Y1 = 0, Y2 = 1, Y3 = 1, Y4 = 0. Note that there is no usage of two different rules for a single nonterminal. literals. Output: 1 if there is a satisfying assignment for φ and 0 otherwise.","We now describe a reduction of 3-SAT to Problem 1. Given an instance of the 3-SAT problem, the reduction will, in polynomial time, create a grammar and a single string such that solving the ViterbiTrain problem for this grammar and string will yield a solution for the instance of the 3-SAT problem.","Let φ =","∧m","i=1 (ai ∨ bi ∨ ci) be an instance of the 3-SAT problem, where ai, bi and ci are literals over the set of variables {Y1, . . . , YN } (a literal refers to a variable Yj or its negation, Ȳj). Let Cj be the jth clause in φ, such that Cj = aj ∨ bj ∨ cj. We define the following context-free grammarGφ and string to parse sφ:","1. The terminals of Gφ are the binary digits Σ = {0, 1}.","2. We create N nonterminals VYr , r ∈ {1, . . . , N } and rules VYr → 0 and VYr → 1.","3. We create N nonterminals V Ȳr , r ∈ {1, . . . , N } and rules V Ȳr → 0 and V Ȳr → 1.","4. We create UYr,1 → VYr V Ȳr and UYr,0 → V Ȳr VYr .","5. We create the rule Sφ1 → A1. For each j ∈ {2, . . . , m}, we create a rule Sφj → Sφj−1Aj where Sφj is a new nonterminal indexed by φj ≜ ∧j","i=1 Ci and Aj is also a new nonterminal","indexed by j ∈ {1, . . . , m}.","6. Let Cj = aj ∨ bj ∨ cj be clause j in φ. Let Y (aj) be the variable that aj mentions. Let (y1, y2, y3) be a satisfying assignment for Cj where yk ∈ {0, 1} and is the value of Y (aj), Y (bj) and Y (cj) respectively for k ∈ {1, 2, 3}. For each such clause-satisfying assignment, we add the rule: Aj → UY (aj),y1UY (bj),y2UY (cj),y3 (6) For each Aj, we would have at most 7 rules of that form, since one rule will be logically inconsistent with aj ∨ bj ∨ cj. 7. The grammar’s start symbol is Sφn. 8. The string to parse is sφ = (10)3m",", i.e. 3m consecutive occurrences of the string 10.","A parse of the string sφ using Gφ will be used to get an assignment by setting Yr = 0 if the rule VYr → 0 or V Ȳr → 1 are used in the derivation of the parse tree, and 1 otherwise. Notice that at this point we do not exclude “contradictions” coming from the parse tree, such as VY3 → 0 used in the tree together with VY3 → 1 or V Ȳ3 → 0. The following lemma gives a condition under which the assignment is consistent (so contradictions do not occur in the parse tree): Lemma 1. Let φ be an instance of the 3-SAT problem, and let Gφ be a probabilistic CFG based on the above grammar with weights θφ. If the (multiplicative) weight of the Viterbi parse of sφ is 1, then the assignment extracted from the parse tree is consistent. Proof. Since the probability of the Viterbi parse is 1, all rules of the form {VYr , V Ȳr } → {0, 1} which appear in the parse tree have probability 1 as well. There are two possible types of inconsistencies. We show that neither exists in the Viterbi parse: 1504","1. For any r, an appearance of both rules of the form VYr → 0 and VYr → 1 cannot occur because all rules that appear in the Viterbi parse tree have probability 1.","2. For any r, an appearance of rules of the form VYr → 1 and V Ȳr → 1 cannot occur, because whenever we have an appearance of the rule VYr → 0, we have an adjacent appearance of the rule V Ȳr → 1 (because we parse substrings of the form 10), and then again we use the fact that all rules in the parse tree have probability 1. The case of VYr → 0 and V Ȳr → 0 is handled analogously. Thus, both possible inconsistencies are ruled out, resulting in a consistent assignment.","Figure 1 gives an example of an application of the reduction. Lemma 2. Define φ, Gφ as before. There exists θφ such that the Viterbi parse of sφ is 1 if and only if φ is satisfiable. Moreover, the satisfying assignment is the one extracted from the parse tree with weight 1 of sφ under θφ. Proof. (=⇒) Assume that there is a satisfying assignment. Each clause Cj = aj ∨ bj ∨ cj is satisfied using a tuple(y1, y2, y3) which assigns value for Y (aj), Y (bj) and Y (cj). This assignment corresponds the following rule Aj → UY (aj),y1UY (bj),y2UY (cj),y3 (7) Set its probability to 1, and set all other rules of Aj to 0. In addition, for each r, if Yr = y, set the probabilities of the rules VYr → y and V Ȳr → 1−y to 1 and V Ȳr → y and VYr → 1 − y to 0. The rest of the weights for Sφj → Sφj−1Aj are set to 1. This assignment of rule probabilities results in a Viterbi parse of weight 1.","(⇐=) Assume that the Viterbi parse has probability 1. From Lemma 1, we know that we can extract a consistent assignment from the Viterbi parse. In addition, for each clause Cj we have a rule Aj → UY (aj),y1UY (bj),y2UY (cj),y3 (8) that is assigned probability 1, for some (y1, y2, y3). One can verify that (y1, y2, y3) are the values of the assignment for the corresponding variables in clause Cj, and that they satisfy this clause. This means that each clause is satisfied by the assignment we extracted.","In order to show an NP-hardness result, we need to “convert” ViterbiTrain to a decision problem. The natural way to do it, following Lemmas 1 and 2, is to state the decision problem for ViterbiTrain as “given G and x1, . . . , xn and α ≥ 0, is the optimized value of the objective function L(θ, z) ≥ α?” and use α = 1 together with Lemmas 1 and 2. (Naturally, an algorithm for solving ViterbiTrain can easily be used to solve its decision problem.) Theorem 3. The decision version of the ViterbiTrain problem is NP-hard."]},{"title":"5 Hardness of Approximation","paragraphs":["A natural path of exploration following the hardness result we showed is determining whether an approximation of ViterbiTrain is also hard. Perhaps there is an efficient approximation algorithm for ViterbiTrain we could use instead of coordinate ascent algorithms such as Viterbi EM. Recall that such algorithms’ main guarantee is identify-ing a local maximum; we know nothing about how far it will be from the global maximum.","We next show that approximating the objective function of ViterbiTrain with a constant factor of ρ is hard for any ρ ∈ ( 1","2 , 1] (i.e., 1/2 + ε approximation is hard for any ε ≤ 1/2). This means that, under the P ̸= NP assumption, there is no efficient algorithm that, given a grammar G and a sample of sentences x1, . . . , xn, returns θ′","and z′","such that:","L(θ′ , z′ ) ≥ ρ · max","θ,z n ∏ i=1 p(xi, zi | θ) (9) We will continue to use the same reduction from §4. Let sφ be the string from that reduction, and let (θ, z) be the optimal solution for ViterbiTrain given Gφ and sφ. We first note that if p(sφ, z | θ) < 1 (implying that there is no satisfying assignment), then there must be a nonterminal which appears along with two different rules in z.","This means that we have a nonterminal B ∈ N with some rule B → α that appears k times, while the nonterminal appears in the parse r ≥ k + 1 times. Given the tree z, the θ that maximizes the objective function is the maximum likelihood estimate (MLE) for z (counting and normalizing the rules).2","We therefore know that the ViterbiTrain objective function, L(θ, z), is at 2 Note that we can only make p(z | θ, x) greater by using","θ to be the MLE for the derivation z. 1505 most ( k r )k , because it includes a factor equal to ( fB→α(z) fB(z) )fB→α(z) , where fB(z) is the number of times nonterminal B appears in z (hence fB(z) = r) and fB→α(z) is the number of times B → α appears in z (hence fB→α(z) = k). For any k ≥ 1, r ≥ k + 1: ( k r )k ≤ (","k k + 1 )k ≤ 1 2 (10) This means that if the value of the objective function of ViterbiTrain is not 1 using the reduction from §4, then it is at most 1","2 . If we had an efficient approximate algorithm with approximation coefficient ρ > 1","2 (Eq. 9 holds), then in order to solve 3-SAT for formula φ, we could run the algorithm on Gφ and sφ and check whether the assignment to (θ, z) that the algorithm returns satisfies φ or not, and return our response accordingly.","If φ were satisfiable, then the true maximal value of L would be 1, and the approximation algorithm would return (θ, z) such that L(θ, z) ≥ ρ > 1","2 . z would have to correspond to a satisfying assignment, and in fact p(z | θ) = 1, because in any other case, the probability of a derivation which does not represent a satisfying assignment is smaller than 1","2 . If φ were not satisfiable, then the approximation algorithm would never return a (θ, z) that results in a satisfying assignment (because such a (θ, z) does not exist).","The conclusion is that an efficient algorithm for approximating the objective function of ViterbiTrain (Eq. 4) within a factor of 1","2 + ε is unlikely to exist. If there were such an algorithm, we could use it to solve 3-SAT using the reduction from §4."]},{"title":"6 Extensions of the Hardness Result","paragraphs":["An alternative problem to Problem 1, a variant of Viterbi-training, is the following (see, for example, Klein and Manning, 2001): Problem 3. ConditionalViterbiTrain Input: G context-free grammar, x1, . . . , xn training instances from L(G) Output: θ and z1, . . . , zn such that","(θ, z1, . . . , zn) = argmax θ,z n ∏ i=1 p(zi | θ, xi) (11)","Here, instead of maximizing the likelihood, we maximize the conditional likelihood. Note that there is a hidden assumption in this problem definition, that xi can be parsed using the grammar G. Otherwise, the quantity p(zi | θ, xi) is not well-defined. We can extendConditionalViterbi-Train to return ⊥ in the case of not having a parse for one of the xi—this can be efficiently checked using a run of a cubic-time parser on each of the strings xi with the grammar G.","An approximate technique for this problem is similar to Viterbi EM, only modifying the M-step to maximize the conditional, rather than joint, likelihood. This new M-step will not have a closed form and may require auxiliary optimization techniques like gradient ascent.","Our hardness result for ViterbiTrain applies to ConditionalViterbiTrain as well. The reason is that if p(z, sφ | θφ) = 1 for a φ with a satisfying assignment, then L(G) = {sφ} and D(G) = {z}. This implies that p(z | θφ, sφ) = 1. If φ is unsatisfiable, then for the optimalθ of ViterbiTrain we have z and z′","such that 0 < p(z, sφ | θφ) < 1 and 0 < p(z′",", sφ | θφ) < 1, and therefore p(z | θφ, sφ) < 1, which means the conditional objective function will not obtain the value 1. (Note that there always exist some parameters θφ that generate sφ.) So, again, given an algorithm for ConditionalViterbiTrain, we can discern between a satisfiable formula and an unsatisfiable formula, using the reduction from §4 with the given algorithm, and identify whether the value of the objective function is 1 or strictly less than 1. We get the result that: Theorem 4. The decision problem of ConditionalViterbiTrain problem is NP-hard. where the decision problem of ConditionalViterbiTrain is defined analogously to the decision problem of ViterbiTrain.","We can similarly show that finding the global maximum of the marginalized likelihood: max θ 1 n n ∑ i=1 log ∑ z p(xi, z | θ) (12) is NP-hard. The reasoning follows. Using the reduction from before, if φ is satisfiable, then Eq. 12 gets value 0. If φ is unsatisfiable, then we would still get value 0 only if L(G) = {sφ}. If Gφ generates a single derivation for (10)3m",", then we actually do have a satisfying assignment from 1506 Lemma 1. Otherwise (more than a single derivation), the optimal θ would have to give fractional probabilities to rules of the form VYr → {0, 1} (or V Ȳr → {0, 1}). In that case, it is no longer true that (10)3m","is the only generated sentence, which is a contradiction.","The quantity in Eq. 12 can be maximized approximately using algorithms like EM, so this gives a hardness result for optimizing the objective function of EM for PCFGs. Day (1983) previously showed that maximizing the marginalized likelihood for hidden Markov models is NP-hard.","We note that the grammar we use for all of our results is not recursive. Therefore, we can encode this grammar as a hidden Markov model, strengthening our result from PCFGs to HMMs.3"]},{"title":"7 Uniform-at-Random Initialization","paragraphs":["In the previous sections, we showed that solving Viterbi training is hard, and therefore requires an approximation algorithm. Viterbi EM, which is an example of such algorithm, is dependent on an initialization of either θ to start with an E-step or z to start with an M-step. In the absence of a betterinformed initializer, it is reasonable to initialize z using a uniform distribution over D(G, xi) for each i. If D(G, xi) is finite, it can be done efficiently by setting θ = 1 (ignoring the normaliza-tion constraint), running the inside algorithm, and sampling from the (unnormalized) posterior given by the chart (Johnson et al., 2007). We turn next to an analysis of this initialization technique that suggests it is well-motivated.","The sketch of our result is as follows: we first give an asymptotic upper bound for the log-likelihood of derivations and sentences. This bound, which has an information-theoretic interpretation, depends on a parameter λ, which depends on the distribution from which the derivations were chosen. We then show that this bound is minimized when we pick λ such that this distribution is (conditioned on the sentence) a uniform distribution over derivations.","Let q(x) be any distribution over L(G) and θ some parameters for G. Let f (z) be some feature function (such as the one that counts the number of appearances of a certain rule in a derivation), and then: Eq,θ[f ] ≜ ∑ x∈L(G) q(x) ∑ z∈D(G,x) p(z | θ, x)f (z) 3 We thank an anonymous reviewer for pointing this out. which gives the expected value of the feature function f (z) under the distribution q(x) × p(z | θ, x). We will make the following assumption about G: Condition 1. There exists some θI such that ∀x ∈ L(G), ∀z ∈ D(G, x), p(z | θI , x) = 1/|D(G, x)|. This condition is satisfied, for example, whenG","is in Chomsky normal form and for all A, A′","∈ N,","|R(A)| = |R(A′",")|. Then, if we set θ","A→α = 1/|R(A)|, we get that all derivations of x will have the same number of rules and hence the same probability. This condition does not hold for grammars with unary cycles because |D(G, x)| may be infinite for some derivations. Such grammars are not commonly used in NLP.","Let us assume that some “correct” parameters θ∗","exist, and that our data were drawn from a distribution parametrized by θ∗",". The goal of this sec-tion is to motivate the following initialization for θ, which we call UniformInit:","1. Initialize z by sampling from the uniform distribution over D(G, xi) for each xi.","2. Update the grammar parameters using maximum likelihood estimation. 7.1 Bounding the Objective To show our result, we require first the following definition due to Freund et al. (1997): Definition 5. A distribution p1 is within λ ≥ 1 of a distribution p2 if for every event A, we have 1 λ ≤ p1(A) p2(A) ≤ λ (13)","For any feature function f (z) and any two sets of parameters θ2 and θ1 for G and for any marginal q(x), if p(z | θ1, x) is within λ of p(z | θ2, x) for all x then:","Eq,θ1[f ] λ ≤ Eq,θ2[f ] ≤ λEq,θ1[f ] (14) Let θ0 be a set of parameters such that we perform the following procedure in initializing Viterbi EM: first, we sample from the posterior distribution p(z | θ0, x), and then update the parameters with maximum likelihood estimate, in a regular M-step. Let λ be such that p(z | θ0, x) is within λ of p(z | θ∗",", x) (for all x ∈ L(G)). (Later we will show that UniformInit is a wise choice for making λ small. Note that UniformInit is equivalent to the procedure mentioned above with θ0 = θI .) 1507 Consider p̃n(x), the empirical distribution over","x1, . . . , xn. As n → ∞, we have that p̃n(x) →","p∗ (x), almost surely, where p∗","is:","p∗ (x) = ∑ z","p∗ (x, z | θ∗",") (15) This means that as n → ∞ we have Ep̃n,θ[f ] → Ep∗",",θ[f ]. Now, let z0 = (z0,1, . . . , z0,n) be samples from p(z | θ0, xi) for i ∈ {1, . . . , n}. Then, from simple MLE computation, we know that the value max θ′ n ∏ i=1","p(xi, z0,i | θ′ ) (16) = ∏ (A→α)∈R ( FA→α(z0) FA(z0) )FA→α(z0) We also know that for θ0, from the consistency of MLE, for large enough samples: FA→α(z0) FA(z0) ≈ Ep̃n,θ0[fA→α] Ep̃n,θ0[fA] (17) which means that we have the following as n grows (starting from the ViterbiTrain objective with initial state z = z0): max θ′ n ∏ i=1","p(xi, z0,i | θ′ ) (18) (Eq. 16) = ∏ (A→α)∈R ( FA→α(z0) FA(z0) )FA→α(z0) (19) (Eq. 17) ≈ ∏ (A→α)∈R ( Ep̃n,θ0[fA→α] Ep̃n,θ0[fA]",")FA→α(z0) (20) We next use the fact that p̃n(x) ≈ p∗","(x) for large n, and apply Eq. 14, noting again our assumption that p(z | θ0, x) is within λ of p(z | θ∗",", x). We also let B = ∑ i |zi|, where |zi| is the number of nodes in the derivation zi. Note that FA(zi) ≤ B. The above quantity (Eq. 20) is approximately bounded above by ∏ (A→α)∈R 1 λ2B (","Ep∗ ,θ∗[fA→α] Ep∗",",θ∗[fA] )FA→α(z0) (21) =","1 λ2|R|B ∏ (A→α)∈R","(θ∗ A→α)FA→α(z0) (22) Eq. 22 follows from: θ∗ A→α =","Ep∗ ,θ∗[fA→α] Ep∗",",θ∗[fA] (23)","If we continue to develop Eq. 22 and apply Eq. 17 and Eq. 23 again, we get that:","1 λ2|R|B ∏ (A→α)∈R","(θ∗ A→α)FA→α(z0) =","1 λ2|R|B ∏ (A→α)∈R","(θ∗","A→α)FA→α(z0)· F A(z","0) F A(z","0) ≈","1 λ2|R|B ∏ (A→α)∈R","(θ∗ A→α)","E","p∗ ,θ 0[f","A→α]","E p∗ ,θ","0[f","A] ·FA(z0) ≥","1 λ2|R|B ∏ (A→α)∈R","(θ∗","A→α)λ2","θ∗","A→αFA(z0) ≥","1 λ2|R|B   ∏ (A→α)∈R","(θ∗ A→α)","nθ∗ A→α  ","} {{ } T (θ∗",",n)","Bλ2 /n (24) = (","1 λ2|R|B )","T (θ∗ , n)Bλ2","/n (25)","≜ d(λ; θ∗ , |R|, B) (26) where Eq. 24 is the result of FA(z0) ≤ B.","For two series {an} and {bn}, let “ an ⪆ bn” denote that limn→∞ an ≥ limn→∞ bn. In other words, an is asymptotically larger than bn. Then, if we changed the representation of the objective function of the ViterbiTrain problem to log-likelihood, for θ′","that maximizes Eq. 18 (with some simple algebra) we have: 1 n n ∑ i=1","log2 p(xi, z0,i | θ′ ) (27) ⪆ −","2|R|B n log2 λ + Bλ2 n ( 1 n","log2 T (θ∗ , n) ) = −","2|R|B n log2 λ − |N| Bλ2 |N|n ∑ A∈N","H(θ∗ , A) (28) where","H(θ∗ , A) = − ∑ (A→α)∈R(A) θ∗ A→α log2 θ∗","A→α","(29) is the entropy of the multinomial for nonterminal A. H(θ∗",", A) can be thought of as the minimal number of bits required to encode a choice of a rule from A, if chosen independently from the other rules. All together, the quantity B |N|n","(∑ A∈N H(θ∗",", A))","is the average number of","bits required to encode a tree in our sample using 1508 θ∗",", while removing dependence among all rules and assuming that each node at the tree is chosen uniformly.4","This means that the log-likelihood, for large n, is bounded from above by a linear function of the (average) number of bits required to optimally encode n trees of total size B, while assuming independence among the rules in a tree. We note that the quantity B/n will tend toward the average size of a tree, which, under Condition 1, must be finite.","Our final approximate bound from Eq. 28 relates the choice of distribution, from which sample z0, to λ. The lower bound in Eq. 28 is a monotonedecreasing function of λ. We seek to make λ as small as possible to make the bound tight. We next show that the uniform distribution optimizes λ in that sense. 7.2 Optimizing λ Note that the optimal choice of λ, for a single x and for candidate initializer θ′",", is","λopt(x, θ∗ ; θ","0) = sup z∈D(G,x) p(z | θ0, x) p(z | θ∗",", x)(30) In order to avoid degenerate cases, we will add an-other condition on the true model, θ∗",": Condition 2. There exists τ > 0 such that, for any x ∈ L(G) and for any z ∈ D(G, x), p(z | θ∗",", x) ≥ τ . This is a strong condition, forcing the cardinality of D(G) to be finite, but it is not unreasonable if natural language sentences are effectively bounded in length.","Without further information about θ∗","(other than that it satisfies Condition 2), we may want to consider the worst-case scenario of possible λ, hence we seek initializer θ0 such that","Λ(x; θ0) ≜ sup θ λopt(x, θ; θ0) (31) is minimized. If θ0 = θI , then we have that p(z | θI , x) = |D(G, x)|−1","≜ μ x. Together with Condition 2, this implies that p(z | θI , x) p(z | θ∗",", x) ≤ μx τ (32) 4 We note that Grenander (1967) describes a (lin-","ear) relationship between the derivational entropy and","H(θ∗",", A). The derivational entropy is defined ash(θ∗",", A) =","− P x,z p(x, z | θ∗",") log p(x, z | θ∗","), where z ranges over","trees that have nonterminal A as the root. It follows im-","mediately from Grenander’s result that P","A H(θ∗ , A) ≤","P A h(θ∗",", A).","and hence λopt(x, θ∗ ) ≤ μ","x/τ for any θ∗",", hence","Λ(x; θI ) ≤ μx/τ . However, if we choose θ0 ̸=","θI , we have that p(z′ | θ","0, x) > μx for some z′",",","hence, for θ∗","such that it assigns probability τ on","z′",", we have that","sup z∈D(G,x) p(z | θ0, x) p(z | θ∗",", x) > μx τ (33) and hence λopt(x, θ∗","; θ′",") > μx/τ , so Λ(x; θ′",") > μx/τ . So, to optimize for the worst-case scenario over true distributions with respect to λ, we are motivated to choose θ0 = θI as defined in Condition 1. Indeed, UniformInit uses θI to initialize the state of Viterbi EM.","We note that if θI was known for a specific grammar, then we could have used it as a direct initializer. However, Condition 1 only guarantees its existence, and does not give a practical way to identify it. In general, as mentioned above, θ = 1 can be used to obtain a weighted CFG that satisfies p(z | θ, x) = 1/|D(G, x)|. Since we require a uniform posterior distribution, the number of derivations of a fixed length is finite. This means that we can converted the weighted CFG with θ = 1 to a PCFG with the same posterior (Smith and Johnson, 2007), and identify the appropriate θI ."]},{"title":"8 Related Work","paragraphs":["Viterbi training is closely related to the k-means clustering problem, where the objective is to find k centroids for a given set of d-dimensional points such that the sum of distances between the points and the closest centroid is minimized. The ana-log for Viterbi EM for the k-means problem is the k-means clustering algorithm (Lloyd, 1982), a coordinate ascent algorithm for solving the k-means problem. It works by iterating between an E-like-step, in which each point is assigned the closest centroid, and an M-like-step, in which the centroids are set to be the center of each cluster.","“ k” in k-means corresponds, in a sense, to the size of our grammar. k-means has been shown to be NP-hard both when k varies and d is fixed and when d varies and k is fixed (Aloise et al., 2009; Mahajan et al., 2009). An open problem relating to our hardness result would be whether ViterbiTrain (or ConditionalViterbiTrain) is hard even if we do not permit grammars of arbitrarily large size, or at least, constrain the number of rules that do not rewrite to terminals (in our current reduction, the 1509 size of the grammar grows as the size of the 3-SAT formula grows).","On a related note to §7, Arthur and Vassilvitskii (2007) described a greedy initialization algorithm for initializing the centroids of k-means, called k-means++. They show that their initialization is O(log k)-competitive; i.e., it approximates the optimal clusters assignment by a factor of O(log k). In §7.1, we showed that uniform-at-random initialization is approximately O(|N|Lλ2","/n)-competitive (modulo an additive constant) for CNF grammars, where n is the number of sentences, L is the total length of sentences and λ is a measure for distance between the true distribution and the uniform distribution.5","Many combinatorial problems in NLP involv-ing phrase-structure trees, alignments, and dependency graphs are hard (Sima’an, 1996; Goodman, 1998; Knight, 1999; Casacuberta and de la Higuera, 2000; Lyngsø and Pederson, 2002; Udupa and Maji, 2006; McDonald and Satta, 2007; DeNero and Klein, 2008, inter alia). Of special relevance to this paper is Abe and Warmuth (1992), who showed that the problem of finding maximum likelihood model of probabilistic automata is hard even for a single string and an automaton with two states. Understanding the complexity of NLP problems, we believe, is crucial as we seek effective practical approximations when necessary."]},{"title":"9 Conclusion","paragraphs":["We described some properties of Viterbi training for probabilistic context-free grammars. We showed that Viterbi training is NP-hard and, in fact, NP-hard to approximate. We gave motivation for uniform-at-random initialization for derivations in the Viterbi EM algorithm."]},{"title":"Acknowledgments","paragraphs":["We acknowledge helpful comments by the anonymous reviewers. This research was supported by NSF grant 0915187."]},{"title":"References","paragraphs":["N. Abe and M. Warmuth. 1992. On the computational complexity of approximating distributions by prob-5","Making the assumption that the grammar is in CNF permits us to use L instead of B, since there is a linear relationship between them in that case. abilistic automata. Machine Learning, 9(2–3):205– 260.","S. Abney. 2007. Semisupervised Learning for Computational Linguistics. CRC Press.","D. Aloise, A. Deshpande, P. Hansen, and P. Popat. 2009. NP-hardness of Euclidean sum-of-squares clustering. Machine Learning, 75(2):245–248.","D. Arthur and S. Vassilvitskii. 2007. k-means++: The advantages of careful seeding. In Proc. of ACM-SIAM symposium on Discrete Algorithms.","F. Casacuberta and C. de la Higuera. 2000. Computational complexity of problems on probabilistic grammars and transducers. In Proc. of ICGI.","E. Charniak. 1997. Statistical parsing with a context-free grammar and word statistics. In Proc. of AAAI.","S. B. Cohen and N. A. Smith. 2009. Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction. In Proc. of HLT-NAACL.","M. Collins. 2003. Head-driven statistical models for natural language processing. Computational Linguistics, 29(4):589–637.","W. H. E. Day. 1983. Computationally difficult parsimony problems in phylogenetic systematics. Journal of Theoretical Biology, 103.","J. DeNero and D. Klein. 2008. The complexity of phrase alignment problems. In Proc. of ACL.","Y. Freund, H. Seung, E. Shamir, and N. Tishby. 1997. Selective sampling using the query by committee algorithm. Machine Learning, 28(2–3):133–168.","S. Goldwater and M. Johnson. 2005. Bias in learning syllable structure. In Proc. of CoNLL.","J. Goodman. 1998. Parsing Inside-Out. Ph.D. thesis, Harvard University.","U. Grenander. 1967. Syntax-controlled probabilities. Technical report, Brown University, Division of Applied Mathematics.","M. Johnson, T. L. Griffiths, and S. Goldwater. 2006. Adaptor grammars: A framework for specifying compositional nonparameteric Bayesian models. In Advances in NIPS.","M. Johnson, T. L. Griffiths, and S. Goldwater. 2007. Bayesian inference for PCFGs via Markov chain Monte Carlo. In Proc. of NAACL.","D. Klein and C. Manning. 2001. Natural language grammar induction using a constituentcontext model. In Advances in NIPS.","K. Knight. 1999. Decoding complexity in wordreplacement translation models. Computational Linguistics, 25(4):607–615.","S. P. Lloyd. 1982. Least squares quantization in PCM. In IEEE Transactions on Information Theory.","R. B. Lyngsø and C. N. S. Pederson. 2002. The consensus string problem and the complexity of comparing hidden Markov models. Journal of Comput-ing and System Science, 65(3):545–569.","M. Mahajan, P. Nimbhorkar, and K. Varadarajan. 2009. The planar k-means problem is NP-hard. In Proc. of International Workshop on Algorithms and Computation. 1510","D. McClosky, E. Charniak, and M. Johnson. 2006a. Effective self-training for parsing. In Proc. of HLT-NAACL.","D. McClosky, E. Charniak, and M. Johnson. 2006b. Reranking and self-training for parser adaptation. In Proc. of COLING-ACL.","R. McDonald and G. Satta. 2007. On the complexity of non-projective data-driven dependency parsing. In Proc. of IWPT.","R. M. Neal and G. E. Hinton. 1998. A view of the EM algorithm that justifies incremental, sparse, and other variants. In Learning and Graphical Models, pages 355–368. Kluwer Academic Publishers.","K. Sima’an. 1996. Computational complexity of probabilistic disambiguation by means of tree-grammars. In In Proc. of COLING.","M. Sipser. 2006. Introduction to the Theory of Computation, Second Edition. Thomson Course Technology.","N. A. Smith and M. Johnson. 2007. Weighted and probabilistic context-free grammars are equally expressive. Computational Linguistics, 33(4):477– 491.","V. I. Spitkovsky, H. Alshawi, D. Jurafsky, and C. D. Manning. 2010. Viterbi training improves unsupervised dependency parsing. In Proc. of CoNLL.","R. Udupa and K. Maji. 2006. Computational complexity of statistical machine translation. In Proc. of EACL.","M. Wang, N. A. Smith, and T. Mitamura. 2007. What is the Jeopardy model? a quasi-synchronous grammar for question answering. In Proc. of EMNLP.","C. Yejin and C. Cardie. 2007. Structured local training and biased potential functions for conditional random fields with application to coreference resolu-tion. In Proc. of HLT-NAACL. 1511"]}],"references":[{"authors":[],"source":"N. Abe and M. Warmuth. 1992. On the computational complexity of approximating distributions by prob-5"},{"authors":[],"source":"Making the assumption that the grammar is in CNF permits us to use L instead of B, since there is a linear relationship between them in that case. abilistic automata. Machine Learning, 9(2–3):205– 260."},{"authors":[{"first":"S.","last":"Abney"}],"year":"2007","title":"Semisupervised Learning for Computational Linguistics","source":"S. Abney. 2007. Semisupervised Learning for Computational Linguistics. CRC Press."},{"authors":[{"first":"D.","last":"Aloise"},{"first":"A.","last":"Deshpande"},{"first":"P.","last":"Hansen"},{"first":"P.","last":"Popat"}],"year":"2009","title":"NP-hardness of Euclidean sum-of-squares clustering","source":"D. Aloise, A. Deshpande, P. Hansen, and P. Popat. 2009. NP-hardness of Euclidean sum-of-squares clustering. Machine Learning, 75(2):245–248."},{"authors":[{"first":"D.","last":"Arthur"},{"first":"S.","last":"Vassilvitskii"}],"year":"2007","title":"k-means++: The advantages of careful seeding","source":"D. Arthur and S. Vassilvitskii. 2007. k-means++: The advantages of careful seeding. In Proc. of ACM-SIAM symposium on Discrete Algorithms."},{"authors":[{"first":"F.","last":"Casacuberta"},{"first":"C.","middle":"de la","last":"Higuera"}],"year":"2000","title":"Computational complexity of problems on probabilistic grammars and transducers","source":"F. Casacuberta and C. de la Higuera. 2000. Computational complexity of problems on probabilistic grammars and transducers. In Proc. of ICGI."},{"authors":[{"first":"E.","last":"Charniak"}],"year":"1997","title":"Statistical parsing with a context-free grammar and word statistics","source":"E. Charniak. 1997. Statistical parsing with a context-free grammar and word statistics. In Proc. of AAAI."},{"authors":[{"first":"S.","middle":"B.","last":"Cohen"},{"first":"N.","middle":"A.","last":"Smith"}],"year":"2009","title":"Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction","source":"S. B. Cohen and N. A. Smith. 2009. Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction. In Proc. of HLT-NAACL."},{"authors":[{"first":"M.","last":"Collins"}],"year":"2003","title":"Head-driven statistical models for natural language processing","source":"M. Collins. 2003. Head-driven statistical models for natural language processing. Computational Linguistics, 29(4):589–637."},{"authors":[{"first":"W.","middle":"H. E.","last":"Day"}],"year":"1983","title":"Computationally difficult parsimony problems in phylogenetic systematics","source":"W. H. E. Day. 1983. Computationally difficult parsimony problems in phylogenetic systematics. Journal of Theoretical Biology, 103."},{"authors":[{"first":"J.","last":"DeNero"},{"first":"D.","last":"Klein"}],"year":"2008","title":"The complexity of phrase alignment problems","source":"J. DeNero and D. Klein. 2008. The complexity of phrase alignment problems. In Proc. of ACL."},{"authors":[{"first":"Y.","last":"Freund"},{"first":"H.","last":"Seung"},{"first":"E.","last":"Shamir"},{"first":"N.","last":"Tishby"}],"year":"1997","title":"Selective sampling using the query by committee algorithm","source":"Y. Freund, H. Seung, E. Shamir, and N. Tishby. 1997. Selective sampling using the query by committee algorithm. Machine Learning, 28(2–3):133–168."},{"authors":[{"first":"S.","last":"Goldwater"},{"first":"M.","last":"Johnson"}],"year":"2005","title":"Bias in learning syllable structure","source":"S. Goldwater and M. Johnson. 2005. Bias in learning syllable structure. In Proc. of CoNLL."},{"authors":[{"first":"J.","last":"Goodman"}],"year":"1998","title":"Parsing Inside-Out","source":"J. Goodman. 1998. Parsing Inside-Out. Ph.D. thesis, Harvard University."},{"authors":[{"first":"U.","last":"Grenander"}],"year":"1967","title":"Syntax-controlled probabilities","source":"U. Grenander. 1967. Syntax-controlled probabilities. Technical report, Brown University, Division of Applied Mathematics."},{"authors":[{"first":"M.","last":"Johnson"},{"first":"T.","middle":"L.","last":"Griffiths"},{"first":"S.","last":"Goldwater"}],"year":"2006","title":"Adaptor grammars: A framework for specifying compositional nonparameteric Bayesian models","source":"M. Johnson, T. L. Griffiths, and S. Goldwater. 2006. Adaptor grammars: A framework for specifying compositional nonparameteric Bayesian models. In Advances in NIPS."},{"authors":[{"first":"M.","last":"Johnson"},{"first":"T.","middle":"L.","last":"Griffiths"},{"first":"S.","last":"Goldwater"}],"year":"2007","title":"Bayesian inference for PCFGs via Markov chain Monte Carlo","source":"M. Johnson, T. L. Griffiths, and S. Goldwater. 2007. Bayesian inference for PCFGs via Markov chain Monte Carlo. In Proc. of NAACL."},{"authors":[{"first":"D.","last":"Klein"},{"first":"C.","last":"Manning"}],"year":"2001","title":"Natural language grammar induction using a constituentcontext model","source":"D. Klein and C. Manning. 2001. Natural language grammar induction using a constituentcontext model. In Advances in NIPS."},{"authors":[{"first":"K.","last":"Knight"}],"year":"1999","title":"Decoding complexity in wordreplacement translation models","source":"K. Knight. 1999. Decoding complexity in wordreplacement translation models. Computational Linguistics, 25(4):607–615."},{"authors":[{"first":"S.","middle":"P.","last":"Lloyd"}],"year":"1982","title":"Least squares quantization in PCM","source":"S. P. Lloyd. 1982. Least squares quantization in PCM. In IEEE Transactions on Information Theory."},{"authors":[{"first":"R.","middle":"B.","last":"Lyngsø"},{"first":"C.","middle":"N. S.","last":"Pederson"}],"year":"2002","title":"The consensus string problem and the complexity of comparing hidden Markov models","source":"R. B. Lyngsø and C. N. S. Pederson. 2002. The consensus string problem and the complexity of comparing hidden Markov models. Journal of Comput-ing and System Science, 65(3):545–569."},{"authors":[{"first":"M.","last":"Mahajan"},{"first":"P.","last":"Nimbhorkar"},{"first":"K.","last":"Varadarajan"}],"year":"2009","title":"The planar k-means problem is NP-hard","source":"M. Mahajan, P. Nimbhorkar, and K. Varadarajan. 2009. The planar k-means problem is NP-hard. In Proc. of International Workshop on Algorithms and Computation. 1510"},{"authors":[{"first":"D.","last":"McClosky"},{"first":"E.","last":"Charniak"},{"first":"M.","last":"Johnson"}],"year":"2006a","title":"Effective self-training for parsing","source":"D. McClosky, E. Charniak, and M. Johnson. 2006a. Effective self-training for parsing. In Proc. of HLT-NAACL."},{"authors":[{"first":"D.","last":"McClosky"},{"first":"E.","last":"Charniak"},{"first":"M.","last":"Johnson"}],"year":"2006b","title":"Reranking and self-training for parser adaptation","source":"D. McClosky, E. Charniak, and M. Johnson. 2006b. Reranking and self-training for parser adaptation. In Proc. of COLING-ACL."},{"authors":[{"first":"R.","last":"McDonald"},{"first":"G.","last":"Satta"}],"year":"2007","title":"On the complexity of non-projective data-driven dependency parsing","source":"R. McDonald and G. Satta. 2007. On the complexity of non-projective data-driven dependency parsing. In Proc. of IWPT."},{"authors":[{"first":"R.","middle":"M.","last":"Neal"},{"first":"G.","middle":"E.","last":"Hinton"}],"year":"1998","title":"A view of the EM algorithm that justifies incremental, sparse, and other variants","source":"R. M. Neal and G. E. Hinton. 1998. A view of the EM algorithm that justifies incremental, sparse, and other variants. In Learning and Graphical Models, pages 355–368. Kluwer Academic Publishers."},{"authors":[{"first":"K.","last":"Sima’an"}],"year":"1996","title":"Computational complexity of probabilistic disambiguation by means of tree-grammars","source":"K. Sima’an. 1996. Computational complexity of probabilistic disambiguation by means of tree-grammars. In In Proc. of COLING."},{"authors":[{"first":"M.","last":"Sipser"}],"year":"2006","title":"Introduction to the Theory of Computation, Second Edition","source":"M. Sipser. 2006. Introduction to the Theory of Computation, Second Edition. Thomson Course Technology."},{"authors":[{"first":"N.","middle":"A.","last":"Smith"},{"first":"M.","last":"Johnson"}],"year":"2007","title":"Weighted and probabilistic context-free grammars are equally expressive","source":"N. A. Smith and M. Johnson. 2007. Weighted and probabilistic context-free grammars are equally expressive. Computational Linguistics, 33(4):477– 491."},{"authors":[{"first":"V.","middle":"I.","last":"Spitkovsky"},{"first":"H.","last":"Alshawi"},{"first":"D.","last":"Jurafsky"},{"first":"C.","middle":"D.","last":"Manning"}],"year":"2010","title":"Viterbi training improves unsupervised dependency parsing","source":"V. I. Spitkovsky, H. Alshawi, D. Jurafsky, and C. D. Manning. 2010. Viterbi training improves unsupervised dependency parsing. In Proc. of CoNLL."},{"authors":[{"first":"R.","last":"Udupa"},{"first":"K.","last":"Maji"}],"year":"2006","title":"Computational complexity of statistical machine translation","source":"R. Udupa and K. Maji. 2006. Computational complexity of statistical machine translation. In Proc. of EACL."},{"authors":[{"first":"M.","last":"Wang"},{"first":"N.","middle":"A.","last":"Smith"},{"first":"T.","last":"Mitamura"}],"year":"2007","title":"What is the Jeopardy model? a quasi-synchronous grammar for question answering","source":"M. Wang, N. A. Smith, and T. Mitamura. 2007. What is the Jeopardy model? a quasi-synchronous grammar for question answering. In Proc. of EMNLP."},{"authors":[{"first":"C.","last":"Yejin"},{"first":"C.","last":"Cardie"}],"year":"2007","title":"Structured local training and biased potential functions for conditional random fields with application to coreference resolu-tion","source":"C. Yejin and C. Cardie. 2007. Structured local training and biased potential functions for conditional random fields with application to coreference resolu-tion. In Proc. of HLT-NAACL. 1511"}],"cites":[{"style":0,"text":"Charniak, 1997","origin":{"pointer":"/sections/2/paragraphs/0","offset":108,"length":14},"authors":[{"last":"Charniak"}],"year":"1997","references":["/references/6"]},{"style":0,"text":"Collins, 2003","origin":{"pointer":"/sections/2/paragraphs/0","offset":124,"length":13},"authors":[{"last":"Collins"}],"year":"2003","references":["/references/8"]},{"style":0,"text":"Johnson et al., 2006","origin":{"pointer":"/sections/2/paragraphs/0","offset":139,"length":20},"authors":[{"last":"Johnson"},{"last":"al."}],"year":"2006","references":["/references/15"]},{"style":0,"text":"Cohen and Smith, 2009","origin":{"pointer":"/sections/2/paragraphs/0","offset":161,"length":21},"authors":[{"last":"Cohen"},{"last":"Smith"}],"year":"2009","references":["/references/7"]},{"style":0,"text":"Neal and Hinton, 1998","origin":{"pointer":"/sections/2/paragraphs/1","offset":108,"length":21},"authors":[{"last":"Neal"},{"last":"Hinton"}],"year":"1998","references":["/references/25"]},{"style":0,"text":"Yejin and Cardie, 2007","origin":{"pointer":"/sections/2/paragraphs/1","offset":448,"length":22},"authors":[{"last":"Yejin"},{"last":"Cardie"}],"year":"2007","references":["/references/32"]},{"style":0,"text":"Wang et al., 2007","origin":{"pointer":"/sections/2/paragraphs/1","offset":472,"length":17},"authors":[{"last":"Wang"},{"last":"al."}],"year":"2007","references":["/references/31"]},{"style":0,"text":"Goldwater and Johnson, 2005","origin":{"pointer":"/sections/2/paragraphs/1","offset":491,"length":27},"authors":[{"last":"Goldwater"},{"last":"Johnson"}],"year":"2005","references":["/references/12"]},{"style":0,"text":"DeNero and Klein, 2008","origin":{"pointer":"/sections/2/paragraphs/1","offset":520,"length":22},"authors":[{"last":"DeNero"},{"last":"Klein"}],"year":"2008","references":["/references/10"]},{"style":0,"text":"Spitkovsky et al., 2010","origin":{"pointer":"/sections/2/paragraphs/1","offset":544,"length":23},"authors":[{"last":"Spitkovsky"},{"last":"al."}],"year":"2010","references":["/references/29"]},{"style":0,"text":"Yejin and Cardie, 2007","origin":{"pointer":"/sections/4/paragraphs/0","offset":98,"length":22},"authors":[{"last":"Yejin"},{"last":"Cardie"}],"year":"2007","references":["/references/32"]},{"style":0,"text":"Wang et al., 2007","origin":{"pointer":"/sections/4/paragraphs/0","offset":122,"length":17},"authors":[{"last":"Wang"},{"last":"al."}],"year":"2007","references":["/references/31"]},{"style":0,"text":"Goldwater and Johnson, 2005","origin":{"pointer":"/sections/4/paragraphs/0","offset":141,"length":27},"authors":[{"last":"Goldwater"},{"last":"Johnson"}],"year":"2005","references":["/references/12"]},{"style":0,"text":"DeNero and Klein, 2008","origin":{"pointer":"/sections/4/paragraphs/0","offset":170,"length":22},"authors":[{"last":"DeNero"},{"last":"Klein"}],"year":"2008","references":["/references/10"]},{"style":0,"text":"Spitkovsky et al., 2010","origin":{"pointer":"/sections/4/paragraphs/0","offset":194,"length":23},"authors":[{"last":"Spitkovsky"},{"last":"al."}],"year":"2010","references":["/references/29"]},{"style":0,"text":"Neal and Hinton (1998)","origin":{"pointer":"/sections/4/paragraphs/7","offset":0,"length":22},"authors":[{"last":"Neal"},{"last":"Hinton"}],"year":"1998","references":["/references/25"]},{"style":0,"text":"Charniak, 1997","origin":{"pointer":"/sections/4/paragraphs/9","offset":115,"length":14},"authors":[{"last":"Charniak"}],"year":"1997","references":["/references/6"]},{"style":0,"text":"McClosky et al., 2006a","origin":{"pointer":"/sections/4/paragraphs/9","offset":131,"length":22},"authors":[{"last":"McClosky"},{"last":"al."}],"year":"2006a","references":["/references/22"]},{"style":0,"text":"McClosky et al., 2006b","origin":{"pointer":"/sections/4/paragraphs/9","offset":155,"length":22},"authors":[{"last":"McClosky"},{"last":"al."}],"year":"2006b","references":["/references/23"]},{"style":0,"text":"Abney (2007)","origin":{"pointer":"/sections/4/paragraphs/9","offset":510,"length":12},"authors":[{"last":"Abney"}],"year":"2007","references":["/references/2"]},{"style":0,"text":"Sipser, 2006","origin":{"pointer":"/sections/5/paragraphs/0","offset":139,"length":12},"authors":[{"last":"Sipser"}],"year":"2006","references":["/references/27"]},{"style":0,"text":"Klein and Manning, 2001","origin":{"pointer":"/sections/7/paragraphs/0","offset":104,"length":23},"authors":[{"last":"Klein"},{"last":"Manning"}],"year":"2001","references":["/references/17"]},{"style":0,"text":"Day (1983)","origin":{"pointer":"/sections/7/paragraphs/10","offset":167,"length":10},"authors":[{"last":"Day"}],"year":"1983","references":["/references/9"]},{"style":0,"text":"Johnson et al., 2007","origin":{"pointer":"/sections/8/paragraphs/0","offset":619,"length":20},"authors":[{"last":"Johnson"},{"last":"al."}],"year":"2007","references":["/references/16"]},{"style":0,"text":"Freund et al. (1997)","origin":{"pointer":"/sections/8/paragraphs/12","offset":166,"length":20},"authors":[{"last":"Freund"},{"last":"al."}],"year":"1997","references":["/references/11"]},{"style":0,"text":"Grenander (1967)","origin":{"pointer":"/sections/8/paragraphs/93","offset":32,"length":16},"authors":[{"last":"Grenander"}],"year":"1967","references":["/references/14"]},{"style":0,"text":"Smith and Johnson, 2007","origin":{"pointer":"/sections/8/paragraphs/122","offset":526,"length":23},"authors":[{"last":"Smith"},{"last":"Johnson"}],"year":"2007","references":["/references/28"]},{"style":0,"text":"Lloyd, 1982","origin":{"pointer":"/sections/9/paragraphs/0","offset":331,"length":11},"authors":[{"last":"Lloyd"}],"year":"1982","references":["/references/19"]},{"style":0,"text":"Aloise et al., 2009","origin":{"pointer":"/sections/9/paragraphs/1","offset":174,"length":19},"authors":[{"last":"Aloise"},{"last":"al."}],"year":"2009","references":["/references/3"]},{"style":0,"text":"Mahajan et al., 2009","origin":{"pointer":"/sections/9/paragraphs/1","offset":195,"length":20},"authors":[{"last":"Mahajan"},{"last":"al."}],"year":"2009","references":["/references/21"]},{"style":0,"text":"Arthur and Vassilvitskii (2007)","origin":{"pointer":"/sections/9/paragraphs/2","offset":25,"length":31},"authors":[{"last":"Arthur"},{"last":"Vassilvitskii"}],"year":"2007","references":["/references/4"]},{"style":0,"text":"Sima’an, 1996","origin":{"pointer":"/sections/9/paragraphs/4","offset":114,"length":13},"authors":[{"last":"Sima’an"}],"year":"1996","references":["/references/26"]},{"style":0,"text":"Goodman, 1998","origin":{"pointer":"/sections/9/paragraphs/4","offset":129,"length":13},"authors":[{"last":"Goodman"}],"year":"1998","references":["/references/13"]},{"style":0,"text":"Knight, 1999","origin":{"pointer":"/sections/9/paragraphs/4","offset":144,"length":12},"authors":[{"last":"Knight"}],"year":"1999","references":["/references/18"]},{"style":0,"text":"Higuera, 2000","origin":{"pointer":"/sections/9/paragraphs/4","offset":180,"length":13},"authors":[{"last":"Higuera"}],"year":"2000","references":[]},{"style":0,"text":"Lyngsø and Pederson, 2002","origin":{"pointer":"/sections/9/paragraphs/4","offset":195,"length":25},"authors":[{"last":"Lyngsø"},{"last":"Pederson"}],"year":"2002","references":["/references/20"]},{"style":0,"text":"Udupa and Maji, 2006","origin":{"pointer":"/sections/9/paragraphs/4","offset":222,"length":20},"authors":[{"last":"Udupa"},{"last":"Maji"}],"year":"2006","references":["/references/30"]},{"style":0,"text":"McDonald and Satta, 2007","origin":{"pointer":"/sections/9/paragraphs/4","offset":244,"length":24},"authors":[{"last":"McDonald"},{"last":"Satta"}],"year":"2007","references":["/references/24"]},{"style":0,"text":"DeNero and Klein, 2008","origin":{"pointer":"/sections/9/paragraphs/4","offset":270,"length":22},"authors":[{"last":"DeNero"},{"last":"Klein"}],"year":"2008","references":["/references/10"]},{"style":0,"text":"Abe and Warmuth (1992)","origin":{"pointer":"/sections/9/paragraphs/4","offset":345,"length":22},"authors":[{"last":"Abe"},{"last":"Warmuth"}],"year":"1992","references":[]}]}
