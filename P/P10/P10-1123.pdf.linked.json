{"sections":[{"title":"","paragraphs":["Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1209–1219, Uppsala, Sweden, 11-16 July 2010. c⃝2010 Association for Computational Linguistics"]},{"title":"Assessing the Role of Discourse References in Entailment Inference Shachar Mirkin, Ido Dagan Bar-Ilan University Ramat-Gan, Israel {mirkins,dagan}@cs.biu.ac.il Sebastian Padó University of Stuttgart Stuttgart, Germany pado@ims.uni-stuttgart.de Abstract","paragraphs":["Discourse references, notably coreference and bridging, play an important role in many text understanding applications, but their impact on textual entailment is yet to be systematically understood. On the ba-sis of an in-depth analysis of entailment instances, we argue that discourse references have the potential of substantially improving textual entailment recognition, and identify a number of research direc-tions towards this goal."]},{"title":"1 Introduction","paragraphs":["The detection and resolution of discourse references such as coreference and bridging anaphora play an important role in text understanding applications, like question answering and information extraction. There, reference resolution is used for the purpose of combining knowledge from multiple sentences. Such knowledge is also important for Textual Entailment (TE), a generic framework for modeling semantic inference. TE reduces the inference requirements of many text understanding applications to the problem of determining whether the meaning of a given textual assertion, termed hypothesis (H), can be inferred from the meaning of certain text (T ) (Dagan et al., 2006).","Consider the following example:","(1) T: “ Not only had he developed an aversion to the President1 and politics in general, Oswald2 was also a failure with Marina, his wife. [...] Their relationship was supposedly responsible for why he2 killed Kennedy1.” H: “ Oswald killed President Kennedy.” The understanding that the second sentence of the text entails the hypothesis draws on two coreference relationships, namely that he is Oswald, and that the Kennedy in question is President Kennedy. However, the utilization of discourse information for such inferences has been so far limited mainly to the substitution of nominal coreferents, while many aspects of the interface between discourse and semantic inference needs remain unexplored.","The recently held Fifth Recognizing Textual Entailment (RTE-5) challenge (Bentivogli et al., 2009a) has introduced a Search task, where the text sentences are interpreted in the context of their full discourse, as in Example 1 above. Accordingly, TE constitutes an interesting framework – and the Search task an adequate dataset – to study the interrelation between discourse and inference.","The goal of this study is to analyze the roles of discourse references for textual entailment inference, to provide relevant findings and insights to developers of both reference resolvers and entailment systems and to highlight promising direc-tions for the better incorporation of discourse phenomena into inference. Our focus is on a manual, in-depth assessment that results in a classification and quantification of discourse reference phenomena and their utilization for inference. On this ba-sis, we develop an account of formal devices for incorporating discourse references into the inference computation. An additional point of interest is the interrelation between entailment knowledge and coreference. E.g., in Example 1 above, knowing that Kennedy was a president can alleviate the need for coreference resolution. Conversely, coreference resolution can often be used to overcome gaps in entailment knowledge. Structure of the paper. In Section 2, we provide background on the use of discourse references in natural language processing (NLP) in general and specifically in TE. Section 3 describes the goals of this study, followed by our analysis scheme (Section 4) and the required inference 1209 mechanisms (Section 5). Section 6 presents quantitative findings and further observations. Conclusions are discussed in Section 7."]},{"title":"2 Background 2.1 Discourse in NLP","paragraphs":["Discourse information plays a role in a range of NLP tasks. It is obviously central to discourse processing tasks such as text segmenta-tion (Hearst, 1997). Reference information provided by discourse is also useful for text understanding tasks such as question answering (QA), information extraction (IE) and information retrieval (IR) (Vicedo and Ferrndez, 2006; Zelenko et al., 2004; Na and Ng, 2009), as well as for the acquisition of lexical-semantic “narrative schema” knowledge (Chambers and Jurafsky, 2009). Discourse references have been the subject of atten-tion in both the Message Understanding Conference (Grishman and Sundheim, 1996) and the Automatic Content Extraction program (Strassel et al., 2008).","The simplest form of information that discourse provides is coreference, i.e., information that two linguistic expressions refer to the same entity or event. Coreference is particularly important for processing pronouns and other anaphoric expressions, such as he in Example 1. Ability to resolve this reference translates directly into, e.g., a QA system’s ability to answer questions like Who killed Kennedy?.","A second, more complex type of information stems from bridging references, such as in the following discourse (Asher and Lascarides, 1998): (2) “ I’ve just arrived. The camel is outside.” While coreference indicates equivalence, bridging points to the existence of a salient semantic relation between two distinct entities or events. Here, it is (informally) ‘means of transport’, which would make the discourse (2) relevant for a question like How did I arrive here?. Other types of bridging relations include set-membership, roles in events and consequence (Clark, 1975).","Note, however, that text understanding systems are generally limited to the resolution of entity (or even just pronoun) coreference, e.g. (Li et al., 2009; Dali et al., 2009). An important reason is the unavailability of tools to resolve the more complex (and difficult) forms of discourse reference such as event coreference and bridging.1","Another reason is uncertainty about their practical importance. 2.2 Discourse in Textual Entailment Textual Entailment has been introduced in Section 1 as a common-sense notion of inference. It has spawned interest in the computational linguistics community as a common denominator of many NLP tasks including IE, summarization and tutoring (Romano et al., 2006; Harabagiu et al., 2007; Nielsen et al., 2009). Architectures for Textual Entailment. Over the course of recent RTE challenges (Giampiccolo et al., 2007; Giampiccolo et al., 2008), the main benchmark for TE technology, two architectures for modeling TE have emerged as dominant: transformations and alignment. The goal of transformation-based TE models is to determine the entailment relation T ⇒ H by find-ing a “proof”, i.e., a sequence of consequents, (T, T1, . . . , Tn), such that Tn=H (Bar-Haim et al., 2008; Harmeling, 2009), and that in each transformation, Ti → Ti+1, the consequent Ti+1 is entailed by Ti. These transformations commonly include lexical modifications and the generation of syntactic alternatives. The second major approach constructs an alignment between the linguistic entities of the trees (or graphs) of T and H, which can represent syntactic structure, semantic structure, or non-hierarchical phrases (Zanzotto et al., 2009; Burchardt et al., 2009; MacCartney et al., 2008). H is assumed to be entailed by T if its entities are aligned “well” to corresponding entities in T . Alignment quality is generally determined based on features that assess the validity of the local replacement of the T entity by the H entity.","While transformation- and alignment-based entailment models look different at first glance, they ultimately have the same goal, namely obtaining a maximal coverage of H by T , i.e. to identify matches of as many elements of H within T as possible.2","To do so, both architectures typically make use of inference rules such as ‘Y was purchased by X → X paid for Y’, either by directly applying them as transformations, or by using them","1","Some studies, e.g. (Markert et al., 2003; Poesio et al., 2004), address the resolution of a few specific kinds of bridging relations; yet, wide-scope systems for bridging resolution are unavailable.","2","Clearly, the details of how the final entailment decision is made based on the attained coverage differ substantially among models. 1210 to score alignments. Rules are generally drawn from external knowledge resources, such as WordNet (Fellbaum, 1998) or DIRT (Lin and Pantel, 2001), although knowledge gaps remain a key obstacle (Bos, 2005; Balahur et al., 2008; Bar-Haim et al., 2008). Discourse in previous RTE challenges. The first two rounds of the RTE challenge used “selfcontained” texts and hypotheses, where discourse considerations played virtually no role. A first step towards a more comprehensive notion of entailment was taken with RTE-3 (Giampiccolo et al., 2007), when paragraph-length texts were first included and constituted 17% of the texts in the test set. Chambers et al. (2007) report that in a sample of T − H pairs drawn from the development set, 25% involved discourse references.","Using the concepts introduced above, the impact of discourse references can be generally described as a coverage problem, independent of the system’s architecture. In Example 1, the hypothesis word Oswald cannot be safely linked to the text pronoun he without further knowledge about he; the same is true for ‘Kennedy → President Kennedy’ which involves a specialization that is only warranted in the specific discourse.","A number of systems have tried to address the question of coreference in RTE as a preprocessing step prior to inference proper, with most systems using off-the-shelf coreference resolvers such as JavaRap (Qiu et al., 2004) or OpenNLP3",". Generally, anaphoric expressions were textually replaced by their antecedents. Results were in-conclusive, however, with several reports about errors introduced by automatic coreference resolution (Agichtein et al., 2008; Adams et al., 2007). Specific evaluations of the contribution of coreference resolution yielded both small negative (Bar-Haim et al., 2008) and insignificant positive (Chambers et al., 2007) results."]},{"title":"3 Motivation and Goals","paragraphs":["The results of recent studies, as reported in Section 2.2, seem to show that current resolution of discourse references in RTE systems hardly affects performance. However, our intuition is that these results can be attributed to four major limitations shared by these studies: (1) the datasets, where discourse phenomena were not well repre-3 http://opennlp.sourceforge.net sented; (2) the off-the-shelf coreference resolution systems which may have been not robust enough; (3) the limitation to nominal coreference; and (4) overly simple integration of reference information into the inference engines.","The goal of this paper is to assess the impact of discourse references on entailment with an annotation study which removes these limitations. To counteract (1), we use the recent RTE-5 Search dataset (details below). To avoid (2), we perform a manual analysis, assuming discourse references as predicted by an oracle. With regards to (3), our annotation scheme covers coreference and bridging relations of all syntactic categories and classifies them. As for (4), we suggest several operations necessary to integrate the discourse information into an entailment engine.","In contrast to the numerous existing datasets annotated for discourse references (Hovy et al., 2006; Strassel et al., 2008), we do not annotate exhaustively. Rather, we are interested specifically in those references instances that impact inference. Furthermore, we analyze each instance from an entailment perspective, characterizing the relevant factors that have an impact on inference. To our knowledge, this is the first such in-depth study.4","The results of our study are of twofold interest. First, they provide guidance for the developers of reference resolvers who might prioritize the scope of their systems to make them more valuable for inference. Second, they point out potential direc-tions for the developers of inference systems by specifying what additional inference mechanisms are needed to utilize discourse information. The RTE-5 Search dataset. We base our annotation on the Search task dataset, a new addition to the recent Fifth RTE challenge (Bentivogli et al., 2009a) that is motivated by the needs of NLP applications and drawn from the TAC summarization track. In the Search task, TE systems are required to find all individual sentences in a given corpus which entail the hypothesis – a setting that is sensible not only for summarization, but also for information access tasks like QA. Sentences are judged individually, but “are to be interpreted in the context of the corpus as they rely on explicit and implicit references to entities, events, dates, places, etc., mentioned elsewhere in the corpus” (Bentivogli et al., 2009b). 4 The guidelines and the dataset are available at","http://www.cs.biu.ac.il/ñlp/downloads/ 1211","Text Hypothesis","i T ′ Once the reform becomes law, Spain will join the Netherlands and Belgium in allowing homosexual marriages. Massachusetts allows homosexual","T Such unions are also legal in six Canadian provinces and the northeastern US state of Massachusetts. marriages T ′","The official name of 2003 UB313 has yet to be determined.","ii T Brown said he expected to find a moon orbiting Xena because","many Kuiper Belt objects are paired with moons. 2003 UB313 is in the Kuiper Belt iii T ′ a All seven aboard the AS-28 submarine appeared to be in satisfactory condition, naval spokesman said. T ′ b British crews were working with Russian naval authorities to maneuver the unmanned robotic vehicle and untangle the AS-28. The AS-28 mini submarine was trapped underwater","T The Russian military was racing against time early Friday to rescue a mini submarine trapped on the seabed.","iv T ′ China seeks solutions to its coal mine safety. A mining accident in China has killed","several minersT A recent accident has cost more than a dozen miners their lives. v","T ′′ A remote-controlled device was lowered to the stricken vessel to","cut the cables in which the AS-28 vehicle is caught.","T ′ The mini submarine was resting on the seabed at a depth of about","200 meters. The AS-28 mini submarine was trapped underwater","T Specialists said it could have become tangled up with a metal","cable or in sunken nets from a fishing trawler.","vi T . . . dried up lakes in Siberia, because the permafrost beneath","them has begun to thaw. The ice is melting in the Arctic Table 1: Examples for discourse-dependent entailment in the RTE-5 dataset, where the inference of H depends on reference information from the discourse sentences T ′","/ T ′′",". Referring terms (in T ) and target terms (in H) are shown in boldface."]},{"title":"4 Analysis Scheme","paragraphs":["For annotating the RTE-5 data, we operationalize reference relations that are relevant for entailment as those that improve coverage. Recall from Section 2.2 that the concept of coverage is applicable to both transformation and alignment models, all of which aim at maximizing coverage of H by T .","We represent T and H as syntactic trees, as common in the RTE literature (Zanzotto et al., 2009; Agichtein et al., 2008). Specifically, we assume MINIPAR-style (Lin, 1993) dependency trees where nodes represent text expressions and edges represent the syntactic relations between them. We use “term” to refer to text expressions, and “components” to refer to nodes, edges, and subtrees. Dependency trees are a popular choice in RTE since they offer a fairly semantics-oriented account of the sentence structure that can still be constructed robustly. In an ideal case of entailment, all nodes and dependency edges of H are covered by T .","For each T − H pair, we annotate all relevant discourse references in terms of three items: the target component in H, the focus term in T , and the reference term which stands in a reference relation to the focus term. By resolving this reference, the target component can usually be inferred; sometimes, however, more than one reference term needs to be found. We now define and illustrate these concepts on examples from Table 1.5","The target component is a tree component in H that cannot be covered by the “local” material from T . An example for a tree component is Example (v), where the target component AS-28 mini submarine in H cannot be inferred from the pronoun it in T . Example (vi) demonstrates an edge as target component. In this case, the edge in H connecting melt with the modifier in the Arctic is not found in T . Although each of the hypothesis’ nodes can be covered separately via knowledge-based rules (e.g. ‘Siberia → Arctic’, ‘permafrost → ice’, ‘thaw ↔ melt’), the resulting fragments in T are unconnected without the (intra-sentential) coreference between them and lakes in Siberia.","For each target component, we identify its focus term as the expression in T that does not cover the target component itself but participates in a reference relation that can help covering it.","We follow the focus term’s reference chain to a reference term which can, either separately or in combination with the focus term, help covering the target component. In Example (ii), where the","5","In our annotation, we assume throughout that some knowledge about basic admissible transformations is available, such as passive to active or derivational transformations; for brevity, we ignore articles in the examples and treat named entities as single nodes. 1212 target component in H is 2003 UB313, Xena is the focus term in T and the reference term is a men-tion of 2003 UB313 in a previous sentence, T ′",". In this case, the reference term covers the entire target component on its own.","An additional attribute that we record for each instance is whether resolving the discourse reference is mandatory for determining entailment, or optional. In Example (v), it is mandatory: the inference cannot be completed without the knowledge provided by the discourse. In contrast, in Example (ii), inferring 2003 UB313 from Xena is optional. It can be done either by identify-ing their coreference relation, or by using background knowledge in the form of an entailment rule, ‘Xena ↔ 2003 UB313’, that is applicable in the context of astronomy. Optional discourse references represent instances where discourse information and TE knowledge are interchangeable. As mentioned, knowledge gaps constitute a major obstacle for TE systems, and we cannot rely on the availability of any ceratin piece of knowledge to the inference process. Thus, in our scheme, mandatory references provide a “lower bound” with regards to the necessity to resolve discourse references, even in the presence of complete knowledge; optional references, on the other hand, set an “upper bound” for the contribution of discourse resolution to inference, when no knowledge is available. At the same time, this scheme allows investigating how much TE knowledge can be replaced by (perfect) discourse processing.","When choosing a reference term, we search the reference chain of the focus term for the nearest expression that is identical to the target component or a subcomponent of it. If we find such an expression, covering the identical part of the target component requires no entailment knowledge. If no identical reference term exists, we choose the semantically ‘closest’ term from the reference chain, i.e. the term which requires the least knowledge to infer the target component. For instance, we may pick permafrost as the semantically closet term to the target ice if the latter is not found in the focus term’s reference chain.","Finally, for each reference relation that we annotate, we record four additional attributes which we assumed to be informative in an evaluation. First, the reference type: Is the relation a coreference or a bridging reference? Second, the syntactic type of the focus and reference terms. Third, the focus/reference terms entailment status – does some kind of entailment relation hold between the two terms? Fourth, the operation that should be performed on the focus and reference terms to obtain coverage of the target component (as specified in Section 5)."]},{"title":"5 Integrating Discourse References into Entailment Recognition","paragraphs":["In initial analysis we found that the standard substitution operation applied by virtually all previous studies for integrating coreference into entailment is insufficient. We identified three distinct cases for the integration of discourse reference knowledge in entailment, which correspond to different relations between the target component, the focus term and the reference term. This section describes the three cases and characterizes them in terms of tree transformations. An initial version of these transformations is described in (Abad et al., 2010). We assume a transformation-based entailment architecture (cf. Section 2.2), although we believe that the key points of our account are also applicable to alignment-based architecture. Transformations create revised trees that cover previously uncovered target components in H. The output of each transformation, T1, is comprised of copies of the components used to construct it, and is appended to the discourse forest, which in-cludes the dependency trees of all sentences and their generated consequents.","We assume that we have access to a dependency tree for H, a dependency forest for T and its discourse context, as well as the output of a perfect discourse processor, i.e., a complete set of both coreference and bridging relations, including the type of bridging relation (e.g. part-of, cause).","We use the following notation. We use x, y for tree nodes, and Sx to denote a (sub-)tree with root x. lab(x) is the label of the incoming edge of x (i.e., its grammatical function). We write C(x, y) for a coreference relation between Sx and Sy, the corresponding trees of the focus and reference terms, respectively. We write Br(x, y) for a bridging relation, where r is its type. (1) Substitution: This is the most intuitive and widely-used transformation, corresponding to the treatment of discourse information in existing systems. It applies to coreference relations, when an expression found elsewhere in the text (the reference term) can cover all missing information (the 1213 be legal alsounion such pred modsubj be legal alsomarriages homosexual pred","mod subj mod T T1 marriages homosexual mod T’ pre Figure 1: The Substitution transformation, demonstrated on the relevant subtrees of Example (i). The dashed line denotes a discourse reference. target component) on its own. In such cases, the reference term can replace the entire focus term. Apparently (cf. Section 6), substitution applies also to some types of bridging relations, such as set-membership, when the member is sufficient for representing the entire set for the necessary inference. For example, in “ I met two people yesterday. The woman told me a story.” (Clark, 1975), substituting two people with woman results in a text which is entailed from the discourse, and which allows inferring “ I met a woman yesterday.”","In a parse tree representation, given a coreference relation C(x, y) (or Br(x, y)), the newly generated tree, T1, consists of a copy of T , where the entire tree Sx is replaced by a copy of Sy . In Figure 1, which shows Example (i) from Table 1, such unions is substituted by homosexual marriages.","Head-substitution. Occasionally, substituting only the head of the focus term is sufficient. In such cases, only the root nodes x and y are substituted. This is the case, for example, with synonymous verbs with identical subcategorization frames (like melt and thaw). As verbs typically constitute tree roots in dependency parses, substituting or merging (see below) their entire trees might be inappropriate or wasteful. In such cases, the simpler head-substitution may be applied. (2) Merge: In contrast to substitution, where a match for the entire target component is found elsewhere in the text, this transformation is required when parts of the missing information are scattered among multiple locations in the text. We distinguish between two types of merge transformations: (a) dependent-merge, and (b) head-merge, depending on the syntactic roles of the merged components.","(a) Dependent-Merge. This operation is applicable when the head of either the focus or reference terms (of both) matches the head node of submarine mini on","trapped mod T T1 submarine AS-28 nn T’a pcomp-n","pnmod mod seabed submarine","mini trapped mod pnmod mod AS-28 nn AS-28 T’b on pcomp-n seabed","Figure 2: The dependent-merge (T ′ a) and head-","merge (T ′ b) transformations (Example (iii)). the target component, but modifiers from both of them are required to cover the target component’s dependents. The modifiers are therefore merged as dependents of a single head node, to create a tree that covers the entire target component. Dependent-merge is illustrated in Figure 2, using Example (iii). The component we wish to cover in H is the noun phrase AS-28 mini submarine. Unfortunately, the focus term in T , “ mini submarine trapped on the seabed”, covers only the modifier mini, but not AS-28. This modifier can however be provided by the coreferent term in T ′","a (left upper corner). Once merged, the inference engine can, e.g., employ the rule ‘on seabed → underwater’ to cover H completely.","Formally, assume without loss of generality that y, the reference term’s head, matches the root node of the target component. Given C(x, y), we define T1 as a copy of T , where (i) the subtree Sx is replaced by Sy, and (ii) for all children c of x, a copy of Sc is placed under the copy of y in T1 with its original edge label, lab(c).","(b) Head-merge. An alternative way to recover the missing information in Example (iii) is to find a reference term whose head word itself (rather than one of its modifiers) matches the target component’s missing dependent, as with AS-28 in Figure 2 in the bottom left corner (T ′","b). In terms of parse trees, we need to add one tree as a dependent of the other. Formally, given C(x, y), similarly to dependent-merge, T1 is created as a copy of T where the subtree Sx is replaced by either Sx or Sy, depending on whichever of x and y matches the target component’s head. Assume it is x, for example. Then, a copy of Sy is added as a new child to x. In our sample, head-merge operations correspond to internal coreferences within nominal target components (such as between AS-28 and mini submarine in this case). The appropriate label, lab(y), in these cases is nn (nominal modi-1214 in T T1T’ pcomp-n China cost have than more comp1 pcomp-n","obj have dozen accident subj recent mod cost have than more comp1 pcomp-n","obj have dozen accident subj recent mod mod Solution seek China tomod pcomp-n safety coal mine","nn nn its gen","obj subj Figure 3: The insertion transformation. Dotted edges mark the newly inserted path (Ex. (iv)). fier). Further analysis is required to specify what other dependencies can hold between such coreferring heads. (3) Insertion: The last transformation, insertion, is used when a relation that is realized in H is missing from T and is only implied via a bridging relation. In Example (iv), the location that is explicitly mentioned in H can only be covered by T by resolving a bridging reference with China in T ′",". To connect the bridging referents, a new tree component representing the bridging relation is inserted into the consequent tree T1. In this example, the component connects China and recent accident via the in preposition. Formally, given a bridging relation Br(x, y), we introduce a new subtree Sr","z into T1, where z is a child of x and lab(z) = labr. Sr","z must contain a variable node that is instantiated with a copy of S(y).","This transformation stands out from the others in that it introduces new material. For each bridging relation, it adds a specific subtrees Sr","via an edge labeled with labr. These two items form the dependency representation of the bridging relation Br and must be provided by the interface between the discourse and the inference systems. Clearly, their exact form depends on the set of bridging relations provided by the discourse resolver as well as the details of the dependency parses.","As shown in Figure 3, the bridging relation located-in (r) is represented by inserting a subtree Sr z headed by in (z) into T1 and connecting it to accident (x) as a modifier (labr). The subtree Sr","z consists of a variable node which is connected to in with a pcomp-n dependency (a nominal head of a prepositional phrase), and which is instantiated with the node China (y) when the transformation is applied. Note that the structure of Sr","z and the way it is inserted into T1 are predefined by the abovementioned interface; only the node to which it is attached and the contents of the variable node are determined at transformation-time.","As another example, consider the following short text from (Clark, 1975): John was murdered yesterday. The knife lay nearby. Here, the bridging relation between the murder event and the in-strument, the knife (x), can be addressed by inserting under x a subtree for the clause with which as Sr","z , with a variable which is instantiated by the parse-tree (headed by murdered, y) of the entire first sentenceJohn was murdered yesterday. Transformation chaining. Since our transformations are defined to be minimal, some cases require the application of multiple transformations to achieve coverage. Consider Example (v), Table 1. We wish to cover AS-28 mini submarine in H from the coreferring it in T , mini submarine in T ′","and AS-28 vehicle in T ′′",". A substitution of it by either coreference does not suffice, since none of the antecedents contains all necessary modifiers. It is therefore necessary to substitute it first by one of the coreferences and then merge it with the other."]},{"title":"6 Results","paragraphs":["We analyzed 120 sentence-hypothesis pairs of the RTE-5 development set (21 different hypotheses, 111 distinct sentences, 53 different documents). Below, we summarize our findings, focusing on the relation between our findings and the assump-tions of previous studies as discussed in Section 3. General statistics. We found that 44% of the pairs contained reference relations whose resolution was mandatory for inference. In another 28%, references could optionally support the inference of the hypothesis. In the remaining 28%, references did not contribute towards inference. The total number of relevant references was 137, and 37 pairs (27%) contained multiple relevant references. These numbers support our assumption that discourse references play an important role in inference. Reference types. 73% of the identified references are coreferences and 27% are bridging relations. The most common bridging relation was the location of events (e.g. Arctic in ice melting events), generally assumed to be known throughout the document. Other bridging relations we encountered include cause (e.g. between injured and attack), event participants and set membership. 1215 (%) Pronoun NE NP VP Focus term 9 19 49 23 Reference term - 43 43 14 Table 2: Syntactic types of discourse references (%) Sub. Merge Insertion Coreference 62 38 - Bridging 30 - 70 Total 54 28 18 Table 3: Distribution of transformation types Syntactic types. Table 2 shows that 77% of all focus terms and 86% of the reference terms were nominal phrases, which justifies their prominent position in work on anaphora and coreference resolution. However, almost a quarter of the focus terms were verbal phrases. We found these focus terms to be frequently crucial for entailment since they included the main predicate of the hypothesis.6","This calls for an increased focus on the resolution of event references. Transformations. Table 3 shows the relative frequencies of all transformations. Again, we found that the “default” transformation, substitution, is the most frequent one, and is helpful for both coreference and bridging relations. Substitution is particularly useful for handling pronouns (14% of all substitution instances), the replacement of named entities by synonymous names (32%), the replacement of other NPs (38%), and the substitution of verbal head nodes in event coreference (16%). Yet, in nearly half the cases, a different transformation had to be applied. Insertion accounts for the majority of bridging cases. Head-merge is necessary to integrate proper nouns as modifiers of other head nouns. Dependent-merge, responsible for 85% of the merge transformations, can be used to complete nominal focus terms with missing modifiers (e.g., adjectives), as well as for merging other dependencies between coreferring predicates. This result indicates the importance of incorporating other transformations into inference systems. Distance of reference terms. The distance between the focus and the reference terms varied considerably, ranging from intra-sentential reference relations and up to several dozen sentences. For more than a quarter of the focus terms, we","6","The lower proportion of VPs among reference terms stems from bridging relations between VPs and nominal dependents, such as the abovementioned “location” relation. had to go to other documents to find reference terms that, possibly in conjunction with the focus term, could cover the target components. Interestingly, all such cases involved coreference (about equally divided between the merge transformations and substitutions), while bridging was always “document-local”. This result reaffirms the usefulness of cross-document coreference resolution for inference (Huang et al., 2009). Discourse resolution as preprocessing? In existing RTE systems, discourse references are typically resolved as a preprocessing step. While our annotation was manual and cannot yield direct results about processing considerations, we observed that discourse relations often hold between complex, and deeply embedded, expressions, which makes their automatic resolution difficult. Of course, many RTE systems attempt to normalize and simplify H and T , e.g., by splitting conjunctions or removing irrelevant clauses, but these operations are usually considered a part of the inference rather the preprocessing phase (cf. e.g., Bar-Haim et al. (2007)). Since the resolution of discourse references is likely to profit from these steps, it seems desirable to “postpone” it until after simplification. In transformation-based systems, it might be natural to add discourse-based transformations to the set of inference operations, while in alignment-based systems, discourse references can be integrated into the computation of alignment scores. Discourse references vs. entailment knowledge. We have stated before that even if a discourse reference is not strictly necessary for entailment, it may be interesting because it represents an alternative to the use of knowledge rules to cover the hypothesis. Sometimes, these rules are generally applicable (e.g., ‘Alaska → Arctic’). However, often they are context-specific. Consider the following sentence as T for the hypothesis H: “ The ice is melting in the Arctic”:","(3) T : “ The scene at the receding edge of the Exit Glacier was part festive gathering, part nature tour with an apocalyptic edge.” While it is possible to cover melting using a rule ‘melting ↔ receding’, this rule is only valid under quite specific conditions (e.g., for the subjectice). Instead of determining the applicability of the rule, a discourse-aware system can take the next sen-1216 tence into account, which contains a coreferring event to receding that can cover melting in H:","(4) T ′",": “ . . . people moved closer to the rope line near the glacier as it shied away, practically groaning and melting before their eyes.” Discourse relations can in fact encode arbitrarily complex world knowledge, as in the following pair:","(5) H: “ The serial killer BTK was accused of at least 7 killings starting in the 1970’s.” T: “ Police say BTK may have killed as many as 10 people between 1974 and 1991.” Here, the H modifierserial, which does not occur in T , can be covered either by world knowledge (a person who killed 10 people is a serial killer), or by resolving the coreference of BTK to the term the serial killer BTK which occurs in the discourse around T . Our conclusion is that not only can discourse references often replace world knowledge in principle, in practice it often seems easier to resolve discourse references than to determine whether a rule is applicable in a given context or to formalize complex world knowledge as inference rules. Our annotation provides further empirical support to this claim: An entailment relation exists between the focus and reference terms in 60% of the focus-reference term pairs, and in many of the remainder, entailment holds between the terms’ heads. Thus, discourse provides relations which are many times equivalent to entailment knowledge rules and can therefore be utilized in their stead."]},{"title":"7 Conclusions","paragraphs":["This work has presented an analysis of the relation between discourse references and textual entailment. We have identified a set of limitations common to the handling of discourse relations in virtually all entailment systems. They include the use of off-the-shelf resolvers that concentrate on nominal coreference, the integration of reference information through substitution, and the RTE evaluation schemes, which played down the role of discourse. Since in practical settings, discourse plays an important role, our goal was to develop an agenda for improving the handling of discourse references in entailment-based inference.","Our manual analysis of the RTE-5 dataset shows that while the majority of discourse references that affect inference are nominal coreference relations, another substantial part is made up by verbal terms and bridging relations. Furthermore, we have demonstrated that substitution alone is in-sufficient to extract all relevant information from the wide range of discourse references that are frequently relevant for inference. We identified three general cases, and suggested matching operations to obtain the relevant inferences, formulated as tree transformations. Furthermore, our evidence suggests that for practical reasons, the resolution of discourse references should be tightly integrated into entailment systems instead of treat-ing it as a preprocessing step.","A particularly interesting result concerns the interplay between discourse references and entailment knowledge. While semantic knowledge (e.g., from WordNet or Wikipedia) has been used beneficially for coreference resolution (Soon et al., 2001; Ponzetto and Strube, 2006), reference resolution has, to our knowledge, not yet been employed to validate entailment rules’ applicability. Our analyses suggest that in the context of deciding textual entailment, reference resolution and entailment knowledge can be seen as complementary ways of achieving the same goal, namely enriching T with additional knowledge to allow the inference of H. Given that both of the technologies are still imperfect, we envisage the way for-ward as a joint strategy, where reference resolution and entailment rules mutually fill each other’s gaps (cf. Example 3).","In sum, our study shows that textual entailment can profit substantially from better discourse handling. The next challenge is to translate the the-oretical gain into practical benefit. Our analysis demonstrates that improvements are necessary both on the side of discourse reference resolution systems, which need to cover more types of references, as well as a better integration of discourse information in entailment systems, even for those relations which are within the scope of available resolvers."]},{"title":"Acknowledgements","paragraphs":["This work was partially supported by the PASCAL-2 Network of Excellence of the European Community FP7-ICT-2007-1-216886 and the Israel Science Foundation grant 1112/08. 1217"]},{"title":"References","paragraphs":["Azad Abad, Luisa Bentivogli, Ido Dagan, Danilo Giampiccolo, Shachar Mirkin, Emanuele Pianta, and Asher Stern. 2010. A resource for investigating the impact of anaphora and coreference on inference. In Proceedings of LREC.","Rod Adams, Gabriel Nicolae, Cristina Nicolae, and Sanda Harabagiu. 2007. Textual entailment through extended lexical overlap and lexico-semantic matching. In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing.","E. Agichtein, W. Askew, and Y. Liu. 2008. Combining lexical, syntactic, and semantic evidence for textual entailment classification. InProceedings of TAC.","Nicholas Asher and Alex Lascarides. 1998. Bridging. Journal of Semantics, 15(1):83–113.","Alexandra Balahur, Elena Lloret, Óscar Ferrández, Andrés Montoyo, Manuel Palomar, and Rafael Muñoz. 2008. The DLSIUAES team’s participation in the TAC 2008 tracks. In Proceedings of TAC.","Roy Bar-Haim, Ido Dagan, Iddo Greental, and Eyal Shnarch. 2007. Semantic inference at the lexical-syntactic level. In Proceedings of AAAI.","Roy Bar-Haim, Jonathan Berant, Ido Dagan, Iddo Greental, Shachar Mirkin, and Eyal Shnarch amd Idan Szpektor. 2008. Efficient semantic deduc-tion and approximate matching over compact parse forests. In Proceedings of TAC.","Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo Giampiccolo, and Bernardo Magnini. 2009a. The fifth pascal recognizing textual entailment challenge. In Proceedings of TAC.","Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo Giampiccolo, Medea Lo Leggio, and Bernardo Magnini. 2009b. Considering discourse references in textual entailment annotation. In Proceedings of the 5th International Conference on Generative Approaches to the Lexicon (GL2009).","Johan Bos. 2005. Recognising textual entailment with logical inference. In Proceedings of EMNLP.","Aljoscha Burchardt, Marco Pennacchiotti, Stefan Thater, and Manfred Pinkal. 2009. Assessing the impact of frame semantics on textual entailment. Journal of Natural Language Engineering, 15(4):527–550.","Nathanael Chambers and Dan Jurafsky. 2009. Unsupervised learning of narrative schemas and their participants. In Proceedings of ACL-IJCNLP.","Nathanael Chambers, Daniel Cer, Trond Grenager, David Hall, Chloe Kiddon, Bill MacCartney, Marie-Catherine de Marneffe, Daniel Ramage, Eric Yeh, and Christopher D. Manning. 2007. Learning alignments and leveraging natural logic. In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing.","Herbert H. Clark. 1975. Bridging. In R. C. Schank and B. L. Nash-Webber, editors, Theoretical issues in natural language processing, pages 169–174. Association of Computing Machinery.","Ido Dagan, Oren Glickman, and Bernardo Magnini. 2006. The PASCAL recognising textual entailment challenge. In Machine Learning Challenges, volume 3944 of Lecture Notes in Computer Science, pages 177–190. Springer.","Lorand Dali, Delia Rusu, Blaz Fortuna, Dunja Mladenic, and Marko Grobelnik. 2009. Question answering based on semantic graphs. In Proceedings of the Workshop on Semantic Search (Sem-Search 2009).","Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database (Language, Speech, and Communication). The MIT Press.","Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. 2007. The third pascal recognizing textual entailment challenge. In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing.","Danilo Giampiccolo, Hoa Trang Dang, Bernardo Magnini, Ido Dagan, and Bill Dolan. 2008. The fourth pascal recognizing textual entailment challenge. In Proceedings of TAC.","Ralph Grishman and Beth Sundheim. 1996. Message Understanding Conference-6: a brief history. In Proceedings of the 16th conference on Computational Linguistics.","Sanda Harabagiu, Andrew Hickl, and Finley Lacatusu. 2007. Satisfying information needs with multidocument summaries. Information Processing & Management, 43:1619–1642.","Stefan Harmeling. 2009. Inferring textual entailment with a probabilistically sound calculus. Journal of Natural Language Engineering, pages 459–477.","Marti A. Hearst. 1997. Segmenting text into multiparagraph subtopic passages. Computational Linguistics, 23(1):33–64.","Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance Ramshaw, and Ralph Weischedel. 2006. Ontonotes: The 90% solution. In Proceedings of HLT-NAACL.","Jian Huang, Sarah M. Taylor, Jonathan L. Smith, Konstantinos A. Fotiadis, and C. Lee Giles. 2009. Profile based cross-document coreference using kernelized fuzzy relational clustering. In Proceedings of ACL-IJCNLP.","Fangtao Li, Yang Tang, Minlie Huang, and Xiaoyan Zhu. 2009. Answering opinion questions with random walks on graphs. In Proceedings of ACL-IJCNLP. 1218","Dekang Lin and Patrick Pantel. 2001. Discovery of inference rules for question answering. Natural Language Engineering, 4(7):343–360.","Dekang Lin. 1993. Principle-based parsing without overgeneration. In Proceedings of ACL.","Bill MacCartney, Michel Galley, and Christopher D. Manning. 2008. A phrase-based alignment model for natural language inference. In Proceedings of EMNLP.","Katja Markert, Malvina Nissim, and Natalia N. Modjeska. 2003. Using the web for nominal anaphora resolution. In Proceedings of EACL Workshop on the Computational Treatment of Anaphora.","Seung-Hoon Na and Hwee Tou Ng. 2009. A 2-poisson model for probabilistic coreference of named entities for improved text retrieval. In Proceedings of SIGIR.","Rodney D. Nielsen, Wayne Ward, and James H. Martin. 2009. Recognizing entailment in intelligent tutoring systems. Natural Language Engineering, 15(4):479–501.","Massimo Poesio, Rahul Mehta, Axel Maroudas, and Janet Hitzeman. 2004. Learning to resolve bridging references. In Proceedings of ACL.","Simone Paolo Ponzetto and Michael Strube. 2006. Exploiting semantic role labeling, WordNet and Wikipedia for coreference resolution. In Proceedings of HLT.","Long Qiu, Min-Yen Kan, and Tat-Seng Chua. 2004. A public reference implementation of the rap anaphora resolution algorithm. In Proceedings of LREC.","Lorenza Romano, Milen Kouylekov, Idan Szpektor, Ido Dagan, and Alberto Lavelli. 2006. Investigat-ing a generic paraphrase-based approach for relation extraction. In Proceedings of EACL.","Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong Lim. 2001. A machine learning approach to coreference resolution of noun phrases. Computational Linguistics, 27(4):521–544.","Stephanie Strassel, Mark Przybocki, Kay Peterson, Zhiyi Song, and Kazuaki Maeda. 2008. Linguistic resources and evaluation techniques for evaluation of cross-document automatic content extraction. In Proceedings of LREC.","Jose L. Vicedo and Antonio Ferrndez. 2006. Coreference in Q&A. In Tomek Strzalkowski and Sanda M. Harabagiu, editors, Advances in Open Do-main Question Answering, pages 71–96. Springer.","Fabio Massimo Zanzotto, Marco Pennacchiotti, and Alessandro Moschitti. 2009. A machine learning approach to textual entailment recognition. Journal of Natural Language Engineering, 15(4):551–582.","Dmitry Zelenko, Chinatsu Aone, and Jason Tibbetts. 2004. Coreference resolution for information extraction. In Proceedings of the ACL Workshop on Reference Resolution and its Applications. 1219"]}],"references":[{"authors":[],"source":"Discourse references, notably coreference and bridging, play an important role in many text understanding applications, but their impact on textual entailment is yet to be systematically understood. On the ba-sis of an in-depth analysis of entailment instances, we argue that discourse references have the potential of substantially improving textual entailment recognition, and identify a number of research direc-tions towards this goal."},{"authors":[{"first":"In","middle":"initial analysis we found that the standard substitution operation applied by virtually all previous studies for integrating coreference into entailment is insufficient. We identified three distinct cases for the integration of discourse reference knowledge in","last":"entailment"},{"first":"which","middle":"correspond to different relations between the target","last":"component"},{"first":"the","middle":"focus","last":"term"},{"first":"the","middle":"reference term. This section describes the three","last":"cases"},{"first":"characterizes","middle":"them in terms of tree transformations. An initial version of these transformations is described in","last":"(Abad"},{"last":"al."}],"year":"2010","title":"We assume a transformation-based entailment architecture (cf","source":"In initial analysis we found that the standard substitution operation applied by virtually all previous studies for integrating coreference into entailment is insufficient. We identified three distinct cases for the integration of discourse reference knowledge in entailment, which correspond to different relations between the target component, the focus term and the reference term. This section describes the three cases and characterizes them in terms of tree transformations. An initial version of these transformations is described in (Abad et al., 2010). We assume a transformation-based entailment architecture (cf. Section 2.2), although we believe that the key points of our account are also applicable to alignment-based architecture. Transformations create revised trees that cover previously uncovered target components in H. The output of each transformation, T1, is comprised of copies of the components used to construct it, and is appended to the discourse forest, which in-cludes the dependency trees of all sentences and their generated consequents."},{"authors":[],"source":"We assume that we have access to a dependency tree for H, a dependency forest for T and its discourse context, as well as the output of a perfect discourse processor, i.e., a complete set of both coreference and bridging relations, including the type of bridging relation (e.g. part-of, cause)."},{"authors":[],"source":"We use the following notation. We use x, y for tree nodes, and Sx to denote a (sub-)tree with root x. lab(x) is the label of the incoming edge of x (i.e., its grammatical function). We write C(x, y) for a coreference relation between Sx and Sy, the corresponding trees of the focus and reference terms, respectively. We write Br(x, y) for a bridging relation, where r is its type. (1) Substitution: This is the most intuitive and widely-used transformation, corresponding to the treatment of discourse information in existing systems. It applies to coreference relations, when an expression found elsewhere in the text (the reference term) can cover all missing information (the 1213 be legal alsounion such pred modsubj be legal alsomarriages homosexual pred"},{"authors":[],"source":"mod subj mod T T1 marriages homosexual mod T’ pre Figure 1: The Substitution transformation, demonstrated on the relevant subtrees of Example (i). The dashed line denotes a discourse reference. target component) on its own. In such cases, the reference term can replace the entire focus term. Apparently (cf. Section 6), substitution applies also to some types of bridging relations, such as set-membership, when the member is sufficient for representing the entire set for the necessary inference. For example, in “ I met two people yesterday. The woman told me a story.” (Clark, 1975), substituting two people with woman results in a text which is entailed from the discourse, and which allows inferring “ I met a woman yesterday.”"},{"authors":[],"source":"In a parse tree representation, given a coreference relation C(x, y) (or Br(x, y)), the newly generated tree, T1, consists of a copy of T , where the entire tree Sx is replaced by a copy of Sy . In Figure 1, which shows Example (i) from Table 1, such unions is substituted by homosexual marriages."},{"authors":[],"source":"Head-substitution. Occasionally, substituting only the head of the focus term is sufficient. In such cases, only the root nodes x and y are substituted. This is the case, for example, with synonymous verbs with identical subcategorization frames (like melt and thaw). As verbs typically constitute tree roots in dependency parses, substituting or merging (see below) their entire trees might be inappropriate or wasteful. In such cases, the simpler head-substitution may be applied. (2) Merge: In contrast to substitution, where a match for the entire target component is found elsewhere in the text, this transformation is required when parts of the missing information are scattered among multiple locations in the text. We distinguish between two types of merge transformations: (a) dependent-merge, and (b) head-merge, depending on the syntactic roles of the merged components."},{"authors":[],"source":"(a) Dependent-Merge. This operation is applicable when the head of either the focus or reference terms (of both) matches the head node of submarine mini on"},{"authors":[],"source":"trapped mod T T1 submarine AS-28 nn T’a pcomp-n"},{"authors":[],"source":"pnmod mod seabed submarine"},{"authors":[],"source":"mini trapped mod pnmod mod AS-28 nn AS-28 T’b on pcomp-n seabed"},{"authors":[],"source":"Figure 2: The dependent-merge (T ′ a) and head-"},{"authors":[],"source":"merge (T ′ b) transformations (Example (iii)). the target component, but modifiers from both of them are required to cover the target component’s dependents. The modifiers are therefore merged as dependents of a single head node, to create a tree that covers the entire target component. Dependent-merge is illustrated in Figure 2, using Example (iii). The component we wish to cover in H is the noun phrase AS-28 mini submarine. Unfortunately, the focus term in T , “ mini submarine trapped on the seabed”, covers only the modifier mini, but not AS-28. This modifier can however be provided by the coreferent term in T ′"},{"authors":[],"source":"a (left upper corner). Once merged, the inference engine can, e.g., employ the rule ‘on seabed → underwater’ to cover H completely."},{"authors":[],"source":"Formally, assume without loss of generality that y, the reference term’s head, matches the root node of the target component. Given C(x, y), we define T1 as a copy of T , where (i) the subtree Sx is replaced by Sy, and (ii) for all children c of x, a copy of Sc is placed under the copy of y in T1 with its original edge label, lab(c)."},{"authors":[],"source":"(b) Head-merge. An alternative way to recover the missing information in Example (iii) is to find a reference term whose head word itself (rather than one of its modifiers) matches the target component’s missing dependent, as with AS-28 in Figure 2 in the bottom left corner (T ′"},{"authors":[],"source":"b). In terms of parse trees, we need to add one tree as a dependent of the other. Formally, given C(x, y), similarly to dependent-merge, T1 is created as a copy of T where the subtree Sx is replaced by either Sx or Sy, depending on whichever of x and y matches the target component’s head. Assume it is x, for example. Then, a copy of Sy is added as a new child to x. In our sample, head-merge operations correspond to internal coreferences within nominal target components (such as between AS-28 and mini submarine in this case). The appropriate label, lab(y), in these cases is nn (nominal modi-1214 in T T1T’ pcomp-n China cost have than more comp1 pcomp-n"},{"authors":[],"source":"obj have dozen accident subj recent mod cost have than more comp1 pcomp-n"},{"authors":[],"source":"obj have dozen accident subj recent mod mod Solution seek China tomod pcomp-n safety coal mine"},{"authors":[],"source":"nn nn its gen"},{"authors":[],"source":"obj subj Figure 3: The insertion transformation. Dotted edges mark the newly inserted path (Ex. (iv)). fier). Further analysis is required to specify what other dependencies can hold between such coreferring heads. (3) Insertion: The last transformation, insertion, is used when a relation that is realized in H is missing from T and is only implied via a bridging relation. In Example (iv), the location that is explicitly mentioned in H can only be covered by T by resolving a bridging reference with China in T ′"},{"authors":[],"source":". To connect the bridging referents, a new tree component representing the bridging relation is inserted into the consequent tree T1. In this example, the component connects China and recent accident via the in preposition. Formally, given a bridging relation Br(x, y), we introduce a new subtree Sr"},{"authors":[],"source":"z into T1, where z is a child of x and lab(z) = labr. Sr"},{"authors":[],"source":"z must contain a variable node that is instantiated with a copy of S(y)."},{"authors":[],"source":"This transformation stands out from the others in that it introduces new material. For each bridging relation, it adds a specific subtrees Sr"},{"authors":[],"source":"via an edge labeled with labr. These two items form the dependency representation of the bridging relation Br and must be provided by the interface between the discourse and the inference systems. Clearly, their exact form depends on the set of bridging relations provided by the discourse resolver as well as the details of the dependency parses."},{"authors":[],"source":"As shown in Figure 3, the bridging relation located-in (r) is represented by inserting a subtree Sr z headed by in (z) into T1 and connecting it to accident (x) as a modifier (labr). The subtree Sr"},{"authors":[],"source":"z consists of a variable node which is connected to in with a pcomp-n dependency (a nominal head of a prepositional phrase), and which is instantiated with the node China (y) when the transformation is applied. Note that the structure of Sr"},{"authors":[],"source":"z and the way it is inserted into T1 are predefined by the abovementioned interface; only the node to which it is attached and the contents of the variable node are determined at transformation-time."},{"authors":[],"source":"As another example, consider the following short text from (Clark, 1975): John was murdered yesterday. The knife lay nearby. Here, the bridging relation between the murder event and the in-strument, the knife (x), can be addressed by inserting under x a subtree for the clause with which as Sr"},{"authors":[],"source":"z , with a variable which is instantiated by the parse-tree (headed by murdered, y) of the entire first sentenceJohn was murdered yesterday. Transformation chaining. Since our transformations are defined to be minimal, some cases require the application of multiple transformations to achieve coverage. Consider Example (v), Table 1. We wish to cover AS-28 mini submarine in H from the coreferring it in T , mini submarine in T ′"},{"authors":[],"source":"and AS-28 vehicle in T ′′"},{"authors":[],"source":". A substitution of it by either coreference does not suffice, since none of the antecedents contains all necessary modifiers. It is therefore necessary to substitute it first by one of the coreferences and then merge it with the other."},{"authors":[{"first":"Azad","last":"Abad"},{"first":"Luisa","last":"Bentivogli"},{"first":"Ido","last":"Dagan"},{"first":"Danilo","last":"Giampiccolo"},{"first":"Shachar","last":"Mirkin"},{"first":"Emanuele","last":"Pianta"},{"first":"Asher","last":"Stern"}],"year":"2010","title":"A resource for investigating the impact of anaphora and coreference on inference","source":"Azad Abad, Luisa Bentivogli, Ido Dagan, Danilo Giampiccolo, Shachar Mirkin, Emanuele Pianta, and Asher Stern. 2010. A resource for investigating the impact of anaphora and coreference on inference. In Proceedings of LREC."},{"authors":[{"first":"Rod","last":"Adams"},{"first":"Gabriel","last":"Nicolae"},{"first":"Cristina","last":"Nicolae"},{"first":"Sanda","last":"Harabagiu"}],"year":"2007","title":"Textual entailment through extended lexical overlap and lexico-semantic matching","source":"Rod Adams, Gabriel Nicolae, Cristina Nicolae, and Sanda Harabagiu. 2007. Textual entailment through extended lexical overlap and lexico-semantic matching. In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing."},{"authors":[{"first":"E.","last":"Agichtein"},{"first":"W.","last":"Askew"},{"first":"Y.","last":"Liu"}],"year":"2008","title":"Combining lexical, syntactic, and semantic evidence for textual entailment classification","source":"E. Agichtein, W. Askew, and Y. Liu. 2008. Combining lexical, syntactic, and semantic evidence for textual entailment classification. InProceedings of TAC."},{"authors":[{"first":"Nicholas","last":"Asher"},{"first":"Alex","last":"Lascarides"}],"year":"1998","title":"Bridging","source":"Nicholas Asher and Alex Lascarides. 1998. Bridging. Journal of Semantics, 15(1):83–113."},{"authors":[{"first":"Alexandra","last":"Balahur"},{"first":"Elena","last":"Lloret"},{"first":"Óscar","last":"Ferrández"},{"first":"Andrés","last":"Montoyo"},{"first":"Manuel","last":"Palomar"},{"first":"Rafael","last":"Muñoz"}],"year":"2008","title":"The DLSIUAES team’s participation in the TAC 2008 tracks","source":"Alexandra Balahur, Elena Lloret, Óscar Ferrández, Andrés Montoyo, Manuel Palomar, and Rafael Muñoz. 2008. The DLSIUAES team’s participation in the TAC 2008 tracks. In Proceedings of TAC."},{"authors":[{"first":"Roy","last":"Bar-Haim"},{"first":"Ido","last":"Dagan"},{"first":"Iddo","last":"Greental"},{"first":"Eyal","last":"Shnarch"}],"year":"2007","title":"Semantic inference at the lexical-syntactic level","source":"Roy Bar-Haim, Ido Dagan, Iddo Greental, and Eyal Shnarch. 2007. Semantic inference at the lexical-syntactic level. In Proceedings of AAAI."},{"authors":[{"first":"Roy","last":"Bar-Haim"},{"first":"Jonathan","last":"Berant"},{"first":"Ido","last":"Dagan"},{"first":"Iddo","last":"Greental"},{"first":"Shachar","last":"Mirkin"},{"first":"Eyal","middle":"Shnarch amd Idan","last":"Szpektor"}],"year":"2008","title":"Efficient semantic deduc-tion and approximate matching over compact parse forests","source":"Roy Bar-Haim, Jonathan Berant, Ido Dagan, Iddo Greental, Shachar Mirkin, and Eyal Shnarch amd Idan Szpektor. 2008. Efficient semantic deduc-tion and approximate matching over compact parse forests. In Proceedings of TAC."},{"authors":[{"first":"Luisa","last":"Bentivogli"},{"first":"Ido","last":"Dagan"},{"first":"Hoa","middle":"Trang","last":"Dang"},{"first":"Danilo","last":"Giampiccolo"},{"first":"Bernardo","last":"Magnini"}],"year":"2009a","title":"The fifth pascal recognizing textual entailment challenge","source":"Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo Giampiccolo, and Bernardo Magnini. 2009a. The fifth pascal recognizing textual entailment challenge. In Proceedings of TAC."},{"authors":[{"first":"Luisa","last":"Bentivogli"},{"first":"Ido","last":"Dagan"},{"first":"Hoa","middle":"Trang","last":"Dang"},{"first":"Danilo","last":"Giampiccolo"},{"first":"Medea","middle":"Lo","last":"Leggio"},{"first":"Bernardo","last":"Magnini"}],"year":"2009b","title":"Considering discourse references in textual entailment annotation","source":"Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo Giampiccolo, Medea Lo Leggio, and Bernardo Magnini. 2009b. Considering discourse references in textual entailment annotation. In Proceedings of the 5th International Conference on Generative Approaches to the Lexicon (GL2009)."},{"authors":[{"first":"Johan","last":"Bos"}],"year":"2005","title":"Recognising textual entailment with logical inference","source":"Johan Bos. 2005. Recognising textual entailment with logical inference. In Proceedings of EMNLP."},{"authors":[{"first":"Aljoscha","last":"Burchardt"},{"first":"Marco","last":"Pennacchiotti"},{"first":"Stefan","last":"Thater"},{"first":"Manfred","last":"Pinkal"}],"year":"2009","title":"Assessing the impact of frame semantics on textual entailment","source":"Aljoscha Burchardt, Marco Pennacchiotti, Stefan Thater, and Manfred Pinkal. 2009. Assessing the impact of frame semantics on textual entailment. Journal of Natural Language Engineering, 15(4):527–550."},{"authors":[{"first":"Nathanael","last":"Chambers"},{"first":"Dan","last":"Jurafsky"}],"year":"2009","title":"Unsupervised learning of narrative schemas and their participants","source":"Nathanael Chambers and Dan Jurafsky. 2009. Unsupervised learning of narrative schemas and their participants. In Proceedings of ACL-IJCNLP."},{"authors":[{"first":"Nathanael","last":"Chambers"},{"first":"Daniel","last":"Cer"},{"first":"Trond","last":"Grenager"},{"first":"David","last":"Hall"},{"first":"Chloe","last":"Kiddon"},{"first":"Bill","last":"MacCartney"},{"first":"Marie-Catherine","last":"de Marneffe"},{"first":"Daniel","last":"Ramage"},{"first":"Eric","last":"Yeh"},{"first":"Christopher","middle":"D.","last":"Manning"}],"year":"2007","title":"Learning alignments and leveraging natural logic","source":"Nathanael Chambers, Daniel Cer, Trond Grenager, David Hall, Chloe Kiddon, Bill MacCartney, Marie-Catherine de Marneffe, Daniel Ramage, Eric Yeh, and Christopher D. Manning. 2007. Learning alignments and leveraging natural logic. In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing."},{"authors":[{"first":"Herbert","middle":"H.","last":"Clark"}],"year":"1975","title":"Bridging","source":"Herbert H. Clark. 1975. Bridging. In R. C. Schank and B. L. Nash-Webber, editors, Theoretical issues in natural language processing, pages 169–174. Association of Computing Machinery."},{"authors":[{"first":"Ido","last":"Dagan"},{"first":"Oren","last":"Glickman"},{"first":"Bernardo","last":"Magnini"}],"year":"2006","title":"The PASCAL recognising textual entailment challenge","source":"Ido Dagan, Oren Glickman, and Bernardo Magnini. 2006. The PASCAL recognising textual entailment challenge. In Machine Learning Challenges, volume 3944 of Lecture Notes in Computer Science, pages 177–190. Springer."},{"authors":[{"first":"Lorand","last":"Dali"},{"first":"Delia","last":"Rusu"},{"first":"Blaz","last":"Fortuna"},{"first":"Dunja","last":"Mladenic"},{"first":"Marko","last":"Grobelnik"}],"year":"2009","title":"Question answering based on semantic graphs","source":"Lorand Dali, Delia Rusu, Blaz Fortuna, Dunja Mladenic, and Marko Grobelnik. 2009. Question answering based on semantic graphs. In Proceedings of the Workshop on Semantic Search (Sem-Search 2009)."},{"authors":[{"first":"Christiane","last":"Fellbaum"},{"last":"editor"}],"year":"1998","title":"WordNet: An Electronic Lexical Database (Language, Speech, and Communication)","source":"Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database (Language, Speech, and Communication). The MIT Press."},{"authors":[{"first":"Danilo","last":"Giampiccolo"},{"first":"Bernardo","last":"Magnini"},{"first":"Ido","last":"Dagan"},{"first":"Bill","last":"Dolan"}],"year":"2007","title":"The third pascal recognizing textual entailment challenge","source":"Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. 2007. The third pascal recognizing textual entailment challenge. In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing."},{"authors":[{"first":"Danilo","last":"Giampiccolo"},{"first":"Hoa","middle":"Trang","last":"Dang"},{"first":"Bernardo","last":"Magnini"},{"first":"Ido","last":"Dagan"},{"first":"Bill","last":"Dolan"}],"year":"2008","title":"The fourth pascal recognizing textual entailment challenge","source":"Danilo Giampiccolo, Hoa Trang Dang, Bernardo Magnini, Ido Dagan, and Bill Dolan. 2008. The fourth pascal recognizing textual entailment challenge. In Proceedings of TAC."},{"authors":[{"first":"Ralph","last":"Grishman"},{"first":"Beth","last":"Sundheim"}],"year":"1996","title":"Message Understanding Conference-6: a brief history","source":"Ralph Grishman and Beth Sundheim. 1996. Message Understanding Conference-6: a brief history. In Proceedings of the 16th conference on Computational Linguistics."},{"authors":[{"first":"Sanda","last":"Harabagiu"},{"first":"Andrew","last":"Hickl"},{"first":"Finley","last":"Lacatusu"}],"year":"2007","title":"Satisfying information needs with multidocument summaries","source":"Sanda Harabagiu, Andrew Hickl, and Finley Lacatusu. 2007. Satisfying information needs with multidocument summaries. Information Processing & Management, 43:1619–1642."},{"authors":[{"first":"Stefan","last":"Harmeling"}],"year":"2009","title":"Inferring textual entailment with a probabilistically sound calculus","source":"Stefan Harmeling. 2009. Inferring textual entailment with a probabilistically sound calculus. Journal of Natural Language Engineering, pages 459–477."},{"authors":[{"first":"Marti","middle":"A.","last":"Hearst"}],"year":"1997","title":"Segmenting text into multiparagraph subtopic passages","source":"Marti A. Hearst. 1997. Segmenting text into multiparagraph subtopic passages. Computational Linguistics, 23(1):33–64."},{"authors":[{"first":"Eduard","last":"Hovy"},{"first":"Mitchell","last":"Marcus"},{"first":"Martha","last":"Palmer"},{"first":"Lance","last":"Ramshaw"},{"first":"Ralph","last":"Weischedel"}],"year":"2006","title":"Ontonotes: The 90% solution","source":"Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance Ramshaw, and Ralph Weischedel. 2006. Ontonotes: The 90% solution. In Proceedings of HLT-NAACL."},{"authors":[{"first":"Jian","last":"Huang"},{"first":"Sarah","middle":"M.","last":"Taylor"},{"first":"Jonathan","middle":"L.","last":"Smith"},{"first":"Konstantinos","middle":"A.","last":"Fotiadis"},{"first":"C.","middle":"Lee","last":"Giles"}],"year":"2009","title":"Profile based cross-document coreference using kernelized fuzzy relational clustering","source":"Jian Huang, Sarah M. Taylor, Jonathan L. Smith, Konstantinos A. Fotiadis, and C. Lee Giles. 2009. Profile based cross-document coreference using kernelized fuzzy relational clustering. In Proceedings of ACL-IJCNLP."},{"authors":[{"first":"Fangtao","last":"Li"},{"first":"Yang","last":"Tang"},{"first":"Minlie","last":"Huang"},{"first":"Xiaoyan","last":"Zhu"}],"year":"2009","title":"Answering opinion questions with random walks on graphs","source":"Fangtao Li, Yang Tang, Minlie Huang, and Xiaoyan Zhu. 2009. Answering opinion questions with random walks on graphs. In Proceedings of ACL-IJCNLP. 1218"},{"authors":[{"first":"Dekang","last":"Lin"},{"first":"Patrick","last":"Pantel"}],"year":"2001","title":"Discovery of inference rules for question answering","source":"Dekang Lin and Patrick Pantel. 2001. Discovery of inference rules for question answering. Natural Language Engineering, 4(7):343–360."},{"authors":[{"first":"Dekang","last":"Lin"}],"year":"1993","title":"Principle-based parsing without overgeneration","source":"Dekang Lin. 1993. Principle-based parsing without overgeneration. In Proceedings of ACL."},{"authors":[{"first":"Bill","last":"MacCartney"},{"first":"Michel","last":"Galley"},{"first":"Christopher","middle":"D.","last":"Manning"}],"year":"2008","title":"A phrase-based alignment model for natural language inference","source":"Bill MacCartney, Michel Galley, and Christopher D. Manning. 2008. A phrase-based alignment model for natural language inference. In Proceedings of EMNLP."},{"authors":[{"first":"Katja","last":"Markert"},{"first":"Malvina","last":"Nissim"},{"first":"Natalia","middle":"N.","last":"Modjeska"}],"year":"2003","title":"Using the web for nominal anaphora resolution","source":"Katja Markert, Malvina Nissim, and Natalia N. Modjeska. 2003. Using the web for nominal anaphora resolution. In Proceedings of EACL Workshop on the Computational Treatment of Anaphora."},{"authors":[{"first":"Seung-Hoon","last":"Na"},{"first":"Hwee","middle":"Tou","last":"Ng"}],"year":"2009","title":"A 2-poisson model for probabilistic coreference of named entities for improved text retrieval","source":"Seung-Hoon Na and Hwee Tou Ng. 2009. A 2-poisson model for probabilistic coreference of named entities for improved text retrieval. In Proceedings of SIGIR."},{"authors":[{"first":"Rodney","middle":"D.","last":"Nielsen"},{"first":"Wayne","last":"Ward"},{"first":"James","middle":"H.","last":"Martin"}],"year":"2009","title":"Recognizing entailment in intelligent tutoring systems","source":"Rodney D. Nielsen, Wayne Ward, and James H. Martin. 2009. Recognizing entailment in intelligent tutoring systems. Natural Language Engineering, 15(4):479–501."},{"authors":[{"first":"Massimo","last":"Poesio"},{"first":"Rahul","last":"Mehta"},{"first":"Axel","last":"Maroudas"},{"first":"Janet","last":"Hitzeman"}],"year":"2004","title":"Learning to resolve bridging references","source":"Massimo Poesio, Rahul Mehta, Axel Maroudas, and Janet Hitzeman. 2004. Learning to resolve bridging references. In Proceedings of ACL."},{"authors":[{"first":"Simone","middle":"Paolo","last":"Ponzetto"},{"first":"Michael","last":"Strube"}],"year":"2006","title":"Exploiting semantic role labeling, WordNet and Wikipedia for coreference resolution","source":"Simone Paolo Ponzetto and Michael Strube. 2006. Exploiting semantic role labeling, WordNet and Wikipedia for coreference resolution. In Proceedings of HLT."},{"authors":[{"first":"Long","last":"Qiu"},{"first":"Min-Yen","last":"Kan"},{"first":"Tat-Seng","last":"Chua"}],"year":"2004","title":"A public reference implementation of the rap anaphora resolution algorithm","source":"Long Qiu, Min-Yen Kan, and Tat-Seng Chua. 2004. A public reference implementation of the rap anaphora resolution algorithm. In Proceedings of LREC."},{"authors":[{"first":"Lorenza","last":"Romano"},{"first":"Milen","last":"Kouylekov"},{"first":"Idan","last":"Szpektor"},{"first":"Ido","last":"Dagan"},{"first":"Alberto","last":"Lavelli"}],"year":"2006","title":"Investigat-ing a generic paraphrase-based approach for relation extraction","source":"Lorenza Romano, Milen Kouylekov, Idan Szpektor, Ido Dagan, and Alberto Lavelli. 2006. Investigat-ing a generic paraphrase-based approach for relation extraction. In Proceedings of EACL."},{"authors":[{"first":"Wee","middle":"Meng","last":"Soon"},{"first":"Hwee","middle":"Tou","last":"Ng"},{"first":"Daniel","middle":"Chung Yong","last":"Lim"}],"year":"2001","title":"A machine learning approach to coreference resolution of noun phrases","source":"Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong Lim. 2001. A machine learning approach to coreference resolution of noun phrases. Computational Linguistics, 27(4):521–544."},{"authors":[{"first":"Stephanie","last":"Strassel"},{"first":"Mark","last":"Przybocki"},{"first":"Kay","last":"Peterson"},{"first":"Zhiyi","last":"Song"},{"first":"Kazuaki","last":"Maeda"}],"year":"2008","title":"Linguistic resources and evaluation techniques for evaluation of cross-document automatic content extraction","source":"Stephanie Strassel, Mark Przybocki, Kay Peterson, Zhiyi Song, and Kazuaki Maeda. 2008. Linguistic resources and evaluation techniques for evaluation of cross-document automatic content extraction. In Proceedings of LREC."},{"authors":[{"first":"Jose","middle":"L.","last":"Vicedo"},{"first":"Antonio","last":"Ferrndez"}],"year":"2006","title":"Coreference in Q&A","source":"Jose L. Vicedo and Antonio Ferrndez. 2006. Coreference in Q&A. In Tomek Strzalkowski and Sanda M. Harabagiu, editors, Advances in Open Do-main Question Answering, pages 71–96. Springer."},{"authors":[{"first":"Fabio","middle":"Massimo","last":"Zanzotto"},{"first":"Marco","last":"Pennacchiotti"},{"first":"Alessandro","last":"Moschitti"}],"year":"2009","title":"A machine learning approach to textual entailment recognition","source":"Fabio Massimo Zanzotto, Marco Pennacchiotti, and Alessandro Moschitti. 2009. A machine learning approach to textual entailment recognition. Journal of Natural Language Engineering, 15(4):551–582."},{"authors":[{"first":"Dmitry","last":"Zelenko"},{"first":"Chinatsu","last":"Aone"},{"first":"Jason","last":"Tibbetts"}],"year":"2004","title":"Coreference resolution for information extraction","source":"Dmitry Zelenko, Chinatsu Aone, and Jason Tibbetts. 2004. Coreference resolution for information extraction. In Proceedings of the ACL Workshop on Reference Resolution and its Applications. 1219"}],"cites":[{"style":0,"text":"Dagan et al., 2006","origin":{"pointer":"/sections/1/paragraphs/0","offset":657,"length":18},"authors":[{"last":"Dagan"},{"last":"al."}],"year":"2006","references":["/references/47"]},{"style":0,"text":"Bentivogli et al., 2009a","origin":{"pointer":"/sections/1/paragraphs/3","offset":74,"length":24},"authors":[{"last":"Bentivogli"},{"last":"al."}],"year":"2009a","references":["/references/40"]},{"style":0,"text":"Hearst, 1997","origin":{"pointer":"/sections/2/paragraphs/0","offset":142,"length":12},"authors":[{"last":"Hearst"}],"year":"1997","references":["/references/55"]},{"style":0,"text":"Vicedo and Ferrndez, 2006","origin":{"pointer":"/sections/2/paragraphs/0","offset":338,"length":25},"authors":[{"last":"Vicedo"},{"last":"Ferrndez"}],"year":"2006","references":["/references/71"]},{"style":0,"text":"Zelenko et al., 2004","origin":{"pointer":"/sections/2/paragraphs/0","offset":365,"length":20},"authors":[{"last":"Zelenko"},{"last":"al."}],"year":"2004","references":["/references/73"]},{"style":0,"text":"Na and Ng, 2009","origin":{"pointer":"/sections/2/paragraphs/0","offset":387,"length":15},"authors":[{"last":"Na"},{"last":"Ng"}],"year":"2009","references":["/references/63"]},{"style":0,"text":"Chambers and Jurafsky, 2009","origin":{"pointer":"/sections/2/paragraphs/0","offset":486,"length":27},"authors":[{"last":"Chambers"},{"last":"Jurafsky"}],"year":"2009","references":["/references/44"]},{"style":0,"text":"Grishman and Sundheim, 1996","origin":{"pointer":"/sections/2/paragraphs/0","offset":619,"length":27},"authors":[{"last":"Grishman"},{"last":"Sundheim"}],"year":"1996","references":["/references/52"]},{"style":0,"text":"Strassel et al., 2008","origin":{"pointer":"/sections/2/paragraphs/0","offset":694,"length":21},"authors":[{"last":"Strassel"},{"last":"al."}],"year":"2008","references":["/references/70"]},{"style":0,"text":"Asher and Lascarides, 1998","origin":{"pointer":"/sections/2/paragraphs/2","offset":111,"length":26},"authors":[{"last":"Asher"},{"last":"Lascarides"}],"year":"1998","references":["/references/36"]},{"style":0,"text":"Clark, 1975","origin":{"pointer":"/sections/2/paragraphs/2","offset":560,"length":11},"authors":[{"last":"Clark"}],"year":"1975","references":["/references/46"]},{"style":0,"text":"Li et al., 2009","origin":{"pointer":"/sections/2/paragraphs/3","offset":139,"length":15},"authors":[{"last":"Li"},{"last":"al."}],"year":"2009","references":["/references/58"]},{"style":0,"text":"Dali et al., 2009","origin":{"pointer":"/sections/2/paragraphs/3","offset":156,"length":17},"authors":[{"last":"Dali"},{"last":"al."}],"year":"2009","references":["/references/48"]},{"style":0,"text":"Romano et al., 2006","origin":{"pointer":"/sections/2/paragraphs/4","offset":342,"length":19},"authors":[{"last":"Romano"},{"last":"al."}],"year":"2006","references":["/references/68"]},{"style":0,"text":"Harabagiu et al., 2007","origin":{"pointer":"/sections/2/paragraphs/4","offset":363,"length":22},"authors":[{"last":"Harabagiu"},{"last":"al."}],"year":"2007","references":["/references/53"]},{"style":0,"text":"Nielsen et al., 2009","origin":{"pointer":"/sections/2/paragraphs/4","offset":387,"length":20},"authors":[{"last":"Nielsen"},{"last":"al."}],"year":"2009","references":["/references/64"]},{"style":0,"text":"Giampiccolo et al., 2007","origin":{"pointer":"/sections/2/paragraphs/4","offset":490,"length":24},"authors":[{"last":"Giampiccolo"},{"last":"al."}],"year":"2007","references":["/references/50"]},{"style":0,"text":"Giampiccolo et al., 2008","origin":{"pointer":"/sections/2/paragraphs/4","offset":516,"length":24},"authors":[{"last":"Giampiccolo"},{"last":"al."}],"year":"2008","references":["/references/51"]},{"style":0,"text":"Bar-Haim et al., 2008","origin":{"pointer":"/sections/2/paragraphs/4","offset":854,"length":21},"authors":[{"last":"Bar-Haim"},{"last":"al."}],"year":"2008","references":["/references/39"]},{"style":0,"text":"Harmeling, 2009","origin":{"pointer":"/sections/2/paragraphs/4","offset":877,"length":15},"authors":[{"last":"Harmeling"}],"year":"2009","references":["/references/54"]},{"style":0,"text":"Zanzotto et al., 2009","origin":{"pointer":"/sections/2/paragraphs/4","offset":1294,"length":21},"authors":[{"last":"Zanzotto"},{"last":"al."}],"year":"2009","references":["/references/72"]},{"style":0,"text":"Burchardt et al., 2009","origin":{"pointer":"/sections/2/paragraphs/4","offset":1317,"length":22},"authors":[{"last":"Burchardt"},{"last":"al."}],"year":"2009","references":["/references/43"]},{"style":0,"text":"MacCartney et al., 2008","origin":{"pointer":"/sections/2/paragraphs/4","offset":1341,"length":23},"authors":[{"last":"MacCartney"},{"last":"al."}],"year":"2008","references":["/references/61"]},{"style":0,"text":"Markert et al., 2003","origin":{"pointer":"/sections/2/paragraphs/8","offset":20,"length":20},"authors":[{"last":"Markert"},{"last":"al."}],"year":"2003","references":["/references/62"]},{"style":0,"text":"Poesio et al., 2004","origin":{"pointer":"/sections/2/paragraphs/8","offset":42,"length":19},"authors":[{"last":"Poesio"},{"last":"al."}],"year":"2004","references":["/references/65"]},{"style":0,"text":"Fellbaum, 1998","origin":{"pointer":"/sections/2/paragraphs/10","offset":236,"length":14},"authors":[{"last":"Fellbaum"}],"year":"1998","references":[]},{"style":0,"text":"Lin and Pantel, 2001","origin":{"pointer":"/sections/2/paragraphs/10","offset":261,"length":20},"authors":[{"last":"Lin"},{"last":"Pantel"}],"year":"2001","references":["/references/59"]},{"style":0,"text":"Bos, 2005","origin":{"pointer":"/sections/2/paragraphs/10","offset":331,"length":9},"authors":[{"last":"Bos"}],"year":"2005","references":["/references/42"]},{"style":0,"text":"Balahur et al., 2008","origin":{"pointer":"/sections/2/paragraphs/10","offset":342,"length":20},"authors":[{"last":"Balahur"},{"last":"al."}],"year":"2008","references":["/references/37"]},{"style":0,"text":"Bar-Haim et al., 2008","origin":{"pointer":"/sections/2/paragraphs/10","offset":364,"length":21},"authors":[{"last":"Bar-Haim"},{"last":"al."}],"year":"2008","references":["/references/39"]},{"style":0,"text":"Giampiccolo et al., 2007","origin":{"pointer":"/sections/2/paragraphs/10","offset":653,"length":24},"authors":[{"last":"Giampiccolo"},{"last":"al."}],"year":"2007","references":["/references/50"]},{"style":0,"text":"Chambers et al. (2007)","origin":{"pointer":"/sections/2/paragraphs/10","offset":778,"length":22},"authors":[{"last":"Chambers"},{"last":"al."}],"year":"2007","references":["/references/45"]},{"style":0,"text":"Qiu et al., 2004","origin":{"pointer":"/sections/2/paragraphs/12","offset":205,"length":16},"authors":[{"last":"Qiu"},{"last":"al."}],"year":"2004","references":["/references/67"]},{"style":0,"text":"Agichtein et al., 2008","origin":{"pointer":"/sections/2/paragraphs/13","offset":200,"length":22},"authors":[{"last":"Agichtein"},{"last":"al."}],"year":"2008","references":["/references/35"]},{"style":0,"text":"Adams et al., 2007","origin":{"pointer":"/sections/2/paragraphs/13","offset":224,"length":18},"authors":[{"last":"Adams"},{"last":"al."}],"year":"2007","references":["/references/34"]},{"style":0,"text":"Bar-Haim et al., 2008","origin":{"pointer":"/sections/2/paragraphs/13","offset":341,"length":21},"authors":[{"last":"Bar-Haim"},{"last":"al."}],"year":"2008","references":["/references/39"]},{"style":0,"text":"Chambers et al., 2007","origin":{"pointer":"/sections/2/paragraphs/13","offset":392,"length":21},"authors":[{"last":"Chambers"},{"last":"al."}],"year":"2007","references":["/references/45"]},{"style":0,"text":"Hovy et al., 2006","origin":{"pointer":"/sections/3/paragraphs/2","offset":82,"length":17},"authors":[{"last":"Hovy"},{"last":"al."}],"year":"2006","references":["/references/56"]},{"style":0,"text":"Strassel et al., 2008","origin":{"pointer":"/sections/3/paragraphs/2","offset":101,"length":21},"authors":[{"last":"Strassel"},{"last":"al."}],"year":"2008","references":["/references/70"]},{"style":0,"text":"Bentivogli et al., 2009a","origin":{"pointer":"/sections/3/paragraphs/3","offset":519,"length":24},"authors":[{"last":"Bentivogli"},{"last":"al."}],"year":"2009a","references":["/references/40"]},{"style":0,"text":"Bentivogli et al., 2009b","origin":{"pointer":"/sections/3/paragraphs/3","offset":1085,"length":24},"authors":[{"last":"Bentivogli"},{"last":"al."}],"year":"2009b","references":["/references/41"]},{"style":0,"text":"Zanzotto et al., 2009","origin":{"pointer":"/sections/4/paragraphs/1","offset":74,"length":21},"authors":[{"last":"Zanzotto"},{"last":"al."}],"year":"2009","references":["/references/72"]},{"style":0,"text":"Agichtein et al., 2008","origin":{"pointer":"/sections/4/paragraphs/1","offset":97,"length":22},"authors":[{"last":"Agichtein"},{"last":"al."}],"year":"2008","references":["/references/35"]},{"style":0,"text":"Lin, 1993","origin":{"pointer":"/sections/4/paragraphs/1","offset":161,"length":9},"authors":[{"last":"Lin"}],"year":"1993","references":["/references/60"]},{"style":0,"text":"Huang et al., 2009","origin":{"pointer":"/sections/5/paragraphs/3","offset":568,"length":18},"authors":[{"last":"Huang"},{"last":"al."}],"year":"2009","references":["/references/57"]},{"style":0,"text":"Bar-Haim et al. (2007)","origin":{"pointer":"/sections/5/paragraphs/3","offset":1214,"length":22},"authors":[{"last":"Bar-Haim"},{"last":"al."}],"year":"2007","references":["/references/38"]},{"style":0,"text":"Soon et al., 2001","origin":{"pointer":"/sections/6/paragraphs/2","offset":226,"length":17},"authors":[{"last":"Soon"},{"last":"al."}],"year":"2001","references":["/references/69"]},{"style":0,"text":"Ponzetto and Strube, 2006","origin":{"pointer":"/sections/6/paragraphs/2","offset":245,"length":25},"authors":[{"last":"Ponzetto"},{"last":"Strube"}],"year":"2006","references":["/references/66"]}]}
