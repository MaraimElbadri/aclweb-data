{"sections":[{"title":"","paragraphs":["Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 886–896, Uppsala, Sweden, 11-16 July 2010. c⃝2010 Association for Computational Linguistics"]},{"title":"Estimating Strictly Piecewise Distributions Jeffrey Heinz University of Delaware Newark, Delaware, USA heinz@udel.edu James Rogers Earlham College Richmond, Indiana, USA jrogers@quark.cs.earlham.edu Abstract","paragraphs":["Strictly Piecewise (SP) languages are a subclass of regular languages which encode certain kinds of long-distance dependencies that are found in natural languages. Like the classes in the Chomsky and Subregular hierarchies, there are many independently converging characterizations of the SP class (Rogers et al., to appear). Here we define SP distributions and show that they can be efficiently estimated from positive data."]},{"title":"1 Introduction","paragraphs":["Long-distance dependencies in natural language are of considerable interest. Although much at-tention has focused on long-distance dependencies which are beyond the expressive power of models with finitely many states (Chomsky, 1956; Joshi, 1985; Shieber, 1985; Kobele, 2006), there are some long-distance dependencies in natural language which permit finite-state characterizations. For example, although it is well-known that vowel and consonantal harmony applies across any arbitrary number of intervening segments (Ringen, 1988; Baković, 2000; Hansson, 2001; Rose and Walker, 2004) and that phonological patterns are regular (Johnson, 1972; Kaplan and Kay, 1994), it is less well-known that harmony patterns are largely characterizable by the Strictly Piecewise languages, a subregular class of languages with independently-motivated, converging characterizations (see Heinz (2007, to appear) and especially Rogers et al. (2009)).","As shown by Rogers et al. (to appear), the Strictly Piecewise (SP) languages, which make distinctions on the basis of (potentially) discontiguous subsequences, are precisely analogous to the Strictly Local (SL) languages (McNaughton and Papert, 1971; Rogers and Pullum, to appear), which make distinctions on the basis of contiguous subsequences. The Strictly Local languages are the formal-language theoretic foundation for n-gram models (Garcia et al., 1990), which are widely used in natural language processing (NLP) in part because such distributions can be estimated from positive data (i.e. a corpus) (Jurafsky and Martin, 2008). N -gram models describe probability distributions over all strings on the basis of the Markov assumption (Markov, 1913): that the probability of the next symbol only depends on the previous contiguous sequence of length n − 1. From the perspective of formal language theory, these distributions are perhaps properly called Strictly k-Local distributions (SLk) where k = n. It is well-known that one limitation of the Markov assumption is its inability to express any kind of long-distance dependency.","This paper defines Strictly k-Piecewise (SPk) distributions and shows how they too can be efficiently estimated from positive data. In contrast with the Markov assumption, our assumption is that the probability of the next symbol is conditioned on the previous set of discontiguous subsequences of length k − 1 in the string. While this suggests the model has too many parameters (one for each subset of all possible subsequences), in fact the model has on the order of |Σ|k+1","parameters because of an independence assumption: there is no interaction between different subsequences. As a result, SP distributions are efficiently computable even though they condition the probability of the next symbol on the occurrences of earlier (possibly very distant) discontiguous subsequences. Essentially, these SP distributions reflect a kind of long-term memory.","On the other hand, SP models have no shortterm memory and are unable to make distinctions on the basis of contiguous subsequences. We do not intend SP models to replace n-gram models, but instead expect them to be used alongside of 886 them. Exactly how this is to be done is beyond the scope of this paper and is left for future research.","Since SP languages are the analogue of SL languages, which are the formal-language theoretical foundation for n-gram models, which are widely used in NLP, it is expected that SP distributions and their estimation will also find wide application. Apart from their interest to problems in theoretical phonology such as phonotactic learning (Coleman and Pierrehumbert, 1997; Hayes and Wilson, 2008; Heinz, to appear), it is expected that their use will have application, in conjunction with n-gram models, in areas that currently use them; e.g. augmentative communication (Newell et al., 1998), part of speech tagging (Brill, 1995), and speech recognition (Jelenik, 1997).","§2 provides basic mathematical notation. §3 provides relevant background on the subregular hierarchy. §4 describes automata-theoretic characterizations of SP languages. §5 defines SP distributions. §6 shows how these distributions can be efficiently estimated from positive data and provides a demonstration. §7 concludes the paper."]},{"title":"2 Preliminaries","paragraphs":["We start with some mostly standard notation. Σ denotes a finite set of symbols and a string over Σ is a finite sequence of symbols drawn from that set. Σk",", Σ≤k",", Σ≥k",", and Σ∗","denote all strings over this alphabet of length k, of length less than or equal to k, of length greater than or equal to k, and of any finite length, respectively. ε denotes the empty string. |w| denotes the length of string w. The prefixes of a string w are Pfx(w) = {v : ∃u ∈ Σ∗","such that vu = w}. When discussing partial functions, the notation ↑ and ↓ indicates that the function is undefined, respectively is defined, for particular arguments.","A language L is a subset of Σ∗",". A stochastic language D is a probability distribution over Σ∗",". The probability p of word w with respect to D is written P rD(w) = p. Recall that all distributions D must satisfy ∑","w∈Σ∗ P rD(w) = 1. If L is lan-","guage then P rD(L) =","∑","w∈L P rD(w).","A Deterministic Finite-state Automaton (DFA) is a tuple M = ⟨Q, Σ, q0, δ, F ⟩ where Q is the state set, Σ is the alphabet, q0 is the start state, δ is a deterministic transition function with do-main Q × Σ and codomain Q, F is the set of accepting states. Let d̂ : Q × Σ∗","→ Q be the (partial) path function of M, i.e., d̂(q, w) is the (unique) state reachable from state q via the sequence w, if any, or d̂(q, w)↑ otherwise. The language recognized by a DFA M is L(M) def = {w ∈ Σ∗","| d̂(q 0, w)↓ ∈ F }. A state is useful iff for all q ∈ Q, there exists","w ∈ Σ∗","such that δ(q","0, w) = q and there exists w ∈ Σ∗","such that δ(q, w) ∈ F . Useless states are not useful. DFAs without useless states are trimmed.","Two strings w and v over Σ are distinguished by a DFA M iff d̂(q0, w) ̸= d̂(q0, v). They are Nerode equivalent with respect to a language L if and only if wu ∈ L ⇐⇒ vu ∈ L for all u ∈ Σ∗",". All DFAs which recognize L must distinguish strings which are inequivalent in this sense, but no DFA recognizing L necessarily distinguishes any strings which are equivalent. Hence the number of equivalence classes of strings over Σ modulo Nerode equivalence with respect to L gives a (tight) lower bound on the number of states required to recognize L.","A DFA is minimal if the size of its state set is minimal among DFAs accepting the same language. The product of n DFAs M1 . . . Mn is given by the standard construction over the state space Q1 × . . . × Qn (Hopcroft et al., 2001).","A Probabilistic Deterministic Finite-state Automaton (PDFA) is a tuple M = ⟨Q, Σ, q0, δ, F, T ⟩ where Q is the state set, Σ is the alphabet, q0 is the start state, δ is a deterministic transition function, F and T are the final-state and transition probabilities. In particular, T : Q × Σ → R+","and F : Q → R+ such that for all q ∈ Q, F (q) + ∑ a∈Σ T (q, a) = 1. (1) Like DFAs, for all w ∈ Σ∗",", there is at most one state reachable from q0. PDFAs are typically represented as labeled directed graphs as in Figure 1.","A PDFA M generates a stochastic language DM. If it exists, the (unique) path for a word w = a0 . . . ak belonging to Σ∗","through a PDFA is a sequence ⟨(q0, a0), (q1, a1), . . . , (qk, ak)⟩, where qi+1 = δ(qi, ai). The probability a PDFA assigns to w is obtained by multiplying the transition probabilities with the final probability along w’s path if 887 A:2/10 b:2/10c:3/10 B:4/9 a:3/10 a:2/9b:2/9c:1/9 Figure 1: A picture of a PDFA with states labeled A and B. The probabilities of T and F are located to the right of the colon. it exists, and zero otherwise. P rDM(w) =","( k ∏ i=1 T (qi−1, ai−1) ) · F (qk+1) (2) if d̂(q0, w)↓ and 0 otherwise A probability distribution is regular deterministic iff there is a PDFA which generates it.","The structural components of a PDFA M are its states Q, its alphabet Σ, its transitions δ, and its initial state q0. By structure of a PDFA, we mean its structural components. Each PDFA M defines a family of distributions given by the possible instantiations of T and F satisfying Equation 1. These distributions have |Q|· (|Σ| + 1) in-dependent parameters (since for each state there are |Σ| possible transitions plus the possibility of finality.)","We define the product of PDFA in terms of co-emission probabilities (Vidal et al., 2005a). Definition 1 Let A be a vector of PDFAs and let |A| = n. For each 1 ≤ i ≤ n let Mi = ⟨Qi, Σ, q0i, δi, Fi, Ti⟩ be the ith PDFA in A. The probability that σ is co-emitted from q1, . . . , qn in Q1, . . . , Qn, respectively, is CT (⟨σ, q1 . . . qn⟩) = n ∏ i=1 Ti(qi, σ). Similarly, the probability that a word simultaneously ends at q1 ∈ Q1 . . . qn ∈ Qn is CF (⟨q1 . . . qn⟩) = n ∏ i=1 Fi(qi). Then ⊗ A = ⟨Q, Σ, q0, δ, F, T ⟩ where 1. Q, q0, and δ are defined as with DFA product.","2. For all ⟨q1 . . . qn⟩ ∈ Q, let Z(⟨q1 . . . qn⟩) = CF (⟨q1 . . . qn⟩) + ∑ σ∈Σ CT (⟨σ, q1 . . . qn⟩) be the normalization term; and (a) let F (⟨q1 . . . qn⟩) = CF (⟨q1 ... qn⟩) Z(⟨q1 ... qn⟩) ; and","(b) for all σ ∈ Σ, let T (⟨q1 . . . qn⟩, σ) =","CT (⟨σ, q1 ... qn⟩) Z(⟨q1 ... qn⟩) In other words, the numerators of T and F are defined to be the co-emission probabilities (Vidal et al., 2005a), and division by Z ensures that M defines a well-formed probability distribution. Statistically speaking, the co-emission product makes an independence assumption: the probability of σ being co-emitted from q1, . . . , qn is exactly what one expects if there is no interaction between the individual factors; that is, between the probabilities of σ being emitted from any qi. Also note order of product is irrelevant up to renaming of the states, and so therefore we also speak of tak-ing the product of a set of PDFAs (as opposed to an ordered vector).","Estimating regular deterministic distributions is well-studied problem (Vidal et al., 2005a; Vidal et al., 2005b; de la Higuera, in press). We limit discussion to cases when the structure of the PDFA is known. Let S be a finite sample of words drawn from a regular deterministic distribution D. The problem is to estimate parameters T and F of M so that DM approaches D. We employ the widely-adopted maximum likelihood (ML) criterion for this estimation.","( T̂ , F̂ ) = argmax T,F ( ∏ w∈S P rM(w) ) (3) It is well-known that if D is generated by some PDFA M′","with the same structural components as M, then optimizing the ML estimate guarantees that DM approaches D as the size of S goes to infinity (Vidal et al., 2005a; Vidal et al., 2005b; de la Higuera, in press).","The optimization problem (3) is simple for deterministic automata with known structural components. Informally, the corpus is passed through the PDFA, and the paths of each word through the corpus are tracked to obtain counts, which are then normalized by state. Let M = ⟨Q, Σ, δ, q0, F, T ⟩ be the PDFA whose parameters F and T are to be estimated. For all states q ∈ Q and symbols a ∈ Σ, The ML estimation of the probability of T (q, a) is obtained by dividing the number of times this transition is used in parsing the sample S by the 888 A:2 b:2c:3 B:4 a:3 a:2b:2c:1 Figure 2: The automata shows the counts obtained by parsing M with sample S = {ab, bba, ε, cab, acb, cc}. SL SP LT PT LTT SF FO Reg MSO Prop +1 < Figure 3: Parallel Sub-regular Hierarchies. number of times state q is encountered in the parsing of S. Similarly, the ML estimation of F (q) is obtained by calculating the relative frequency of state q being final with state q being encountered in the parsing of S. For both cases, the division is normalizing; i.e. it guarantees that there is a well-formed probability distribution at each state. Figure 2 illustrates the counts obtained for a machine M with sample S = {ab, bba, ε, cab, acb, cc}.1 Figure 1 shows the PDFA obtained after normaliz-ing these counts."]},{"title":"3 Subregular Hierarchies","paragraphs":["Within the class of regular languages there are dual hierarchies of language classes (Figure 3), one in which languages are defined in terms of their contiguous substrings (up to some length k, known as k-factors), starting with the languages that are Locally Testable in the Strict Sense (SL), and one in which languages are defined in terms of their not necessarily contiguous subsequences, starting with the languages that are Piecewise 1","Technically, this acceptor is neither a simple DFA or PDFA; rather, it has been called a Frequency DFA. We do not formally define them here, see (de la Higuera, in press). Testable in the Strict Sense (SP). Each language class in these hierarchies has independently motivated, converging characterizations and each has been claimed to correspond to specific, fundamental cognitive capabilities (McNaughton and Papert, 1971; Brzozowski and Simon, 1973; Simon, 1975; Thomas, 1982; Perrin and Pin, 1986; Garcı́a and Ruiz, 1990; Beauquier and Pin, 1991; Straub-ing, 1994; Garcı́a and Ruiz, 1996; Rogers and Pullum, to appear; Kontorovich et al., 2008; Rogers et al., to appear).","Languages in the weakest of these classes are defined only in terms of the set of factors (SL) or subsequences (SP) which are licensed to occur in the string (equivalently the complement of that set with respect to Σ≤k",", the forbidden factors or forbidden subsequences). For example, the set containing the forbidden 2-factors {ab, ba} defines a Strictly 2-Local language which includes all strings except those with contiguous substrings {ab, ba}. Similarly since the parameters of n-gram models (Jurafsky and Martin, 2008) assign probabilities to symbols given the preceding contiguous substrings up to length n − 1, we say they describe Strictly n-Local distributions.","These hierarchies have a very attractive model-theoretic characterization. The Locally Testable (LT) and Piecewise Testable languages are exactly those that are definable by propositional formulae in which the atomic formulae are blocks of symbols interpreted factors (LT) or subsequences (PT) of the string. The languages that are testable in the strict sense (SL and SP) are exactly those that are definable by formulae of this sort restricted to conjunctions of negative literals. Going the other way, the languages that are definable by First-Order formulae with adjacency (successor) but not precedence (less-than) are exactly the Locally Threshold Testable (LTT) languages. The Star-Free languages are those that are First-Order definable with precedence alone (adjacency being FO definable from precedence). Finally, by extending to Monadic Second-Order formulae (with either signature, since they are MSO definable from each other), one obtains the full class of Regular languages (McNaughton and Papert, 1971; Thomas, 1982; Rogers and Pullum, to appear; Rogers et al., to appear).","The relation between strings which is fundamental along the Piecewise branch is the subse-889","quence relation, which is a partial order on Σ∗ : w ⊑ v def ⇐⇒ w = ε or w = σ1 · · · σn and","(∃w0, . . . , wn ∈ Σ∗",")[v = w0σ1w1 · · · σnwn].","in which case we say w is a subsequence of v. For w ∈ Σ∗",", let Pk(w) def = {v ∈ Σk","| v ⊑ w} and P≤k(w) def = {v ∈ Σ≤k","| v ⊑ w}, the set of subsequences of length k, respectively length no greater than k, of w. Let Pk(L) and P≤k(L) be the natural extensions of these to sets of strings. Note that P0(w) = {ε}, for all w ∈ Σ∗",", that P1(w) is the set of symbols occurring in w and that P≤k(L) is finite, for all L ⊆ Σ∗",".","Similar to the Strictly Local languages, Strictly Piecewise languages are defined only in terms of the set of subsequences (up to some length k) which are licensed to occur in the string. Definition 2 (SPk Grammar, SP) A SPk grammar is a pair G = ⟨Σ, G⟩ where G ⊆ Σk",". The language licensed by a SPk grammar is L(G) def = {w ∈ Σ∗","| P≤k(w) ⊆ P≤k(G)}. A language is SPk iff it is L(G) for some SPk grammar G. It is SP iff it is SPk for some k.","This paper is primarily concerned with estimating Strictly Piecewise distributions, but first we examine in greater detail properties of SP languages, in particular DFA representations."]},{"title":"4 DFA representations of SP Languages","paragraphs":["Following Sakarovitch and Simon (1983), Lothaire (1997) and Kontorovich, et al. (2008), we call the set of strings that contain w as a subsequence the principal shuffle ideal2","of w:","SI(w) = {v ∈ Σ∗ | w ⊑ v}. The shuffle ideal of a set of strings is defined as SI(S) = ∪w∈SSI(w) Rogers et al. (to appear) establish that the SP languages have a variety of characteristic properties. Theorem 1 The following are equivalent:3","2","Properly SI(w) is the principal ideal generated by {w} wrt the inverse of ⊑.","3","For a complete proof, see Rogers et al. (to appear). We only note that 5 implies 1 by DeMorgan’s theorem and the fact that every shuffle ideal is finitely generated (see also Lothaire (1997)). 1 bc 2 a bc Figure 4: The DFA representation of SI(aa). 1. L =","⋂ w∈S[SI(w)], S finite, 2. L ∈ SP 3. (∃k)[P≤k(w) ⊆ P≤k(L) ⇒ w ∈ L],","4. w ∈ L and v ⊑ w ⇒ v ∈ L (L is subsequence closed),","5. L = SI(X), X ⊆ Σ∗","(L is the complement of a shuffle ideal). The DFA representation of the complement of a shuffle ideal is especially important. Lemma 1 Let w ∈ Σk",", w = σ","1 · · · σk, and MSI(w) = ⟨Q, Σ, q0, δ, F ⟩, where Q = {i | 1 ≤ i ≤ k}, q0 = 1, F = Q and for all qi ∈ Q, σ ∈ Σ: δ(qi, σ) =    qi+1 if σ = σi and i < k, ↑ if σ = σi and i = k, qi otherwise. Then MSI(w) is a minimal, trimmed DFA that recognizes the complement of SI(w), i.e., SI(w) = L(MSI(w)).","Figure 4 illustrates the DFA representation of the complement of SI(aa) with Σ = {a, b, c}. It is easy to verify that the machine in Figure 4 accepts all and only those words which do not contain an aa subsequence.","For any SPk language L = L(⟨Σ, G⟩) ̸= Σ∗",", the first characterization (1) in Theorem 1 above yields a non-deterministic finite-state representation of L, which is a set A of DFA representations of complements of principal shuffle ideals of the elements of G. The trimmed automata product of this set yields a DFA, with the properties below (Rogers et al., to appear). Lemma 2 Let M be a trimmed DFA recognizing a SPk language constructed as described above. Then: 1. All states of M are accepting states: F = Q. 890 a b c b c b a c a b b c b b a b ε ε,a ε,b ε,c ε,a,b ε,b,c ε,a,c ε,a,b,c Figure 5: The DFA representation of the of the SP language given by G = ⟨{a, b, c}, {aa, bc}⟩. Names of the states reflect subsets of subsequences up to length 1 of prefixes of the language. Note this DFA is trimmed, but not minimal.","2. For all q1, q2 ∈ Q and σ ∈ Σ, if d̂(q1, σ)↑ and d̂(q1, w) = q2 for some w ∈ Σ∗","then d̂(q2, σ)↑. (Missing edges propagate down.)","Figure 5 illustrates with the DFA representation of the of the SP2 language given by G = ⟨{a, b, c}, {aa, bc}⟩. It is straightforward to verify that this DFA is identical (modulo relabeling of state names) to one obtained by the trimmed product of the DFA representations of the complement of the principal shuffle ideals of aa and bc, which are the prohibited subsequences.","States in the DFA in Figure 5 correspond to the subsequences up to length 1 of the prefixes of the language. With this in mind, it follows that the DFA of Σ∗","= L(Σ, Σk",") has states which correspond to the subsequences up to length k − 1 of the prefixes of Σ∗",". Figure 6 illustrates such a DFA when k = 2 and Σ = {a, b, c}.","In fact, these DFAs reveal the differences between SP languages and PT languages: they are exactly those expressed in Lemma 2. Within the state space defined by the subsequences up to length k − 1 of the prefixes of the language, if the conditions in Lemma 2 are violated, then the DFAs describe languages that are PT but not SP. Pictorially, P T2 languages are obtained by arbitrarily removing arcs, states, and the finality of states from the DFA in Figure 6, and SP2 ones are obtained by non-arbitrarily removing them in accordance with Lemma 2. The same applies straightforwardly for any k (see Definition 3 below). a b c a b c b a c c a b ab c ac b bc a abc ε ε,a ε,b ε,c ε,a,b ε,b,c ε,a,c ε,a,b,c Figure 6: A DFA representation of the of the SP2 language given by G = ⟨{a, b, c}, Σ2","⟩. Names of the states reflect subsets of subsequences up to length 1 of prefixes of the language. Note this DFA is trimmed, but not minimal."]},{"title":"5 SP Distributions","paragraphs":["In the same way that SL distributions (n-gram models) generalize SL languages, SP distributions generalize SP languages. Recall that SP languages are characterizable by the intersection of the complements of principal shuffle ideals. SP distributions are similarly characterized.","We begin with Piecewise-Testable distributions. Definition 3 A distribution D is k-Piecewise Testable (written D ∈ PTDk) def ⇐⇒ D can be de-","scribed by a PDFA M = ⟨Q, Σ, q0, δ, F, T ⟩ with","1. Q = {P≤k−1(w) : w ∈ Σ∗ } 2. q0 = P≤k−1(ε)","3. For all w ∈ Σ∗","and all σ ∈ Σ, δ(P≤k−1(w), a) = P≤k−1(wa) 4. F and T satisfy Equation 1. In other words, a distribution is k-Piecewise Testable provided it can be represented by a PDFA whose structural components are the same (modulo renaming of states) as those of the DFA discussed earlier where states corresponded to the subsequences up to length k − 1 of the prefixes of the language. The DFA in Figure 6 shows the 891 structure of a PDFA which describes a PT2 distribution as long as the assigned probabilities satisfy Equation 1.","The following lemma follows directly from the finite-state representation of PTk distributions. Lemma 3 Let D belong to PTDk and let M = ⟨Q, Σ, q0, δ, F, T ⟩ be a PDFA representing D defined according to Definition 3.","P rD(σ1 . . . σn) = T (P≤k−1(ε), σ1) ·   ∏ 2≤i≤n T (P≤k−1(σ1 . . . σi−1), σi)   (4) · F (P≤k−1(w)) PTk distributions have 2|Σ|k−1 (|Σ|+1) parameters (since there are 2|Σ|k−1","states and |Σ| + 1 possible events, i.e. transitions and finality).","Let P r(σ | #) and P r(# | P≤k(w)) denote the probability (according to some D ∈ PTDk) that a word begins with σ and ends after observ-ing P≤k(w). Then Equation 4 can be rewritten in terms of conditional probability as","P rD(σ1 . . . σn) = P r(σ1 | #) ·   ∏ 2≤i≤n P r(σi | P≤k−1(σ1 . . . σi−1))  (5) · P r(# | P≤k−1(w)) Thus, the probability assigned to a word depends not on the observed contiguous sequences as in a Markov model, but on observed subsequences.","Like SP languages, SP distributions can be defined in terms of the product of machines very similar to the complement of principal shuffle ideals. Definition 4 Let w ∈ Σk−1","and w = σ","1 · · · σk−1. Mw = ⟨Q, Σ, q0, δ, F, T ⟩ is a w-subsequence-distinguishing PDFA (w-SD-PDFA) iff Q = Pfx(w), q0 = ε, for all u ∈ Pfx(w) and each σ ∈ Σ,","δ(u, σ) = uσ iff uσ ∈ Pfx(w) and u otherwise and F and T satisfy Equation 1.","Figure 7 shows the structure of Ma which is almost the same as the complement of the principal shuffle ideal in Figure 4. The only difference is the additional self-loop labeled a on the right-most state labeled a. Ma defines a family of distributions over Σ∗",", and its states distinguish those bc a a abc ε Figure 7: The structure of PDFA Ma. It is the same (modulo state names) as the DFA in Figure 4 except for the self-loop labeled a on state a. strings which contain a (state a) from those that do not (state ε). A set of PDFAs is a k-set of SD-PDFAs iff, for each w ∈ Σ≤k−1",", it contains exactly one w-SD-PDFA.","In the same way that missing edges propagate down in DFA representations of SP languages (Lemma 2), the final and transitional probabilities must propagate down in PDFA representations of SPk distributions. In other words, the final and transitional probabilities at states further along paths beginning at the start state must be de-termined by final and transitional probabilities at earlier states non-increasingly. This is captured by defining SP distributions as a product of k-sets of SD-PDFAs (see Definition 5 below).","While the standard product based on co-emission probability could be used for this purpose, we adopt a modified version of it defined for k-sets of SD-PDFAs: the positive co-emission probability. The automata product based on the positive co-emission probability not only ensures that the probabilities propagate as necessary, but also that such probabilities are made on the basis of observed subsequences, and not unobserved ones. This idea is familiar from n-gram models: the probability of σn given the immediately preceding sequence σ1 . . . σn−1 does not depend on the probability of σn given the other (n − 1)-long sequences which do not immediately precede it, though this is a logical possibility.","Let A be a k-set of SD-PDFAs. For each w ∈ Σ≤k−1",", let Mw = ⟨Qw, Σ, q0w, δw, Fw, Tw⟩ be the w-subsequence-distinguishing PDFA in A. The positive co-emission probability that σ is simultaneously emitted from states qε, . . . , qu from the statesets Qε, . . . Qu, respectively, of each SD-892 PDFA in A is P CT (⟨σ, qε . . . qu⟩) = ∏","qw∈⟨qε...qu⟩ qw=w Tw(qw, σ) (6) Similarly, the probability that a word simultaneously ends at n states qε ∈ Qε, . . . , qu ∈ Qu is P CF (⟨qε . . . qu⟩) = ∏","qw∈⟨qε...qu⟩ qw=w Fw(qw) (7) In other words, the positive co-emission probability is the product of the probabilities restricted to those assigned to the maximal states in each Mw. For example, consider a 2-set of SD-PDFAs A with Σ = {a, b, c}. A contains four PDFAs Mε, Ma, Mb, Mc. Consider state q = ⟨ε, ε, b, c⟩ ∈ ⊗","A (this is the state labeled ε, b, c in Figure 6). Then CT (a, q) = Tε(ε, a)· Ta(ε, a)· Tb(b, a)· Tc(c, a) but P CT (a, q) = Tε(ε, a)· Tb(b, a)· Tc(c, a) since in PDFA Ma, the state ε is not the maximal state.","The positive co-emission product (⊗+",") is defined just as with co-emission probabilities, substituting PCT and PCF for CT and CF, respectively, in Definition 1. The definition of ⊗+","ensures that the probabilities propagate on the basis of observed subsequences, and not on the basis of unobserved ones. Lemma 4 Let k ≥ 1 and let A be a k-set of SD-PDFAs. Then ⊗+","S defines a well-formed probability distribution over Σ∗",". Proof Since Mε belongs to A, it is always the case that PCT and PCF are defined. Wellformedness follows from the normalization term as in Definition 1. ⊣⊣⊣ Definition 5 A distribution D is k-Strictly Piecewise (written D ∈ SPDk)","def","⇐⇒ D can be described by a PDFA which is the positive co-emission product of a k-set of subsequence-distinguishing PDFAs. By Lemma 4, SP distributions are well-formed. Unlike PDFAs for PT distributions, which distinguish 2|Σ|k−1","states, the number of states in a k-set of SD-PDFAs is","∑ i<k(i + 1)|Σ|i",", which is","Θ(|Σ|k+1 ). Furthermore, since each SD-PDFA","only has one state contributing |Σ|+1 probabilities","to the product, and since there are |Σ≤k","| = |Σ|k","−1","|Σ|−1 many SD-PDFAs in a k-set, there are |Σ|k","− 1 |Σ| − 1 · (|Σ| + 1) =","|Σ|k+1 + |Σ|k","− |Σ| − 1 |Σ| − 1","parameters, which is Θ(|Σ|k ). Lemma 5 Let D ∈ SPDk. Then D ∈ PTDk. Proof Since D ∈ SPDk, there is a k-set of subsequence-distinguishing PDFAs. The product of this set has the same structure as the PDFA given in Definition 3. ⊣⊣⊣ Theorem 2 A distribution D ∈ SPDk if D can be described by a PDFA M = ⟨Q, Σ, q0, δ, F, T ⟩ satisfying Definition 3 and the following.","For all w ∈ Σ∗","and all σ ∈ Σ, let Z(w) = ∏ s∈P≤k−1(w) F (P≤k−1(s)) + ∑ σ′ ∈Σ   ∏ s∈P≤k−1(w)","T (P≤k−1(s), σ′ )   (8)","(This is the normalization term.) Then T must sat-","isfy: T (P≤k−1(w), σ) =","∏","s∈P≤k−1(w) T (P≤k−1(s), σ) Z(w) (9)","and F must satisfy: F (P≤k−1(w)) = ∏","s∈P≤k−1(w) F (P≤k−1(s)) Z(w) (10) Proof That SPDk satisfies Definition 3 Follows directly from Lemma 5. Equations 8-10 follow from the definition of positive co-emission probability. ⊣⊣⊣","The way in which final and transitional probabilities propagate down in SP distributions is reflected in the conditional probability as defined by Equations 9 and 10. In terms of conditional probability, Equations 9 and 10 mean that the probability that σi follows a sequence σ1 . . . σi−1 is not only a function of P≤k−1(σ1 . . . σi−1) (Equation 4) but further that it is a function of each subsequence in σ1 . . . σi−1 up to length k − 1. 893 In particular, P r(σi | P≤k−1(σ1 . . . σi−1)) is obtained by substituting P r(σi | P≤ k−1(s)) for T (P≤ k−1(s), σ) and P r(# | P≤ k−1(s)) for F (P≤k−1(s)) in Equations 8, 9 and 10. For example, for a SP2 distribution, the probability of a given P≤1(bc) (state ε, b, c in Figure 6) is the normalized product of the probabilities of a given P≤1(ε), a given P≤1(b), and a given P≤1(c).","To summarize, SP and PT distributions are regular deterministic. Unlike PT distributions, how-ever, SP distributions can be modeled with only Θ(|Σ|k",") parameters and Θ(|Σ|k+1",") states. This is true even though SP distributions distinguish 2|Σ|k−1","states! Since SP distributions can be represented by a single PDFA, computing P r(w) occurs in only Θ(|w|) for such PDFA. While such PDFA might be too large to be practical, P r(w) can also be computed from the k-set of SD-PDFAs in Θ(|w|k",") (essentially building the path in the product machine on the fly using Equations 4, 8, 9 and 10)."]},{"title":"6 Estimating SP Distributions","paragraphs":["The problem of ML estimation of SPk distributions is reduced to estimating the parameters of the SD-PDFAs. Training (counting and normalization) occurs over each of these machines (i.e. each machine parses the entire corpus), which gives the ML estimates of the parameters of the distribution. It trivially follows that this training successfully estimates any D ∈ SPDk. Theorem 3 For any D ∈ SPDk, let D generate sample S. Let A be the k-set of SD-PDFAs which describes exactly D. Then optimizing the MLE of S with respect to each M ∈ A guarantees that the distribution described by the positive co-emission product of ⊗+ A approaches D as |S| increases. Proof The MLE estimate of S with respect to SPDk returns the parameter values that maximize the likelihood of S. The parameters of D ∈ SPDk are found on the maximal states of each M ∈ A. By definition, each M ∈ A describes a probability distribution over Σ∗",", and similarly defines a family of distributions. Therefore finding the MLE of S with respect to SPDk means finding the MLE estimate of S with respect to each of the family of distributions which each M ∈ A defines, respectively.","Optimizing the ML estimate of S for each M ∈ A means that as |S| increases, the estimates T̂M and F̂M approach the true values TM and FM. It follows that as |S| increases, T̂N+ A and F̂N+ A approach the true values of TN+ A and FN+ A and consequently DN+ A approaches D. ⊣⊣⊣","We demonstrate learning long-distance dependencies by estimating SP2 distributions given a corpus from Samala (Chumash), a language with sibilant harmony.4","There are two classes of sibilants in Samala: [-anterior] sibilants like [s] and [> ts] and [+anterior] sibilants like [S] and [>","tS].5 Samala words are subject to a phonological process wherein the last sibilant requires earlier sibilants to have the same value for the feature [an-terior], no matter how many sounds intervene (Applegate, 1972). As a consequence of this rule, there are generally no words in Samala where [-anterior] sibilants follow [+anterior]. E.g. [StojonowonowaS] ‘it stood upright’ (Applegate 1972:72) is licit but not *[Stojonowonowas].","The results of estimating D ∈ SPD2 with the corpus is shown in Table 6. The results clearly demonstrate the effectiveness of the model: the probability of a [α anterior] sibilant given P≤1([-α anterior]) sounds is orders of magnitude less than given P≤1(α anterior]) sounds. x P r(x | P≤1(y)) s > ts S",">","tS s 0.0335 0.0051 0.0011 0.0002 ts 0.0218 0.0113 0.0009 0.","y S 0.0009 0. 0.0671 0.0353 > tS 0.0006 0. 0.0455 0.0313 Table 1: Results of SP2 estimation on the Samala corpus. Only sibilants are shown."]},{"title":"7 Conclusion","paragraphs":["SP distributions are the stochastic version of SP languages, which model long-distance dependencies. Although SP distributions distinguish 2|Σ|k−1 states, they do so with tractably many parameters and states because of an assumption that distinct subsequences do not interact. As shown, these distributions are efficiently estimable from positive data. As previously mentioned, we anticipate these models to find wide application in NLP. 4 The corpus was kindly provided by Dr. Richard Apple-","gate and drawn from his 2007 dictionary of Samala. 5 Samala actually contrasts glottalized, aspirated, and","plain variants of these sounds (Applegate, 1972). These la-","ryngeal distinctions are collapsed here for easier exposition. 894"]},{"title":"References","paragraphs":["R.B. Applegate. 1972. Ineseño Chumash Grammar. Ph.D. thesis, University of California, Berkeley.","R.B. Applegate. 2007. Samala-English dictionary : a guide to the Samala language of the Ineseño Chumash People. Santa Ynez Band of Chumash Indians.","Eric Baković. 2000. Harmony, Dominance and Control. Ph.D. thesis, Rutgers University.","D. Beauquier and Jean-Eric Pin. 1991. Languages and scanners. Theoretical Computer Science, 84:3–21.","Eric Brill. 1995. Transformation-based error-driven learning and natural language processing: A case study in part-of-speech tagging. Computational Linguistics, 21(4):543–566.","J. A. Brzozowski and Imre Simon. 1973. Characterizations of locally testable events. Discrete Mathematics, 4:243–271.","Noam Chomsky. 1956. Three models for the descrip-tion of language. IRE Transactions on Information Theory. IT-2.","J. S. Coleman and J. Pierrehumbert. 1997. Stochastic phonological grammars and acceptability. In Computational Phonology, pages 49–56. Somerset, NJ: Association for Computational Linguistics. Third Meeting of the ACL Special Interest Group in Computational Phonology.","Colin de la Higuera. in press. Grammatical Inference: Learning Automata and Grammars. Cambridge University Press.","Pedro Garcı́a and José Ruiz. 1990. Inference of k-testable languages in the strict sense and applica-tions to syntactic pattern recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 9:920–925.","Pedro Garcı́a and José Ruiz. 1996. Learning k-piecewise testable languages from positive data. In Laurent Miclet and Colin de la Higuera, editors, Grammatical Interference: Learning Syntax from Sentences, volume 1147 of Lecture Notes in Computer Science, pages 203–210. Springer.","Pedro Garcia, Enrique Vidal, and José Oncina. 1990. Learning locally testable languages in the strict sense. In Proceedings of the Workshop on Algorithmic Learning Theory, pages 325–338.","Gunnar Hansson. 2001. Theoretical and typological issues in consonant harmony. Ph.D. thesis, University of California, Berkeley.","Bruce Hayes and Colin Wilson. 2008. A maximum entropy model of phonotactics and phonotactic learning. Linguistic Inquiry, 39:379–440.","Jeffrey Heinz. 2007. The Inductive Learning of Phonotactic Patterns. Ph.D. thesis, University of California, Los Angeles.","Jeffrey Heinz. to appear. Learning long distance phonotactics. Linguistic Inquiry.","John Hopcroft, Rajeev Motwani, and Jeffrey Ullman. 2001. Introduction to Automata Theory, Languages, and Computation. Addison-Wesley.","Frederick Jelenik. 1997. Statistical Methods for Speech Recognition. MIT Press.","C. Douglas Johnson. 1972. Formal Aspects of Phonological Description. The Hague: Mouton.","A. K. Joshi. 1985. Tree-adjoining grammars: How much context sensitivity is required to provide reasonable structural descriptions? In D. Dowty, L. Karttunen, and A. Zwicky, editors, Natural Language Parsing, pages 206–250. Cambridge University Press.","Daniel Jurafsky and James Martin. 2008. Speech and Language Processing: An Introduction to Natural Language Processing, Speech Recognition, and Computational Linguistics. Prentice-Hall, 2nd edi-tion.","Ronald Kaplan and Martin Kay. 1994. Regular models of phonological rule systems. Computational Linguistics, 20(3):331–378.","Gregory Kobele. 2006. Generating Copies: An In-vestigation into Structural Identity in Language and Grammar. Ph.D. thesis, University of California, Los Angeles.","Leonid (Aryeh) Kontorovich, Corinna Cortes, and Mehryar Mohri. 2008. Kernel methods for learning languages. Theoretical Computer Science, 405(3):223 – 236. Algorithmic Learning Theory.","M. Lothaire, editor. 1997. Combinatorics on Words. Cambridge University Press, Cambridge, UK, New York.","A. A. Markov. 1913. An example of statistical study on the text of ‘eugene onegin’ illustrating the linking of events to a chain.","Robert McNaughton and Simon Papert. 1971. Counter-Free Automata. MIT Press.","A. Newell, S. Langer, and M. Hickey. 1998. The rôle of natural language processing in alternative and augmentative communication. Natural Language Engineering, 4(1):1–16.","Dominique Perrin and Jean-Eric Pin. 1986. First-Order logic and Star-Free sets. Journal of Computer and System Sciences, 32:393–406.","Catherine Ringen. 1988. Vowel Harmony: Theoretical Implications. Garland Publishing, Inc. 895","James Rogers and Geoffrey Pullum. to appear. Aural pattern recognition experiments and the subregular hierarchy. Journal of Logic, Language and Information.","James Rogers, Jeffrey Heinz, Matt Edlefsen, Dylan Leeman, Nathan Myers, Nathaniel Smith, Molly Visscher, and David Wellcome. to appear. On languages piecewise testable in the strict sense. In Proceedings of the 11th Meeting of the Assocation for Mathematics of Language.","Sharon Rose and Rachel Walker. 2004. A typology of consonant agreement as correspondence. Language, 80(3):475–531.","Jacques Sakarovitch and Imre Simon. 1983. Sub-words. In M. Lothaire, editor, Combinatorics on Words, volume 17 of Encyclopedia of Mathematics and Its Applications, chapter 6, pages 105–134. Addison-Wesley, Reading, Massachusetts.","Stuart Shieber. 1985. Evidence against the contextfreeness of natural language. Linguistics and Philosophy, 8:333–343.","Imre Simon. 1975. Piecewise testable events. In Automata Theory and Formal Languages: 2nd Grammatical Inference conference, pages 214–222, Berlin ; New York. Springer-Verlag.","Howard Straubing. 1994. Finite Automata, Formal Logic and Circuit Complexity. Birkhäuser.","Wolfgang Thomas. 1982. Classifying regular events in symbolic logic. Journal of Computer and Systems Sciences, 25:360–376.","Enrique Vidal, Franck Thollard, Colin de la Higuera, Francisco Casacuberta, and Rafael C. Carrasco. 2005a. Probabilistic finite-state machines-part I. IEEE Transactions on Pattern Analysis and Machine Intelligence, 27(7):1013–1025.","Enrique Vidal, Frank Thollard, Colin de la Higuera, Francisco Casacuberta, and Rafael C. Carrasco. 2005b. Probabilistic finite-state machines-part II. IEEE Transactions on Pattern Analysis and Machine Intelligence, 27(7):1026–1039. 896"]}],"references":[{"authors":[{"first":"R.","middle":"B.","last":"Applegate"}],"year":"1972","title":"Ineseño Chumash Grammar","source":"R.B. Applegate. 1972. Ineseño Chumash Grammar. Ph.D. thesis, University of California, Berkeley."},{"authors":[{"first":"R.","middle":"B.","last":"Applegate"}],"year":"2007","title":"Samala-English dictionary : a guide to the Samala language of the Ineseño Chumash People","source":"R.B. Applegate. 2007. Samala-English dictionary : a guide to the Samala language of the Ineseño Chumash People. Santa Ynez Band of Chumash Indians."},{"authors":[{"first":"Eric","last":"Baković"}],"year":"2000","title":"Harmony, Dominance and Control","source":"Eric Baković. 2000. Harmony, Dominance and Control. Ph.D. thesis, Rutgers University."},{"authors":[{"first":"D.","last":"Beauquier"},{"first":"Jean-Eric","last":"Pin"}],"year":"1991","title":"Languages and scanners","source":"D. Beauquier and Jean-Eric Pin. 1991. Languages and scanners. Theoretical Computer Science, 84:3–21."},{"authors":[{"first":"Eric","last":"Brill"}],"year":"1995","title":"Transformation-based error-driven learning and natural language processing: A case study in part-of-speech tagging","source":"Eric Brill. 1995. Transformation-based error-driven learning and natural language processing: A case study in part-of-speech tagging. Computational Linguistics, 21(4):543–566."},{"authors":[{"first":"J.","middle":"A.","last":"Brzozowski"},{"first":"Imre","last":"Simon"}],"year":"1973","title":"Characterizations of locally testable events","source":"J. A. Brzozowski and Imre Simon. 1973. Characterizations of locally testable events. Discrete Mathematics, 4:243–271."},{"authors":[{"first":"Noam","last":"Chomsky"}],"year":"1956","title":"Three models for the descrip-tion of language","source":"Noam Chomsky. 1956. Three models for the descrip-tion of language. IRE Transactions on Information Theory. IT-2."},{"authors":[{"first":"J.","middle":"S.","last":"Coleman"},{"first":"J.","last":"Pierrehumbert"}],"year":"1997","title":"Stochastic phonological grammars and acceptability","source":"J. S. Coleman and J. Pierrehumbert. 1997. Stochastic phonological grammars and acceptability. In Computational Phonology, pages 49–56. Somerset, NJ: Association for Computational Linguistics. Third Meeting of the ACL Special Interest Group in Computational Phonology."},{"authors":[],"source":"Colin de la Higuera. in press. Grammatical Inference: Learning Automata and Grammars. Cambridge University Press."},{"authors":[{"first":"Pedro","last":"Garcı́a"},{"first":"José","last":"Ruiz"}],"year":"1990","title":"Inference of k-testable languages in the strict sense and applica-tions to syntactic pattern recognition","source":"Pedro Garcı́a and José Ruiz. 1990. Inference of k-testable languages in the strict sense and applica-tions to syntactic pattern recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 9:920–925."},{"authors":[{"first":"Pedro","last":"Garcı́a"},{"first":"José","last":"Ruiz"}],"year":"1996","title":"Learning k-piecewise testable languages from positive data","source":"Pedro Garcı́a and José Ruiz. 1996. Learning k-piecewise testable languages from positive data. In Laurent Miclet and Colin de la Higuera, editors, Grammatical Interference: Learning Syntax from Sentences, volume 1147 of Lecture Notes in Computer Science, pages 203–210. Springer."},{"authors":[{"first":"Pedro","last":"Garcia"},{"first":"Enrique","last":"Vidal"},{"first":"José","last":"Oncina"}],"year":"1990","title":"Learning locally testable languages in the strict sense","source":"Pedro Garcia, Enrique Vidal, and José Oncina. 1990. Learning locally testable languages in the strict sense. In Proceedings of the Workshop on Algorithmic Learning Theory, pages 325–338."},{"authors":[{"first":"Gunnar","last":"Hansson"}],"year":"2001","title":"Theoretical and typological issues in consonant harmony","source":"Gunnar Hansson. 2001. Theoretical and typological issues in consonant harmony. Ph.D. thesis, University of California, Berkeley."},{"authors":[{"first":"Bruce","last":"Hayes"},{"first":"Colin","last":"Wilson"}],"year":"2008","title":"A maximum entropy model of phonotactics and phonotactic learning","source":"Bruce Hayes and Colin Wilson. 2008. A maximum entropy model of phonotactics and phonotactic learning. Linguistic Inquiry, 39:379–440."},{"authors":[{"first":"Jeffrey","last":"Heinz"}],"year":"2007","title":"The Inductive Learning of Phonotactic Patterns","source":"Jeffrey Heinz. 2007. The Inductive Learning of Phonotactic Patterns. Ph.D. thesis, University of California, Los Angeles."},{"authors":[],"source":"Jeffrey Heinz. to appear. Learning long distance phonotactics. Linguistic Inquiry."},{"authors":[{"first":"John","last":"Hopcroft"},{"first":"Rajeev","last":"Motwani"},{"first":"Jeffrey","last":"Ullman"}],"year":"2001","title":"Introduction to Automata Theory, Languages, and Computation","source":"John Hopcroft, Rajeev Motwani, and Jeffrey Ullman. 2001. Introduction to Automata Theory, Languages, and Computation. Addison-Wesley."},{"authors":[{"first":"Frederick","last":"Jelenik"}],"year":"1997","title":"Statistical Methods for Speech Recognition","source":"Frederick Jelenik. 1997. Statistical Methods for Speech Recognition. MIT Press."},{"authors":[{"first":"C.","middle":"Douglas","last":"Johnson"}],"year":"1972","title":"Formal Aspects of Phonological Description","source":"C. Douglas Johnson. 1972. Formal Aspects of Phonological Description. The Hague: Mouton."},{"authors":[{"first":"A.","middle":"K.","last":"Joshi"}],"year":"1985","title":"Tree-adjoining grammars: How much context sensitivity is required to provide reasonable structural descriptions? In D","source":"A. K. Joshi. 1985. Tree-adjoining grammars: How much context sensitivity is required to provide reasonable structural descriptions? In D. Dowty, L. Karttunen, and A. Zwicky, editors, Natural Language Parsing, pages 206–250. Cambridge University Press."},{"authors":[{"first":"Daniel","last":"Jurafsky"},{"first":"James","last":"Martin"}],"year":"2008","title":"Speech and Language Processing: An Introduction to Natural Language Processing, Speech Recognition, and Computational Linguistics","source":"Daniel Jurafsky and James Martin. 2008. Speech and Language Processing: An Introduction to Natural Language Processing, Speech Recognition, and Computational Linguistics. Prentice-Hall, 2nd edi-tion."},{"authors":[{"first":"Ronald","last":"Kaplan"},{"first":"Martin","last":"Kay"}],"year":"1994","title":"Regular models of phonological rule systems","source":"Ronald Kaplan and Martin Kay. 1994. Regular models of phonological rule systems. Computational Linguistics, 20(3):331–378."},{"authors":[{"first":"Gregory","last":"Kobele"}],"year":"2006","title":"Generating Copies: An In-vestigation into Structural Identity in Language and Grammar","source":"Gregory Kobele. 2006. Generating Copies: An In-vestigation into Structural Identity in Language and Grammar. Ph.D. thesis, University of California, Los Angeles."},{"authors":[{"first":"Leonid","middle":"(Aryeh)","last":"Kontorovich"},{"first":"Corinna","last":"Cortes"},{"first":"Mehryar","last":"Mohri"}],"year":"2008","title":"Kernel methods for learning languages","source":"Leonid (Aryeh) Kontorovich, Corinna Cortes, and Mehryar Mohri. 2008. Kernel methods for learning languages. Theoretical Computer Science, 405(3):223 – 236. Algorithmic Learning Theory."},{"authors":[{"first":"M.","last":"Lothaire"},{"last":"editor"}],"year":"1997","title":"Combinatorics on Words","source":"M. Lothaire, editor. 1997. Combinatorics on Words. Cambridge University Press, Cambridge, UK, New York."},{"authors":[{"first":"A.","middle":"A.","last":"Markov"}],"year":"1913","title":"An example of statistical study on the text of ‘eugene onegin’ illustrating the linking of events to a chain","source":"A. A. Markov. 1913. An example of statistical study on the text of ‘eugene onegin’ illustrating the linking of events to a chain."},{"authors":[{"first":"Robert","last":"McNaughton"},{"first":"Simon","last":"Papert"}],"year":"1971","title":"Counter-Free Automata","source":"Robert McNaughton and Simon Papert. 1971. Counter-Free Automata. MIT Press."},{"authors":[{"first":"A.","last":"Newell"},{"first":"S.","last":"Langer"},{"first":"M.","last":"Hickey"}],"year":"1998","title":"The rôle of natural language processing in alternative and augmentative communication","source":"A. Newell, S. Langer, and M. Hickey. 1998. The rôle of natural language processing in alternative and augmentative communication. Natural Language Engineering, 4(1):1–16."},{"authors":[{"first":"Dominique","last":"Perrin"},{"first":"Jean-Eric","last":"Pin"}],"year":"1986","title":"First-Order logic and Star-Free sets","source":"Dominique Perrin and Jean-Eric Pin. 1986. First-Order logic and Star-Free sets. Journal of Computer and System Sciences, 32:393–406."},{"authors":[{"first":"Catherine","last":"Ringen"}],"year":"1988","title":"Vowel Harmony: Theoretical Implications","source":"Catherine Ringen. 1988. Vowel Harmony: Theoretical Implications. Garland Publishing, Inc. 895"},{"authors":[],"source":"James Rogers and Geoffrey Pullum. to appear. Aural pattern recognition experiments and the subregular hierarchy. Journal of Logic, Language and Information."},{"authors":[],"source":"James Rogers, Jeffrey Heinz, Matt Edlefsen, Dylan Leeman, Nathan Myers, Nathaniel Smith, Molly Visscher, and David Wellcome. to appear. On languages piecewise testable in the strict sense. In Proceedings of the 11th Meeting of the Assocation for Mathematics of Language."},{"authors":[{"first":"Sharon","last":"Rose"},{"first":"Rachel","last":"Walker"}],"year":"2004","title":"A typology of consonant agreement as correspondence","source":"Sharon Rose and Rachel Walker. 2004. A typology of consonant agreement as correspondence. Language, 80(3):475–531."},{"authors":[{"first":"Jacques","last":"Sakarovitch"},{"first":"Imre","last":"Simon"}],"year":"1983","title":"Sub-words","source":"Jacques Sakarovitch and Imre Simon. 1983. Sub-words. In M. Lothaire, editor, Combinatorics on Words, volume 17 of Encyclopedia of Mathematics and Its Applications, chapter 6, pages 105–134. Addison-Wesley, Reading, Massachusetts."},{"authors":[{"first":"Stuart","last":"Shieber"}],"year":"1985","title":"Evidence against the contextfreeness of natural language","source":"Stuart Shieber. 1985. Evidence against the contextfreeness of natural language. Linguistics and Philosophy, 8:333–343."},{"authors":[{"first":"Imre","last":"Simon"}],"year":"1975","title":"Piecewise testable events","source":"Imre Simon. 1975. Piecewise testable events. In Automata Theory and Formal Languages: 2nd Grammatical Inference conference, pages 214–222, Berlin ; New York. Springer-Verlag."},{"authors":[{"first":"Howard","last":"Straubing"}],"year":"1994","title":"Finite Automata, Formal Logic and Circuit Complexity","source":"Howard Straubing. 1994. Finite Automata, Formal Logic and Circuit Complexity. Birkhäuser."},{"authors":[{"first":"Wolfgang","last":"Thomas"}],"year":"1982","title":"Classifying regular events in symbolic logic","source":"Wolfgang Thomas. 1982. Classifying regular events in symbolic logic. Journal of Computer and Systems Sciences, 25:360–376."},{"authors":[{"first":"Enrique","last":"Vidal"},{"first":"Franck","last":"Thollard"},{"first":"Colin","middle":"de la","last":"Higuera"},{"first":"Francisco","last":"Casacuberta"},{"first":"Rafael","middle":"C.","last":"Carrasco"}],"year":"2005a","title":"Probabilistic finite-state machines-part I","source":"Enrique Vidal, Franck Thollard, Colin de la Higuera, Francisco Casacuberta, and Rafael C. Carrasco. 2005a. Probabilistic finite-state machines-part I. IEEE Transactions on Pattern Analysis and Machine Intelligence, 27(7):1013–1025."},{"authors":[{"first":"Enrique","last":"Vidal"},{"first":"Frank","last":"Thollard"},{"first":"Colin","middle":"de la","last":"Higuera"},{"first":"Francisco","last":"Casacuberta"},{"first":"Rafael","middle":"C.","last":"Carrasco"}],"year":"2005b","title":"Probabilistic finite-state machines-part II","source":"Enrique Vidal, Frank Thollard, Colin de la Higuera, Francisco Casacuberta, and Rafael C. Carrasco. 2005b. Probabilistic finite-state machines-part II. IEEE Transactions on Pattern Analysis and Machine Intelligence, 27(7):1026–1039. 896"}],"cites":[{"style":0,"text":"Chomsky, 1956","origin":{"pointer":"/sections/2/paragraphs/0","offset":219,"length":13},"authors":[{"last":"Chomsky"}],"year":"1956","references":["/references/6"]},{"style":0,"text":"Joshi, 1985","origin":{"pointer":"/sections/2/paragraphs/0","offset":234,"length":11},"authors":[{"last":"Joshi"}],"year":"1985","references":["/references/19"]},{"style":0,"text":"Shieber, 1985","origin":{"pointer":"/sections/2/paragraphs/0","offset":247,"length":13},"authors":[{"last":"Shieber"}],"year":"1985","references":["/references/34"]},{"style":0,"text":"Kobele, 2006","origin":{"pointer":"/sections/2/paragraphs/0","offset":262,"length":12},"authors":[{"last":"Kobele"}],"year":"2006","references":["/references/22"]},{"style":0,"text":"Ringen, 1988","origin":{"pointer":"/sections/2/paragraphs/0","offset":519,"length":12},"authors":[{"last":"Ringen"}],"year":"1988","references":["/references/29"]},{"style":0,"text":"Baković, 2000","origin":{"pointer":"/sections/2/paragraphs/0","offset":533,"length":13},"authors":[{"last":"Baković"}],"year":"2000","references":["/references/2"]},{"style":0,"text":"Hansson, 2001","origin":{"pointer":"/sections/2/paragraphs/0","offset":548,"length":13},"authors":[{"last":"Hansson"}],"year":"2001","references":["/references/12"]},{"style":0,"text":"Rose and Walker, 2004","origin":{"pointer":"/sections/2/paragraphs/0","offset":563,"length":21},"authors":[{"last":"Rose"},{"last":"Walker"}],"year":"2004","references":["/references/32"]},{"style":0,"text":"Johnson, 1972","origin":{"pointer":"/sections/2/paragraphs/0","offset":630,"length":13},"authors":[{"last":"Johnson"}],"year":"1972","references":["/references/18"]},{"style":0,"text":"Kaplan and Kay, 1994","origin":{"pointer":"/sections/2/paragraphs/0","offset":645,"length":20},"authors":[{"last":"Kaplan"},{"last":"Kay"}],"year":"1994","references":["/references/21"]},{"style":0,"text":"Rogers et al. (2009)","origin":{"pointer":"/sections/2/paragraphs/0","offset":912,"length":20},"authors":[{"last":"Rogers"},{"last":"al."}],"year":"2009","references":[]},{"style":0,"text":"McNaughton and Papert, 1971","origin":{"pointer":"/sections/2/paragraphs/1","offset":222,"length":27},"authors":[{"last":"McNaughton"},{"last":"Papert"}],"year":"1971","references":["/references/26"]},{"style":0,"text":"Garcia et al., 1990","origin":{"pointer":"/sections/2/paragraphs/1","offset":440,"length":19},"authors":[{"last":"Garcia"},{"last":"al."}],"year":"1990","references":["/references/11"]},{"style":0,"text":"Jurafsky and Martin, 2008","origin":{"pointer":"/sections/2/paragraphs/1","offset":609,"length":25},"authors":[{"last":"Jurafsky"},{"last":"Martin"}],"year":"2008","references":["/references/20"]},{"style":0,"text":"Markov, 1913","origin":{"pointer":"/sections/2/paragraphs/1","offset":743,"length":12},"authors":[{"last":"Markov"}],"year":"1913","references":["/references/25"]},{"style":0,"text":"Coleman and Pierrehumbert, 1997","origin":{"pointer":"/sections/2/paragraphs/5","offset":339,"length":31},"authors":[{"last":"Coleman"},{"last":"Pierrehumbert"}],"year":"1997","references":["/references/7"]},{"style":0,"text":"Hayes and Wilson, 2008","origin":{"pointer":"/sections/2/paragraphs/5","offset":372,"length":22},"authors":[{"last":"Hayes"},{"last":"Wilson"}],"year":"2008","references":["/references/13"]},{"style":0,"text":"Newell et al., 1998","origin":{"pointer":"/sections/2/paragraphs/5","offset":570,"length":19},"authors":[{"last":"Newell"},{"last":"al."}],"year":"1998","references":["/references/27"]},{"style":0,"text":"Brill, 1995","origin":{"pointer":"/sections/2/paragraphs/5","offset":616,"length":11},"authors":[{"last":"Brill"}],"year":"1995","references":["/references/4"]},{"style":0,"text":"Jelenik, 1997","origin":{"pointer":"/sections/2/paragraphs/5","offset":654,"length":13},"authors":[{"last":"Jelenik"}],"year":"1997","references":["/references/17"]},{"style":0,"text":"Hopcroft et al., 2001","origin":{"pointer":"/sections/3/paragraphs/22","offset":207,"length":21},"authors":[{"last":"Hopcroft"},{"last":"al."}],"year":"2001","references":["/references/16"]},{"style":0,"text":"Vidal et al., 2005a","origin":{"pointer":"/sections/3/paragraphs/30","offset":69,"length":19},"authors":[{"last":"Vidal"},{"last":"al."}],"year":"2005a","references":["/references/38"]},{"style":0,"text":"Vidal et al., 2005a","origin":{"pointer":"/sections/3/paragraphs/33","offset":126,"length":19},"authors":[{"last":"Vidal"},{"last":"al."}],"year":"2005a","references":["/references/38"]},{"style":0,"text":"Vidal et al., 2005a","origin":{"pointer":"/sections/3/paragraphs/34","offset":72,"length":19},"authors":[{"last":"Vidal"},{"last":"al."}],"year":"2005a","references":["/references/38"]},{"style":0,"text":"Vidal et al., 2005b","origin":{"pointer":"/sections/3/paragraphs/34","offset":93,"length":19},"authors":[{"last":"Vidal"},{"last":"al."}],"year":"2005b","references":["/references/39"]},{"style":0,"text":"Vidal et al., 2005a","origin":{"pointer":"/sections/3/paragraphs/36","offset":141,"length":19},"authors":[{"last":"Vidal"},{"last":"al."}],"year":"2005a","references":["/references/38"]},{"style":0,"text":"Vidal et al., 2005b","origin":{"pointer":"/sections/3/paragraphs/36","offset":162,"length":19},"authors":[{"last":"Vidal"},{"last":"al."}],"year":"2005b","references":["/references/39"]},{"style":0,"text":"McNaughton and Papert, 1971","origin":{"pointer":"/sections/4/paragraphs/1","offset":395,"length":27},"authors":[{"last":"McNaughton"},{"last":"Papert"}],"year":"1971","references":["/references/26"]},{"style":0,"text":"Brzozowski and Simon, 1973","origin":{"pointer":"/sections/4/paragraphs/1","offset":424,"length":26},"authors":[{"last":"Brzozowski"},{"last":"Simon"}],"year":"1973","references":["/references/5"]},{"style":0,"text":"Simon, 1975","origin":{"pointer":"/sections/4/paragraphs/1","offset":452,"length":11},"authors":[{"last":"Simon"}],"year":"1975","references":["/references/35"]},{"style":0,"text":"Thomas, 1982","origin":{"pointer":"/sections/4/paragraphs/1","offset":465,"length":12},"authors":[{"last":"Thomas"}],"year":"1982","references":["/references/37"]},{"style":0,"text":"Perrin and Pin, 1986","origin":{"pointer":"/sections/4/paragraphs/1","offset":479,"length":20},"authors":[{"last":"Perrin"},{"last":"Pin"}],"year":"1986","references":["/references/28"]},{"style":0,"text":"Garcı́a and Ruiz, 1990","origin":{"pointer":"/sections/4/paragraphs/1","offset":501,"length":22},"authors":[{"last":"Garcı́a"},{"last":"Ruiz"}],"year":"1990","references":["/references/9"]},{"style":0,"text":"Beauquier and Pin, 1991","origin":{"pointer":"/sections/4/paragraphs/1","offset":525,"length":23},"authors":[{"last":"Beauquier"},{"last":"Pin"}],"year":"1991","references":["/references/3"]},{"style":0,"text":"Straub-ing, 1994","origin":{"pointer":"/sections/4/paragraphs/1","offset":550,"length":16},"authors":[{"last":"Straub-ing"}],"year":"1994","references":[]},{"style":0,"text":"Garcı́a and Ruiz, 1996","origin":{"pointer":"/sections/4/paragraphs/1","offset":568,"length":22},"authors":[{"last":"Garcı́a"},{"last":"Ruiz"}],"year":"1996","references":["/references/10"]},{"style":0,"text":"Kontorovich et al., 2008","origin":{"pointer":"/sections/4/paragraphs/1","offset":622,"length":24},"authors":[{"last":"Kontorovich"},{"last":"al."}],"year":"2008","references":["/references/23"]},{"style":0,"text":"Jurafsky and Martin, 2008","origin":{"pointer":"/sections/4/paragraphs/3","offset":279,"length":25},"authors":[{"last":"Jurafsky"},{"last":"Martin"}],"year":"2008","references":["/references/20"]},{"style":0,"text":"McNaughton and Papert, 1971","origin":{"pointer":"/sections/4/paragraphs/4","offset":990,"length":27},"authors":[{"last":"McNaughton"},{"last":"Papert"}],"year":"1971","references":["/references/26"]},{"style":0,"text":"Thomas, 1982","origin":{"pointer":"/sections/4/paragraphs/4","offset":1019,"length":12},"authors":[{"last":"Thomas"}],"year":"1982","references":["/references/37"]},{"style":0,"text":"Sakarovitch and Simon (1983)","origin":{"pointer":"/sections/5/paragraphs/0","offset":10,"length":28},"authors":[{"last":"Sakarovitch"},{"last":"Simon"}],"year":"1983","references":["/references/33"]},{"style":0,"text":"Lothaire (1997)","origin":{"pointer":"/sections/5/paragraphs/0","offset":40,"length":15},"authors":[{"last":"Lothaire"}],"year":"1997","references":[]},{"style":0,"text":"Kontorovich, et al. (2008)","origin":{"pointer":"/sections/5/paragraphs/0","offset":60,"length":26},"authors":[{"last":"Kontorovich"},{"last":"al."}],"year":"2008","references":["/references/23"]},{"style":0,"text":"Lothaire (1997)","origin":{"pointer":"/sections/5/paragraphs/6","offset":175,"length":15},"authors":[{"last":"Lothaire"}],"year":"1997","references":[]},{"style":0,"text":"Applegate, 1972","origin":{"pointer":"/sections/7/paragraphs/5","offset":199,"length":15},"authors":[{"last":"Applegate"}],"year":"1972","references":["/references/0"]},{"style":0,"text":"Applegate, 1972","origin":{"pointer":"/sections/8/paragraphs/2","offset":32,"length":15},"authors":[{"last":"Applegate"}],"year":"1972","references":["/references/0"]}]}
