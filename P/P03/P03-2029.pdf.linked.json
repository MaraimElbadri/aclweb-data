{"sections":[{"title":"Word Sense Disambiguation Using Pairwise Alignment Koichi Yamashita Keiichi Yoshida Faculty of Administration and Informatics University of Hamamatsu 1230 Miyakoda-cho, Hamamatsu, Shizuoka, Japan yamasita@hamamatsu-u.ac.jp Yukihiro Itoh Abstract","paragraphs":["In this paper, we proposed a new supervised word sense disambiguation (WSD) method based on a pairwise alignment technique, which is used generally to measure a similarity between DNA sequences. The new method obtained 2.8%-14.2% improvements of the accuracy in our experiment for WSD."]},{"title":"1 Introduction","paragraphs":["WSD has been recognized as one of the most important subjects in natural language processing, especially in machine translation, information retrieval, and so on (Ide and Véronis, 1998). Most of previous supervised methods can be classified into two major ones; approach based on association, and approach based on selectional restriction. The former uses some words around a target word, represented by n-word window. The latter uses some syntactic relations, say, verb-object, including necessarily a target word.","However, there are some words that one approach gets good result for them while another gets worse, and vice versa. For example, suppose that we want to distinguish between “go off or discharge” and “terminate the employment” as a sense of “fire”. Consider the sentence in Brown Corpus1",": My Cousin Simmons carried a musket, but he had loaded it with bird shot, and as the officer came opposite him, he rose up behind the wall and fired. 1 In this case, we consider only one sentential context for the","simplicity. The words such as “musket”, “loaded” and “bird shot” would seem useful in deciding the sense of “fire”, and serve as clue to leading the sense to “go off or discharge”. It seems that there is no clue to another sense. For this case, an approach based on association is useful for WSD. However, an approach based on selectional restriction would not be appropriate, because these clues do not have the direct syntactic dependencies on “fire”. On the other hand, consider the sentence in EDR Corpus: Police said Haga was immediately fired from the force. The most significant fact is that “Haga” (a person’s name) appears as the direct object of “fire”. A selectional restriction approach would use this clue appropriately, because there is the direct dependency between “fire” and “Haga”. However, an association approach would make an error in deciding the sense, because “Police” and “force” tend to be a noise, from the point of view of an unordered set of words. Generally, an association does not use a syntactic dependency, and a selectional restriction uses only a part of words appeared in a sentence.","In this paper, we present a new method for WSD, which uses syntactic dependencies for a whole sentence as a clue. They contain both of all words in-cluded in a sentence and all syntactic dependencies in it. Our method is based on a technique of pairwise alignment, and described in the following two sections. Using our method, we have gotten appropriate sense for various cases including above examples. In section 4, we describe our experimental result for WSD on some verbs in SENSEVAL-1 (Kilgarriff, 1998)."]},{"title":"2 Our Method","paragraphs":["Our method has the features on an association and a selectional restriction approach both. It can be applied with the various sentence types because our method can treat a local (direct) and a whole sentence dependency. Our method is based on the following steps;","Step 1. Parse the input sentence with syntactic parser2",", and find all paths from root to leaves in the resulting dependency tree.","Step 2. Compare the paths from Step 1. with prototype paths prepared for each sense of the target word.","Step 3. Find a summation of similarity between each prototype and input path for each sense.","Step 4. Select the sense with the maximum value of the summation. We describe our method in detail in the followings.","In our method, we consider paths from root to leaves in a dependency tree. For example, consider the sentence “we consider a path in a graph”. This sentence has three leaves in the dependency structure, and consequently has three paths from root to leaves; (consider, SUB, we), (consider, OBJ, path, a) and (consider, OBJ, path, in, graph, a). “SUB” and “OBJ” in the paths are the elements added automatically using some rules in order to make a remarkable difference between verb-subject and verb-object. We think this sequence structure of word would serve as a clue to WSD very well, and we regard a set of the sequences obtained from an input sentence as the context of a target word.","The general intuition for WSD is that words with similar context have the same sense (Charniak, 1993; Lin, 1997). That is, once we prepare the prototype sequences for each sense, we can determine the sense of the target word as one with the most similar prototype set. We measure a similarity between a set of prototype sequences T and a set of sequences from input sentence T  . Let T and T ","have a set of sequences, PT","p1 p2","pn","and 2 We assume that we can get the correct syntactic structure","here. (See section 4)","fire: go off or discharge fire, SUB, person fire, OBJ, [weapon, rocket] fire, [on, upon, at], physical object fire, *, load, [into, with], weapon fire, *, set up, OBJ, weapon","fire: terminate the employment fire, SUB, company fire, OBJ, [person, people, staff] fire, from, organization fire, *, hire fire, *, job Figure 1: Prototype sequence for verb “fire”","PT p 1","p ","2 p ","m respectively. pi and p ","j are sequences of words. We define the similarity between T and T ",", sim T","T  , as following:","sim T","T    pi PT","fi max p j P","T","alignment pi","p  j  (1)","sim T","T  ","is not commutative. That is, sim T","T   ","sim T   T ",". alignment pi","p  j ","is an alignment score between the sequences pi and p ","j, defined in the next section. fi is a weight function characteristic of the sequence pi, defined as following: fi ","ui if max p j P","T","alignment pi","p  j  ti vi otherwise (2) where ui and vi are arbitrary constants and ti is arbitrary threshold.","Using equation (1), we can estimate a similarity between the context of a target word and prototype context, and can determine the sense of a target word by selecting the prototype with the maximum similarity.","An example of the prototype sequences for verb “fire” is shown in Figure 1. A prototype sequence is represented like a regular expression. For the present, we obtain the sequence by hand. The basic policy to obtain prototypes is to observe the common features on dependency trees in which target word is used in the same sense. We have some ideas about a method to obtain prototypes automatically."]},{"title":"3 Pairwise Alignment","paragraphs":["We attempt to apply the method of pairwise alignment to measuring the similarity between sequences. Recently, the technique of pairwise alignment is worked at composition the is make at home 1.000 0.500 1.000 0.595 -1 -1 -1 -1-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1-1-1 -1 -1 -1 -1 -1-1 -1","-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1","alignment : (worked) ( ) (at)(at) (composition)","( ) (the) (make)is home- - score : 0.595 = (worked, at, composition, the) = (is, make, at, home) p p’ Figure 2: Pairwise alignment used generally in molecular biology research as a basic method to measure the similarity between proteins or DNA sequences (Mitaku and Kanehisa, 1995).","There have been several ways to find the pairwise alignment, such as the method based on Dynamic Programming, one based on Finite State Automaton, and so on (Durbinet al., 1998). In our method, we apply the method using DP matrix, as in Figure 2. We have shown the pairwise alignment between sequences p","(worked, at, composition, the) and p  ","(is, make, at, home) as an example.","In a matrix, a vertical and horizontal transition means a gap and is assigned a gap score. A diagonal transition means a substitution and is assigned a score based on the similarity between two words corresponding to that point in the matrix. Actually, the following value is calculated in each node, using values which have been calculated in its three previous nodes.","Fi","j","max"," Fi 1","j","subst","-","w  j ","Fi","j 1 subst","wi","- ","Fi","1","j 1","subst","wi","w  j  (3)","where subst -","w  j ","and subst wi","-  represent respectively to substitute w ","j and wi with a gap (-),","and return the gap score. subst","wi","w  j  represent the score of substituting wi with w  j or vice versa.","Now let the word w has synsets s1 s2","sk and w  has s ","1 s ","2 s ","l on WordNet hierarchy (Miller et","al., 1990). For simplicity, we define the subst","w","w   as following, based on the semantic distance (Stetina and Nagao, 1998).","subst w","w   ","2 max i","j"," sd si","s  j  1 (4)","where sd si","s  j ","is the semantic distance between two synsets si and s ","j. Because 0 sd si","s  j "," 1, ","1","subst","w","w  ","","1. The score of the substitution","between identical words is 1, and one between two","words with no common ancestor in the hierarchy is 1. We simply define the gap score as  1."]},{"title":"4 Experimental Result","paragraphs":["Up to the present, we have obtained the experimental results on 7 verbs in SENSEVAL-13",". In our experiment, for all sentences including target word in the training and test corpus of SENSEVAL-1, we make a parsing using Apple Pie Parser (Sekine, 1996) and additional vertices using some rules automatically. If the resulted parsing includes some errors, we remove them by hand. Then we obtain the sequence patterns by hand from training data and attempt WSD using equation (1) for test data. Because of various length of sequence, we assign score zero to the preceding and right-end gaps in an alignment.","We show our experimental results in Table 1. In SENSEVAL-1, precisions and recalls are calculated by three scoring ways, fine-grained, mixed-grained and coarse-grained scoring. We show the results only by fine-grained scoring which is evaluated by distinguishing word sense in the strictest way. It is impossible to make simple comparison with the participants in SENSEVAL-1 because our method needs supervised learning by hand. However, 2.8%- 14.2% improvements of the accuracy compared with the best system seems significant, suggesting that our method is promising for WSD."]},{"title":"5 Future Works","paragraphs":["There are two major limitations in our method; one of syntactic information and of knowledge acquisi-","3","We have experimented on verbs in SENSEVAL-1 one by one alphabetically. The word “amaze” is omitted because it has only one verbal sense.","Table 1: Experimental results for some verbs (in","fine-grained scoring) bet the numbers of test instance:117","precision (recall) our method 0.880 (0.880) best system in SENSEVAL-1 0.778 (0.778) human 0.924 (0.916) bother the numbers of test instance:209","precision (recall) our method 0.900 (0.900) best system in SENSEVAL-1 0.866 (0.866) human 0.976 (0.976) bury the numbers of test instance:201","precision (recall) our method 0.667 (0.667) best system in SENSEVAL-1 0.572 (0.572) human 0.928 (0.923) calculate the numbers of test instance:218","precision (recall) our method 0.950 (0.950) best system in SENSEVAL-1 0.922 (0.922) human 0.954 (0.950) consume the numbers of test instance:186","precision (recall) our method 0.645 (0.645) best system in SENSEVAL-1 0.503 (0.500) human 0.944 (0.939) derive the numbers of test instance:217","precision (recall) our method 0.751 (0.751) best system in SENSEVAL-1 0.664 (0.664) human 0.965 (0.961) float the numbers of test instance:229","precision (recall) our method 0.616 (0.616) best system in SENSEVAL-1 0.555 (0.555) human 0.927 (0.923) tion by hand.","The former is that our method assumes we can get the correct syntactic information. In fact, the accuracy and performance of syntactic analyzer are being improved more and more, consequently this disadvantage would become a minor problem. Because a similarity between sequences derived from syntactic dependencies is calculated as a numerical value, our method would also be suitable for integration with a probabilistic syntactic analyzer.","The latter, which is more serious, is that the sequence patterns used as clue to WSD are acquired by hand at the present. In molecular biology research, several attempts to obtain sequence patterns automatically have been reported, which can be expected to motivate ours for WSD. We plan to construct an algorithm for an automatic pattern acquisi-tion from large scale corpora based on those biological approaches."]},{"title":"References","paragraphs":["Eugene Charniak. 1993. Statistical Language Learning. MIT Press, Cambridge.","Richard Durbin, Sean R. Eddy, Andrew Krogh and Graeme Mitchison. 1998. Biological Sequence Analysis: Probabilistic Models of Proteins and Nucleic Acids. Cambridge University Press.","Marti A. Hearst. 1991. Noun Homograph Disambiguation Using Local Context in Large Text Corpora. In Proceedings of the 7th Annual Conference of the University of Waterloo Center for the New OED and Text Research, pp.1-22.","Nancy Ide and Jean Véronis. 1998. Introduction to the Special Issue on Word Sense Disambiguation: The State of the Art. Computational Linguistics, 24(1):1-40.","Adam Kilgarriff. 1998. Senseval: An exercise in evaluating word sense disambiguation programs. In Proceedings of the 1st International Conference on Language Resources and Evaluation (LREC98), volume 1, pp.581-585.","Dekang Lin. 1997. Using Syntactic Dependency as Local Context to Resolve Word Sense Ambiguity. In Proceedings of ACL/EACL-97, pp.64-71.","Christopher D. Manning and Hinrich Schütze. 1999. Foundations of Statistical Natural Language Processing. MIT Press, Cambridge.","George A. Miller, Richard Beckwith, Christiane Fellbaum, Derek Gross and Katherine J. Miller. 1990. Introduction to WordNet: an on-line lexical database. In International Journal of Lexicography, 3(4):235-244.","Shigeki Mitaku and Minoru Kanehisa (ed). 1995. Human Genom Project and Knowledge Information Processing (in Japanese). Baifukan.","Satoshi Sekine. 1996. Manual of Apple Pie Parser. URL: http://nlp.cs.nyu.edu/app/.","Jiri Stetina and Makoto Nagao. 1998. General Word Sense Disambiguation Method Based on a Full Sentential Context. In Journal of Natural Language Processing, 5(2):47-74."]}],"references":[{"authors":[{"first":"Eugene","last":"Charniak"}],"year":"1993","title":"Statistical Language Learning","source":"Eugene Charniak. 1993. Statistical Language Learning. MIT Press, Cambridge."},{"authors":[{"first":"Richard","last":"Durbin"},{"first":"Sean","middle":"R.","last":"Eddy"},{"first":"Andrew","last":"Krogh"},{"first":"Graeme","last":"Mitchison"}],"year":"1998","title":"Biological Sequence Analysis: Probabilistic Models of Proteins and Nucleic Acids","source":"Richard Durbin, Sean R. Eddy, Andrew Krogh and Graeme Mitchison. 1998. Biological Sequence Analysis: Probabilistic Models of Proteins and Nucleic Acids. Cambridge University Press."},{"authors":[{"first":"Marti","middle":"A.","last":"Hearst"}],"year":"1991","title":"Noun Homograph Disambiguation Using Local Context in Large Text Corpora","source":"Marti A. Hearst. 1991. Noun Homograph Disambiguation Using Local Context in Large Text Corpora. In Proceedings of the 7th Annual Conference of the University of Waterloo Center for the New OED and Text Research, pp.1-22."},{"authors":[{"first":"Nancy","last":"Ide"},{"first":"Jean V","last":"éronis"}],"year":"1998","title":"Introduction to the Special Issue on Word Sense Disambiguation: The State of the Art","source":"Nancy Ide and Jean Véronis. 1998. Introduction to the Special Issue on Word Sense Disambiguation: The State of the Art. Computational Linguistics, 24(1):1-40."},{"authors":[{"first":"Adam","last":"Kilgarriff"}],"year":"1998","title":"Senseval: An exercise in evaluating word sense disambiguation programs","source":"Adam Kilgarriff. 1998. Senseval: An exercise in evaluating word sense disambiguation programs. In Proceedings of the 1st International Conference on Language Resources and Evaluation (LREC98), volume 1, pp.581-585."},{"authors":[{"first":"Dekang","last":"Lin"}],"year":"1997","title":"Using Syntactic Dependency as Local Context to Resolve Word Sense Ambiguity","source":"Dekang Lin. 1997. Using Syntactic Dependency as Local Context to Resolve Word Sense Ambiguity. In Proceedings of ACL/EACL-97, pp.64-71."},{"authors":[{"first":"Christopher","middle":"D.","last":"Manning"},{"first":"Hinrich","last":"Schütze"}],"year":"1999","title":"Foundations of Statistical Natural Language Processing","source":"Christopher D. Manning and Hinrich Schütze. 1999. Foundations of Statistical Natural Language Processing. MIT Press, Cambridge."},{"authors":[{"first":"George","middle":"A.","last":"Miller"},{"first":"Richard","last":"Beckwith"},{"first":"Christiane","last":"Fellbaum"},{"first":"Derek","last":"Gross"},{"first":"Katherine","middle":"J.","last":"Miller"}],"year":"1990","title":"Introduction to WordNet: an on-line lexical database","source":"George A. Miller, Richard Beckwith, Christiane Fellbaum, Derek Gross and Katherine J. Miller. 1990. Introduction to WordNet: an on-line lexical database. In International Journal of Lexicography, 3(4):235-244."},{"authors":[{"first":"Shigeki","last":"Mitaku"},{"first":"Minoru","middle":"Kanehisa","last":"(ed)"}],"year":"1995","title":"Human Genom Project and Knowledge Information Processing (in Japanese)","source":"Shigeki Mitaku and Minoru Kanehisa (ed). 1995. Human Genom Project and Knowledge Information Processing (in Japanese). Baifukan."},{"authors":[{"first":"Satoshi","last":"Sekine"}],"year":"1996","title":"Manual of Apple Pie Parser","source":"Satoshi Sekine. 1996. Manual of Apple Pie Parser. URL: http://nlp.cs.nyu.edu/app/."},{"authors":[{"first":"Jiri","last":"Stetina"},{"first":"Makoto","last":"Nagao"}],"year":"1998","title":"General Word Sense Disambiguation Method Based on a Full Sentential Context","source":"Jiri Stetina and Makoto Nagao. 1998. General Word Sense Disambiguation Method Based on a Full Sentential Context. In Journal of Natural Language Processing, 5(2):47-74."}],"cites":[{"style":0,"text":"Ide and Véronis, 1998","origin":{"pointer":"/sections/1/paragraphs/0","offset":163,"length":21},"authors":[{"last":"Ide"},{"last":"Véronis"}],"year":"1998","references":[]},{"style":0,"text":"Kilgarriff, 1998","origin":{"pointer":"/sections/1/paragraphs/4","offset":492,"length":16},"authors":[{"last":"Kilgarriff"}],"year":"1998","references":["/references/4"]},{"style":0,"text":"Charniak, 1993","origin":{"pointer":"/sections/2/paragraphs/7","offset":86,"length":14},"authors":[{"last":"Charniak"}],"year":"1993","references":["/references/0"]},{"style":0,"text":"Lin, 1997","origin":{"pointer":"/sections/2/paragraphs/7","offset":102,"length":9},"authors":[{"last":"Lin"}],"year":"1997","references":["/references/5"]},{"style":0,"text":"Mitaku and Kanehisa, 1995","origin":{"pointer":"/sections/3/paragraphs/3","offset":255,"length":25},"authors":[{"last":"Mitaku"},{"last":"Kanehisa"}],"year":"1995","references":[]},{"style":0,"text":"Stetina and Nagao, 1998","origin":{"pointer":"/sections/3/paragraphs/41","offset":50,"length":23},"authors":[{"last":"Stetina"},{"last":"Nagao"}],"year":"1998","references":["/references/10"]},{"style":0,"text":"Sekine, 1996","origin":{"pointer":"/sections/4/paragraphs/1","offset":150,"length":12},"authors":[{"last":"Sekine"}],"year":"1996","references":["/references/9"]}]}
