{"sections":[{"title":"Query-Relevant Summarization using FAQs Adam Berger Vibhu O. Mittal School of Computer Science Just Research Carnegie Mellon University 4616 Henry Street Pittsburgh, PA 15213 Pittsburgh, PA 15213 aberger@cs.cmu.edu mittal@justresearch.com Abstract","paragraphs":["This paper introduces a statistical model for query-relevant summarization: succinctly characterizing the relevance of a document to a query. Learning parameter values for the proposed model requires a large collection of summarized documents, which we do not have, but as a proxy, we use a collection of FAQ (frequently-asked question) documents. Taking a learning approach enables a principled, quantitative evaluation of the proposed system, and the results of some initial experiments—on a collection of Usenet FAQs and on a FAQ-like set of customer-submitted questions to several large retail companies—suggest the plausibility of learning for summarization."]},{"title":"1 Introduction","paragraphs":["An important distinction in document summarization is between generic summaries, which capture the central ideas of the document in much the same way that the abstract of this paper was designed to distill its salient points, and query-relevant summaries, which reflect the relevance of a document to a user-specified query. This paper discusses query-relevant summarization, sometimes also called “user-focused summarization” (Mani and Bloedorn, 1998).","Query-relevant summaries are especially important in the “needle(s) in a haystack” document retrieval problem: a user has an information need expressed as a query (What countries export smoked salmon?), and a retrieval system must locate within a large collection of documents those documents most likely to fulfill this need. Many interactive retrieval systems—web search engines like Altavista, for instance—present the user with a small set of candidate relevant documents, each summarized; the user must then perform a kind of triage to identify likely relevant documents from this set. The web page summaries presented by most search engines are generic, not query-relevant, and thus provide very little guidance to the user in assessing relevance. Query-relevant summarization (QRS) aims to provide a more effective characterization of a document by accounting for the user’s information need when generating a summary. Search for relevant documents          Summarize documents relative to Q"]},{"title":"s      s     s     s     ","paragraphs":["(a) (b) Figure 1: One promising setting for query-relevant summarization is large-scale document retrieval. Given a user query",", search engines typically first (a) identify a set of documents which appear potentially relevant to the query, and then (b) produce a short characterization","of each document’s relevance to",". The purpose of","is to as-sist the user in finding documents that merit a more detailed inspection.","As with almost all previous work on summarization, this paper focuses on the task of extractive summarization: selecting as summaries text spans—either complete sentences or paragraphs—from the original document. 1.1 Statistical models for summarization","From a document","and query",", the task of query-","relevant summarization is to extract a portion","from","","which best reveals how the document relates to","the query. To begin, we start with a collection","of"," triplets, where","is a human-constructed sum-","mary of relative to the query",". From such a collec-Snow is not unusual in France... D1 S1","Q1 = Weather in Paris in December D2 Some parents elect to teach their children at home... S2","Q2 = Home schooling D3 Good Will Hunting is about... S3","Q3 = Academy award winners in 1998     ","","","","","","      ","","","","","     ","     ","","","      ","","","      ","","","","","      . . .",".",".",".",".",".",". . . . . . . . . . Figure 2: Learning to perform query-relevant summarization requires a set of documents summarized with respect to queries. Here we show three imaginary triplets",", but the statistical learning techniques described in Section 2 require thousands of examples. tion of data, we fit the best function mapping document/query pairs to summaries. The mapping we use is a probabilistic one, meaning the system assigns a value","to every possible summary","of",". The QRS system will summarize a","pair by selecting  def","  There are at least two ways to interpret",". First, one could view","as a “degree of be-lief” that the correct summary of","relative to","is",". Of course, what constitutes a good summary in any setting is subjective: any two people performing the same summarization task will likely disagree on which part of the document to extract. We could, in principle, ask a large number of people to perform the same task. Doing so would impose a distribution","over candidate summaries. Under the second, or “frequentist” interpretation,","is the fraction of people who would select","—equivalently, the probability that a person selected at random would prefer","as the summary.","The statistical model","is parametric, the values of which are learned by inspection of the ","triplets. The learning process involves maximum-likelihood estimation of probabilistic language models and the statistical technique of shrinkage (Stein, 1955).","This probabilistic approach easily generalizes to the generic summarization setting, where there is no query. In that case, the training data consists of  ","pairs, where","is a summary of the document",". The","goal, in this case, is to learn and apply a mapping"," ","from documents to summaries. That is,   A single FAQ document     Summary of document with respect to Q2 .  .  . What is amniocentesis? Amniocenteses, or amnio, is a prenatal test in which... What can it detect? One of the main uses of amniocentesis is to detect chromosomal abnormalities... What are the risks of amnio? The main risk of amnio is that it may increase the chance of miscarriage... Figure 3: FAQs consist of a list of questions and answers on a single topic; the FAQ depicted here is part of an in-formational document on amniocentesis. This paper views answers in a FAQ as different summaries of the FAQ: the answer to the","th question is a summary of the FAQ relative to that question. find  def","  1.2 Using FAQ data for summarization We have proposed using statistical learning to construct a summarization system, but have not yet discussed the one crucial ingredient of any learning procedure: training data. The ideal training data would contain a large number of heterogeneous documents, a large number of queries, and summaries of each document relative to each query. We know of no such publicly-available collection. Many studies on text summarization have focused on the task of summarizing newswire text, but there is no obvious way to use news articles for query-relevant summarization within our proposed framework.","In this paper, we propose a novel data collection for training a QRS model: frequently-asked question documents. Each frequently-asked question document (FAQ) is comprised of questions and answers about a specific topic. We view each answer in a FAQ as a summary of the document relative to the question which preceded it. That is, an FAQ with","question/answer pairs comes equipped with","different queries and summaries: the answer to the","th question is a summary of the document relative to the","th question. While a somewhat unorthodox perspective, this insight allows us to enlist FAQs as labeled training data for the purpose of learning the parameters of a statistical QRS model.","FAQ data has some properties that make it particularly attractive for text learning:"," There exist a large number of Usenet FAQs— several thousand documents—publicly available on the Web1",". Moreover, many large companies maintain their own FAQs to streamline the customer-response process."," FAQs are generally well-structured documents, so the task of extracting the constituent parts (queries and answers) is amenable to automation. There have even been proposals for standardized FAQ formats, such as RFC1153 and the Minimal Digest Format (Wancho, 1990)."," Usenet FAQs cover an astonishingly wide variety of topics, ranging from extraterrestrial visitors to mutual-fund investing. If there’s an online community of people with a common interest, there’s likely to be a Usenet FAQ on that subject.","There has been a small amount of published work involving question/answer data, including (Sato and Sato, 1998) and (Lin, 1999). Sato and Sato used FAQs as a source of summarization corpora, although in quite a different context than that presented here. Lin used the datasets from a question/answer task within the Tipster project, a dataset of considerably smaller size than the FAQs we employ. Neither of these paper focused on a statistical machine learning approach to summarization."]},{"title":"2 A probabilistic model of summarization","paragraphs":["Given a query","and document",", the query-relevant","summarization task is to find  ","  the a posteriori most probable summary for",". Using Bayes’ rule, we can rewrite this expression as  "," "," "," ","  ",""," relevance   "," fidelity"," (1) where the last line follows by dropping the dependence on","in",". Equation (1) is a search problem: find the summary   which maximizes the product of two factors:","1. The relevance","of the query to the summary: A document may contain some portions directly relevant to the query, and other sections bearing little or no relation to the query. Consider, for instance, the problem of summarizing a 1 Two online sources for FAQ data are www.faqs.org","and rtfm.mit.edu. survey on the history of organized sports relative to the query “Who was Lou Gehrig?” A summary mentioning Lou Gehrig is probably more relevant to this query than one describing the rules of volleyball, even if two-thirds of the survey happens to be about volleyball.","2. The fidelity","of the summary to the document: Among a set of candidate summaries whose relevance scores are comparable, we should prefer that summary","which is most representative of the document as a whole. Sum-maries of documents relative to a query can of-ten mislead a reader into overestimating the relevance of an unrelated document. In particular, very long documents are likely (by sheer luck) to contain some portion which appears related to the query. A document having nothing to do with Lou Gehrig may include a mention of his name in passing, perhaps in the context of amyotropic lateral sclerosis, the disease from which he suffered. The fidelity term guards against this occurrence by rewarding or penalizing candidate summaries, depending on whether they are germane to the main theme of the document. More generally, the fidelity term represents a prior, query-independent distribution over candidate summaries. In addition to enforcing fidelity, this term could serve to distinguish between more and less fluent candidate summaries, in much the same way that traditional language models steer a speech dictation system towards more fluent hypothesized transcriptions.","In words, (1) says that the best summary of a document relative to a query is relevant to the query (exhibits a large","value) and also representative of the document from which it was extracted (exhibits a large","value). We now describe the parametric form of these models, and how one can determine optimal values for these parameters using maximum-likelihood estimation. 2.1 Language modeling The type of statistical model we employ for both ","and","is a unigram probability distribution over words; in other words, a language model. Stochastic models of language have been used extensively in speech recognition, optical character recogni-tion, and machine translation (Jelinek, 1997; Berger et al., 1994). Language models have also started to find their way into document retrieval (Ponte and Croft, 1998; Ponte, 1998). The fidelity model One simple statistical characterization of an","-word document","     "," is the frequency of each word in","—in other words, a marginal distribution over words. That is, if word","appears","times in  , then ","",". This is not only intuitive, but","also the maximum-likelihood estimate for",".","Now imagine that, when asked to summarize","rel-","ative to",", a person generates a summary from","in the","following way:"," Select a length","for the summary according to some distribution","","."," Do for"," :","– Select a word","at random according to the","distribution","",". (That is, throw all the words","in","into a bag, pull one out, and then re-","place it.)","– Set",".","In following this procedure, the person will generate the summary","       with probability        ","  ","   ","(2) Denoting by","the set of all known words, and by  ","the number of times that word","appears in  , one can also write (2) as a multinomial distribution:       ",""," (3)","In the text classification literature, this characterization of","is known as a “bag of words” model, since the distribution","does not take account of the order of the words within the document",", but rather views","as an unordered set (“bag”) of words. Of course, ignoring word order amounts to discarding potentially valuable information. In Figure 3, for instance, the second question contains an anaphoric reference to the preceding question: a sophisticated context-sensitive model of language might be able to detect that it in this context refers to amniocentesis, but a context-free model will not. The relevance model","In principle, one could proceed analogously to (2), and take        ","   ","(4)","for a length-query","     ","",". But this strategy suffers from a sparse estimation problem. In contrast to a document, which we expect will typically contain a few hundred words, a normal-sized summary contains just a handful of words. What this means is that","","will assign zero probability to most words, and        ","","","","","","","","","","","","    ","","","","","","","       ","","","","","","","","","","","","","","","","","","","","","Figure 4: The relevance","","of a query to the","th an-","swer in document is a convex combination of five distribu-","tions: (1) a uniform model",". (2) a corpus-wide model",";","(3) a model","constructed from the document containing","","","","; (4) a model ","constructed from","","","and the neigh-","boring sentences in","; (5) a model","","","constructed from","","","","alone. (The distribution is omitted for clarity.)","any query containing a word not in the summary will","receive a relevance score of zero. (The fidelity model doesn’t suffer from zero-","probabilities, at least not in the extractive summariza-","tion setting. Since a summary","is part of its contain-","ing document",", every word in","also appears in",",","and therefore",""," "," for every word ","",". But","we have no guarantee, for the relevance model, that a","summary contains all the words in the query.) We address this zero-probability problem by inter-","polating or “smoothing” the","","model with four more","robustly estimated unigram word models. Listed in","order of decreasing variance but increasing bias away","from","",", they are:","",": a probability distribution constructed using","not only , but also all words within the six sum-","maries (answers) surrounding","in",". Since","","is calculated using more text than just","alone, its","parameter estimates should be more robust that","those of  . On the other hand, the","","model is,","by construction, biased away from","",", and there-","fore provides only indirect evidence for the rela-","tion between","and",".","",": a probability distribution constructed over the","entire document","containing",". This model has","even less variance than",", but is even more bi-","ased away from","",".  : a probability distribution constructed over all documents","."," : the uniform distribution over all words. Figure 4 is a hierarchical depiction of the various","language models which come into play in calculating","",". Each summary model","","lives at a leaf node,","and the relevance","of a query to that summary is","a convex combination of the distributions at each node Algorithm: Shrinkage for  estimation","Input: Distributions","","","",",","","","","","(not used to","estimate","","  ","","",") Output Model weights","  ","            1. Set          ","    2. Repeat until  converges: 3. Set "," for  ","5. (E-step)  ","","         ","","(similarly for ) 6. (M-step) ",""," ","    (similarly for ","          )","along a path from the leaf to the root2 :","","","","","","","","","","","(5)       We calculate the weighting coefficients","            ","","","using the statistical technique","known as shrinkage (Stein, 1955), a simple form of","the EM algorithm (Dempster et al., 1977).","As a practical matter, if one assumes the","","model","assigns probabilities independently of",", then we can","drop the  term when ranking candidate summaries,","since the score of all candidate summaries will re-","ceive an identical contribution from the","","term. We","make this simplifying assumption in the experiments","reported in the following section."]},{"title":"3 Results","paragraphs":["To gauge how well our proposed summarization technique performs, we applied it to two different realworld collections of answered questions: Usenet FAQs: A collection of ","frequently-asked question documents from the comp.* Usenet hierarchy. The documents contained    questions/answer pairs in total. Call-center data: A collection of questions submitted by customers to the companies Air Canada, Ben and Jerry, Iomagic, and Mylex, along with the answers supplied by company 2 By incorporating a","model into the relevance model,","equation (6) has implicitly resurrected the dependence on","which we dropped, for the sake of simplicity, in deriving (1). representatives. These four documents contain    ","question/answer pairs.","We conducted an identical, parallel set of experi-","ments on both. First, we used a randomly-selected","subset of 70% of the question/answer pairs to calcu-","late the language models  ","","","","","","—a simple","matter of counting word frequencies. Then, we used","this same set of data to estimate the model weights","","  ","   ","        ","","using shrinkage. We reserved the remaining 30% of the question/answer pairs to evaluate the performance of the system, in a manner described below.","Figure 5 shows the progress of the EM algorithm in calculating maximum-likelihood values for the smoothing coefficients ",", for the first of the three runs on the Usenet data. The quick convergence and the final ","values were essentially identical for the other partitions of this dataset. The call-center data’s convergence behavior was similar, although the final ","values were quite different. Figure 6 shows the final model weights for the first of the three experiments on both datasets. For the Usenet FAQ data, the corpus language model is the best predictor of the query and thus receives the highest weight. This may seem counterintuitive; one might suspect that answer to the query (",", that is) would be most similar to, and therefore the best predictor of, the query. But the corpus model, while certainly biased away from the distribution of words found in the query, contains (by construction) no zeros, whereas each summary model is typically very sparse.","In the call-center data, the corpus model weight is lower at the expense of a higher document model weight. We suspect this arises from the fact that the documents in the Usenet data were all quite similar to one another in lexical content, in contrast to the call-center documents. As a result, in the call-center data the document containing","will appear much more relevant than the corpus as a whole.","To evaluate the performance of the trained QRS model, we used the previously-unseen portion of the FAQ data in the following way. For each test pair, we recorded how highly the system ranked the correct summary","","—the answer to","in","—relative to the other answers in",". We repeated this entire sequence three times for both the Usenet and the call-center data.","For these datasets, we discovered that using a uniform fidelity term in place of the","","model described above yields essentially the same result. This is not surprising: while the fidelity term is an important component of a real summarization system, our evaluation was conducted in an answer-locating framework, and in this context the fidelity term—enforcing that the summary be similar to the entire document from which 0 0.1 0.2 0.3 0.4 0.5","1 2 3 4 5 6 7 8 9 10 iteration m o d e l  w e i g h t uniform corpus FAQ nearby answers answer -6.9 -6.8 -6.7 -6.6 -6.5 -6.4 -6.3","1 2 3 4 5 6 7 8 9 10 Iteration L o g - l i k e l i h o o d Test Training Figure 5: Estimating the weights of the five constituent models in (6) using the EM algorithm. The values here were computed using a single, randomly-selected 70% portion of the Usenet FAQ dataset. Left: The weights","for the models are initialized to ",", but within a few iterations settle to their final values. Right: The progression of the likelihood of the training data during the execution of the EM algorithm; almost all of the improvement comes in the first five iterations.","","","","  ","","","","","Usenet FAQ","  "," ","    call-center ","     ","  ","     Summary 29% Neighbors 10% Document 14% Corpus 47%","Uniform","0% Summary 11% Neighbors","0% Document 40% Corpus 42% Uniform 7% Figure 6: Maximum-likelihood weights for the various components of the relevance model","",". Left: Weights assigned to the constituent models from the Usenet FAQ data. Right: Corresponding breakdown for the call-center data. These weights were calculated using shrinkage.","it was drawn—is not so important. From a set of rankings  ","     ","",", one can","measure the the quality of a ranking algorithm using","the harmonic mean rank:"," def","        A lower number indicates better performance;"," ",", which is optimal, means that the algorithm consistently assigns the first rank to the correct answer. Table 1 shows the harmonic mean rank on the two collections. The third column of Table 1 shows the result of a QRS system using a uniform fidelity model, the fourth corresponds to a standard tfidf-based ranking method (Ponte, 1998), and the last column reflects the performance of randomly guessing the correct summary from all answers in the document.","trial # trials LM tfidf random Usenet 1 554 1.41 2.29 4.20 FAQ 2 549 1.38 2.42 4.25 data 3 535 1.40 2.30 4.19 Call 1 1020 4.8 38.7 1335 center 2 1055 4.0 22.6 1335 data 3 1037 4.2 26.0 1321 Table 1: Performance of query-relevant extractive summarization on the Usenet and call-center datasets. The numbers reported in the three rightmost columns are harmonic mean ranks: lower is better."]},{"title":"4 Extensions 4.1 Question-answering","paragraphs":["The reader may by now have realized that our approach to the QRS problem may be portable to the problem of question-answering. By question-answering, we mean a system which automatically extracts from a potentially lengthy document (or set of documents) the answer to a user-specified question. Devising a highquality question-answering system would be of great service to anyone lacking the inclination to read an entire user’s manual just to find the answer to a single question. The success of the various automated question-answering services on the Internet (such as AskJeeves) underscores the commercial importance of this task.","One can cast answer-finding as a traditional document retrieval problem by considering each candidate answer as an isolated document and ranking each candidate answer by relevance to the query. Traditional tfidf-based ranking of answers will reward candidate answers with many words in common with the query. Employing traditional vector-space retrieval to find answers seems attractive, since tfidf is a standard, timetested algorithm in the toolbox of any IR professional.","What this paper has described is a first step towards more sophisticated models of question-answering. First, we have dispensed with the simplifying assumption that the candidate answers are independent of one another by using a model which explicitly accounts for the correlation between text blocks—candidate answers—within a single document. Second, we have put forward a principled statistical model for answer-ranking;  ","","","has a probabilistic in-terpretation as the best answer to","within","is",".","Question-answering and query-relevant summarization are of course not one and the same. For one, the criterion of containing an answer to a question is rather stricter than mere relevance. Put another way, only a small number of documents actually contain the answer to a given query, while every document can in principle be summarized with respect to that query. Second, it would seem that the","term, which acts as a prior on summaries in (1), is less appropriate in a question-answering setting, where it is less important that a candidate answer to a query bears resemblance to the document containing it. 4.2 Generic summarization Although this paper focuses on the task of query-relevant summarization, the core ideas—formulating a probabilistic model of the problem and learning the values of this model automatically from FAQ-like data—are equally applicable to generic summarization. In this case, one seeks the summary which best typifies the document. Applying Bayes’ rule as in (1),","  "," "," ","  ",""," generative   "," prior (6) The first term on the right is a generative model of documents from summaries, and the second is a prior distribution over summaries. One can think of this factorization in terms of a dialogue. Alice, a newspaper editor, has an idea","for a story, which she relates to Bob. Bob researches and writes the story",", which we can view as a “corruption” of Alice’s original idea",". The task of generic summarization is to recover",", given only the generated document",", a model","of how the Alice generates summaries from documents, and a prior distribution","on ideas",".","The central problem in information theory is reliable communication through an unreliable channel. We can interpret Alice’s idea","as the original signal, and the process by which Bob turns this idea into a document ","as the channel, which corrupts the original message. The summarizer’s task is to “decode” the original, condensed message from the document.","We point out this source-channel perspective be-cause of the increasing influence that information theory has exerted on language and information-related applications. For instance, the source-channel model has been used for non-extractive summarization, generating titles automatically from news articles (Witbrock and Mittal, 1999).","The factorization in (6) is superficially similar to (1), but there is an important difference:","","is a generative, from a summary to a larger document, whereas  ","is compressive, from a summary to a smaller query. This distinction is likely to translate in practice into quite different statistical models and training procedures in the two cases."]},{"title":"5 Summary","paragraphs":["The task of summarization is difficult to define and even more difficult to automate. Historically, a rewarding line of attack for automating language-related problems has been to take a machine learning perspective: let a computer learn how to perform the task by “watching” a human perform it many times. This is the strategy we have pursued here.","There has been some work on learning a probabilistic model of summarization from text; some of the earliest work on this was due to Kupiec et al. (1995), who used a collection of manually-summarized text to learn the weights for a set of features used in a generic summarization system. Hovy and Lin (1997) present another system that learned how the position of a sentence affects its suitability for inclusion in a summary of the document. More recently, there has been work on building more complex, structured models—probabilistic syntax trees—to compress single sentences (Knight and Marcu, 2000). Mani and Bloedorn (1998) have recently proposed a method for automatically constructing decision trees to predict whether a sentence should or should not be included in a document’s summary. These previous approaches focus mainly on the generic summarization task, not query relevant summarization.","The language modelling approach described here does suffer from a common flaw within text processing systems: the problem of synonymy. A candidate answer containing the term Constantinople is likely to be relevant to a question about Istanbul, but recognizing this correspondence requires a step beyond word frequency histograms. Synonymy has received much attention within the document retrieval community recently, and researchers have applied a variety of heuristic and statistical techniques—including pseudo-relevance feedback and local context analysis (Efthimiadis and Biron, 1994; Xu and Croft, 1996). Some recent work in statistical IR has extended the basic language modelling approaches to account for word synonymy (Berger and Lafferty, 1999).","This paper has proposed the use of two novel datasets for summarization: the frequently-asked questions (FAQs) from Usenet archives and question/answer pairs from the call centers of retail companies. Clearly this data isn’t a perfect fit for the task of building a QRS system: after all, answers are not summaries. However, we believe that the FAQs represent a reasonable source of query-related document condensations. Furthermore, using FAQs allows us to assess the effectiveness of applying standard statistical learning machinery—maximum-likelihood estimation, the EM algorithm, and so on—to the QRS problem. More importantly, it allows us to evaluate our results in a rigorous, non-heuristic way. Although this work is meant as an opening salvo in the battle to conquer summarization with quantitative, statistical weapons, we expect in the future to enlist linguistic, semantic, and other non-statistical tools which have shown promise in condensing text."]},{"title":"Acknowledgments","paragraphs":["This research was supported in part by an IBM University Partnership Award and by Claritech Corporation. The authors thank Right Now Tech for the use of the call-center question database. We also acknowledge thoughtful comments on this paper by Inderjeet Mani."]},{"title":"References","paragraphs":["A. Berger and J. Lafferty. 1999. Information retrieval as statistical translation. In Proc. of ACM SIGIR-99.","A. Berger, P. Brown, S. Della Pietra, V. Della Pietra, J. Gillett, J. Lafferty, H. Printz, and L. Ures. 1994. The CANDIDE system for machine translation. In Proc. of the ARPA Human Language Technology Workshop.","Y. Chali, S. Matwin, and S. Szpakowicz. 1999. Query-biased text summarization as a question-answering technique. In Proc. of the AAAI Fall Symp. on Ques-tion Answering Systems, pages 52–56.","A. Dempster, N. Laird, and D. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, 39B:1–38.","E. Efthimiadis and P. Biron. 1994. UCLA-Okapi at TREC-2: Query expansion experiments. In Proc. of the Text Retrieval Conference (TREC-2).","E. Hovy and C. Lin. 1997. Automated text summarization in SUMMARIST. In Proc. of the ACL Wkshp on Intelligent Text Summarization, pages 18–24.","F. Jelinek. 1997. Statistical methods for speech recognition. MIT Press.","K. Knight and D. Marcu. 2000. Statistics-based summarization—Step one: Sentence compression. In Proc. of AAAI-00. AAAI.","J. Kupiec, J. Pedersen, and F. Chen. 1995. A trainable document summarizer. In Proc. SIGIR-95, pages 68–73, July.","Chin-Yew Lin. 1999. Training a selection function for extraction. In Proc. of the Eighth ACM CIKM Conference, Kansas City, MO.","I. Mani and E. Bloedorn. 1998. Machine learning of generic and user-focused summarization. In Proc. of AAAI-98, pages 821–826.","J. Ponte and W. Croft. 1998. A language modeling approach to information retrieval. In Proc. of SIGIR-98, pages 275–281.","J. Ponte. 1998. A language modelling approach to information retrieval. Ph.D. thesis, University of Massachusetts at Amherst.","S. Sato and M. Sato. 1998. Rewriting saves extracted summaries. In Proc. of the AAAI Intelligent Text Summarization Workshop, pages 76–83.","C. Stein. 1955. Inadmissibility of the usual estimator for the mean of a multivariate normal distribution. In Proc. of the Third Berkeley symposium on mathematical statistics and probability, pages 197–206. F. Wancho. 1990. RFC 1153: Digest message format.","M. Witbrock and V. Mittal. 1999. Headline Genera-tion: A framework for generating highly-condensed non-extractive summaries. In Proc. of ACM SIGIR-99, pages 315–316.","J. Xu and B. Croft. 1996. Query expansion using local and global document analysis. In Proc. of ACM SIGIR-96."]}],"references":[{"authors":[{"first":"A.","last":"Berger"},{"first":"J.","last":"Lafferty"}],"year":"1999","title":"Information retrieval as statistical translation","source":"A. Berger and J. Lafferty. 1999. Information retrieval as statistical translation. In Proc. of ACM SIGIR-99."},{"authors":[{"first":"A.","last":"Berger"},{"first":"P.","last":"Brown"},{"first":"S.","middle":"Della","last":"Pietra"},{"first":"V.","middle":"Della","last":"Pietra"},{"first":"J.","last":"Gillett"},{"first":"J.","last":"Lafferty"},{"first":"H.","last":"Printz"},{"first":"L.","last":"Ures"}],"year":"1994","title":"The CANDIDE system for machine translation","source":"A. Berger, P. Brown, S. Della Pietra, V. Della Pietra, J. Gillett, J. Lafferty, H. Printz, and L. Ures. 1994. The CANDIDE system for machine translation. In Proc. of the ARPA Human Language Technology Workshop."},{"authors":[{"first":"Y.","last":"Chali"},{"first":"S.","last":"Matwin"},{"first":"S.","last":"Szpakowicz"}],"year":"1999","title":"Query-biased text summarization as a question-answering technique","source":"Y. Chali, S. Matwin, and S. Szpakowicz. 1999. Query-biased text summarization as a question-answering technique. In Proc. of the AAAI Fall Symp. on Ques-tion Answering Systems, pages 52–56."},{"authors":[{"first":"A.","last":"Dempster"},{"first":"N.","last":"Laird"},{"first":"D.","last":"Rubin"}],"year":"1977","title":"Maximum likelihood from incomplete data via the EM algorithm","source":"A. Dempster, N. Laird, and D. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, 39B:1–38."},{"authors":[{"first":"E.","last":"Efthimiadis"},{"first":"P.","last":"Biron"}],"year":"1994","title":"UCLA-Okapi at TREC-2: Query expansion experiments","source":"E. Efthimiadis and P. Biron. 1994. UCLA-Okapi at TREC-2: Query expansion experiments. In Proc. of the Text Retrieval Conference (TREC-2)."},{"authors":[{"first":"E.","last":"Hovy"},{"first":"C.","last":"Lin"}],"year":"1997","title":"Automated text summarization in SUMMARIST","source":"E. Hovy and C. Lin. 1997. Automated text summarization in SUMMARIST. In Proc. of the ACL Wkshp on Intelligent Text Summarization, pages 18–24."},{"authors":[{"first":"F.","last":"Jelinek"}],"year":"1997","title":"Statistical methods for speech recognition","source":"F. Jelinek. 1997. Statistical methods for speech recognition. MIT Press."},{"authors":[{"first":"K.","last":"Knight"},{"first":"D.","last":"Marcu"}],"year":"2000","title":"Statistics-based summarization—Step one: Sentence compression","source":"K. Knight and D. Marcu. 2000. Statistics-based summarization—Step one: Sentence compression. In Proc. of AAAI-00. AAAI."},{"authors":[{"first":"J.","last":"Kupiec"},{"first":"J.","last":"Pedersen"},{"first":"F.","last":"Chen"}],"year":"1995","title":"A trainable document summarizer","source":"J. Kupiec, J. Pedersen, and F. Chen. 1995. A trainable document summarizer. In Proc. SIGIR-95, pages 68–73, July."},{"authors":[{"first":"Chin-Yew","last":"Lin"}],"year":"1999","title":"Training a selection function for extraction","source":"Chin-Yew Lin. 1999. Training a selection function for extraction. In Proc. of the Eighth ACM CIKM Conference, Kansas City, MO."},{"authors":[{"first":"I.","last":"Mani"},{"first":"E.","last":"Bloedorn"}],"year":"1998","title":"Machine learning of generic and user-focused summarization","source":"I. Mani and E. Bloedorn. 1998. Machine learning of generic and user-focused summarization. In Proc. of AAAI-98, pages 821–826."},{"authors":[{"first":"J.","last":"Ponte"},{"first":"W.","last":"Croft"}],"year":"1998","title":"A language modeling approach to information retrieval","source":"J. Ponte and W. Croft. 1998. A language modeling approach to information retrieval. In Proc. of SIGIR-98, pages 275–281."},{"authors":[{"first":"J.","last":"Ponte"}],"year":"1998","title":"A language modelling approach to information retrieval","source":"J. Ponte. 1998. A language modelling approach to information retrieval. Ph.D. thesis, University of Massachusetts at Amherst."},{"authors":[{"first":"S.","last":"Sato"},{"first":"M.","last":"Sato"}],"year":"1998","title":"Rewriting saves extracted summaries","source":"S. Sato and M. Sato. 1998. Rewriting saves extracted summaries. In Proc. of the AAAI Intelligent Text Summarization Workshop, pages 76–83."},{"authors":[{"first":"C.","last":"Stein"}],"year":"1955","title":"Inadmissibility of the usual estimator for the mean of a multivariate normal distribution","source":"C. Stein. 1955. Inadmissibility of the usual estimator for the mean of a multivariate normal distribution. In Proc. of the Third Berkeley symposium on mathematical statistics and probability, pages 197–206. F. Wancho. 1990. RFC 1153: Digest message format."},{"authors":[{"first":"M.","last":"Witbrock"},{"first":"V.","last":"Mittal"}],"year":"1999","title":"Headline Genera-tion: A framework for generating highly-condensed non-extractive summaries","source":"M. Witbrock and V. Mittal. 1999. Headline Genera-tion: A framework for generating highly-condensed non-extractive summaries. In Proc. of ACM SIGIR-99, pages 315–316."},{"authors":[{"first":"J.","last":"Xu"},{"first":"B.","last":"Croft"}],"year":"1996","title":"Query expansion using local and global document analysis","source":"J. Xu and B. Croft. 1996. Query expansion using local and global document analysis. In Proc. of ACM SIGIR-96."}],"cites":[{"style":0,"text":"Mani and Bloedorn, 1998","origin":{"pointer":"/sections/1/paragraphs/0","offset":428,"length":23},"authors":[{"last":"Mani"},{"last":"Bloedorn"}],"year":"1998","references":["/references/10"]},{"style":0,"text":"Stein, 1955","origin":{"pointer":"/sections/2/paragraphs/68","offset":147,"length":11},"authors":[{"last":"Stein"}],"year":"1955","references":["/references/14"]},{"style":0,"text":"Wancho, 1990","origin":{"pointer":"/sections/2/paragraphs/86","offset":252,"length":12},"authors":[{"last":"Wancho"}],"year":"1990","references":[]},{"style":0,"text":"Sato and Sato, 1998","origin":{"pointer":"/sections/2/paragraphs/88","offset":91,"length":19},"authors":[{"last":"Sato"},{"last":"Sato"}],"year":"1998","references":["/references/13"]},{"style":0,"text":"Lin, 1999","origin":{"pointer":"/sections/2/paragraphs/88","offset":117,"length":9},"authors":[{"last":"Lin"}],"year":"1999","references":["/references/9"]},{"style":0,"text":"Jelinek, 1997","origin":{"pointer":"/sections/3/paragraphs/26","offset":221,"length":13},"authors":[{"last":"Jelinek"}],"year":"1997","references":["/references/6"]},{"style":0,"text":"Berger et al., 1994","origin":{"pointer":"/sections/3/paragraphs/26","offset":236,"length":19},"authors":[{"last":"Berger"},{"last":"al."}],"year":"1994","references":["/references/1"]},{"style":0,"text":"Ponte and Croft, 1998","origin":{"pointer":"/sections/3/paragraphs/26","offset":335,"length":21},"authors":[{"last":"Ponte"},{"last":"Croft"}],"year":"1998","references":["/references/11"]},{"style":0,"text":"Ponte, 1998","origin":{"pointer":"/sections/3/paragraphs/26","offset":358,"length":11},"authors":[{"last":"Ponte"}],"year":"1998","references":["/references/12"]},{"style":0,"text":"Stein, 1955","origin":{"pointer":"/sections/3/paragraphs/259","offset":20,"length":11},"authors":[{"last":"Stein"}],"year":"1955","references":["/references/14"]},{"style":0,"text":"Dempster et al., 1977","origin":{"pointer":"/sections/3/paragraphs/260","offset":18,"length":21},"authors":[{"last":"Dempster"},{"last":"al."}],"year":"1977","references":["/references/3"]},{"style":0,"text":"Ponte, 1998","origin":{"pointer":"/sections/4/paragraphs/73","offset":323,"length":11},"authors":[{"last":"Ponte"}],"year":"1998","references":["/references/12"]},{"style":0,"text":"Witbrock and Mittal, 1999","origin":{"pointer":"/sections/5/paragraphs/29","offset":307,"length":25},"authors":[{"last":"Witbrock"},{"last":"Mittal"}],"year":"1999","references":["/references/15"]},{"style":0,"text":"Kupiec et al. (1995)","origin":{"pointer":"/sections/6/paragraphs/1","offset":132,"length":20},"authors":[{"last":"Kupiec"},{"last":"al."}],"year":"1995","references":["/references/8"]},{"style":0,"text":"Hovy and Lin (1997)","origin":{"pointer":"/sections/6/paragraphs/1","offset":287,"length":19},"authors":[{"last":"Hovy"},{"last":"Lin"}],"year":"1997","references":["/references/5"]},{"style":0,"text":"Knight and Marcu, 2000","origin":{"pointer":"/sections/6/paragraphs/1","offset":578,"length":22},"authors":[{"last":"Knight"},{"last":"Marcu"}],"year":"2000","references":["/references/7"]},{"style":0,"text":"Mani and Bloedorn (1998)","origin":{"pointer":"/sections/6/paragraphs/1","offset":603,"length":24},"authors":[{"last":"Mani"},{"last":"Bloedorn"}],"year":"1998","references":["/references/10"]},{"style":0,"text":"Efthimiadis and Biron, 1994","origin":{"pointer":"/sections/6/paragraphs/2","offset":560,"length":27},"authors":[{"last":"Efthimiadis"},{"last":"Biron"}],"year":"1994","references":["/references/4"]},{"style":0,"text":"Xu and Croft, 1996","origin":{"pointer":"/sections/6/paragraphs/2","offset":589,"length":18},"authors":[{"last":"Xu"},{"last":"Croft"}],"year":"1996","references":["/references/16"]},{"style":0,"text":"Berger and Lafferty, 1999","origin":{"pointer":"/sections/6/paragraphs/2","offset":728,"length":25},"authors":[{"last":"Berger"},{"last":"Lafferty"}],"year":"1999","references":["/references/0"]}]}
