{"sections":[{"title":"","paragraphs":["Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 341–344, Suntec, Singapore, 4 August 2009. c⃝2009 ACL and AFNLP"]},{"title":"A Succinct N-gram Language Model Taro Watanabe Hajime Tsukada Hideki Isozaki NTT Communication Science Laboratories 2-4 Hikaridai Seika-cho Soraku-gun Kyoto 619-0237 Japan {taro,tsukada,isozaki}@cslab.kecl.ntt.co.jp Abstract","paragraphs":["Efficient processing of tera-scale text data is an important research topic. This paper proposes lossless compression of N - gram language models based on LOUDS, a succinct data structure. LOUDS succinctly represents a trie with M nodes as a 2M +1bit string. We compress it further for the N -gram language model structure. We also use ‘variable length coding’and ‘block-wise compression’ to compress values associated with nodes. Experimental results for three large-scale N -gram compression tasks achieved a significant compression rate without any loss."]},{"title":"1 Introduction","paragraphs":["There has been an increase in available N -gram data and a large amount of web-scaled N -gram data has been successfully deployed in statistical machine translation. However, we need either a machine with hundreds of gigabytes of memory or a large computer cluster to handle them.","Either pruning (Stolcke, 1998; Church et al., 2007) or lossy randomizing approaches (Talbot and Brants, 2008) may result in a compact representation for the application run-time. However, the lossy approaches may reduce accuracy, and tuning is necessary. A lossless approach is obviously better than a lossy one if other conditions are the same. In addtion, a lossless approach can easly combined with pruning. Therefore, lossless representation of N -gram is a key issue even for lossy approaches.","Raj and Whittaker (2003) showed a general N - gram language model structure and introduced a lossless algorithm that compressed a sorted integer vector by recursively shifting a certain number of bits and by emitting index-value inverted vectors. However, we need more compact representation.","In this work, we propose a succinct way to represent the N -gram language model structure based on LOUDS (Jacobson, 1989; Delpratt et al., 2006). It was first introduced by Jacobson (1989) and requires only a small space close to the information-theoretic lower bound. For an M node ordinal trie, its information-theoretical lower bound is 2M − O(lg M ) bits (lg(x)=log2(x)) 1-gram 2-gram 3-gram","probability back-offpointer","word id probabilityback-off","pointer","word id probabilityback-off","pointer Figure 1: Data structure for language model and LOUDS succinctly represents it by a 2M +1 bit string. The space is further reduced by considering the N -gram structure. We also use variable length coding and block-wise compression to compress the values associated with each node, such as word ids, probabilities or counts.","We experimented with English Web 1T 5-gram from LDC consisting of 25 GB of gzipped raw text N -gram counts. By using 8-bit floating point quantization 1",", N -gram language models are compressed into 10 GB, which is comparable to a lossy representation (Talbot and Brants, 2008)."]},{"title":"2 N -gram Language Model","paragraphs":["We assume a back-off N -gram language model in which the conditional probability Pr(wn|wn−1","1 ) for an arbitrary N -gram wn","1 =(w1, ..., wn) is recursively computed as follows.","α (wn 1 ) if wn","1 exists.","β (wn−1","1 )Pr(wn|wn−1","2 ) if wn−1","1 exists.","Pr(wn|wn−1 2 ) otherwise. α (wn","1 ) and β (wn","1 ) are smoothed probabilities and back-off coefficients, respectively.","The N -grams are stored in a trie structure as shown in Figure 1. N -grams of different orders are stored in different tables and each row corresponds to a particular wn","1 , consisting of a word id for wn, α (wn","1 ), β (wn","1 ) and a pointer to the first position of the succeeding (n +1)-grams that share thesameprefixwn","1 . The succeeding (n+1)-grams are stored in a contiguous region and sorted by the word id of wn+1. The boundary of the region is determined by the pointer of the next N -gram in the","1","The compact representation of the floating point is out of the scope of this paper. Therefore, we use the term lossless even when using floating point quantization. 341 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 (a) Trie structure node id 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 bit position 01234567891011 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 LOUDS bit 1011110111 0 1100 101100 100 1100 0 0 0 0 0","(b) Corresponding LOUDS bit string 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 (c) Trie structure for N-gram node id 0 1 2 3 4 5 6 7 8 9 bit position 01234567 8910 11 12 13 14 15 16 17 18 19 20 LOUDS bit 11101100 10 1100 100 1100","(d) Corresponding N-gram optimized LOUDS bit string Figure 2: Optimization of LOUDS bit string for N -gram data row. When an N -gram is traversed, binary search is performed N times. If each word id corresponds to its node position in the unigram table, we can remove the word ids for the first order.","Our implementation merges across different orders of N -grams, then separates into multiple tables such as word ids, smoothed probabilities, back-off coefficients, and pointers. The starting positions of different orders are memorized to allow access to arbitrary orders. To store N -gram counts, we use three tables for word ids, counts and pointers. We share the same tables for word ids and pointers with additional probability and back-off coefficient tables.","To support distributed computation (Brants et al., 2007), we further split the N -gram data into “shards” by hash values of the first bigram. Uni-gram data are shared across shards for efficiency."]},{"title":"3 Succinct N -gram Structure","paragraphs":["The table of pointers described in the previous section represents a trie. We use a succinct data structure LOUDS (Jacobson, 1989; Delpratt et al., 2006) for compact representation of the trie.","For an M node ordinal trie, there exist","1 2M+1","(2M+1 M )","different tries. Therefore, its information-theoretical lower bound is lg ⌈","1 2M+1","(2M+1 M )⌉","≈ 2M − O(lg M ) bits. LOUDS represents a trie with M nodes as a 2M + O(M ) bit string.","The LOUDS bit string is constructed as follows. Starting from the root node, we traverse a trie in level order. For each node with d ≥ 0 children, the bit string 1d","0 is emitted. In addition, 10 is prefixed to the bit string emitted by an imaginary super-root node pointing to the root node. Figure 2(a) shows an example trie structure. The nodes are numbered inlevelorder,andfromlefttoright. Thecorresponding LOUDS bit string is shown in Figure 2(b). Since the root node 0 has four child nodes, it emits four 1s followed by 0, which marks the end of the node. Before the root node, we assume an imaginary super root node emits 10 for its only child, i.e., the root node. After the root node, its first child or node 1 follows. Since (M +1)0sand M 1s are emitted for a trie with M nodes, LOUDS occupies 2M +1bits.","We define a basic operation on the bit string. sel1(i) returns the position of the i-th 1.Wecan also define similar operations over zero bit strings, sel0(i).Givenselb, we define two operations for a node x. parent(x) gives x’s parent node and firstch(x) gives x’s first child node: parent(x)=sel1(x +1)− x − 1, (1) firstch(x)=sel0(x +1)− x. (2) To test whether a child node exists, we simply check firstch(x) ̸= firstch(x +1). Similarly, the child node range is determined by [firstch(x), firstch(x +1)). 3.1 Optimizing N -gram Structure for Space We propose removing redundant bits from the baseline LOUDS representation assuming N - gram structures. Since we do not store any information in the root node, we can safely remove the root so that the imaginary super-root node directly points to unigram nodes. The node ids are renumbered and the first unigram is 0. In this way, 2 bits are saved.","The N -gram data structure has a fixed depth N and takes a flat structure. Since the highest order N -grams have no child nodes, they emit 0NN in the tail of the bit stream, where Nn stands for the number of n-grams. By memorizing the starting position of the highest order N -grams, we can completely remove NN bits.","The imaginary super-root emits 1N1","0 at the beginning of the bit stream. By memorizing the bi-gram starting position, we can remove the N1 +1 bits.","Finally, parent(x) and firstch(x) are rewritten as 342 integer seq. 52 156 260 364 coding 0x34 0x9c 0x01 0x04 0x01 0x6c boundary 1 1 0101 Figure 3: Example of variable length coding follows: parent(x)=sel1(x +1−N1)+N1 − x, (3) firstch(x)=sel0(x)+N1 +1− x. (4) Figure 2(c) shows the N -gram optimized trie structure (N =3) from Figure 2 with N1 =4 and N3 =5. The parent of node 8 is found by sel1(8+1−4) = 5 and 5+4−8=1. The first child is located by sel0(8) = 16 and 16+4+1−8=13.","When accessing the N -gram data structure, selb(i) operations are used extensively. We use an auxiliary dictionary structure proposed by Kim et al. (2005) and Jacobson (1989) that supports an efficient sel1(i) (sel0(i)) with the dictionary. We omit the details due to lack of space. 3.2 Variable Length Coding The above method compactly represents pointers, but not associated values, such as word ids or counts. Raj and Whittaker (2003) proposed integer compression on each range of the word id sequence that shared the same N -gram prefix.","Here, we introduce a simple but more effective variable length coding for integer sequences of word ids and counts. The basic idea comes from encoding each integer by the smallest number of required bytes. Specifically, an integer within the range of 0 to 255 is coded as a 1-byte integer, the integers within the range of 256 to 65,535 are stored as 2-byte integers, and so on. We use an additional bit vector to indicate the boundary of the byte sequences. Figure 3 presents an example integer sequence, 52, 156, 260 and 364 with coded integers in hex decimals with boundary bits.","In spite of the length variability, the system can directly access a value at index i as bytes in [sel1(i)+1, sel1(i +1)+1)by the efficient sel1 operation assuming that sel1(0) yields −1. For example, the value 260 at index 2 in Figure 3 is mapped onto the byte range of [sel1(2) + 1, sel1(3) + 1) = [2, 4). 3.3 Block-wise Compression We further compress every 8K-byte data block of all tables in N -grams by using a generic compression library, zlib, employed in UNIX gzip. We treat a sequence of 4-byte floats in the probability table as a byte stream, and compress every 8K-byte block. To facilitate random access to the compressed block, we keep track of the compressed block’s starting offsets. Since the offsets are in sorted order, we can apply sorted integer compression (Raj and Whittaker, 2003). Since N - gram language model access preserves some locality, N -gram with block compression is still practical enough to be usable in our system."]},{"title":"4 Experiments","paragraphs":["We applied the proposed representation to 5-gram trainedby“English Gigaword 3rd Edition,” “English Web 1T 5-gram” from LDC, and “Japanese We b 1 T 7-gram” from GSK. Since their tendencies are the same, we only report in this paper the results on English Web 1T 5-gram, where the size of the count data in gzipped raw text format is 25GB, the number of N-grams is 3.8G, the vocabulary size is 13.6M words, and the number of the highest order N-grams is 1.2G.","We implemented an N -gram indexer/estimator using MPI inspired by the MapReduce implementation of N -gram language model index-ing/estimation pipeline (Brants et al., 2007).","Table 1 summarizes the overall results. We show the initial indexed counts and the final language model size by differentiating compression strategies for the pointers, namely the 4-byte raw value (Trie), the sorted integer compression (Integer) and our succinct representation (Succinct). The “block” indicates block compression. For the sake of implementation simplicity, the sorted integer compression used a fixed 8-bit shift amount, although the original paper proposed recursively determined optimum shift amounts (Raj and Whittaker, 2003). 8-bit quantization was performed for probabilities and back-off coefficients using a simple binning approach (Federico and Cettolo, 2007).","N -gram counts were reduced from 23.59GB to 10.57GB by our succinct representation with block compression. N -gram language models of 42.65GB were compressed to 18.37GB. Finally, the 8-bit quantized N -gram language models are represented by 9.83GB of space.","Table 2 shows the compression ratio for the pointer table alone. Block compression employed on raw 4-byte pointers attained a large reduction that was almost comparable to sorted integer compression. Since large pointer value tables are sorted, even a generic compression algorithm could achieve better compression. Using our succinct representation, 2.4 bits are required for each N -gram. By using the “flat” trie structure, we approach closer to its information-theoretic lower bound beyond the LOUDS baseline. With block compression, we achieved 1.8 bits per N -gram.","Table 3 shows the effect of variable length coding and block compression for the word ids, counts, probabilities and back-off coefficients. After variable-length coding, the word id is almost half its original size. We assign a word id for each 343","w/o block w/ block Counts Trie 23.59 GB 12.21 GB","Integer 14.59 GB 11.18 GB","Succinct 12.62 GB 10.57 GB Language Trie 42.65 GB 20.01 GB model Integer 33.65 GB 18.98 GB","Succinct 31.67 GB 18.37 GB Quantized Trie 24.73 GB 11.47 GB language Integer 15.73 GB 10.44 GB model Succinct 13.75 GB 9.83 GB Table 1: Summary of N -gram compression","total per N-gram 4-byte Pointer 12.04 GB 27.24 bits +block compression 2.42 GB 5.48 bits Sorted Integer 3.04 GB 6.87 bits +block compression 1.39 GB 3.15 bits Succinct 1.06 GB 2.40 bits +block compression 0.78 GB 1.76 bits Table 2: Compression ratio for pointers word according to its reverse sorted order of frequency. Therefore, highly frequent words are as-signed smaller values, which in turn occupies less space in our variable length coding. With block compression, we achieved further 1 GB reduction in space. Since the word id sequence preserves local ordering for a certain range, even a generic compression algorithm is effective.","The most frequently observed count in N -gram data is one. Therefore, we can reduce the space by the variable length coding. Large compression rates are achieved for both probabilities and back-off coefficients."]},{"title":"5 Conclusion","paragraphs":["We provided a succinct representation of the N - gram language model without any loss. Our method approaches closer to the information-theoretic lower bound beyond the LOUDS baseline. Experimental results showed our succinct representation drastically reduces the space for the pointers compared to the sorted integer compression approach. Furthermore, the space of N -grams was significantly reduced by variable","total per N-gram","word id size (4 bytes) 14.09 GB 31.89 bits","+variable length 6.72 GB 15.20 bits +block compression 5.57 GB 12.60 bits","count size (8 bytes) 28.28 GB 64.00 bits","+variable length 4.85 GB 10.96 bits +block compression 4.22 GB 9.56 bits","probability size (4 bytes) 14.14 GB 32.00 bits +block compression 9.55 GB 21.61 bits","8-bit quantization 3.54 GB 8.00 bits +block compression 2.64 GB 5.97 bits","backoff size (4 bytes) 9.76 GB 22.08 bits +block compression 2.48 GB 5.61 bits","8-bit quantization 2.44 GB 5.52 bits +block compression 0.85 GB 1.92 bits Table 3: Effects of block compression length coding and block compression. A large amount of N -gram data is reduced from unindexed gzipped 25 GB text counts to 10 GB of indexed language models. Our representation is practical enough though we did not experimentally investigate the runtime efficiency in this paper. The proposed representation enables us to utilize a web-scaled N -gram in our MT competition system (Watanabe et al., 2008). Our succinct representation will encourage new research on web-scaled N -gram data without requiring a larger computer cluster or hundreds of gigabytes of memory."]},{"title":"Acknowledgments","paragraphs":["We would like to thank Daisuke Okanohara for his open source implementation and extensive documentation of LOUDS, which helped our original coding."]},{"title":"References","paragraphs":["T. Brants, A. C. Popat, P. Xu, F. J. Och, and J. Dean. 2007. Large language models in machine translation. In Proc. of EMNLP-CoNLL 2007.","K. Church, T. Hart, and J. Gao. 2007. Compressing trigram language models with Golomb coding. In Proc. of EMNLP-CoNLL 2007.","O. Delpratt, N. Rahman, and R. Raman. 2006. Engineering the LOUDS succinct tree representation. In Proc. of the 5th International Workshop on Experimental Algorithms.","M. Federico and M. Cettolo. 2007. Efficient handling of n-gram language models for statistical machine translation. In Proc. of the 2nd Workshop on Statistical Machine Translation.","G. Jacobson. 1989. Space-efficient static trees and graphs. In 30th Annual Symposium on Foundations of Computer Science,Nov.","D. K. Kim, J. C. Na, J. E. Kim, and K. Park. 2005. Efficient implementation of rank and select functions for succinct representation. In Proc. of the 5th International Workshop on Experimental Algorithms.","B. Raj and E. W. D. Whittaker. 2003. Lossless compression of language model structure and word identifiers. In Proc. of ICASSP 2003, volume 1.","A. Stolcke. 1998. Entropy-based pruning of backoff language models. In Proc. of the ARPA Workshop on Human Language Technology.","D. Talbot and T. Brants. 2008. Randomized language models via perfect hash functions. In Proc. of ACL-08: HLT.","T. Watanabe, H. Tsukada, and H. Isozaki. 2008. NTT SMT system 2008 at NTCIR-7. In Proc. of the 7th NTCIR Workshop, pages 420–422. 344"]}],"references":[{"authors":[{"first":"T.","last":"Brants"},{"first":"A.","middle":"C.","last":"Popat"},{"first":"P.","last":"Xu"},{"first":"F.","middle":"J.","last":"Och"},{"first":"J.","last":"Dean"}],"year":"2007","title":"Large language models in machine translation","source":"T. Brants, A. C. Popat, P. Xu, F. J. Och, and J. Dean. 2007. Large language models in machine translation. In Proc. of EMNLP-CoNLL 2007."},{"authors":[{"first":"K.","last":"Church"},{"first":"T.","last":"Hart"},{"first":"J.","last":"Gao"}],"year":"2007","title":"Compressing trigram language models with Golomb coding","source":"K. Church, T. Hart, and J. Gao. 2007. Compressing trigram language models with Golomb coding. In Proc. of EMNLP-CoNLL 2007."},{"authors":[{"first":"O.","last":"Delpratt"},{"first":"N.","last":"Rahman"},{"first":"R.","last":"Raman"}],"year":"2006","title":"Engineering the LOUDS succinct tree representation","source":"O. Delpratt, N. Rahman, and R. Raman. 2006. Engineering the LOUDS succinct tree representation. In Proc. of the 5th International Workshop on Experimental Algorithms."},{"authors":[{"first":"M.","last":"Federico"},{"first":"M.","last":"Cettolo"}],"year":"2007","title":"Efficient handling of n-gram language models for statistical machine translation","source":"M. Federico and M. Cettolo. 2007. Efficient handling of n-gram language models for statistical machine translation. In Proc. of the 2nd Workshop on Statistical Machine Translation."},{"authors":[{"first":"G.","last":"Jacobson"}],"year":"1989","title":"Space-efficient static trees and graphs","source":"G. Jacobson. 1989. Space-efficient static trees and graphs. In 30th Annual Symposium on Foundations of Computer Science,Nov."},{"authors":[{"first":"D.","middle":"K.","last":"Kim"},{"first":"J.","middle":"C.","last":"Na"},{"first":"J.","middle":"E.","last":"Kim"},{"first":"K.","last":"Park"}],"year":"2005","title":"Efficient implementation of rank and select functions for succinct representation","source":"D. K. Kim, J. C. Na, J. E. Kim, and K. Park. 2005. Efficient implementation of rank and select functions for succinct representation. In Proc. of the 5th International Workshop on Experimental Algorithms."},{"authors":[{"first":"B.","last":"Raj"},{"first":"E.","middle":"W. D.","last":"Whittaker"}],"year":"2003","title":"Lossless compression of language model structure and word identifiers","source":"B. Raj and E. W. D. Whittaker. 2003. Lossless compression of language model structure and word identifiers. In Proc. of ICASSP 2003, volume 1."},{"authors":[{"first":"A.","last":"Stolcke"}],"year":"1998","title":"Entropy-based pruning of backoff language models","source":"A. Stolcke. 1998. Entropy-based pruning of backoff language models. In Proc. of the ARPA Workshop on Human Language Technology."},{"authors":[{"first":"D.","last":"Talbot"},{"first":"T.","last":"Brants"}],"year":"2008","title":"Randomized language models via perfect hash functions","source":"D. Talbot and T. Brants. 2008. Randomized language models via perfect hash functions. In Proc. of ACL-08: HLT."},{"authors":[{"first":"T.","last":"Watanabe"},{"first":"H.","last":"Tsukada"},{"first":"H.","last":"Isozaki"}],"year":"2008","title":"NTT SMT system 2008 at NTCIR-7","source":"T. Watanabe, H. Tsukada, and H. Isozaki. 2008. NTT SMT system 2008 at NTCIR-7. In Proc. of the 7th NTCIR Workshop, pages 420–422. 344"}],"cites":[{"style":0,"text":"Stolcke, 1998","origin":{"pointer":"/sections/2/paragraphs/1","offset":16,"length":13},"authors":[{"last":"Stolcke"}],"year":"1998","references":["/references/7"]},{"style":0,"text":"Church et al., 2007","origin":{"pointer":"/sections/2/paragraphs/1","offset":31,"length":19},"authors":[{"last":"Church"},{"last":"al."}],"year":"2007","references":["/references/1"]},{"style":0,"text":"Talbot and Brants, 2008","origin":{"pointer":"/sections/2/paragraphs/1","offset":85,"length":23},"authors":[{"last":"Talbot"},{"last":"Brants"}],"year":"2008","references":["/references/8"]},{"style":0,"text":"Raj and Whittaker (2003)","origin":{"pointer":"/sections/2/paragraphs/2","offset":0,"length":24},"authors":[{"last":"Raj"},{"last":"Whittaker"}],"year":"2003","references":["/references/6"]},{"style":0,"text":"Jacobson, 1989","origin":{"pointer":"/sections/2/paragraphs/3","offset":106,"length":14},"authors":[{"last":"Jacobson"}],"year":"1989","references":["/references/4"]},{"style":0,"text":"Delpratt et al., 2006","origin":{"pointer":"/sections/2/paragraphs/3","offset":122,"length":21},"authors":[{"last":"Delpratt"},{"last":"al."}],"year":"2006","references":["/references/2"]},{"style":0,"text":"Jacobson (1989)","origin":{"pointer":"/sections/2/paragraphs/3","offset":173,"length":15},"authors":[{"last":"Jacobson"}],"year":"1989","references":["/references/4"]},{"style":0,"text":"Talbot and Brants, 2008","origin":{"pointer":"/sections/2/paragraphs/10","offset":100,"length":23},"authors":[{"last":"Talbot"},{"last":"Brants"}],"year":"2008","references":["/references/8"]},{"style":0,"text":"Brants et al., 2007","origin":{"pointer":"/sections/3/paragraphs/22","offset":36,"length":19},"authors":[{"last":"Brants"},{"last":"al."}],"year":"2007","references":["/references/0"]},{"style":0,"text":"Jacobson, 1989","origin":{"pointer":"/sections/4/paragraphs/0","offset":115,"length":14},"authors":[{"last":"Jacobson"}],"year":"1989","references":["/references/4"]},{"style":0,"text":"Delpratt et al., 2006","origin":{"pointer":"/sections/4/paragraphs/0","offset":131,"length":21},"authors":[{"last":"Delpratt"},{"last":"al."}],"year":"2006","references":["/references/2"]},{"style":0,"text":"Kim et al. (2005)","origin":{"pointer":"/sections/4/paragraphs/15","offset":137,"length":17},"authors":[{"last":"Kim"},{"last":"al."}],"year":"2005","references":["/references/5"]},{"style":0,"text":"Jacobson (1989)","origin":{"pointer":"/sections/4/paragraphs/15","offset":159,"length":15},"authors":[{"last":"Jacobson"}],"year":"1989","references":["/references/4"]},{"style":0,"text":"Raj and Whittaker (2003)","origin":{"pointer":"/sections/4/paragraphs/15","offset":413,"length":24},"authors":[{"last":"Raj"},{"last":"Whittaker"}],"year":"2003","references":["/references/6"]},{"style":0,"text":"Raj and Whittaker, 2003","origin":{"pointer":"/sections/4/paragraphs/17","offset":780,"length":23},"authors":[{"last":"Raj"},{"last":"Whittaker"}],"year":"2003","references":["/references/6"]},{"style":0,"text":"Brants et al., 2007","origin":{"pointer":"/sections/5/paragraphs/1","offset":152,"length":19},"authors":[{"last":"Brants"},{"last":"al."}],"year":"2007","references":["/references/0"]},{"style":0,"text":"Raj and Whittaker, 2003","origin":{"pointer":"/sections/5/paragraphs/2","offset":521,"length":23},"authors":[{"last":"Raj"},{"last":"Whittaker"}],"year":"2003","references":["/references/6"]},{"style":0,"text":"Federico and Cettolo, 2007","origin":{"pointer":"/sections/5/paragraphs/2","offset":657,"length":26},"authors":[{"last":"Federico"},{"last":"Cettolo"}],"year":"2007","references":["/references/3"]},{"style":0,"text":"Watanabe et al., 2008","origin":{"pointer":"/sections/6/paragraphs/9","offset":492,"length":21},"authors":[{"last":"Watanabe"},{"last":"al."}],"year":"2008","references":["/references/9"]}]}
