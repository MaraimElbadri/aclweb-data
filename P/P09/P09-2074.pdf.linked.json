{"sections":[{"title":"","paragraphs":["Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 293–296, Suntec, Singapore, 4 August 2009. c⃝2009 ACL and AFNLP"]},{"title":"Markov Random Topic Fields Hal Daumé III School of Computing University of Utah Salt Lake City, UT 84112 me@hal3.name Abstract","paragraphs":["Most approaches to topic modeling as-sume an independence between documents that is frequently violated. We present an topic model that makes use of one or more user-specified graphs describing relationships between documents. These graph are encoded in the form of a Markov random field over topics and serve to encourage related documents to have similar topic structures. Experiments on show upwards of a 10% improvement in modeling performance."]},{"title":"1 Introduction","paragraphs":["One often wishes to apply topic models to large document collections. In these large collections, we usually have meta-information about how one document relates to another. Perhaps two documents share an author; perhaps one document cites another; perhaps two documents are published in the same journal or conference. We often believe that documents related in such a way should have similar topical structures. We encode this in a probabilistic fashion by imposing an (undirected) Markov random field (MRF) on top of a standard topic model (see Section 3). The edge potentials in the MRF encode the fact that “connected” documents should share similar topic structures, measured by some parameterized distance function. Inference in the resulting model is complicated by the addition of edge potentials in the MRF. We demonstrate that a hybrid Gibbs/Metropolis-Hastings sampler is able to efficiently explore the posterior distribution (see Section 4).","In experiments (Section 5), we explore several variations on our basic model. The first is to explore the importance of being able to tune the strength of the potentials in the MRF as part of the inference procedure. This turns out to be of utmost importance. The second is to study the importance of the form of the distance metric used to specify the edge potentials. Again, this has a significant impact on performance. Finally, we consider the use of multiple graphs for a single model and find that the power of combined graphs also leads to significantly better models."]},{"title":"2 Background","paragraphs":["Probabilistic topic models propose that text can be considered as a mixture of words drawn from one or more “topics” (Deerwester et al., 1990; Blei et al., 2003). The model we build on is latent Dirichlet allocation (Blei et al., 2003) (henceforth, LDA). LDA stipulates the following generative model for a document collection: 1. For each document d = 1 . . . D:","(a) Choose a topic mixture θd ∼ Dir(α)","(b) For each word in d, n = 1 . . . Nd:","i. Choose a topic zdn ∼ Mult(θd)","ii. Choose a word wdn ∼ Mult(βzdn) Here, α is a hyperparameter vector of length K, where K is the desired number of topics. Each document has a topic distribution θd over these K topics and each word is associated with precisely one topic (indicated by zdn). Each topic k = 1 . . . K is a unigram distribution over words (aka, a multinomial) parameterized by a vector βk. The associated graphical model for LDA is shown in Figure 1. Here, we have added a few additional hyperparameters: we place a Gam(a, b) prior independently on each component of α and a Dir(η, . . . , η) prior on each of the βs.","The joint distribution over all random variables specified by LDA is: p(α, θ, z, β, w) = Y k Gam(αk | a, b)Dir(βk | η) (1) Y d Dir(θd | α) Y n","Mult(zdn | θd)Mult(wdn | βz dn)","Many inference methods have been developed for this model; the approach upon which we 293 wzθα N D β K a b η Figure 1: Graphical model for LDA. build is the collapsed Gibbs sampler (Griffiths and Steyvers, 2006). Here, the random variables β and θ are analytically integrated out. The main sampling variables are the zdn indicators (as well as the hyperparameters: η and a, b). The conditional distribution for zdn conditioned on all other variables in the model gives the following Gibbs sampling distribution p(zdn = k): #−dn z=k + αk","P","k′(#−dn","z=k′ + αk′",") #−dn z=k,w=w","dn + η","P","k′(#−dn z=k′",",w=w","dn + η) (2)","Here, #−dn","χ denotes the number of times event χ occurs in the entire corpus, excluding word n in document d. Intuitively, the first term is a (smoothed) relative frequency of topic k occurring; the second term is a (smoothed) relative frequency of topic k giving rise to word wdn.","A Markov random field specifies a joint distribution over a collection of random variables x1, . . . , xN . An undirected graph structure stipulates how the joint distribution factorizes over these variables. Given a graph G = (V, E), where V = {x1, . . . , xN }, let C denote a subset of all the cliques of G. Then, the MRF specifies the joint distribution as: p(x) = 1","Z ∏ c∈C ψc(xc). Here, Z =","∑ x ∏","c∈C ψc(xc) is the partition function, xc is the subset of x contained in clique c and ψc is any non-negative function that measures how “good” a particular configuration of variables xc is. The ψs are called potential functions."]},{"title":"3 Markov Random Topic Fields","paragraphs":["Suppose that we have access to a collection of documents, but do not believe that these documents are all independent. In this case, the generative story of LDA no longer makes sense: related documents are more likely to have “similar” topic structures. For instance, in the scientific community, if paper A cites paper B, we would (a priori) expect the topic distributions for papers A and B to be related. Similarly, if two papers share an author, we might expect them to be topically related. Doc 1 Doc 2 Doc 3","Doc 4 Doc 5 Doc 6 wzθ N wzθ N wzθ N wzθ N wzθ N wzθ N Figure 2: Example Markov Random Topic Field (variables α and β are excluded for clarify). Of if they are both published at EMNLP. Or if they are published in the same year, or come out of the same institution, or many other possibilities.","Regardless of the source of this notion of similarity, we suppose that we can represent the relationship between documents in the form of a graph G = (V, E). The vertices in this graph are the documents and the edges indicate relatedness. Note that the resulting model will not be fully generative, but is still probabilistically well defined. 3.1 Single Graph There are multiple possibilities for augmenting LDA with such graph structure. We could “link” the topic distributions θ over related documents; we could “like” the topic indicators z over related documents. We consider the former because it leads to a more natural model. The idea is to “unroll” the D-plate in the graphical model for LDA (Figure 1) and connect (via undirected links) the θ variables associated with connected documents. Figure 2 shows an example MRTF over six documents, with thick edges connecting the θ variables of “related” documents. Note that each θ still has α as a parent and each w has β as a parent: these are left off for figure clarity.","The model is a straightforward “integration” of LDA and an MRF specified by the document relationships G. We begin with the joint distribution specified by LDA (see Eq (1)) and add in edge potentials for each edge in the document graph G that “encourage” the topic distributions of neighboring documents to be similar. The potentials all have the form: ψd,d′(θd, θd′) = exp [ −ld,d′ρ(θd, θd′) ] (3)","Here, ld,d′ is a “measure of strength” of the importance of the connection between d and d′","(and will be inferred as part of the model). ρ is a distance metric measuring the dissimilarity between θd and θd′. For now, this is Euclidean distance 294 (i.e., ρ(θd, θd′) = ||θd − θd′||); later, we show that alternative distance metrics are preferable.","Adding the graph structure necessitates the addition of hyperparameters le for every edge e ∈ E. We place an exponential prior on each 1/le with parameter λ: p(le | λ) = λ exp(−λ/le). Finally, we place a vague Gam(λa, λb) prior on λ. 3.2 Multiple Graphs In many applications, there may be multiple graphs that apply to the same data set, G1, . . . , GJ . In this case, we construct a single MRF based on the union of these graph structures. Each edge now has L-many parameters (one for each graph j) l","j","e.","Each graph also has its own exponential prior pa-","rameter λj. Together, this yields: ψd,d′(θd, θd′) = exp [ − ∑ j l j d,d′ρ(θd, θd′) ] (4) Here, the sum ranges only over those graphs","that have (d, d′",") in their edge set."]},{"title":"4 Inference","paragraphs":["Inference in MRTFs is somewhat complicated from inference in LDA, due to the introduction of the additional potential functions. In particular, while it is possible to analytically integrate out θ in LDA (due to multinomial/Dirichlet conjugacy), this is no longer possible in MRTFs. This means that we must explicitly represent (and sample over) the topic distributions θ in the MRTF.","This means that we must sample over the following set of variables: α, θ, z, l and λ. Sampling for α remains unchanged from the LDA case. Sampling for variables except θ is easy: zdn = k : θdk","#−dn z=k,w=wdn + η","∑","k′(#−dn","z=k′",",w=wdn + η) (5) 1/ld,d′ ∼ Exp ( λ + ρ(θd, θd′) ) (6) λ ∼ Gam ( λa + |E| , λb + ∑ e le ) (7)","The latter two follow from simple conjugacy. When we use multiple graphs, we assign a separate λ for each graph.","For sampling θ, we resort to a Metropolis-Hastings step. Our proposal distribution is the Dirichlet posterior over θ, given all the current as-signments. The acceptance probability then just depends on the graph distances. In particular, once θd is drawn from the posterior Dirichlet, the acceptance probability becomes ∏ d′ ∈N (d) ψd,d′,","where N (d) denotes the neighbors of d. For each 0 200 400 600 800 80 90 100 110 120 130 140 # of iterations perplexity   auth book cite http time *none* year Figure 3: Held-out perplexity for different graphs. document, we run 10 Metropolis steps; the acceptance rates are roughly 25%."]},{"title":"5 Experiments","paragraphs":["Our experiments are on a collection for 7441 document abstracts crawled from CiteSeer. The crawl was seeded with a collection of ten documents from each of: ACL, EMNLP, SIGIR, ICML, NIPS, UAI. This yields 650 thousand words of text after remove stop words. We use the following graphs (number in parens is the number of edges): auth: shared author (47k) book: shared booktitle/journal (227k) cite: one cites the other (18k) http: source file from same domain (147k) time: published within one year (4122k) year: published in the same year (2101k) Other graph structures are of course possible, but these were the most straightforward to cull.","The first thing we look at is convergence of the samplers for the different graphs. See Figure 3. Here, we can see that the author graph and the citation graph provide improved perplexity to the straightforward LDA model (called “*none*”), and that convergence occurs in a few hundred iterations. Due to their size, the final two graphs led to significantly slower inference than the first four, so results with those graphs are incomplete. Tuning Graph Parameters. The next item we investigate is whether it is important to tune the graph connectivity weights (the l and λ variables). It turns out this is incredibly important; see Figure 4. This is the same set of results as Figure 3, but without l and λ tuning. We see that the graph-based methods do not improve over the baseline. 295 0 200 400 600 800 80 90 100 110 120 130 140 # of iterations perplexity   auth book cite http *none* time year Figure 4: Held-out perplexity for difference graph structures without graph parameter tuning. 0 200 400 600 800 80 90 100 110 120 130 140 # of iterations perplexity   Bhattacharyya Hellinger Euclidean Logit Figure 5: Held-out perplexity for different distance metrics. Distance Metric. Next, we investigate the use of different distance metrics. We experiments with Bhattacharyya, Hellinger, Euclidean and logistic-Euclidean. See Figure 5 (this is just for the auth graph). Here, we see that Bhattacharyya and Hellinger (well motivated distances for probability distributions) outperform the Euclidean metrics. Using Multiple Graphs Finally, we compare results using combinations of graphs. Here, we run every sampler for 500 iterations and compute standard deviations based on ten runs (year and time are excluded). The results are in Table 1. Here, we can see that adding graphs (almost) always helps and never hurts. By adding all the graphs together, we are able to achieve an absolute reduction in perplexity of 9 points (roughly 10%). As discussed, this hinges on the tuning of the graph parameters to allow different graphs to have different amounts of influence."]},{"title":"6 Discussion","paragraphs":["We have presented a graph-augmented model for topic models and shown that a simple combined Gibbs/MH sampler is efficient in these models.","*none* 92.1","http 92.2","book 90.2","cite 88.4","auth 87.9 book+http 89.9 cite+http 88.6 auth+http 88.0 book+cite 86.9 auth+book 85.1 auth+cite 84.3","book+cite+http 87.9","auth+cite+http 85.5","auth+book+http 85.3","auth+book+cite 83.7","all 83.1 Table 1: Comparison of held-out perplexities for varying graph structures with two standard deviation error bars; grouped by number of graphs. Grey bars are indistinguish-able from best model in previous group; blue bars are at least two stddevs better; red bars are at least four stddevs better. Using data from the scientific domain, we have shown that we can achieve significant reductions in perplexity on held-out data using these models. Our model resembles recent work on hyper-text topic models (Gruber et al., 2008; Sun et al., 2008) and blog influence (Nallapati and Cohen, 2008), but is specifically tailored toward undirected models. Ours is an alternative to the recently proposed Markov Topic Models approach (Wang et al., 2009). While the goal of these two models is similar, the approaches differ fairly dramatically: we use the graph structure to inform the per-document topic distributions; they use the graph structure to inform the unigram models associated with each topic. It would be worthwhile to directly compare these two approaches."]},{"title":"References","paragraphs":["David Blei, Andrew Ng, and Michael Jordan. 2003. Latent Dirichlet allocation. JMLR, 3.","Scott C. Deerwester, Susan T. Dumais, Thomas K. Landauer, George W. Furnas, and Richard A. Harshman. 1990. In-dexing by latent semantic analysis. JASIS, 41(6).","Tom Griffiths and Mark Steyvers. 2006. Probabilistic topic models. In Latent Semantic Analysis: A Road to Meaning.","Amit Gruber, Michal Rosen-Zvi, , and Yair Weiss. 2008. Latent topic models for hypertext. In UAI.","Ramesh Nallapati and William Cohen. 2008. Link-PLSA-LDA: A new unsupervised model for topics and influence of blogs. In Conference for Webblogs and Social Media.","Congkai Sun, Bin Gao, Zhenfu Cao, and Hang Li. 2008. HTM: A topic model for hypertexts. In EMNLP.","Chong Wang, Bo Thiesson, Christopher Meek, and David Blei. 2009. Markov topic models. In AI-Stats. 296"]}],"references":[{"authors":[{"first":"David","last":"Blei"},{"first":"Andrew","last":"Ng"},{"first":"Michael","last":"Jordan"}],"year":"2003","title":"Latent Dirichlet allocation","source":"David Blei, Andrew Ng, and Michael Jordan. 2003. Latent Dirichlet allocation. JMLR, 3."},{"authors":[{"first":"Scott","middle":"C.","last":"Deerwester"},{"first":"Susan","middle":"T.","last":"Dumais"},{"first":"Thomas","middle":"K.","last":"Landauer"},{"first":"George","middle":"W.","last":"Furnas"},{"first":"Richard","middle":"A.","last":"Harshman"}],"year":"1990","title":"In-dexing by latent semantic analysis","source":"Scott C. Deerwester, Susan T. Dumais, Thomas K. Landauer, George W. Furnas, and Richard A. Harshman. 1990. In-dexing by latent semantic analysis. JASIS, 41(6)."},{"authors":[{"first":"Tom","last":"Griffiths"},{"first":"Mark","last":"Steyvers"}],"year":"2006","title":"Probabilistic topic models","source":"Tom Griffiths and Mark Steyvers. 2006. Probabilistic topic models. In Latent Semantic Analysis: A Road to Meaning."},{"authors":[{"first":"Amit","last":"Gruber"},{"first":"Michal","last":"Rosen-Zvi"},{"first":"Yair","last":"Weiss"}],"year":"2008","title":"Latent topic models for hypertext","source":"Amit Gruber, Michal Rosen-Zvi, , and Yair Weiss. 2008. Latent topic models for hypertext. In UAI."},{"authors":[{"first":"Ramesh","last":"Nallapati"},{"first":"William","last":"Cohen"}],"year":"2008","title":"Link-PLSA-LDA: A new unsupervised model for topics and influence of blogs","source":"Ramesh Nallapati and William Cohen. 2008. Link-PLSA-LDA: A new unsupervised model for topics and influence of blogs. In Conference for Webblogs and Social Media."},{"authors":[{"first":"Congkai","last":"Sun"},{"first":"Bin","last":"Gao"},{"first":"Zhenfu","last":"Cao"},{"first":"Hang","last":"Li"}],"year":"2008","title":"HTM: A topic model for hypertexts","source":"Congkai Sun, Bin Gao, Zhenfu Cao, and Hang Li. 2008. HTM: A topic model for hypertexts. In EMNLP."},{"authors":[{"first":"Chong","last":"Wang"},{"first":"Bo","last":"Thiesson"},{"first":"Christopher","last":"Meek"},{"first":"David","last":"Blei"}],"year":"2009","title":"Markov topic models","source":"Chong Wang, Bo Thiesson, Christopher Meek, and David Blei. 2009. Markov topic models. In AI-Stats. 296"}],"cites":[{"style":0,"text":"Deerwester et al., 1990","origin":{"pointer":"/sections/3/paragraphs/0","offset":118,"length":23},"authors":[{"last":"Deerwester"},{"last":"al."}],"year":"1990","references":["/references/1"]},{"style":0,"text":"Blei et al., 2003","origin":{"pointer":"/sections/3/paragraphs/0","offset":143,"length":17},"authors":[{"last":"Blei"},{"last":"al."}],"year":"2003","references":["/references/0"]},{"style":0,"text":"Blei et al., 2003","origin":{"pointer":"/sections/3/paragraphs/0","offset":217,"length":17},"authors":[{"last":"Blei"},{"last":"al."}],"year":"2003","references":["/references/0"]},{"style":0,"text":"Griffiths and Steyvers, 2006","origin":{"pointer":"/sections/3/paragraphs/7","offset":182,"length":28},"authors":[{"last":"Griffiths"},{"last":"Steyvers"}],"year":"2006","references":["/references/2"]},{"style":0,"text":"Gruber et al., 2008","origin":{"pointer":"/sections/7/paragraphs/10","offset":513,"length":19},"authors":[{"last":"Gruber"},{"last":"al."}],"year":"2008","references":["/references/3"]},{"style":0,"text":"Sun et al., 2008","origin":{"pointer":"/sections/7/paragraphs/10","offset":534,"length":16},"authors":[{"last":"Sun"},{"last":"al."}],"year":"2008","references":["/references/5"]},{"style":0,"text":"Nallapati and Cohen, 2008","origin":{"pointer":"/sections/7/paragraphs/10","offset":572,"length":25},"authors":[{"last":"Nallapati"},{"last":"Cohen"}],"year":"2008","references":["/references/4"]},{"style":0,"text":"Wang et al., 2009","origin":{"pointer":"/sections/7/paragraphs/10","offset":733,"length":17},"authors":[{"last":"Wang"},{"last":"al."}],"year":"2009","references":["/references/6"]}]}
