{"sections":[{"title":"","paragraphs":["Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 737–745, Suntec, Singapore, 2-7 August 2009. c⃝2009 ACL and AFNLP"]},{"title":"Answering Opinion Questions with Random Walks on Graphs Fangtao Li, Yang Tang, Minlie Huang, and Xiaoyan Zhu State Key Laboratory on Intelligent Technology and Systems Tsinghua National Laboratory for Information Science and Technology Department of Computer Sci. and Tech., Tsinghua University, Beijing 100084, China {fangtao06,tangyang9}@gmail.com,{aihuang,zxy-dcs}@tsinghua.edu.cn Abstract","paragraphs":["Opinion Question Answering (Opinion QA), which aims to find the authors’ sentimental opinions on a specific target, is more challenging than traditional fact-based question answering problems. To extract the opinion oriented answers, we need to consider both topic relevance and opinion sentiment issues. Current solu-tions to this problem are mostly ad-hoc combinations of question topic information and opinion information. In this paper, we propose an Opinion PageRank model and an Opinion HITS model to fully explore the information from different relations among questions and answers, answers and answers, and topics and opinions. By fully exploiting these relations, the experiment results show that our proposed algorithms outperform several state of the art baselines on benchmark data set. A gain of over 10% in F scores is achieved as compared to many other systems."]},{"title":"1 Introduction","paragraphs":["Question Answering (QA), which aims to provide answers to human-generated questions automatically, is an important research area in natural language processing (NLP) and much progress has been made on this topic in previous years. However, the objective of most state-of-the-art QA systems is to find answers to factual questions, such as “What is the longest river in the United States?” and “Who is Andrew Carnegie?” In fact, rather than factual information, people would also like to know about others’ opinions, thoughts and feelings toward some specific objects, people and events. Some examples of these questions are: “How is Bush’s decision not to ratify the Kyoto Protocol looked upon by Japan and other US allies?”(Stoyanov et al., 2005) and “Why do people like Subway Sandwiches?” from TAC 2008 (Dang, 2008). Systems designed to deal with such questions are called opinion QA systems. Researchers (Stoyanov et al., 2005) have found that opinion questions have very different characteristics when compared with fact-based questions: opinion questions are often much longer, more likely to represent partial answers rather than complete answers and vary much more widely. These features make opinion QA a harder problem to tackle than fact-based QA. Also as shown in (Stoyanov et al., 2005), directly applying previous systems designed for fact-based QA onto opinion QA tasks would not achieve good performances.","Similar to other complex QA tasks (Chen et al., 2006; Cui et al., 2007), the problem of opinion QA can be viewed as a sentence ranking problem. The Opinion QA task needs to consider not only the topic relevance of a sentence (to identify whether this sentence matches the topic of the question) but also the sentiment of a sentence (to identify the opinion polarity of a sentence). Current solu-tions to opinion QA tasks are generally in ad hoc styles: the topic score and the opinion score are usually separately calculated and then combined via a linear combination (Varma et al., 2008) or just filter out the candidate without matching the question sentiment (Stoyanov et al., 2005). However, topic and opinion are not independent in reality. The opinion words are closely associated with their contexts. Another problem is that exist-ing algorithms compute the score for each answer candidate individually, in other words, they do not consider the relations between answer candidates. The quality of a answer candidate is not only determined by the relevance to the question, but also by other candidates. For example, the good answer may be mentioned by many candidates.","In this paper, we propose two models to address the above limitations of previous sentence 737 ranking models. We incorporate both the topic relevance information and the opinion sentiment information into our sentence ranking procedure. Meanwhile, our sentence ranking models could naturally consider the relationships between different answer candidates. More specifically, our first model, called Opinion PageRank, incorporates opinion sentiment information into the graph model as a condition. The second model, called Opinion HITS model, considers the sentences as authorities and both question topic information and opinion sentiment information as hubs. The experiment results on the TAC QA data set demonstrate the effectiveness of the proposed Random Walk based methods. Our proposed method performs better than the best method in the TAC 2008 competition.","The rest of this paper is organized as follows: Section 2 introduces some related works. We will discuss our proposed models in Section 3. In Section 4, we present an overview of our opinion QA system. The experiment results are shown in Section 5. Finally, Section 6 concludes this paper and provides possible directions for future work."]},{"title":"2 Related Work","paragraphs":["Few previous studies have been done on opinion QA. To our best knowledge, (Stoyanov et al., 2005) first created an opinion QA corpus OpQA. They find that opinion QA is a more challenging task than factual question answering, and they point out that traditional fact-based QA approaches may have difficulty on opinion QA tasks if unchanged. (Somasundaran et al., 2007) argues that making finer grained distinction of subjective types (sentiment and arguing) further improves the QA system. For non-English opinion QA, (Ku et al., 2007) creates a Chinese opinion QA corpus. They classify opinion questions into six types and construct three components to retrieve opinion answers. Relevant answers are further processed by focus detection, opinion scope identification and polarity detection. Some works on opinion min-ing are motivated by opinion question answering. (Yu and Hatzivassiloglou, 2003) discusses a necessary component for an opinion question answering system: separating opinions from fact at both the document and sentence level. (Soo-Min and Hovy, 2005) addresses another important component of opinion question answering: finding opinion holders.","More recently, TAC 2008 QA track (evolved from TREC) focuses on finding answers to opinion questions (Dang, 2008). Opinion questions retrieve sentences or passages as answers which are relevant for both question topic and question sentiment. Most TAC participants employ a strategy of calculating two types of scores for answer candidates, which are the topic score measure and the opinion score measure (the opinion information expressed in the answer candidate). However, most approaches simply combined these two scores by a weighted sum, or removed candidates that didn’t match the polarity of questions, in order to extract the opinion answers.","Algorithms based on Markov Random Walk have been proposed to solve different kinds of ranking problems, most of which are inspired by the PageRank algorithm (Page et al., 1998) and the HITS algorithm (Kleinberg, 1999). These two algorithms were initially applied to the task of Web search and some of their variants have been proved successful in a number of applications, including fact-based QA and text summarization (Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Otterbacher et al., 2005; Wan and Yang, 2008). Generally, such models would first construct a directed or undirected graph to represent the relationship between sentences and then certain graph-based ranking methods are applied on the graph to compute the ranking score for each sentence. Sentences with high scores are then added into the answer set or the summary. However, to the best of our knowledge, all previous Markov Random Walk-based sentence ranking models only make use of topic relevance information, i.e. whether this sentence is relevant to the fact we are looking for, thus they are limited to fact-based QA tasks. To solve the opinion QA problems, we need to consider both topic and sentiment in a non-trivial manner."]},{"title":"3 Our Models for Opinion Sentence Ranking","paragraphs":["In this section, we formulate the opinion question answering problem as a topic and sentiment based sentence ranking task. In order to naturally integrate the topic and opinion information into the graph based sentence ranking framework, we propose two random walk based models for solving the problem, i.e. an Opinion PageRank model and an Opinion HITS model. 738 3.1 Opinion PageRank Model In order to rank sentence for opinion question answering, two aspects should be taken into account. First, the answer candidate is relevant to the question topic; second, the answer candidate is suitable for question sentiment.","Considering Question Topic: We first introduce how to incorporate the question topic into the Markov Random Walk model, which is similar as the Topic-sensitive LexRank (Otterbacher et al., 2005). Given the set Vs = {vi} containing all the sentences to be ranked, we construct a graph where each node represents a sentence and each edge weight between sentence vi and sentence vj is induced from sentence similarity measure as follows: p(i → j) = f(i→j)","P|Vs| k=1 f(i→k) , where f (i → j) represents the similarity between sentence vi and sentence vj, here is cosine similarity (Baeza-Yates and Ribeiro-Neto, 1999). We define f (i → i) = 0 to avoid self transition. Note that p(i → j) is usually not equal to p(j → i). We also compute the similarity rel(vi|q) of a sentence vi to the question topic q using the cosine measure. This relevance score is then normalized as follows to make the sum of all relevance values of the sentences equal to 1: rel′","(vi|q) = rel(vi|q)","P|Vs| k=1 rel(vk|q) .","The saliency score Score(vi) for sentence vi can be calculated by mixing topic relevance score and scores of all other sentences linked with it as follows: Score(vi) = μ","∑ j̸=i Score(vj ) · p(j → i) + (1 − μ)rel′","(vi|q), where μ is the damping factor as in the PageRank algorithm.","The matrix form is: p̃ = μ M̃ T","p̃ + (1 − μ)⃗α, where p̃ = [Score(vi)]|Vs|×1 is the vector of saliency scores for the sentences; M̃ = [p(i → j)]|Vs|×|Vs| is the graph with each entry corresponding to the transition probability; ⃗α = [rel′","(vi|q)]|Vs|×1 is the vector containing the relevance scores of all the sentences to the question. The above process can be considered as a Markov chain by taking the sentences as the states and the corresponding transition matrix is given by A′","= μ M̃ T","+ (1 − μ)⃗e⃗αT",".","Considering Topics and Sentiments To-gether: In order to incorporate the opinion information and topic information for opinion sentence ranking in an unified framework, we propose an Opinion PageRank model (Figure 1) based on a two-layer link graph (Liu and Ma, 2005; Wan and Yang, 2008). In our opinion PageRank model, the Figure 1: Opinion PageRank first layer contains all the sentiment words from a lexicon to represent the opinion information, and the second layer denotes the sentence relationship in the topic sensitive Markov Random Walk model discussed above. The dashed lines between these two layers indicate the conditional influence between the opinion information and the sentences to be ranked.","Formally, the new representation for the two-layer graph is denoted as G∗","= ⟨Vs, Vo, Ess, Eso⟩, where Vs = {vi} is the set of sentences and Vo = {oj } is the set of sentiment words representing the opinion information; Ess = {eij |vi, vj ∈ Vs} corresponds to all links between sentences and Eso = {eij |vi ∈ Vs, oj ∈ Vo} corresponds to the opinion correlation between a sentence and the sentiment words. For further discussions, we let π(oj ) ∈ [0, 1] denote the sentiment strength of word oj , and let ω(vi, oj ) ∈ [0, 1] denote the strength of the correlation between sentence vi and word oj. We incorporate the two factors into the transition probability from vi to vj and the new transition probability p(i → j|Op(vi), Op(vj)) is defined as f(i→j|Op(vi),Op(vj ))","P|Vs| k=1 f(i→k|Op(vi),Op(vk)) when ∑ f ̸= 0, and defined as 0 otherwise, where Op(vi) is denoted as the opinion information of sentence vi, and f (i → j|Op(vi), Op(vj )) is the new similarity score between two sentences vi and vj, conditioned on the opinion information expressed by the sentiment words they contain. We propose to compute the conditional similarity score by linearly combining the scores conditioned on the source opinion (i.e. f (i → j|Op(vi))) and the destination opinion (i.e. f (i → j|Op(vj ))) as follows: f(i → j|Op(vi), Op(vj)) = λ · f(i → j|Op(vi)) + (1 − λ) · f(i → j|Op(vj)) = λ · X o k∈Op(v","i) f(i → j) · π(ok) · ω(ok, vi) + (1 − λ) · X o k′ ∈Op(v","j))","(i → j) · π(ok′ ) · ω(ok′",", vj) (1) where λ ∈ [0, 1] is the combination weight controlling the relative contributions from the source 739 opinion and the destination opinion. In this study, for simplicity, we define π(oj ) as 1, if oj exists in the sentiment lexicon, otherwise 0. And ω(vi, oj ) is described as an indicative function. In other words, if word oj appears in the sentence vi, ω(vi, oj ) is equal to 1. Otherwise, its value is 0. Then the new row-normalized matrix M̃ ∗","is defined as follows: M̃ ∗","ij = p(i → j|Op(i), Opj).","The final sentence score for Opinion PageRank model is then denoted by: Score(vi) = μ ·∑","j̸=i Score(vj ) · M̃ ∗","ji + (1 − μ) · rel′","(s","i|q). The matrix form is: p̃ = μ M̃ ∗T","p̃ + (1 − μ) · ⃗α.","The final transition matrix is then denoted as:","A∗ = μ M̃ ∗T","+(1−μ)⃗e⃗αT","and the sentence scores","are obtained by the principle eigenvector of the","new transition matrix A∗",". 3.2 Opinion HITS Model The word’s sentiment score is fixed in Opinion PageRank. This may encounter problem when the sentiment score definition is not suitable for the specific question. We propose another opinion sentence ranking model based on the popular graph ranking algorithm HITS (Kleinberg, 1999). This model can dynamically learn the word sentiment score towards a specific question. HITS algorithm distinguishes the hubs and authorities in the objects. A hub object has links to many authorities, and an authority object has high-quality content and there are many hubs linking to it. The hub scores and authority scores are computed in a recursive way. Our proposed opinion HITS algorithm contains three layers. The upper level contains all the sentiment words from a lexicon, which represent their opinion information. The lower level contains all the words, which represent their topic information. The middle level contains all the opinion sentences to be ranked. We consider both the opinion layer and topic layer as hubs and the sentences as authorities. Figure 2 gives the bipartite graph representation, where the upper opinion layer is merged with lower topic layer together as the hubs, and the middle sentence layer is considered as the authority.","Formally, the representation for the bipartite graph is denoted as G#","= ⟨Vs, Vo, Vt, Eso, Est⟩, where Vs = {vi} is the set of sentences. Vo = {oj } is the set of all the sentiment words representing opinion information, Vt = {tj} is the set of all the words representing topic information. Eso = {eij |vi ∈ Vs, oj ∈ Vo} corresponds to the Figure 2: Opinion HITS model correlations between sentence and opinion words. Each edge eij is associated with a weight owij denoting the strength of the relationship between the sentence vi and the opinion word oj . The weight owij is 1 if the sentence vi contains word oj, otherwise 0. Est denotes the relationship between sentence and topic word. Its weight twij is calculated by tf · idf (Otterbacher et al., 2005).","We define two matrixes O = (Oij )|Vs|×|Vo| and T = (Tij )|Vs|×|Vt| as follows, for Oij = owij , and if sentence i contains word j, therefore owij is assigned 1, otherwise owij is 0. Tij = twij = tfj · idfj (Otterbacher et al., 2005).","Our new opinion HITS model is different from the basic HITS algorithm in two aspects. First, we consider the topic relevance when computing the sentence authority score based on the topic hub level as follows: Authsen(vi) ∝","∑","twij>0 twij · topic score(j)·hubtopic(j), where topic score(j) is empirically defined as 1, if the word j is in the topic set (we will discuss in next section), and 0.1 otherwise.","Second, in our opinion HITS model, there are two aspects to boost the sentence authority score: we simultaneously consider both topic information and opinion information as hubs.","The final scores for authority sentence, hub topic and hub opinion in our opinion HITS model are defined as:","Auth(n+1) sen (vi) = (2) γ · X","tw ij>0","twij · topic score(j) · Hub(n) topic(tj) + (1 − γ) · X","ow ij>0","owij · Hub(n) opinion(oj)","Hub(n+1) topic (ti) = X","tw ki>0","twki · Auth(n) sen(vi) (3)","Hub(n+1) opinion(oi) = X","ow ki>0","owki · Auth(n) sen(vi) (4) 740 Figure 3: Opinion Question Answering System The matrix form is:","a(n+1)","= γ · T · e · tT s · I · h(n)","t + (1 − γ) · O · h(n)","o (5)","h(n+1)","t = T T","· a(n)","(6)","h(n+1)","o = OT","· a(n)","(7) where e is a |Vt|×1 vector with all elements equal to 1 and I is a |Vt| × |Vt| identity matrix, ts = [topic score(j)]|Vt|×1 is the score vector for topic words, a(n)","= [Auth(n)","sen(vi)]|Vs|×1 is the vector authority scores for the sentence in the nth","iteration, and the same as h(n)","t = [Hub(n)","topic(tj )]|Vt|×1, h(n) o = [Hub","(n)","opinion(tj )]|Vo|×1. In order to guarantee the convergence of the iterative form, authority score and hub score are normalized after each iteration.","For computation of the final scores, the initial scores of all nodes, including sentences, topic words and opinion words, are set to 1 and the above iterative steps are used to compute the new scores until convergence. Usually the convergence of the iteration algorithm is achieved when the difference between the scores computed at two successive iterations for any nodes falls below a given threshold (10e-6 in this study). We use the authority scores as the saliency scores in the Opinion HITS model. The sentences are then ranked by their saliency scores."]},{"title":"4 System Description","paragraphs":["In this section, we introduce the opinion question answering system based on the proposed graph methods. Figure 3 shows five main modules:","Question Analysis: It mainly includes two components. 1).Sentiment Classification: We classify all opinion questions into two categories: positive type or negative type. We extract several types of features, including a set of pattern features, and then design a classifier to identify sentiment polarity for each question (similar as (Yu and Hatzivassiloglou, 2003)). 2).Topic Set Expansion: The opinion question asks opinions about a particular target. Semantic role labeling based (Carreras and Marquez, 2005) and rule based techniques can be employed to extract this target as topic word. We also expand the topic word with several external knowledge bases: Since all the entity synonyms are redirected into the same page in Wikipedia (Rodrigo et al., 2007), we collect these redirection synonym words to expand topic set. We also collect some related lists as topic words. For example, given question “What reasons did people give for liking Ed Norton’s movies?”, we collect all the Norton’s movies from IMDB as this question’s topic words.","Document Retrieval: The PRISE search engine, supported by NIST (Dang, 2008), is employed to retrieve the documents with topic word.","Answer Candidate Extraction: We split retrieved documents into sentences, and extract sentences containing topic words. In order to improve recall, we carry out the following process to handle the problem of coreference resolution: We classify the topic word into four categories: male, female, group and other. Several pronouns are defined for each category, such as ”he”, ”him”, ”his” for male category. If a sentence is determined to contain the topic word, and its next sentence contains the corresponding pronouns, then the next sentence is also extracted as an answer candidate, similar as (Chen et al., 2006).","Answer Ranking: The answer candidates are ranked by our proposed Opinion PageRank method or Opinion HITS method.","Answer Selection by Removing Redundancy: We incrementally add the top ranked sentence into the answer set, if its cosine similarity with every extracted answer doesn’t exceed a predefined threshold, until the number of selected sentence (here is 40) is reached."]},{"title":"5 Experiments 5.1 Experiment Step 5.1.1 Dataset","paragraphs":["We employ the dataset from the TAC 2008 QA track. The task contains a total of 87 squishy 741 opinion questions.1","These questions have simple forms, and can be easily divided into positive type or negative type, for example “Why do people like Mythbusters?” and “What were the specific ac-tions or reasons given for a negative attitude towards Mahmoud Ahmadinejad?”. The initial topic word for each question (called target in TAC) is also provided. Since our work in this paper focuses on sentence ranking for opinion QA, these characteristics of TAC data make it easy to process question analysis. Answers for all questions must be retrieved from the TREC Blog06 collection (Craig Macdonald and Iadh Ounis, 2006). The collection is a large sample of the blog sphere, crawled over an eleven-week period from December 6, 2005 until February 21, 2006. We retrieve the top 50 documents for each question. 5.1.2 Evaluation Metrics We adopt the evaluation metrics used in the TAC squishy opinion QA task (Dang, 2008). The TAC assessors create a list of acceptable information nuggets for each question. Each nugget will be assigned a normalized weight based on the number of assessors who judged it to be vital. We use these nuggets and corresponding weights to assess our approach. Three human assessors complete the evaluation process. Every question is scored using nugget recall (NR) and an approximation to nugget precision (NP) based on length. The final score will be calculated using F measure with TAC official value β = 3 (Dang, 2008). This means recall is 3 times as important as precision: F (β = 3) =","(32","+ 1) · NP · NR","32","· NP + NR where N P is the sum of weights of nuggets returned in response over the total sum of weights of all nuggets in nugget list, and N P = 1 − (length − allowance)/(length) if length is no less than allowance and 0 otherwise. Here allowance = 100 × (♯nuggets returned) and length equals to the number of non-white characters in strings. We will use average F Score to evaluate the performance for each system. 5.1.3 Baseline The baseline combines the topic score and opinion score with a linear weight for each answer candidate, similar to the previous ad-hoc algorithms:","final score = (1 − α) × opinion score + α × topic score","(8) 1 3 questions were dropped from the evaluation due to no","correct answers found in the corpus The topic score is computed by the cosine similarity between question topic words and answer candidate. The opinion score is calculated using the number of opinion words normalized by the total number of words in candidate sentence. 5.2 Performance Evaluation 5.2.1 Performance on Sentimental Lexicons Lexicon Neg Pos Description Name Size Size","1 HowNet 2700 2009 English translation","of positive/negative","Chinese words","2 Senti- 4800 2290 Words with a positive WordNet or negative score","above 0.6","3 Intersec- 640 518 Words appeared in tion both 1 and 2","4 Union 6860 3781 Words appeared in","1 or 2","5 All 10228 10228 All words appeared","in 1 or 2 without","distinguishing pos","or neg Table 1: Sentiment lexicon description","For lexicon-based opinion analysis, the selec-tion of opinion thesaurus plays an important role in the final performance. HowNet2","is a knowledge database of the Chinese language, and provides an online word list with tags of positive and negative polarity. We use the English translation of those sentiment words as the sentimental lexicon. SentiWordNet (Esuli and Sebastiani, 2006) is another popular lexical resource for opinion mining. Table 1 shows the detail information of our used sentiment lexicons. In our models, the positive opinion words are used only for positive questions, and negative opinion words just for negative questions. We initially set parameter λ in Opinion PageRank as 0 as (Liu and Ma, 2005), and other parameters simply as 0.5, including μ in Opinion PageRank, γ in Opinion HITS, and α in baseline. The experiment results are shown in Figure 4.","We can make three conclusions from Figure 4: 1. Opinion PageRank and Opinion HITS are both effective. The best results of Opinion PageRank and Opinion HITS respectively achieve around 35.4% (0.199 vs 0.145), and 34.7% (0.195 vs 0.145) improvements in terms of F score over the best baseline result. We believe this is because our proposed models not only incorporate the topic information and opinion information, but also con-","2","http://www.keenage.com/zhiwang/e zhiwang.html 742 0 15 0.2 0.25 HowNet SentiWordNet Intersection Union All 0 0.05 0.1 0.15 Baseline OpinionPageRank OpinionHITS Figure 4: Sentiment Lexicon Performance sider the relationship between different answers. The experiment results demonstrate the effectiveness of these relations. 2. Opinion PageRank and Opinion HITS are comparable. Among five sentimental lexicons, Opinion PageRank achieves the best results when using HowNet and Union lexicons, and Opinion HITS achieves the best results using the other three lexicons. This may be because when the sentiment lexicon is defined appropriately for the specific question set, the opinion PageRank model performs better. While when the sentiment lexicon is not suitable for these questions, the opinion HITS model may dynamically learn a temporal sentiment lexicon and can yield a satisfied performance. 3. Hownet achieves the best overall performance among five sentiment lexicons. In HowNet, English translations of the Chinese sentiment words are annotated by nonnative speakers; hence most of them are common and popular terms, which maybe more suitable for the Blog environment (Zhang and Ye, 2008). We will use HowNet as the sentiment thesaurus in the following experiments.","In baseline, the parameter α shows the relative contributions for topic score and opinion score. We vary α from 0 to 1 with an interval of 0.1, and find that the best baseline result 0.170 is achieved when α=0.1. This is because the topic information has been considered during candidate extrac-tion, the system considering more opinion information (lower α) achieves better. We will use this best result as baseline score in following experiments. Since F(3) score is more related with recall, F score and recall will be demonstrated. In the next two sections, we will present the performances of the parameters in each model. For simplicity, we denote Opinion PageRank as PR, Opinion HITS as HITS, baseline as Base, Recall as r, F score as F. 0 . 2 2 0 . 2 4","0",".","2","6 P","R","_","r","P","R","_","F","B","a","s","e","_","r","B","a","s","e","_","F","F(3) 0 . 1 2 0 . 1 4 0 . 1 6 0 . 1 8 0 . 2","0","0",".","2","0",".","4","0",".","6","0",".","8","1  Figure 5: Opinion PageRank Performance with varying parameter λ (μ = 0.5) 0 . 2 2 0 . 2 4","0",".","2","6 P","R","_","r","P","R","_","F","B","a","s","e","_","r","B","a","s","e","_","F","F(3) 0 . 1 2 0 . 1 4 0 . 1 6 0 . 1 8 0 . 2","0","0",".","2","0",".","4","0",".","6","0",".","8","1  Figure 6: Opinion PageRank Performance with varying parameter μ (λ = 0.2) 5.2.2 Opinion PageRank Performance In Opinion PageRank model, the value λ combines the source opinion and the destination opinion. Figure 5 shows the experiment results on parameter λ. When we consider lower λ, the system performs better. This demonstrates that the destination opinion score contributes more than source opinion score in this task.","The value of μ is a trade-off between answer reinforcement relation and topic relation to calculate the scores of each node. For lower value of μ, we give more importance to the relevance to the question than the similarity with other sentences. The experiment results are shown in Figure 6. The best result is achieved when μ = 0.8. This figure also shows the importance of reinforcement between answer candidates. If we don’t consider the sentence similarity(μ = 0), the performance drops significantly. 5.2.3 Opinion HITS Performance The parameter γ combines the opinion hub score and topic hub score in the Opinion HITS model. The higher γ is, the more contribution is given 743 0 . 2 2 0 . 2 4","0",".","2","6 H","I","T","S","_","r H I T S _ F","B","a","s","e","_","r","B","a","s","e","_","F","F(3) 0 . 1 2 0 . 1 4 0 . 1 6 0 . 1 8 0 . 2","0","0",".","2","0",".","4","0",".","6","0",".","8","1  Figure 7: Opinion HITS Performance with varying parameter γ to topic hub level, while the less contribution is given to opinion hub level. The experiment results are shown in Figure 7. Similar to baseline parameter α, since the answer candidates are extracted based on topic information, the systems considering opinion information heavily (α=0.1 in baseline, γ=0.2) perform best.","Opinion HITS model ranks the sentences by authority scores. It can also rank the popular opinion words and popular topic words from the topic hub layer and opinion hub layer, towards a specific question. Take the question 1024.3 “What reasons do people give for liking Zillow?” as an example, its topic word is “Zillow”, and its sentiment polarity is positive. Based on the final hub scores, the top 10 topic words and opinion words are shown as Table 2. Opinion real, like, accurate, rich, right, interesting, Words better, easily, free, good Topic zillow, estate, home, house, data, value, Words site, information, market, worth Table 2: Question-specific popular topic words and opinion words generated by Opinion HITS","Zillow is a real estate site for users to see the value of houses or homes. People like it because it is easily used, accurate and sometimes free. From the Table 2, we can see that the top topic words are the most related with question topic, and the top opinion words are question-specific sentiment words, such as “accurate”, “easily”, “free”, not just general opinion words, like “great”, “excellent” and “good”. 5.2.4 Comparisons with TAC Systems We are also interested in the performance comparison with the systems in TAC QA 2008. From Table 3, we can see Opinion PageRank and Opinion System Precision Recall F(3) OpPageRank 0.109 0.242 0.200 OpHITS 0.102 0.256 0.205 System 1 0.079 0.235 0.186 System 2 0.053 0.262 0.173 System 3 0.109 0.216 0.172 Table 3: Comparison results with TAC 2008 Three Top Ranked Systems (system 1-3 demonstrate top 3 systems in TAC) HITS respectively achieve around 10% improve-ment compared with the best result in TAC 2008, which demonstrates that our algorithm is indeed performing much better than the state-of-the-art opinion QA methods."]},{"title":"6 Conclusion and Future Works","paragraphs":["In this paper, we proposed two graph based sentence ranking methods for opinion question answering. Our models, called Opinion PageRank and Opinion HITS, could naturally incorporate topic relevance information and the opinion sentiment information. Furthermore, the relationships between different answer candidates can be considered. We demonstrate the usefulness of these relations through our experiments. The experiment results also show that our proposed methods outperform TAC 2008 QA Task top ranked systems by about 10% in terms of F score.","Our random walk based graph methods integrate topic information and sentiment information in a unified framework. They are not limited to the sentence ranking for opinion question answering. They can be used in general opinion document search. Moreover, these models can be more generalized to the ranking task with two types of influencing factors.","Acknowledgments: Special thanks to Derek Hao Hu and Qiang Yang for their valuable comments and great help on paper prepara-tion. We also thank Hongning Wang, Min Zhang, Xiaojun Wan and the anonymous reviewers for their useful comments, and thank Hoa Trang Dang for providing the TAC evaluation results. The work was supported by 973 project in China(2007CB311003), NSFC project(60803075), Microsoft joint project ”Opinion Summarization toward Opinion Search”, and a grant from the International Development Re-search Center, Canada. 744"]},{"title":"References","paragraphs":["Ricardo Baeza-Yates and Berthier Ribeiro-Neto. 1999. Modern Information Retrieval. Addison Wesley, May.","Xavier Carreras and Lluis Marquez. 2005. Introduc-tion to the conll-2005 shared task: Semantic role labeling.","Yi Chen, Ming Zhou, and Shilong Wang. 2006. Reranking answers for definitional qa using language modeling. In ACL-CoLing, pages 1081–1088.","Hang Cui, Min-Yen Kan, and Tat-Seng Chua. 2007. Soft pattern matching models for definitional question answering. ACM Trans. Inf. Syst., 25(2):8.","Hoa Trang Dang. 2008. Overview of the tac 2008 opinion question answering and summarization tasks (draft). In TAC.","Günes Erkan and Dragomir R. Radev. 2004. Lexpagerank: Prestige in multi-document text summarization. In EMNLP.","Andrea Esuli and Fabrizio Sebastiani. 2006. Sentiwordnet: A publicly available lexical resource for opinion mining. In LREC.","Jon M. Kleinberg. 1999. Authoritative sources in a hyperlinked environment. J. ACM, 46(5):604–632.","Lun-Wei Ku, Yu-Ting Liang, and Hsin-Hsi Chen. 2007. Question analysis and answer passage retrieval for opinion question answering systems. In ROCLING.","Tie-Yan Liu and Wei-Ying Ma. 2005. Webpage importance analysis using conditional markov random walk. In Web Intelligence, pages 515–521.","Rada Mihalcea and Paul Tarau. 2004. Textrank: Bringing order into texts. In EMNLP.","Jahna Otterbacher, Günes Erkan, and Dragomir R. Radev. 2005. Using random walks for question-focused sentence retrieval. In HLT/EMNLP.","Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. 1998. The pagerank citation ranking: Bringing order to the web. Technical report, Stanford University.","Swapna Somasundaran, Theresa Wilson, Janyce Wiebe, and Veselin Stoyanov. 2007. Qa with attitude: Exploiting opinion type analysis for improv-ing question answering in online discussions and the news. In ICWSM.","Kim Soo-Min and Eduard Hovy. 2005. Identifying opinion holders for question answering in opinion texts. In AAAI 2005 Workshop.","Veselin Stoyanov, Claire Cardie, and Janyce Wiebe. 2005. Multi-perspective question answering using the opqa corpus. In HLT/EMNLP.","Vasudeva Varma, Prasad Pingali, Rahul Katragadda, and et al. 2008. Iiit hyderabad at tac 2008. In Text Analysis Conference.","X. Wan and J Yang. 2008. Multi-document summarization using cluster-based link analysis. In SIGIR, pages 299–306.","Hong Yu and Vasileios Hatzivassiloglou. 2003. To-wards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences. In EMNLP.","Min Zhang and Xingyao Ye. 2008. A generation model to unify topic relevance and lexicon-based sentiment for opinion retrieval. In SIGIR, pages 411–418. 745"]}],"references":[{"authors":[{"first":"Ricardo","last":"Baeza-Yates"},{"first":"Berthier","last":"Ribeiro-Neto"}],"year":"1999","title":"Modern Information Retrieval","source":"Ricardo Baeza-Yates and Berthier Ribeiro-Neto. 1999. Modern Information Retrieval. Addison Wesley, May."},{"authors":[{"first":"Xavier","last":"Carreras"},{"first":"Lluis","last":"Marquez"}],"year":"2005","title":"Introduc-tion to the conll-2005 shared task: Semantic role labeling","source":"Xavier Carreras and Lluis Marquez. 2005. Introduc-tion to the conll-2005 shared task: Semantic role labeling."},{"authors":[{"first":"Yi","last":"Chen"},{"first":"Ming","last":"Zhou"},{"first":"Shilong","last":"Wang"}],"year":"2006","title":"Reranking answers for definitional qa using language modeling","source":"Yi Chen, Ming Zhou, and Shilong Wang. 2006. Reranking answers for definitional qa using language modeling. In ACL-CoLing, pages 1081–1088."},{"authors":[{"first":"Hang","last":"Cui"},{"first":"Min-Yen","last":"Kan"},{"first":"Tat-Seng","last":"Chua"}],"year":"2007","title":"Soft pattern matching models for definitional question answering","source":"Hang Cui, Min-Yen Kan, and Tat-Seng Chua. 2007. Soft pattern matching models for definitional question answering. ACM Trans. Inf. Syst., 25(2):8."},{"authors":[{"first":"Hoa","middle":"Trang","last":"Dang"}],"year":"2008","title":"Overview of the tac 2008 opinion question answering and summarization tasks (draft)","source":"Hoa Trang Dang. 2008. Overview of the tac 2008 opinion question answering and summarization tasks (draft). In TAC."},{"authors":[{"first":"Günes","last":"Erkan"},{"first":"Dragomir","middle":"R.","last":"Radev"}],"year":"2004","title":"Lexpagerank: Prestige in multi-document text summarization","source":"Günes Erkan and Dragomir R. Radev. 2004. Lexpagerank: Prestige in multi-document text summarization. In EMNLP."},{"authors":[{"first":"Andrea","last":"Esuli"},{"first":"Fabrizio","last":"Sebastiani"}],"year":"2006","title":"Sentiwordnet: A publicly available lexical resource for opinion mining","source":"Andrea Esuli and Fabrizio Sebastiani. 2006. Sentiwordnet: A publicly available lexical resource for opinion mining. In LREC."},{"authors":[{"first":"Jon","middle":"M.","last":"Kleinberg"}],"year":"1999","title":"Authoritative sources in a hyperlinked environment","source":"Jon M. Kleinberg. 1999. Authoritative sources in a hyperlinked environment. J. ACM, 46(5):604–632."},{"authors":[{"first":"Lun-Wei","last":"Ku"},{"first":"Yu-Ting","last":"Liang"},{"first":"Hsin-Hsi","last":"Chen"}],"year":"2007","title":"Question analysis and answer passage retrieval for opinion question answering systems","source":"Lun-Wei Ku, Yu-Ting Liang, and Hsin-Hsi Chen. 2007. Question analysis and answer passage retrieval for opinion question answering systems. In ROCLING."},{"authors":[{"first":"Tie-Yan","last":"Liu"},{"first":"Wei-Ying","last":"Ma"}],"year":"2005","title":"Webpage importance analysis using conditional markov random walk","source":"Tie-Yan Liu and Wei-Ying Ma. 2005. Webpage importance analysis using conditional markov random walk. In Web Intelligence, pages 515–521."},{"authors":[{"first":"Rada","last":"Mihalcea"},{"first":"Paul","last":"Tarau"}],"year":"2004","title":"Textrank: Bringing order into texts","source":"Rada Mihalcea and Paul Tarau. 2004. Textrank: Bringing order into texts. In EMNLP."},{"authors":[{"first":"Jahna","last":"Otterbacher"},{"first":"Günes","last":"Erkan"},{"first":"Dragomir","middle":"R.","last":"Radev"}],"year":"2005","title":"Using random walks for question-focused sentence retrieval","source":"Jahna Otterbacher, Günes Erkan, and Dragomir R. Radev. 2005. Using random walks for question-focused sentence retrieval. In HLT/EMNLP."},{"authors":[{"first":"Lawrence","last":"Page"},{"first":"Sergey","last":"Brin"},{"first":"Rajeev","last":"Motwani"},{"first":"Terry","last":"Winograd"}],"year":"1998","title":"The pagerank citation ranking: Bringing order to the web","source":"Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. 1998. The pagerank citation ranking: Bringing order to the web. Technical report, Stanford University."},{"authors":[{"first":"Swapna","last":"Somasundaran"},{"first":"Theresa","last":"Wilson"},{"first":"Janyce","last":"Wiebe"},{"first":"Veselin","last":"Stoyanov"}],"year":"2007","title":"Qa with attitude: Exploiting opinion type analysis for improv-ing question answering in online discussions and the news","source":"Swapna Somasundaran, Theresa Wilson, Janyce Wiebe, and Veselin Stoyanov. 2007. Qa with attitude: Exploiting opinion type analysis for improv-ing question answering in online discussions and the news. In ICWSM."},{"authors":[{"first":"Kim","last":"Soo-Min"},{"first":"Eduard","last":"Hovy"}],"year":"2005","title":"Identifying opinion holders for question answering in opinion texts","source":"Kim Soo-Min and Eduard Hovy. 2005. Identifying opinion holders for question answering in opinion texts. In AAAI 2005 Workshop."},{"authors":[{"first":"Veselin","last":"Stoyanov"},{"first":"Claire","last":"Cardie"},{"first":"Janyce","last":"Wiebe"}],"year":"2005","title":"Multi-perspective question answering using the opqa corpus","source":"Veselin Stoyanov, Claire Cardie, and Janyce Wiebe. 2005. Multi-perspective question answering using the opqa corpus. In HLT/EMNLP."},{"authors":[{"first":"Vasudeva","last":"Varma"},{"first":"Prasad","last":"Pingali"},{"first":"Rahul","last":"Katragadda"},{"last":"al"}],"year":"2008","title":"Iiit hyderabad at tac 2008","source":"Vasudeva Varma, Prasad Pingali, Rahul Katragadda, and et al. 2008. Iiit hyderabad at tac 2008. In Text Analysis Conference."},{"authors":[{"first":"X.","last":"Wan"},{"first":"J","last":"Yang"}],"year":"2008","title":"Multi-document summarization using cluster-based link analysis","source":"X. Wan and J Yang. 2008. Multi-document summarization using cluster-based link analysis. In SIGIR, pages 299–306."},{"authors":[{"first":"Hong","last":"Yu"},{"first":"Vasileios","last":"Hatzivassiloglou"}],"year":"2003","title":"To-wards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences","source":"Hong Yu and Vasileios Hatzivassiloglou. 2003. To-wards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences. In EMNLP."},{"authors":[{"first":"Min","last":"Zhang"},{"first":"Xingyao","last":"Ye"}],"year":"2008","title":"A generation model to unify topic relevance and lexicon-based sentiment for opinion retrieval","source":"Min Zhang and Xingyao Ye. 2008. A generation model to unify topic relevance and lexicon-based sentiment for opinion retrieval. In SIGIR, pages 411–418. 745"}],"cites":[{"style":0,"text":"Stoyanov et al., 2005","origin":{"pointer":"/sections/2/paragraphs/0","offset":725,"length":21},"authors":[{"last":"Stoyanov"},{"last":"al."}],"year":"2005","references":["/references/15"]},{"style":0,"text":"Dang, 2008","origin":{"pointer":"/sections/2/paragraphs/0","offset":807,"length":10},"authors":[{"last":"Dang"}],"year":"2008","references":["/references/4"]},{"style":0,"text":"Stoyanov et al., 2005","origin":{"pointer":"/sections/2/paragraphs/0","offset":909,"length":21},"authors":[{"last":"Stoyanov"},{"last":"al."}],"year":"2005","references":["/references/15"]},{"style":0,"text":"Stoyanov et al., 2005","origin":{"pointer":"/sections/2/paragraphs/0","offset":1277,"length":21},"authors":[{"last":"Stoyanov"},{"last":"al."}],"year":"2005","references":["/references/15"]},{"style":0,"text":"Chen et al., 2006","origin":{"pointer":"/sections/2/paragraphs/1","offset":35,"length":17},"authors":[{"last":"Chen"},{"last":"al."}],"year":"2006","references":["/references/2"]},{"style":0,"text":"Cui et al., 2007","origin":{"pointer":"/sections/2/paragraphs/1","offset":54,"length":16},"authors":[{"last":"Cui"},{"last":"al."}],"year":"2007","references":["/references/3"]},{"style":0,"text":"Varma et al., 2008","origin":{"pointer":"/sections/2/paragraphs/1","offset":569,"length":18},"authors":[{"last":"Varma"},{"last":"al."}],"year":"2008","references":["/references/16"]},{"style":0,"text":"Stoyanov et al., 2005","origin":{"pointer":"/sections/2/paragraphs/1","offset":663,"length":21},"authors":[{"last":"Stoyanov"},{"last":"al."}],"year":"2005","references":["/references/15"]},{"style":0,"text":"Stoyanov et al., 2005","origin":{"pointer":"/sections/3/paragraphs/0","offset":75,"length":21},"authors":[{"last":"Stoyanov"},{"last":"al."}],"year":"2005","references":["/references/15"]},{"style":0,"text":"Somasundaran et al., 2007","origin":{"pointer":"/sections/3/paragraphs/0","offset":341,"length":25},"authors":[{"last":"Somasundaran"},{"last":"al."}],"year":"2007","references":["/references/13"]},{"style":0,"text":"Ku et al., 2007","origin":{"pointer":"/sections/3/paragraphs/0","offset":518,"length":15},"authors":[{"last":"Ku"},{"last":"al."}],"year":"2007","references":["/references/8"]},{"style":0,"text":"Yu and Hatzivassiloglou, 2003","origin":{"pointer":"/sections/3/paragraphs/0","offset":867,"length":29},"authors":[{"last":"Yu"},{"last":"Hatzivassiloglou"}],"year":"2003","references":["/references/18"]},{"style":0,"text":"Soo-Min and Hovy, 2005","origin":{"pointer":"/sections/3/paragraphs/0","offset":1044,"length":22},"authors":[{"last":"Soo-Min"},{"last":"Hovy"}],"year":"2005","references":["/references/14"]},{"style":0,"text":"Dang, 2008","origin":{"pointer":"/sections/3/paragraphs/1","offset":102,"length":10},"authors":[{"last":"Dang"}],"year":"2008","references":["/references/4"]},{"style":0,"text":"Page et al., 1998","origin":{"pointer":"/sections/3/paragraphs/2","offset":158,"length":17},"authors":[{"last":"Page"},{"last":"al."}],"year":"1998","references":["/references/12"]},{"style":0,"text":"Kleinberg, 1999","origin":{"pointer":"/sections/3/paragraphs/2","offset":201,"length":15},"authors":[{"last":"Kleinberg"}],"year":"1999","references":["/references/7"]},{"style":0,"text":"Erkan and Radev, 2004","origin":{"pointer":"/sections/3/paragraphs/2","offset":421,"length":21},"authors":[{"last":"Erkan"},{"last":"Radev"}],"year":"2004","references":["/references/5"]},{"style":0,"text":"Mihalcea and Tarau, 2004","origin":{"pointer":"/sections/3/paragraphs/2","offset":444,"length":24},"authors":[{"last":"Mihalcea"},{"last":"Tarau"}],"year":"2004","references":["/references/10"]},{"style":0,"text":"Otterbacher et al., 2005","origin":{"pointer":"/sections/3/paragraphs/2","offset":470,"length":24},"authors":[{"last":"Otterbacher"},{"last":"al."}],"year":"2005","references":["/references/11"]},{"style":0,"text":"Wan and Yang, 2008","origin":{"pointer":"/sections/3/paragraphs/2","offset":496,"length":18},"authors":[{"last":"Wan"},{"last":"Yang"}],"year":"2008","references":["/references/17"]},{"style":0,"text":"Otterbacher et al., 2005","origin":{"pointer":"/sections/4/paragraphs/1","offset":169,"length":24},"authors":[{"last":"Otterbacher"},{"last":"al."}],"year":"2005","references":["/references/11"]},{"style":0,"text":"Baeza-Yates and Ribeiro-Neto, 1999","origin":{"pointer":"/sections/4/paragraphs/2","offset":125,"length":34},"authors":[{"last":"Baeza-Yates"},{"last":"Ribeiro-Neto"}],"year":"1999","references":["/references/0"]},{"style":0,"text":"Liu and Ma, 2005","origin":{"pointer":"/sections/4/paragraphs/14","offset":250,"length":16},"authors":[{"last":"Liu"},{"last":"Ma"}],"year":"2005","references":["/references/9"]},{"style":0,"text":"Wan and Yang, 2008","origin":{"pointer":"/sections/4/paragraphs/14","offset":268,"length":18},"authors":[{"last":"Wan"},{"last":"Yang"}],"year":"2008","references":["/references/17"]},{"style":0,"text":"Kleinberg, 1999","origin":{"pointer":"/sections/4/paragraphs/36","offset":289,"length":15},"authors":[{"last":"Kleinberg"}],"year":"1999","references":["/references/7"]},{"style":0,"text":"Otterbacher et al., 2005","origin":{"pointer":"/sections/4/paragraphs/38","offset":662,"length":24},"authors":[{"last":"Otterbacher"},{"last":"al."}],"year":"2005","references":["/references/11"]},{"style":0,"text":"Otterbacher et al., 2005","origin":{"pointer":"/sections/4/paragraphs/39","offset":207,"length":24},"authors":[{"last":"Otterbacher"},{"last":"al."}],"year":"2005","references":["/references/11"]},{"style":0,"text":"Yu and Hatzivassiloglou, 2003","origin":{"pointer":"/sections/5/paragraphs/1","offset":336,"length":29},"authors":[{"last":"Yu"},{"last":"Hatzivassiloglou"}],"year":"2003","references":["/references/18"]},{"style":0,"text":"Carreras and Marquez, 2005","origin":{"pointer":"/sections/5/paragraphs/1","offset":485,"length":26},"authors":[{"last":"Carreras"},{"last":"Marquez"}],"year":"2005","references":["/references/1"]},{"style":0,"text":"Rodrigo et al., 2007","origin":{"pointer":"/sections/5/paragraphs/1","offset":740,"length":20},"authors":[{"last":"Rodrigo"},{"last":"al."}],"year":"2007","references":[]},{"style":0,"text":"Dang, 2008","origin":{"pointer":"/sections/5/paragraphs/2","offset":64,"length":10},"authors":[{"last":"Dang"}],"year":"2008","references":["/references/4"]},{"style":0,"text":"Chen et al., 2006","origin":{"pointer":"/sections/5/paragraphs/3","offset":597,"length":17},"authors":[{"last":"Chen"},{"last":"al."}],"year":"2006","references":["/references/2"]},{"style":0,"text":"Ounis, 2006","origin":{"pointer":"/sections/6/paragraphs/1","offset":587,"length":11},"authors":[{"last":"Ounis"}],"year":"2006","references":[]},{"style":0,"text":"Dang, 2008","origin":{"pointer":"/sections/6/paragraphs/1","offset":886,"length":10},"authors":[{"last":"Dang"}],"year":"2008","references":["/references/4"]},{"style":0,"text":"Dang, 2008","origin":{"pointer":"/sections/6/paragraphs/1","offset":1414,"length":10},"authors":[{"last":"Dang"}],"year":"2008","references":["/references/4"]},{"style":0,"text":"Esuli and Sebastiani, 2006","origin":{"pointer":"/sections/6/paragraphs/22","offset":225,"length":26},"authors":[{"last":"Esuli"},{"last":"Sebastiani"}],"year":"2006","references":["/references/6"]},{"style":0,"text":"Liu and Ma, 2005","origin":{"pointer":"/sections/6/paragraphs/22","offset":572,"length":16},"authors":[{"last":"Liu"},{"last":"Ma"}],"year":"2005","references":["/references/9"]},{"style":0,"text":"Zhang and Ye, 2008","origin":{"pointer":"/sections/6/paragraphs/25","offset":1175,"length":18},"authors":[{"last":"Zhang"},{"last":"Ye"}],"year":"2008","references":["/references/19"]}]}
