{"sections":[{"title":"","paragraphs":["Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 201–204, Suntec, Singapore, 4 August 2009. c⃝2009 ACL and AFNLP"]},{"title":"Mining Association Language Patterns for Negative Life Event Classification   Liang-Chih Yu 1 , Chien-Lung Chan1 , Chung-Hsien Wu2 and Chao-Cheng Lin 3","paragraphs":["1"]},{"title":"Department of Information Management, Yuan Ze University, Taiwan, R.O.C.","paragraphs":["2"]},{"title":"Department of CSIE, National Cheng Kung University, Taiwan, R.O.C.","paragraphs":["3"]},{"title":"Department of Psychiatry, National Taiwan University Hospital, Taiwan, R.O.C.","paragraphs":["{lcyu, clchan}@saturn.yzu.edu.tw, chwu@csie.ncku.edu.tw, linchri@gmail.com   "]},{"title":"Abstract ","paragraphs":["Negative life events, such as death of a family member, argument with a spouse and loss of a job, play an important role in triggering depressive episodes. Therefore, it is worth to develop psychiatric services that can automatically identify such events. In this paper, we propose the use of association language patterns, i.e., meaningful combinations of words (e.g., <loss, job>), as features to classify sentences with negative life events into predefined categories (e.g., Family, Love, Work). The language patterns are discovered using a data mining algorithm, called association pattern mining, by incrementally associating frequently co-occurred words in the sentences annotated with negative life events. The discovered patterns are then combined with single words to train classifiers. Experimental results show that association language patterns are significant features, thus yielding better performance than the baseline system using single words alone."]},{"title":"1 Introduction","paragraphs":["With the increased incidence of depressive disorders, many psychiatric websites have developed community-based services such as message boards, web forums and blogs for public access. Through these services, individuals can describe their stressful or negative life events such as death of a family member, argument with a spouse and loss of a job, along with depressive symptoms, such as depressive mood, suicidal tendencies and anxiety. Such psychiatric texts (e.g., forum posts) contain large amounts of natural language expressions related to negative life events, making them useful resources for build-ing more effective psychiatric services. For in-stance, a psychiatric retrieval service can retrieve relevant forum or blog posts according to the negative life events experienced by users so that they can be aware that they are not alone because many people have suffered from the same or similar problems. The users can then create a community discussion to share their experiences with each other. Additionally, a dialog system can generate supportive responses like “Don’t worry”, “That’s really sad” and “Cheer up” if it can understand the negative life events embedded in the example sentences shown in Table 1. Therefore, this study proposes a framework for negative life event classification. We formulate this problem as a sentence classification task; that is, classify sentences according to the type of negative life events within them. The class labels used herein are presented in Table 1, which are derived from Brostedt and Pedersen (2003).","Traditional approaches to sentence classification (Khoo et al., 2006; Naughton et al., 2008) or text categorization (Sebastiani 2002) usually adopt bag-of-words as baseline features to train classifiers. Since the bag-of-words approach treats each word independently without consider-ing the relationships of words in sentences, some researchers have investigated the use of n-grams to capture sequential relations between words to boost classification performance (Chitturi and Hansen, 2008; Li and Zong, 2008). The use of n-grams is effective in capturing local dependencies of words, but tends to suffer from data sparseness problem in capturing long-distance dependencies since higher-order n-grams require large training data to obtain reliable estimation. For our task, the expressions of negative life events can be characterized by association language patterns, i.e., meaningful combinations of words, such as <worry, children, health>, <break up, boyfriend>, <argue, friend>, <loss, job>, and 201 <school, teacher, blame> in the example sentences in Table 1. Such language patterns are not necessarily composed of continuous words. In-stead, they are usually composed of the words with long-distance dependencies, which cannot be easily captured by n-grams.","Therefore, the aim of this study is two-fold: (1) to automatically discover association language patterns from the sentences annotated with negative life events; and (2) to classify sentences with negative life events using the discovered patterns. To discover association language patterns, we incorporate the measure mutual information (MI) into a data mining algorithm, called association pattern mining, to incrementally derive frequently co-occurred words in sentences (Section 2). The discovered patterns are then combined with single words as features to train classifiers for negative life event classification (Section 3). Experimental results are presented in Section 4. Conclusions are finally drawn in Section 5."]},{"title":"2 Association Language Pattern Mining","paragraphs":["The problem of language pattern acquisition can be converted into the problem of association pattern mining, where each sales transaction in a database can be considered as a sentence in the corpora, and each item in a transaction denotes a word in a sentence. An association language pattern is defined herein as a combination of multiple associated words, denoted by 1,..., kww<>. Thus, the task of association pattern mining is to mine the language patterns of frequently associated words from the training sentences. For this purpose, we adopt the Apriori algorithm (Agrawal and Srikant, 1994) and modified it slightly to fit our application. Its basic concept is to identify frequent word sets recursively, and then generate association language patterns from the frequent word sets. For simplicity, only the combinations of nouns and verbs are considered, and the length is restricted to at most 4 words, i.e., 2-word, 3-word and 4-word combinations. The detailed procedure is described as follows. 2.1 Find frequent word sets A word set is frequent if it possesses a minimum support. The support of a word set is defined as the number of training sentences containing the word set. For instance, the support of a two-word set { iw , jw } denotes the number of training sentences containing the word pair ( iw , jw ). The","frequent k-word sets are discovered from (k-1)-","word sets. First, the support of each word, i.e.,","word frequency, in the training corpus is counted.","The set of frequent one-word sets, denoted as 1L ,","is then generated by choosing the words with a","minimum support level. To calculate kL , the fol-","lowing two-step process is performed iteratively","until no more frequent k-word sets are found.","z Join step: A set of candidate k-word sets, denoted as kC , is first generated by merg-ing frequent word sets of 1kL − , in which only the word sets whose first (k-2) words are identical can be merged.","z Prune step: The support of each candidate word set in kC is then counted to determine which candidate word sets are frequent. Finally, the candidate word sets with a support count greater than or equal to the minimum support are considered to form kL . The candidate word sets with a subset that is not frequent are eliminated. Figure 1 shows an example of generating kL . Label Description Example Sentence","Family Serious illness of a family member;","Son or daughter leaving home I am very worried about my children’s health.","Love Spouse/mate engaged in infidelity; Broke up with a boyfriend or girlfriend I broke up with my dear but cruel boyfriend recently.","School Examination failed or grade dropped; Unable to enter/stay in school I hate to go to school because my teacher always blames me.","Work Laid off or fired from a job; Demotion and salary reduction I lost my job in this economic recession a few months ago.","Social Substantial conflicts with a friend; Difficulties in social activities I argued with my best friend and was upset. Table 1. Classification of negative life events. 202","2.2 Generate association patterns from frequent word sets Association language patterns can be generated via a confidence measure once the frequent word sets have been identified. The confidence of an association language pattern of k words is defined as the mutual information of the k words, as shown below. 11 1 1 1 (,... ) (,...) ( ,... ) ( ,... )log () kk k k k i i Confww MIww Pw w Pw w Pw = <>= ="]},{"title":"∏ ","paragraphs":["(1) where 1( ,... )kPw w denotes the probability of the k words co-occurring in a sentence in the training set, and ()iPw denotes the probability of a single word occurring in the training set. Accordingly, each frequent word set in kL is assigned a mutual information score. In order to generate a set of association language patterns, all frequent word sets are sorted in the descending order of the mutual information scores. The minimum confidence (a threshold at percentage) is then applied to select top N percent frequent word sets as the resulting language patterns. This threshold is determined empirically by maximizing classification performance (Section 4). Figure 1 (righthand side) shows an example of generating the association language patterns from kL ."]},{"title":"3 Sentence Classification","paragraphs":["The classifiers used in this study include Support Vector Machine (SVM), C4.5, and Naïve Bayes (NB) classifier, which is provided by Weka Package (Witten and Frank, 2005). The feature set includes: Bag-of-Words (BOW): Each single word in sentences. Association language patterns (ALP): The top N percent association language patterns acquired in the previous section. Ontology expansion (Onto): The top N percent association language patterns are expanded by mapping the constituent words into their synonyms. For example, the pattern <boss, conflict> can be expanded as <chief, conflict> since the words boss and chief are synonyms. Here we use the HowNet (http://www.keenage.com), a Chinese lexical ontology, for pattern expansion."]},{"title":"4 Experimental Results Data set:","paragraphs":["A total of 2,856 sentences were collected from the Internet-based Self-assessment Program for Depression (ISP-D) database of the PsychPark (http://www.psychpark.org), a virtual psychiatric clinic, maintained by a group of volunteer professionals of Taiwan Association of Mental Health Informatics (Bai et al., 2001). Each sentence was then annotated by trained an-notators with one of the five types of negative life events. Table 2 shows the break-down of the distribution of sentence types.","The data set was randomly split into a training set, a development set, and a test set with an 8:1:1 ratio. The training set was used for language pattern generation. The development set was used to optimize the threshold (Section 2.2) for the classifiers (SVM, C4.5 and NB). Each classifier was implemented using three different levels of features, namely BOW, BOW+ALP, Prune Step (min. support) Sorting and Thresholding <Boyfriend, Conflict> <Boyfriend, Break up> <Boss, Conflict> <Conflict, Break up> Find Frequent Word Sets Generate Association Language Patterns Join Step Prune Step (min. support) Prune Step (min. support) <Boyfriend, Conflict, Break up>Join Step Figure 1. Example of generating association language patterns.","Sentence Type % in Corpus Family 28.8 Love 22.8 School 13.3 Work 14.3 Social 20.8","Table 2. Distribution of sentence types. 203 and BOW+ALP+Onto, to examine the effective-ness of association language patterns. The classification performance is measured by accuracy, i.e., the number of correctly classified sentences divided by the total number of test sentences. 4.1 Evaluation on threshold selection Since not all discovered association language patterns contribute to the classification task, the threshold described in Section 2.2 is used to select top N percent patterns for classification. This experiment is to determine an optimal threshold for each involved classifier by maximizing its classification accuracy on the development set. Figure 2 shows the classification accuracy of NB against different threshold values.","When using association language patterns as features (BOW+ALP), the accuracy increased with increasing the threshold value up to 0.6, indicating that the top 60% discovered patterns contained more useful patterns for classification. By contrast, the accuracy decreased when the threshold value was above 0.6, indicating that the remaining 40% contained more noisy patterns that may increase the ambiguity in classification. When using the ontology expansion approach (BOW+ALP+Onto), both the number and diversity of discovered patterns are increased. Therefore, the accuracy was improved and the optimal accuracy was achieved at 0.5. However, the accuracy dropped significantly when the threshold value was above 0.5. This finding indicates that expansion on noisy patterns may produce more noisy patterns and thus decrease performance. 4.2 Results of classification performance The results of each classifier were obtained from the test set using its own threshold optimized in the previous section. Table 3 shows the comparative results of different classifiers with different levels of features. The incorporation of association language patterns improved the accuracy of NB, C4.5, and SVM by 3.9%, 1.9%, and 2.2%, respectively, and achieved an average improvement of 2.7%. Additionally, the use of ontology expansion can further improve the performance by 1.6% in average. This finding indicates that association language patterns are significant features for negative life event classification."]},{"title":"5 Conclusion","paragraphs":["This work has presented a framework that uses a data mining algorithm and ontology expansion method to acquire association language patterns for negative life event classification. The association language patterns can capture word relationships in sentences, thus yielding higher performance than the baseline system using single words alone. Future work will focus on devising a semi-supervised or unsupervised method for language pattern acquisition from web resources so as to reduce reliance on annotated corpora."]},{"title":"References","paragraphs":["R. Agrawal and R. Srikant. 1994. Fast Algorithms for Min-ing Association Rules. In Proc. Int’l Conf. Very Large Data Bases (VLDB), pages 487-499.","Y. M. Bai, C. C. Lin, J. Y. Chen, and W. C. Liu. 2001. Virtual Psychiatric Clinics. American Journal of Psychiatry, vol. 158, no. 7, pp. 1160-1161.","E. M. Brostedt and N. L. Pedersen. 2003. Stressful Life Events and Affective Illness. Acta Psychiatrica Scandinavica, vol. 107, pp. 208-215.","R. Chitturi and J. H.L. Hansen. 2008. Dialect Classification for online podcasts fusing Acoustic and Language based Structural and Semantic Information. In Proc. of ACL-08, pages 21-24.","A. Khoo, Y. Marom and D. Albrecht. 2006. Experiments with Sentence Classification. In Proc. of Australasian Language Technology Workshop, pages 18-25.","S. Li and C. Zong. 2008. Multi-domain Sentiment Classification. In Proc. of ACL-08, pages 257-260.","M. Naughton, N. Stokes, and J. Carthy. 2008. Investigating Statistical Techniques for Sentence-Level Event Classification. In Proc. of COLING-08, pages 617-624.","F. Sebastiani. 2002. Machine Learning in Automated Text Categorization. ACM Computing Surveys, vol. 34, no. 1, pp. 1-47.","I. H. Witten and E. Frank. 2005. Data Mining: Practical Machine Learning Tools and Techniques, 2nd Edition, Morgan Kaufmann, San Francisco.","NB C4.5 SVM BOW 0.717 0.741 0.787 BOW+ALP 0.745 0.755 0.804 BOW+ALP+Onto 0.759 0.766 0.815 Table 3. Accuracy of classifiers on testing data. 0.62 0.64 0.66 0.68 0.70 0.72 0.74 0.76 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Threshold A ccur a c y BOW+ALP BOW+ALP+Onto  Figure 2. Threshold selection. 204"]}],"references":[{"authors":[{"first":"R.","last":"Agrawal"},{"first":"R.","last":"Srikant"}],"year":"1994","title":"Fast Algorithms for Min-ing Association Rules","source":"R. Agrawal and R. Srikant. 1994. Fast Algorithms for Min-ing Association Rules. In Proc. Int’l Conf. Very Large Data Bases (VLDB), pages 487-499."},{"authors":[{"first":"Y.","middle":"M.","last":"Bai"},{"first":"C.","middle":"C.","last":"Lin"},{"first":"J.","middle":"Y.","last":"Chen"},{"first":"W.","middle":"C.","last":"Liu"}],"year":"2001","title":"Virtual Psychiatric Clinics","source":"Y. M. Bai, C. C. Lin, J. Y. Chen, and W. C. Liu. 2001. Virtual Psychiatric Clinics. American Journal of Psychiatry, vol. 158, no. 7, pp. 1160-1161."},{"authors":[{"first":"E.","middle":"M.","last":"Brostedt"},{"first":"N.","middle":"L.","last":"Pedersen"}],"year":"2003","title":"Stressful Life Events and Affective Illness","source":"E. M. Brostedt and N. L. Pedersen. 2003. Stressful Life Events and Affective Illness. Acta Psychiatrica Scandinavica, vol. 107, pp. 208-215."},{"authors":[{"first":"R.","last":"Chitturi"},{"first":"J.","middle":"H. L.","last":"Hansen"}],"year":"2008","title":"Dialect Classification for online podcasts fusing Acoustic and Language based Structural and Semantic Information","source":"R. Chitturi and J. H.L. Hansen. 2008. Dialect Classification for online podcasts fusing Acoustic and Language based Structural and Semantic Information. In Proc. of ACL-08, pages 21-24."},{"authors":[{"first":"A.","last":"Khoo"},{"first":"Y.","last":"Marom"},{"first":"D.","last":"Albrecht"}],"year":"2006","title":"Experiments with Sentence Classification","source":"A. Khoo, Y. Marom and D. Albrecht. 2006. Experiments with Sentence Classification. In Proc. of Australasian Language Technology Workshop, pages 18-25."},{"authors":[{"first":"S.","last":"Li"},{"first":"C.","last":"Zong"}],"year":"2008","title":"Multi-domain Sentiment Classification","source":"S. Li and C. Zong. 2008. Multi-domain Sentiment Classification. In Proc. of ACL-08, pages 257-260."},{"authors":[{"first":"M.","last":"Naughton"},{"first":"N.","last":"Stokes"},{"first":"J.","last":"Carthy"}],"year":"2008","title":"Investigating Statistical Techniques for Sentence-Level Event Classification","source":"M. Naughton, N. Stokes, and J. Carthy. 2008. Investigating Statistical Techniques for Sentence-Level Event Classification. In Proc. of COLING-08, pages 617-624."},{"authors":[{"first":"F.","last":"Sebastiani"}],"year":"2002","title":"Machine Learning in Automated Text Categorization","source":"F. Sebastiani. 2002. Machine Learning in Automated Text Categorization. ACM Computing Surveys, vol. 34, no. 1, pp. 1-47."},{"authors":[{"first":"I.","middle":"H.","last":"Witten"},{"first":"E.","last":"Frank"}],"year":"2005","title":"Data Mining: Practical Machine Learning Tools and Techniques, 2nd Edition, Morgan Kaufmann, San Francisco","source":"I. H. Witten and E. Frank. 2005. Data Mining: Practical Machine Learning Tools and Techniques, 2nd Edition, Morgan Kaufmann, San Francisco."},{"authors":[],"source":"NB C4.5 SVM BOW 0.717 0.741 0.787 BOW+ALP 0.745 0.755 0.804 BOW+ALP+Onto 0.759 0.766 0.815 Table 3. Accuracy of classifiers on testing data. 0.62 0.64 0.66 0.68 0.70 0.72 0.74 0.76 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Threshold A ccur a c y BOW+ALP BOW+ALP+Onto  Figure 2. Threshold selection. 204"}],"cites":[{"style":0,"text":"Brostedt and Pedersen (2003)","origin":{"pointer":"/sections/6/paragraphs/0","offset":1535,"length":28},"authors":[{"last":"Brostedt"},{"last":"Pedersen"}],"year":"2003","references":["/references/2"]},{"style":0,"text":"Khoo et al., 2006","origin":{"pointer":"/sections/6/paragraphs/1","offset":51,"length":17},"authors":[{"last":"Khoo"},{"last":"al."}],"year":"2006","references":["/references/4"]},{"style":0,"text":"Naughton et al., 2008","origin":{"pointer":"/sections/6/paragraphs/1","offset":70,"length":21},"authors":[{"last":"Naughton"},{"last":"al."}],"year":"2008","references":["/references/6"]},{"style":0,"text":"Chitturi and Hansen, 2008","origin":{"pointer":"/sections/6/paragraphs/1","offset":466,"length":25},"authors":[{"last":"Chitturi"},{"last":"Hansen"}],"year":"2008","references":["/references/3"]},{"style":0,"text":"Li and Zong, 2008","origin":{"pointer":"/sections/6/paragraphs/1","offset":493,"length":17},"authors":[{"last":"Li"},{"last":"Zong"}],"year":"2008","references":["/references/5"]},{"style":0,"text":"Agrawal and Srikant, 1994","origin":{"pointer":"/sections/7/paragraphs/0","offset":571,"length":25},"authors":[{"last":"Agrawal"},{"last":"Srikant"}],"year":"1994","references":["/references/0"]},{"style":0,"text":"Witten and Frank, 2005","origin":{"pointer":"/sections/9/paragraphs/0","offset":147,"length":22},"authors":[{"last":"Witten"},{"last":"Frank"}],"year":"2005","references":["/references/8"]},{"style":0,"text":"Bai et al., 2001","origin":{"pointer":"/sections/10/paragraphs/0","offset":298,"length":16},"authors":[{"last":"Bai"},{"last":"al."}],"year":"2001","references":["/references/1"]}]}
