{"sections":[{"title":"","paragraphs":["Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 93–96, Suntec, Singapore, 4 August 2009. c⃝2009 ACL and AFNLP"]},{"title":"Automatic Generation of Information-seeking Questions Using Concept Clusters Shuguang Li Department of Computer Science University of York, YO10 5DD, UK sgli@cs.york.ac.uk Suresh Manandhar Department of Computer Science University of York, YO10 5DD, UK suresh@cs.york.ac.uk Abstract","paragraphs":["One of the basic problems of efficiently generating information-seeking dialogue in interactive question answering is to find the topic of an information-seeking question with respect to the answer documents. In this paper we propose an approach to solving this problem using concept clusters. Our empirical results on TREC collections and our ambiguous question collection shows that this approach can be successfully employed to handle ambiguous and list questions."]},{"title":"1 Introduction","paragraphs":["Question Answering systems have received a lot of interest from NLP researchers during the past years. But it is often the case that traditional QA systems cannot satisfy the information needs of the users as the question processing part may fail to properly classify the question or the information needed for extracting and generating the answer is either implicit or not present in the question. In such cases, interactive dialogue is needed to clarify the information needs and reformulate the question in a way that will help the system to find the correct answer.","Due to the fact that casual users often ask questions with ambiguity and vagueness, and most of the questions have multiple answers, current QA systems return a list of answers for most questions. The answers for one question usually belong to different topics. In order to satisfy the information needs of the user, information-seeking dialogue should take advantage of the inherent grouping of the answers.","Several methods have been investigated for generating topics for questions in information-seeking dialogue. Hori et al. (2003) proposed a method for generating the topics for disambiguation questions. The scores are computed purely based on the syntactic ambiguity present in the question. Phrases that are not modified by other phrases are considered to be highly ambiguous while phrases that are modified are considered less ambiguous. Small et al. (2004) utilizes clarification dialogue to reduce the misunderstanding of the questions between the HITIQA system and the user. The topics for such clarification questions are based on manually constructed topic frames. Similarly in (Hickl et al., 2006), suggestions are made to users in the form of predictive question and answer pairs (known as QUABs) which are either generated automatically from the set of documents returned for a query (using techniques first described in (Harabagiu et al., 2005), or are selected from a large database of questions-answer pairs created offline (prior to a dialogue) by human annotators. In Curtis et al. (2005), query expansion of the question based on Cyc Knowledge is used to generate topics for clarification questions. In Duan et al. (2008), the tree-cutting model is used to select topics from a set of relevant questions from Yahoo Answers.","None of the above methods consider the contexts of the list of answers in the documents returned by QA systems. The topic of a good information-seeking question should not only be relevant to the original question but also should be able to distinguish each answer from the others so that the new information can reduce the ambiguity and vagueness in the original question. Instead of using traditional clustering methods on categorization of web results, we present a new topic generation approach using concept clusters and a separability scoring mechanism for ranking the topics."]},{"title":"2 Topic Generation Based on Concept Clustering","paragraphs":["Text categorization and clustering especially hierarchical clustering are predominant approaches to organizing large amounts of information into top-93 ics or categories. But the main issue of categorization is that it is still difficult to automatically construct a good category structure, and manually formed hierarchies are usually small. And the main challenge of clustering algorithms is that the automatically formed cluster hierarchy may be unreadable or meaningless for human users. In order to overcome the limits of the above methods, we propose a concept clusters method and choose the labels of the clusters as topics.","Recent research on automatically extracting concepts and clusters of words from large database makes it feasible to grow a big set of concept clusters. Clustering by Committee (CBC) in Pantel et al. (2002) made use of the fact that words in the same cluster tend to appear in similar contexts. Pasca et al. (2008) utilized Google logs and lexico-syntactic patterns to get clusters with labels simultaneously. Google also released Google Sets which can be used to grow concept clusters with different sizes.","Currently our clusters are the union of the sets generated by the above three approaches, and we label them using the method described in Pasca et al. (2008). We define the concept clusters in our collection as {C1, C2, ..., Cn}. Ci={ei1, ei2, ..., eim}, eij is jth","subtopic of cluster Ci and m is the size of Ci.","We designed our system to take a question and its corresponding list of answers as input and then retrieve Google snippet documents for each of the answers with respect to the question. In a vectorspace model, a document is represented by a vector of keywords extracted from the document, with associated weights representing the importance of the keywords in the document and within the whole document collection. A document Dj in the collection is represented as {W0j, W1j, ..., Wnj}, and Wij is the weight of word i in document j. Here we use our concept clusters to create concept cluster vectors. A document Dj now is represented as <W C1j, W C2j, ..., W Cnj>, and W Cij is the score vector of document Dj for concept cluster Ci: W Cij = <Scorej(ei1), Scorej(ei2), ...Scorej(eim)> Scorej(eip) is the weight of subtopic eip of cluster Ci in document Dj. Currently we use tf-idf scheme (Yang et al., 1999) to calculate the weight of subtopics."]},{"title":"3 Concept Cluster Separability Measure","paragraphs":["We view different concept clusters from the contexts of the answers as different groups of features that can be used to classify the answers documents. We rank different context features by their separability on the answers. Currently our system retrieves the answers from Google search snippets, and each snippet is quite short. So we combine the top 50 snippets for one answer into one document. One answer is associated with one such big document. We propose the following interclass measure to compare the separability of different clusters: Score(Ci) = D N N ∑ p<q Dis(Dp, Dq),","D is the Dimension Penalty score, D = 1 M , M is the size of cluster Ci, N is the combined total number of classes from all the answers Dis(Dp, Dq) =","√ n ∑ m=0 (Scorep(eim) − Scoreq(eim))2","We introduce D, the ”Dimension Penalty” score which gives higher penalty to bigger clusters. Currently we use the reciprocal of the size of the cluster. The second part is the average pairwise distance between answers. N is the total number of classes of the answers. Next we describe in detail how to use the concept cluster vectors and separability measure to rank clusters."]},{"title":"4 Cluster Ranking Algorithm","paragraphs":["Input: Answer set A = {A1, A2, ..., Ap}; Documents set D = {D1, D2, ..., Dp} associated with answer set A; Concept cluster set CS = {Ci | some of the subtopics from Ci occurs in D}; Threshold Θ1, Θ2; The question Q; Concept cluster set QS = {Ci | some of the subtopics from Ci occurs in Q} Output: T = {< Ci, Score >}, a set of pairs of a concept cluster and its ranking score; QS; Variables: X, Y ; Steps: 1. CS = CS − QS 2. For each cluster Ci in CS 3. X = No. of answers in which context subtopics from Ci are present; 4. Y = No. of subtopics from Ci that occurs in the answers’ contexts; 5. If X < Θ1 or Y < Θ2 6. delete Ci from CS 7. continue 8. Represent every document as a concept cluster vector on Ci (see","section 2) 9. Calculate the Score(Ci) using our separability measure 10. Store < Ci, Score > in T 11. return T the medoid. Figure 1: Concept Cluster Ranking Algorithm","Figure 1 describes the algorithm for ranking concept clusters based on their separability score. This algorithm starts by deleting all 94 the clusters which are in QS from CS so that we only focus on the context clusters whose subtopics are present in the answers. However in some cases this assumption is incorrect1",". Taking the question shown in Table 2 for example, there are 6 answers for question LQ1, and in Step 1 CS = {C41American State, C1522Times, C414Tournament, C10004Y ear, ...} and QS = {C4545Event}. Using cluster C414 (see Table 2), D = {D1{Daytona 500, 24 Hours of Daytona, 24 Hours of Le Mans, ...}, D2{3M Performance 400, Cummins 200, ...}, D3{Indy 500, Truck series, ...}, ...}, and hence the vector representa-tion for a given document Dj using C414 will be <Scorej(indy 500), Scorej(Cummins 200), Scorej(daytona 500), ...>.","In Step 2 through 11 from Figure 1, for each context cluster Ci in CS we calculate X (the number of answers in which context subtopics from Ci are present), and Y (the number of subtopics from Ci that occurs in the answers’ contexts). We would like the clusters to hold two characteristics: (a) at least occur in Θ1 answers as we want to have a cluster whose subtopics are widely distributed in the answers. Currently we set Θ1 as half the number of the answers; (b) at least have Θ2 subtopics occurring in the answers’ documents. We set Θ2 as the number of the answers. For example, for cluster C414, X = 6, Y = 10, Θ1 = 3 and Θ2 = 6, so this cluster has the above two characteristics. If a cluster has the above two characteristics, we use our separability measure described in section 3 to calculate a score for this cluster. The size of C414 is 11, so Score(C414) = 1","11×6","∑N","p<q Dis(Dp, Dq). Ranking the clusters based on this separability score means we will select a cluster which has several subtopics occurring in the answers and the answers are distinguished from each other because they belong to these different subtopics. The top three clusters for question LQ1 is shown in Table 2."]},{"title":"5 Experiment 5.1 Data Set and Baseline Method","paragraphs":["To the best of our knowledge, the only available test data of multiple answer questions are list questions from TREC 2004-2007 Data. For our first","1","For the question ”In which movies did Christopher Reeve acted?”, cluster Actor{Christopher Reeve, michael caine, anthony hopkins, ...} is quite useful. While for ”Which country won the football world cup?” cluster Sports{football, hockey, ...} is useless. list question collection we randomly selected 200 questions which have at least 3 answers. We changed the list questions to factoid ones with additional words from their context questions to eliminate ellipsis and reference. For the ambiguous questions, we manually choose 200 questions from TREC 1999-2007 data and some questions discussed as examples in Hori et al. (2003) and Burger et al. (2001).","We compare our approach with a baseline method. Our baseline system does not rank the clusters by the above separability score instead it prefers the cluster which occurs in more answers and have more subtopics distributed in the answer documents. If we still use X to represent the number of answers in which context subtopics from one cluster are present and Y to represent the number of subtopics from this cluster that occurs in the answers’ contexts, for the baseline system, we will use X × Y to rank all the concept clusters found in the contexts. 5.2 Results and Error Analysis We applied our algorithm on the two collections of questions. Two assessors were involved in the manual judgments with an inter-rater agreement of 97%. For each approach, we obtained the top 20 clusters based on their scores. Given a cluster with its subtopics in the contexts of the answers, an assessor manually labeled each cluster ’good’ or ’bad’. If it is labeled ’good’, the cluster is deemed relevant to the question and the cluster’s label could be used as dialogue seeking question’s topic to distinguish one answer from the others. Otherwise, the assessor will label a cluster as ’bad’. We use the above two ranking approaches to rank the clusters for each question. Table 1 provides the statistics of the performance on the the two question collection. List B means the baseline method on the list question set while Ambiguous S means our separability method on the ambiguous questions. The ’MAP’ column is the mean of average precisions over the set of clusters. The ’P@1’ column is the precision of the top one cluster while the ’P@3’ column is the precision of the top three clusters2",". The ’Err@3’ column is the percentage of questions whose top three clusters are all labeled ’bad’. One example associated with the manually constructed desirable questions 2 ’P@3’ is the number of ’good’ clusters out of the top","three clusters 95","Table 1: Experiment results Methods MAP P@1 P@3 Err@3 List B 41.3% 42.1% 27.7% 33.0% List S 60.3% 90.0% 81.3% 11.0%","Ambiguous B 31.1% 33.2% 21.8% 47.1%","Ambiguous S 53.6% 71.1% 64.2% 29.7% Table 2: TREC Question Examples LQ1: Who is the winners of the NASCAR races? 1st","C414(Tournament):{indy 500, Cummins 200, day-","tona 500, ...} Q1 Which Tournament are you interested in? 2nd","C41(American State):{houston, baltimore, los an-","geles, ...} Q2 Which American State were the races held? 3rd","C1522(Times):{once, twice, three times, ...} Q3 How many times did the winner win? is shown in Table 2.","From Table 1, we can see that our approach outperforms the baseline approach in terms of all the measures. We can see that 11% of the questions have no ‘good’ clusters. Further analysis of the answer documents shows that the ‘bad’ clusters fall into four categories. First, there are noisy subtopics in some clusters. Second, some questions’ clusters are all labeled ‘bad’ because the contexts for different answers are too similar. Third, unstructured web document soften contain multiple subtopics. This means that different subtopics are in the context of the same answer. Currently we only look for context words while not using any scheme to specify whether there is a relationship between the answer and the subtopics. Finally, for other ‘bad’ cases and the questions with no good clusters all of the separability scores are quite low. This is because the answers fall into different topics which do not share a common topic in our cluster collection."]},{"title":"6 Conclusion and Discussion","paragraphs":["This paper proposes a new approach to solve the problem of generating an information-seeking question’s topic using concept clusters that can be used in a clarification dialogue to handle ambiguous questions. Our empirical results show that this approach leads to good performance on TREC collections and our ambiguous question collections. The contribution of this paper are: (1) a new concept cluster method that maps a document into a vector of subtopics; (2) a new ranking scheme to rank the context clusters according to their separability. The labels of the chosen clusters can be used as topics in an information-seeking question. Finally our approach shows significant improvement (nearly 48% points) over comparable baseline system.","But currently we only consider the context clusters while ignoring the clusters associated with the questions. In the future, we will further investigate the relationships between the concept clusters in the question and the answers."]},{"title":"References","paragraphs":["Tiphaine Dalmas, Bonnie L. Webber: Answer comparison in automated question answering. J. Applied Logic (JAPLL) 5(1):104-120, (2007).","Chiori Hori, Sadaoki Furui: A new approach to automatic speech summarization. IEEE Transactions on Multimedia (TMM) 5(3):368-378, (2003).","Sharon Small and Tomek Strzalkowski, HITIQA: A Data Driven Approach to Interactive Analytical Question Answering, in Proceedings of HLT-NAACL 2004: Short Papers, (2004).","Andrew Hickl, Patrick Wang, John Lehmann, Sanda M. Harabagiu: FERRET: Interactive Question-Answering for Real-World Environments. ACL, (2006).","Sanda M. Harabagiu, Andrew Hickl, John Lehmann, Dan I. Moldovan: Experiments with Interactive Question-Answering. ACL, (2005).","John Burger et al.: Issues, Tasks and Program Structures to Roadmap Research in Question and Answering (Q&A),DARPA/NSF committee publica-tion, (2001).","Patrick Pantel, Dekang Lin: Document clustering with committees. SIGIR 2002:199-206, (2002).","Marius Pasca and Benjamin Van Durme: Weakly-Supervised Acquisition of Open-Domain Classes and Class Attributes from Web Documents and Query Logs. ACL, (2008).","Sanda M. Harabagiu, Andrew Hickl, V. Finley Lacatusu: Satisfying information needs with multi-document summaries. Inf. Process. Manage. (IPM) 43(6):1619-1642, (2007).","Huizhong Duan, Yunbo Cao, Chin-Yew Lin and Yong Yu: Searching Questions by Identifying Question Topic and Question Focus. ACL, (2008).","Jon Curtis, G. Matthews and D. Baxter: On the Effective Use of Cyc in a Question Answering System. IJCAI Workshop on Knowledge and Reasoning for Answering Questions, Edinburgh, (2005). 96"]}],"references":[{"authors":[],"source":"Tiphaine Dalmas, Bonnie L. Webber: Answer comparison in automated question answering. J. Applied Logic (JAPLL) 5(1):104-120, (2007)."},{"authors":[],"source":"Chiori Hori, Sadaoki Furui: A new approach to automatic speech summarization. IEEE Transactions on Multimedia (TMM) 5(3):368-378, (2003)."},{"authors":[],"source":"Sharon Small and Tomek Strzalkowski, HITIQA: A Data Driven Approach to Interactive Analytical Question Answering, in Proceedings of HLT-NAACL 2004: Short Papers, (2004)."},{"authors":[],"source":"Andrew Hickl, Patrick Wang, John Lehmann, Sanda M. Harabagiu: FERRET: Interactive Question-Answering for Real-World Environments. ACL, (2006)."},{"authors":[],"source":"Sanda M. Harabagiu, Andrew Hickl, John Lehmann, Dan I. Moldovan: Experiments with Interactive Question-Answering. ACL, (2005)."},{"authors":[],"source":"John Burger et al.: Issues, Tasks and Program Structures to Roadmap Research in Question and Answering (Q&A),DARPA/NSF committee publica-tion, (2001)."},{"authors":[],"source":"Patrick Pantel, Dekang Lin: Document clustering with committees. SIGIR 2002:199-206, (2002)."},{"authors":[],"source":"Marius Pasca and Benjamin Van Durme: Weakly-Supervised Acquisition of Open-Domain Classes and Class Attributes from Web Documents and Query Logs. ACL, (2008)."},{"authors":[],"source":"Sanda M. Harabagiu, Andrew Hickl, V. Finley Lacatusu: Satisfying information needs with multi-document summaries. Inf. Process. Manage. (IPM) 43(6):1619-1642, (2007)."},{"authors":[],"source":"Huizhong Duan, Yunbo Cao, Chin-Yew Lin and Yong Yu: Searching Questions by Identifying Question Topic and Question Focus. ACL, (2008)."},{"authors":[],"source":"Jon Curtis, G. Matthews and D. Baxter: On the Effective Use of Cyc in a Question Answering System. IJCAI Workshop on Knowledge and Reasoning for Answering Questions, Edinburgh, (2005). 96"}],"cites":[{"style":0,"text":"Hori et al. (2003)","origin":{"pointer":"/sections/2/paragraphs/2","offset":108,"length":18},"authors":[{"last":"Hori"},{"last":"al."}],"year":"2003","references":[]},{"style":0,"text":"Small et al. (2004)","origin":{"pointer":"/sections/2/paragraphs/2","offset":438,"length":19},"authors":[{"last":"Small"},{"last":"al."}],"year":"2004","references":[]},{"style":0,"text":"Hickl et al., 2006","origin":{"pointer":"/sections/2/paragraphs/2","offset":684,"length":18},"authors":[{"last":"Hickl"},{"last":"al."}],"year":"2006","references":[]},{"style":0,"text":"Harabagiu et al., 2005","origin":{"pointer":"/sections/2/paragraphs/2","offset":930,"length":22},"authors":[{"last":"Harabagiu"},{"last":"al."}],"year":"2005","references":[]},{"style":0,"text":"Curtis et al. (2005)","origin":{"pointer":"/sections/2/paragraphs/2","offset":1081,"length":20},"authors":[{"last":"Curtis"},{"last":"al."}],"year":"2005","references":[]},{"style":0,"text":"Duan et al. (2008)","origin":{"pointer":"/sections/2/paragraphs/2","offset":1217,"length":18},"authors":[{"last":"Duan"},{"last":"al."}],"year":"2008","references":[]},{"style":0,"text":"Pantel et al. (2002)","origin":{"pointer":"/sections/3/paragraphs/1","offset":185,"length":20},"authors":[{"last":"Pantel"},{"last":"al."}],"year":"2002","references":[]},{"style":0,"text":"Pasca et al. (2008)","origin":{"pointer":"/sections/3/paragraphs/1","offset":294,"length":19},"authors":[{"last":"Pasca"},{"last":"al."}],"year":"2008","references":[]},{"style":0,"text":"Pasca et al. (2008)","origin":{"pointer":"/sections/3/paragraphs/2","offset":138,"length":19},"authors":[{"last":"Pasca"},{"last":"al."}],"year":"2008","references":[]},{"style":0,"text":"Yang et al., 1999","origin":{"pointer":"/sections/3/paragraphs/4","offset":890,"length":17},"authors":[{"last":"Yang"},{"last":"al."}],"year":"1999","references":[]},{"style":0,"text":"Hori et al. (2003)","origin":{"pointer":"/sections/6/paragraphs/2","offset":612,"length":18},"authors":[{"last":"Hori"},{"last":"al."}],"year":"2003","references":[]},{"style":0,"text":"Burger et al. (2001)","origin":{"pointer":"/sections/6/paragraphs/2","offset":635,"length":20},"authors":[{"last":"Burger"},{"last":"al."}],"year":"2001","references":[]}]}
