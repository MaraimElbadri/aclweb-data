{"sections":[{"title":"","paragraphs":["Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 165–168, Suntec, Singapore, 4 August 2009. c⃝2009 ACL and AFNLP"]},{"title":"Hierarchical Multi-Class Text Categorization with Global Margin Maximization Xipeng Qiu School of Computer Science Fudan University xpqiu@fudan.edu.cn Wenjun Gao School of Computer Science Fudan University wjgao616@gmail.com Xuanjing Huang School of Computer Science Fudan University xjhuang@fudan.edu.cn Abstract","paragraphs":["Text categorization is a crucial and wellproven method for organizing the collec-tion of large scale documents. In this paper, we propose a hierarchical multi-class text categorization method with global margin maximization. We not only maximize the margins among leaf categories, but also maximize the margins among their ancestors. Experiments show that the performance of our algorithm is competitive with the recently proposed hierarchical multi-class classification algorithms."]},{"title":"1 Introduction","paragraphs":["In the past serval years, hierarchical text categorization has become an active research topic in database area (Koller and Sahami, 1997; Weigend et al., 1999) and machine learning area (Rousu et al., 2006; Cai and Hofmann, 2007).","Hierarchical categorization methods can be divided in two types: local and global approaches (Wang et al., 1999; Sun and Lim, 2001). A local approach usually proceeds in a top-down fashion, which firstly picks the most relevant categories of the top level and then recursively making the choice among the low-level categories. The global approach builds only one classifier to discriminate all categories in a hierarchy. Due that the global hierarchical categorization can avoid the drawbacks about those high-level irrecoverable error, it is more popular in the machine learning domain.","The essential idea behind global approach is that the close classes(nodes) have some common underlying factors. Especially, the descendant classes can share the characteristics of the ancestor classes, which is similar with multi-task learning(Caruana, 1997). A key problem for global hierarchical categorization is how to combine these underlying factors.","In this paper, we propose an method for hierarchical multi-class text categorization with global margin maximization. We emphasize that it is important to separate all the nodes of the correct path in the class hierarchy from their sibling node, then we incorporate such information into the formulation of hierarchical support vector machine.","The rest of the paper is organized as follows. Section 2 describes the basic model of multi-class hierarchical categorization with maximizing margin. Then we propose our improved versions in section 3. Section 4 gives the experimental analysis. Section 5 concludes the paper."]},{"title":"2 Hierarchical Multi-Class Text Categorization","paragraphs":["Multiclass SVM can be generalized to the problem of hierarchical categorization (Cai and Hofmann, 2007), which has more than two categories in most of the case. Denote Yi as the multilabels of xi and Ȳi the multilabels set not in Yi. The separation margin of w, with respect to xi, can be approximated as:","γi(w) = min","y∈Y i,ȳ∈  ̄","Y","i ⟨Φ(xi, y) − Φ(xi, ȳ), w⟩ (1)","The loss function can be accommodated to multi-class SVM to scale the penalties for margin violations proportional to the loss. This is motivated by the fact that margin violations involving an incorrect class with high loss should be penalized more severely. So the cost-sensitive hierarchical multiclass formulation takes takes the following form: min w,ξ 1 2","||w||2 + C n ∑ i=1 ξi (2)","s.t.⟨w,δΦi(y,ȳ)⟩≥1− ξ","i l(y,ȳ) , (∀i,y∈Yi,ȳ∈ Ȳi) ξi ≥ 0(∀i) 165 where δΦi(y, ȳ) = Φ(xi, y) − Φ(xi, ȳ), l(y, ȳ) > 0 and Φ(x, y) is the joint feature of in-put x and output y, which can be represented as: Φ(x, y) = Λ(y) ⊗ φ(x) (3) where ⊗ is the tensor product. Λ(y) is the feature representation of y. Thus, we can classify a document x to label y⋆",":","y⋆ = arg max","y F (w, Φ(x, y)) (4) where F (·) is a map function.","There are different kinds of loss functions l(y, ȳ).","One is thezero-one loss, l0/1(y, u) = [y ̸= u].","Another is specially designed for the hierarchy is tree loss(Dekel et al., 2004). Tree loss is defined as the length of the path between two multilabels with positive microlabels, ltr = |path(i : yi = 1, j : uj = 1)| (5)","(Rousu et al., 2006) proposed a simplified version of lH , namely l H̃ : l Ĥ = ∑ j cj[yj ̸= uj&ypa(j) = upa(j)], (6) that penalizes a mistake in a child only if the label of the parent was correct. There are some different choices for setting cj. One naive idea is to use a uniform weighting (cj = 1). Another possible choice is to divide the loss among the sibling: croot = 1, cj = cP arent(j)/(|Sib(j)| + 1) (7) Another possible choice is to scale the loss by the proportion of the hierarchy that is in the subtree T (j) rooted by j: cj = |T (j)|/|T (root)| (8) Using these scaling weights, the derived losses are referred as l ûni,l ŝib and l ŝub respectively."]},{"title":"3 Hierarchical Multi-Class Text Categorization with Global Margin Maximization","paragraphs":["In previous literature (Cai and Hofmann, 2004; Tsochantaridis et al., 2005), they focused on separating the correct path from those incorrect path. Inspired by the example in Figure 1, we emphasize it is also important to separate the ancestor node in the correct path from their sibling node.","The vector w can be decomposed in to the set of wi for each node (category) in the hierarchy. In Figure 1, the example hierarchy has 7 nodes and 4 of them are leaf nodes. The category is encode as an integer, 1, . . . , 7. Suppose that the training pattern x belongs to category 4. Both w in the Figure 1a and Figure 1b can successfully classify x into category 4, since F (w, Φ(x, y4)) =∑","1,2,4 ⟨wi, x⟩ is the maximal among all the possible discriminate functions. So both learned parameter w is acceptable in current hierarchical support vector machine. Here we claim the w in Figure 1b is better than the w in Figure 1a. Since we notice in Figure 1a, the discriminate function ⟨w2, x⟩ is smaller than the discriminate function ⟨w3, x⟩. The discriminate function ⟨wi, x⟩ measures the similarity of x to category i. The larger the discriminate function is, the more similar x is to category i. Since category 2 is in the path from the root to the correct category and category 3 is not, intuitively, x should be closer to category 2 than category 3. But the discriminate function in Figure 1a is contradictive to this assumption. But such information is reflected correctly in Figure 1b. So we conclude w in Fig. 1b is superior to w in 1a.","Here we propose a novel formulation to incorporate such information. Denote Ai as the multilabel in Yi that corresponds to the nonleaf categories and Sib(z) denotes the sibling nodes of z, that is the set of nodes that have the same parent with z, except z itself. Implementing the above idea, we can get the following formulation: min w,ξ,ζ 1 2","∥w∥2 + C 1 ∑ i ξi + C2 ∑ i ζi (9) s.t.⟨w, δΦi(y, ȳ)⟩ ≥ 1 −","ξi l(y, ȳ) , (∀i, y ∈ Yi ȳ ∈ Ȳi ) ⟨w, δΦi(z, z̄)⟩ ≥ 1 −","ζi l(z, z̄) , (∀i, z ∈ A(i) z̄ ∈ Sib(z) ) ξi ≥ 0(∀i) ζi ≥ 0(∀i)","It arrives at the following Lagrangian: L(w, ξ1, ..., ξn, ζ1, ..., ζn) = 1 2","∥w∥2 + C 1 X i ξi + C2 X i ζi − X i X y∈Y","i ̄ y∈  ̄ Y","i αiyȳ(⟨w, δΦi(y, ȳ)⟩ − 1 +","ξi l(y, ȳ) ) 166 1 2 3 4 5 6 7","10, 1 xw 3,","2 xw 5,","3 xw","5,","4 xw 1,","7 xw1, 5 xw 2,","6 xw 1 2 3 4 5 6 7","10, 1 xw","7,","2 xw 3,","3","xw","5,","4 xw 1,","7 xw1, 5 xw 2,","6 xw a) b) Figure 1: Two different discriminant function in a hierarchy − X i X z∈A","i ̄ z∈Sib(z) βizz̄(⟨w, δΦi(z, z̄)⟩ − 1 +","ζi l(z, z̄) ) − X i ciξi − X i diζi (10) The dual QP becomes max α Θ(α) = ∑ i ∑","y∈Y i","ȳ∈  ̄ Y i αiyȳ + ∑ i ∑ z∈A","i z̄∈Sib(z) βizz̄ − 1 2 ∑ i,j ∑","y∈Y i","ȳ∈  ̄ Y i ∑","r∈Y j","r̄∈  ̄ Y j θ1 i,j,y,ȳ,r,r̄ (11) − 1 2 ∑ i,j ∑ z∈A","i z̄∈Sib(z) ∑ k∈A","j ̄ k∈Sib(k) θ2 i,j,z,z̄,k,k̄, s.t.αiyȳ ≥ 0, (12) βjzz̄ ≥ 0, (13) ∑","y∈Y i","ȳ∈  ̄ Y i αiyȳ l(y, ȳ) ≤ C1, (14) ∑ z∈A","i z̄∈Sib(z) βizz̄ l(z, z̄) ≤ C2, (15)","where θ1","i,j,y,ȳ,r,r̄ =","αiyȳαjrr̄⟨δΦi(y, ȳ), δΦj(r, r̄)⟩ and θ2","i,j,z,z̄,k,k̄ = βizz̄βjkk̄⟨δΦi(z, z̄), δΦj(k, k̄)⟩. 3.1 Optimization Algorithm The derived QP can be very large, since the number of α and β variables is up to O(n ∗ 2N","), where n is number of training pattern and N is the number of nodes in the hierarchy. But two properties of the dual problem can be exploited to design a much more efficient optimization.","First, the constraints in the dual problem Eq. 11 - Eq. 15 factorize over the instance index for both α-variables and β-variables. The constraints in Eq. 14 do not couple α-variables and β-variables together. Further, dual variables αiyȳ and αjy′ ȳ′ belonging to different training instances i and j do not join in a same constraints. This inspired an optimization procedure which iteratively performs subspace optimization over all dual variables αiyȳ belonging to the same training instance. This will in general reduced to a much smaller QP, since it freezes all αjyȳ with j ̸= i and β-variables at their current values. This strategy can be applied in solving β-variables.","Secondly, the number of active constraints at the solution is expected to be relatively small, since only a small fraction of categories ȳ ∈ Ȳi ( or ȳ ∈ Sib(y) when y ∈ Ai) will typically fail to achieve the required margin. The expected sparseness of the variable for the dual problem can be exploited by employing a variable selection strategy. Equivalently, this corresponds to a cutting plane algorithm for the primal QP. Intuitively, we will identify the most violated margin constraint with index (i, y, ȳ) and then add the correspond-ing variable to the optimization problem. This means that we start with extremely sparse problems and only successively increase the number of variables in the active set. This general approach to deal with large linear or quadratic optimization problems is also known as column selection. In practice, it is often not necessary to optimize until final convergence, which adds to the attractiveness of this approach.","We have used the LOQO optimization package (Vanderbei, 1999) in our experiments."]},{"title":"4 Experiment","paragraphs":["We evaluate our proposed model on the section D in the WIPO-alpha collection1",", which consists of the 1372 training and 358 testing document. The 1 World Intellectual Property Organization (WIPO) 167 Table 1: Prediction losses (%) obtained on WIPO. The values per column is calculated with the different loss function.","Train Test l 0/1 l∆ ltr luni lsib lsub HSVM 48.6 188.8 94.4 97.2 5.4 7.5","l0/1 HSVM-S 48.3 186.6 93.3 96.6 5.2 7.4 HSVM 49.7 187.7 93.9 99.4 5.0 7.1","l∆ HSVM-S 47.8 165.3 89.7 90.5 4.8 6.9 HM3 70.9 167.0 - 89.1 5.0 7.0 HSVM 49.4 186.0 93.0 98.9 5.0 7.5","ltr HSVM-S 48.9 181.4 90.2 97.8 4.9 7.1 HSVM 47.2 181.0 90.5 94.4 5.0 7.0","l ˆ","uni HSVM-S 46.9 179.3 88.7 91.9 4.9 6.9 HM3 70.1 172.1 - 88.8 5.2 7.4 HSVM 49.4 184.9 92.5 98.9 4.8 7.4","l ˆ","sib HSVM-S 48.9 170.2 91.6 90.8 4.7 7.4 HM3 64.8 172.9 - 92.7 4.8 7.1 HSVM 50.6 189.9 95.0 101.1 5.2 7.5","l ˆ","sub HSVM-S 47.2 169.4 85.2 89.4 4.3 6.6 HM3 65.0 170.9 - 91.9 4.8 7.2 number of nodes in the hierarchy is 188, with max-imum depth 3.","We compared the performance of our proposed method HSVM-S with two algorithms: HSVM(Cai and Hofmann, 2007) and HM3(Rousu et al., 2006). 4.1 Effect of Different Loss Function We compare the methods based on different loss functions, l0/1, l∆, ltr, lûni, lŝib and lŝub. The performances for three algorithms can be seen in Table 1. Those empty cells, denoted by “-”, are not available in (Rousu et al., 2006).","As expected, l0/1 is inferior to other hierarchical losses by getting poorest performance in all the testing losses, since it can not take into account the hierarchical information between categories. The results suggests that training with a hierarchical losses function, like lŝib or lûni, would lead to a better reduced l0/1 on the test set as well as in terms of the hierarchical loss. In Table 1, we can also point out that when training with the same hierarchical loss, the performance of HSVM-S is better than HSVM under the measure of most hierarchical losses, since HSVM-S includes more hierarchical information,the relationship between the sibling categories, than HSVM which only separate the leave categories."]},{"title":"5 Conclusion","paragraphs":["In this paper we present a hierarchical multi-class document categorization, which focus on maximize the margin of the classes at the different levels in the class hierarchy. In future work, we plan to extend the proposed hierarchical learning method to the case where the hierarchy is a DAG instead of tree and scale up the method further."]},{"title":"Acknowledgments","paragraphs":["This work was (partially) funded by Chinese NSF 60673038, Doctoral Fund of Ministry of Education of China 200802460066, and Shanghai Science and Technology Development Funds 08511500302."]},{"title":"References","paragraphs":["L. Cai and T Hofmann. 2004. Hierarchical document categorization with support vector machines. In Proceedings of the ACM Conference on Information and Knowledge Management.","L. Cai and T. Hofmann. 2007. Exploiting known taxonomies in learning overlapping concepts. In Proceedings of International Joint Conferences on Artificial Intelligence.","R. Caruana. 1997. Multi-task learning. Machine Learning, 28(1):41–75.","Ofer Dekel, Joseph Keshet, and Yoram Singer. 2004. Large margin hierarchical classification. In Proceedings of the 21 st International Conference on Machine Learning.","D. Koller and M Sahami. 1997. Hierarchically classifying documents using very few words. In Proceedings of the International Conference on Machine Learning (ICML).","Juho Rousu, Craig Saunders, Sandor Szedmak, and John Shawe-Taylor. 2006. Kernel-based learning of hierarchical multilabel classification models. In Journal of Machine Learning Research.","A. Sun and E.-P Lim. 2001. Hierarchical text classification and evaluation. In Proceedings of the IEEE International Conference on Data Mining (ICDM).","Ioannis Tsochantaridis, Thorsten Joachims, Thomas Hofmann, and Yasemin Altun. 2005. Large margin methods for structured and interdependent output variables. In Journal of Machine Learning.","R. J. Vanderbei. 1999. Loqo: An interior point code for quadratic programming. In Optimization Methods and Software.","K. Wang, S. Zhou, and S Liew. 1999. Building hierarchical classifiers using class proximities. In Proceedings of the International Conference on Very Large Data Bases (VLDB).","A. Weigend, E. Wiener, and J Pedersen. 1999. Exploiting hierarchy in text categorization. In Information Retrieval. 168"]}],"references":[{"authors":[{"first":"L.","last":"Cai"},{"first":"T","last":"Hofmann"}],"year":"2004","title":"Hierarchical document categorization with support vector machines","source":"L. Cai and T Hofmann. 2004. Hierarchical document categorization with support vector machines. In Proceedings of the ACM Conference on Information and Knowledge Management."},{"authors":[{"first":"L.","last":"Cai"},{"first":"T.","last":"Hofmann"}],"year":"2007","title":"Exploiting known taxonomies in learning overlapping concepts","source":"L. Cai and T. Hofmann. 2007. Exploiting known taxonomies in learning overlapping concepts. In Proceedings of International Joint Conferences on Artificial Intelligence."},{"authors":[{"first":"R.","last":"Caruana"}],"year":"1997","title":"Multi-task learning","source":"R. Caruana. 1997. Multi-task learning. Machine Learning, 28(1):41–75."},{"authors":[{"first":"Ofer","last":"Dekel"},{"first":"Joseph","last":"Keshet"},{"first":"Yoram","last":"Singer"}],"year":"2004","title":"Large margin hierarchical classification","source":"Ofer Dekel, Joseph Keshet, and Yoram Singer. 2004. Large margin hierarchical classification. In Proceedings of the 21 st International Conference on Machine Learning."},{"authors":[{"first":"D.","last":"Koller"},{"first":"M","last":"Sahami"}],"year":"1997","title":"Hierarchically classifying documents using very few words","source":"D. Koller and M Sahami. 1997. Hierarchically classifying documents using very few words. In Proceedings of the International Conference on Machine Learning (ICML)."},{"authors":[{"first":"Juho","last":"Rousu"},{"first":"Craig","last":"Saunders"},{"first":"Sandor","last":"Szedmak"},{"first":"John","last":"Shawe-Taylor"}],"year":"2006","title":"Kernel-based learning of hierarchical multilabel classification models","source":"Juho Rousu, Craig Saunders, Sandor Szedmak, and John Shawe-Taylor. 2006. Kernel-based learning of hierarchical multilabel classification models. In Journal of Machine Learning Research."},{"authors":[{"first":"A.","last":"Sun"},{"first":"E.","middle":"-P","last":"Lim"}],"year":"2001","title":"Hierarchical text classification and evaluation","source":"A. Sun and E.-P Lim. 2001. Hierarchical text classification and evaluation. In Proceedings of the IEEE International Conference on Data Mining (ICDM)."},{"authors":[{"first":"Ioannis","last":"Tsochantaridis"},{"first":"Thorsten","last":"Joachims"},{"first":"Thomas","last":"Hofmann"},{"first":"Yasemin","last":"Altun"}],"year":"2005","title":"Large margin methods for structured and interdependent output variables","source":"Ioannis Tsochantaridis, Thorsten Joachims, Thomas Hofmann, and Yasemin Altun. 2005. Large margin methods for structured and interdependent output variables. In Journal of Machine Learning."},{"authors":[{"first":"R.","middle":"J.","last":"Vanderbei"}],"year":"1999","title":"Loqo: An interior point code for quadratic programming","source":"R. J. Vanderbei. 1999. Loqo: An interior point code for quadratic programming. In Optimization Methods and Software."},{"authors":[{"first":"K.","last":"Wang"},{"first":"S.","last":"Zhou"},{"first":"S","last":"Liew"}],"year":"1999","title":"Building hierarchical classifiers using class proximities","source":"K. Wang, S. Zhou, and S Liew. 1999. Building hierarchical classifiers using class proximities. In Proceedings of the International Conference on Very Large Data Bases (VLDB)."},{"authors":[{"first":"A.","last":"Weigend"},{"first":"E.","last":"Wiener"},{"first":"J","last":"Pedersen"}],"year":"1999","title":"Exploiting hierarchy in text categorization","source":"A. Weigend, E. Wiener, and J Pedersen. 1999. Exploiting hierarchy in text categorization. In Information Retrieval. 168"}],"cites":[{"style":0,"text":"Koller and Sahami, 1997","origin":{"pointer":"/sections/2/paragraphs/0","offset":113,"length":23},"authors":[{"last":"Koller"},{"last":"Sahami"}],"year":"1997","references":["/references/4"]},{"style":0,"text":"Weigend et al., 1999","origin":{"pointer":"/sections/2/paragraphs/0","offset":138,"length":20},"authors":[{"last":"Weigend"},{"last":"al."}],"year":"1999","references":["/references/10"]},{"style":0,"text":"Rousu et al., 2006","origin":{"pointer":"/sections/2/paragraphs/0","offset":187,"length":18},"authors":[{"last":"Rousu"},{"last":"al."}],"year":"2006","references":["/references/5"]},{"style":0,"text":"Cai and Hofmann, 2007","origin":{"pointer":"/sections/2/paragraphs/0","offset":207,"length":21},"authors":[{"last":"Cai"},{"last":"Hofmann"}],"year":"2007","references":["/references/1"]},{"style":0,"text":"Wang et al., 1999","origin":{"pointer":"/sections/2/paragraphs/1","offset":94,"length":17},"authors":[{"last":"Wang"},{"last":"al."}],"year":"1999","references":["/references/9"]},{"style":0,"text":"Sun and Lim, 2001","origin":{"pointer":"/sections/2/paragraphs/1","offset":113,"length":17},"authors":[{"last":"Sun"},{"last":"Lim"}],"year":"2001","references":["/references/6"]},{"style":0,"text":"Caruana, 1997","origin":{"pointer":"/sections/2/paragraphs/2","offset":244,"length":13},"authors":[{"last":"Caruana"}],"year":"1997","references":["/references/2"]},{"style":0,"text":"Cai and Hofmann, 2007","origin":{"pointer":"/sections/3/paragraphs/0","offset":81,"length":21},"authors":[{"last":"Cai"},{"last":"Hofmann"}],"year":"2007","references":["/references/1"]},{"style":0,"text":"Dekel et al., 2004","origin":{"pointer":"/sections/3/paragraphs/14","offset":61,"length":18},"authors":[{"last":"Dekel"},{"last":"al."}],"year":"2004","references":["/references/3"]},{"style":0,"text":"Rousu et al., 2006","origin":{"pointer":"/sections/3/paragraphs/15","offset":1,"length":18},"authors":[{"last":"Rousu"},{"last":"al."}],"year":"2006","references":["/references/5"]},{"style":0,"text":"Cai and Hofmann, 2004","origin":{"pointer":"/sections/4/paragraphs/0","offset":24,"length":21},"authors":[{"last":"Cai"},{"last":"Hofmann"}],"year":"2004","references":["/references/0"]},{"style":0,"text":"Tsochantaridis et al., 2005","origin":{"pointer":"/sections/4/paragraphs/0","offset":47,"length":27},"authors":[{"last":"Tsochantaridis"},{"last":"al."}],"year":"2005","references":["/references/7"]},{"style":0,"text":"Vanderbei, 1999","origin":{"pointer":"/sections/4/paragraphs/49","offset":44,"length":15},"authors":[{"last":"Vanderbei"}],"year":"1999","references":["/references/8"]},{"style":0,"text":"Cai and Hofmann, 2007","origin":{"pointer":"/sections/5/paragraphs/12","offset":84,"length":21},"authors":[{"last":"Cai"},{"last":"Hofmann"}],"year":"2007","references":["/references/1"]},{"style":0,"text":"Rousu et al., 2006","origin":{"pointer":"/sections/5/paragraphs/12","offset":115,"length":18},"authors":[{"last":"Rousu"},{"last":"al."}],"year":"2006","references":["/references/5"]},{"style":0,"text":"Rousu et al., 2006","origin":{"pointer":"/sections/5/paragraphs/12","offset":387,"length":18},"authors":[{"last":"Rousu"},{"last":"al."}],"year":"2006","references":["/references/5"]}]}
