{"sections":[{"title":"","paragraphs":["Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 558–566, Suntec, Singapore, 2-7 August 2009. c⃝2009 ACL and AFNLP"]},{"title":"Improving Tree-to-Tree Translation with Packed Forests Yang Liu and Yajuan L ü and Qun Liu Key Laboratory of Intelligent Information Processing Institute of Computing Technology Chinese Academy of Sciences P.O. Box 2704, Beijing 100190, China {yliu,lvyajuan,liuqun}@ict.ac.cn Abstract","paragraphs":["Current tree-to-tree models suffer from parsing errors as they usually use only 1-best parses for rule extraction and decoding. We instead propose a forest-based tree-to-tree model that uses packed forests. The model is based on a probabilistic synchronous tree substitution gram-mar (STSG), which can be learned from aligned forest pairs automatically. The decoder finds ways of decomposing trees in the source forest into elementary trees using the source projection of STSG while building target forest in parallel. Comparable to the state-of-the-art phrase-based system Moses, using packed forests in tree-to-tree translation results in a significant absolute improvement of 3.6 BLEU points over using 1-best trees."]},{"title":"1 Introduction","paragraphs":["Approaches to syntax-based statistical machine translation make use of parallel data with syntactic annotations, either in the form of phrase structure trees or dependency trees. They can be roughly divided into three categories: string-to-tree models (e.g., (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008)), tree-to-string models (e.g., (Liu et al., 2006; Huang et al., 2006)), and tree-to-tree models (e.g., (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008)). By modeling the syntax of both source and target languages, tree-to-tree approaches have the potential benefit of providing rules linguistically better motivated. However, while string-to-tree and tree-to-string models demonstrate promising results in empirical evaluations, tree-to-tree models have still been underachieving.","We believe that tree-to-tree models face two major challenges. First, tree-to-tree models are more vulnerable to parsing errors. Obtaining syntactic annotations in quantity usually entails running automatic parsers on a parallel corpus. As the amount and domain of the data used to train parsers are relatively limited, parsers will inevitably output ill-formed trees when handling real-world text. Guided by such noisy syntactic in-formation, syntax-based models that rely on 1-best parses are prone to learn noisy translation rules in training phase and produce degenerate translations in decoding phase (Quirk and Corston-Oliver, 2006). This situation aggravates for tree-to-tree models that use syntax on both sides.","Second, tree-to-tree rules provide poorer rule coverage. As a tree-to-tree rule requires that there must be trees on both sides, tree-to-tree models lose a larger amount of linguistically unmotivated mappings. Studies reveal that the absence of such non-syntactic mappings will impair translation quality dramatically (Marcu et al., 2006; Liu et al., 2007; DeNeefe et al., 2007; Zhang et al., 2008).","Compactly encoding exponentially many parses, packed forests prove to be an excellent fit for alleviating the above two problems (Mi et al., 2008; Mi and Huang, 2008). In this paper, we propose a forest-based tree-to-tree model. To learn STSG rules from aligned forest pairs, we introduce a series of notions for identifying minimal tree-to-tree rules. Our decoder first converts the source forest to a translation forest and then finds the best derivation that has the source yield of one source tree in the forest. Comparable to Moses, our forest-based tree-to-tree model achieves an absolute improvement of 3.6 BLEU points over conventional tree-based model. 558 IP1 NP2 VP3","PP4 VP-B5","NP-B6 NP-B7","NP-B8","NR9","CC10","P 11 NR12","VV13","AS14","NN15 bushi yu shalong juxing le huitan Bush held a talk with Sharon","NNP16","VBD17","DT18","NN19","IN 20","NNP21","NP22 NP23","NP24","NP25 PP26 NP27 VP28 S 29 Figure 1: An aligned packed forest pair. Each node is assigned a unique identity for reference. The solid lines denote hyperedges and the dashed lines denote word alignments. Shaded nodes are frontier nodes."]},{"title":"2 Model","paragraphs":["Figure 1 shows an aligned forest pair for a Chinese sentence and an English sentence. The solid lines denote hyperedges and the dashed lines denote word alignments between the two forests. Each node is assigned a unique identity for reference. Each hyperedge is associated with a probability, which we omit in Figure 1 for clarity. In a forest, a node usually has multiple incoming hyperedges. We use IN (v) to denote the set of incoming hyperedges of node v. For example, the source node “IP1","” has following two incoming hyperedges: 1","e1 = ⟨(NP-B6",", VP3","), IP1","⟩","e2 = ⟨(NP2",", VP-B5","), IP1","⟩","1","As there are both source and target forests, it might be confusing by just using a span to refer to a node. In addition, some nodes will often have the same labels and spans. There-fore, it is more convenient to use an identity for referring to a node. The notation “IP1","” denotes the node that has a label of “IP” and has an identity of “1”.","Formally, a packed parse forest is a compact representation of all the derivations (i.e., parse trees) for a given sentence under a context-free grammar. Huang and Chiang (2005) define a forest as a tuple ⟨V, E, v̄, R⟩, where V is a finite set of nodes, E is a finite set of hyperedges, v̄ ∈ V is a distinguished node that denotes the goal item in parsing, and R is the set of weights. For a given sentence w1:l = w1 . . . wl, each node v ∈ V is in the form of Xi,j , which denotes the recognition of non-terminal X spanning the substring from positions i through j (that is, wi+1 . . . wj). Each hyperedge e ∈ E is a triple e = ⟨T (e), h(e), f (e)⟩, where h(e) ∈ V is its head, T (e) ∈ V ∗","is a vector of tail nodes, and f (e) is a weight function from R|T (e)|","to R.","Our forest-based tree-to-tree model is based on a probabilistic STSG (Eisner, 2003). Formally, an STSG can be defined as a quintuple G = ⟨Fs, Ft, Ss, St, P ⟩, where","• Fs and Ft are the source and target alphabets, respectively,","• Ss and St are the source and target start symbols, and","• P is a set of production rules. A rule r is a triple ⟨ts, tt, ∼⟩ that describes the correspondence ∼ between a source tree ts and a target tree tt.","To integrate packed forests into tree-to-tree translation, we model the process of synchronous generation of a source forest Fs and a target forest Ft using a probabilistic STSG grammar: P r(Fs, Ft) = ∑ Ts∈Fs ∑ Tt∈Ft P r(Ts, Tt) = ∑ Ts∈Fs ∑ Tt∈Ft ∑ d∈D P r(d) = ∑ Ts∈Fs ∑ Tt∈Ft ∑ d∈D ∏ r∈d p(r) (1) where Ts is a source tree, Tt is a target tree, D is the set of all possible derivations that transform Ts into Tt, d is one such derivation, and r is a tree-to-tree rule.","Table 1 shows a derivation of the forest pair in Figure 1. A derivation is a sequence of tree-to-tree rules. Note that we use x to represent a nonterminal. 559 (1) IP(x1:NP-B, x2:VP) → S(x1:NP, x2:VP) (2) NP-B(x1:NR) → NP(x1:NNP) (3) NR(bushi) → NNP(Bush) (4) VP(x1:PP, VP-B(x2:VV, AS(le), x3:NP-B)) → VP(x2:VBD, NP(DT(a), x3:NP), x1:PP) (5) PP(x1:P, x2:NP-B) → PP(x1:IN, x2:NP) (6) P(yu) → IN(with) (7) NP-B(x1:NR) → NP(x1:NP) (8) NR(shalong) → NNP(Sharon) (9) VV(juxing) → VBD(held) (10) NP-B(x1:NN) → NP(x1:NN) (11) NN(huitan) → NN(talk) Table 1: A minimal derivation of the forest pair in Figure 1. id span cspan complement consistent frontier counterparts 1 1-6 1-2, 4-6 1 1 29 2 1-3 1, 5-6 2, 4 0 0 3 2-6 2, 4-6 1 1 1 28 4 2-3 5-6 1-2, 4 1 1 25, 26 5 4-6 2, 4 1, 5-6 1 0 6 1-1 1 2, 4-6 1 1 16, 22 7 3-3 6 1-2, 4-5 1 1 21, 24 8 6-6 4 1-2, 5-6 1 1 19, 23 9 1-1 1 2, 4-6 1 1 16, 22 10 2-2 5 1-2, 4, 6 1 1 20 11 2-2 5 1-2, 4, 6 1 1 20 12 3-3 6 1-2, 4-5 1 1 21, 24 13 4-4 2 1, 4-6 1 1 17 14 5-5 1-2, 4-6 1 0 15 6-6 4 1-2, 5-6 1 1 19, 23 16 1-1 1 2-4, 6 1 1 6, 9 17 2-2 4 1-3, 6 1 1 13 18 3-3 1-4, 6 1 0 19 4-4 6 1-4 1 1 8, 15 20 5-5 2 1, 3-4, 6 1 1 10, 11 21 6-6 3 1-2, 4, 6 1 1 7, 12 22 1-1 1 2-4, 6 1 1 6, 9 23 3-4 6 1-4 1 1 8, 15 24 6-6 3 1-2, 4, 6 1 1 7, 12 25 5-6 2-3 1, 4, 6 1 1 4 26 5-6 2-3 1, 4, 6 1 1 4 27 3-6 2-3, 6 1, 4 0 0 28 2-6 2-4, 6 1 1 1 3 29 1-6 1-4, 6 1 1 1 Table 2: Node attributes of the example forest pair."]},{"title":"3 Rule Extraction","paragraphs":["Given an aligned forest pair as shown in Figure 1, how to extract all valid tree-to-tree rules that explain its synchronous generation process? By constructing a theory that gives formal semantics to word alignments, Galley et al. (2004) give principled answers to these questions for extracting tree-to-string rules. Their GHKM procedure draws connections among word alignments, derivations, and rules. They first identify the tree nodes that subsume tree-string pairs consistent with word alignments and then extract rules from these nodes. By this means, GHKM proves to be able to extract all valid tree-to-string rules from training instances. Although originally developed for the tree-to-string case, it is possible to extend GHKM to extract all valid tree-to-tree rules from aligned packed forests.","In this section, we introduce our tree-to-tree rule extraction method adapted from GHKM, which involves four steps: (1) identifying the correspondence between the nodes in forest pairs, (2) identifying minimum rules, (3) inferring composed rules, and (4) estimating rule probabilities.","3.1 Identifying Correspondence Between Nodes To learn tree-to-tree rules, we need to find aligned tree pairs in the forest pairs. To do this, the start-ing point is to identify the correspondence between nodes. We propose a number of attributes for nodes, most of which derive from GHKM, to facilitate the identification. Definition 1 Given a node v, its span σ(v) is an index set of the words it covers.","For example, the span of the source node “VP-B5","” is {4, 5, 6} as it covers three source words: “juxing”, “le”, and “huitan”. For convenience, we use {4-6} to denotes a contiguous span {4, 5, 6}. Definition 2 Given a node v, its corresponding span γ(v) is the index set of aligned words on another side. For example, the corresponding span of the source node “VP-B5","” is {2, 4}, corresponding to the target words “held” and “talk”. Definition 3 Given a node v, its complement span δ(v) is the union of corresponding spans of nodes that are neither antecedents nor descendants of v. For example, the complement span of the source node “VP-B5","” is {1, 5-6}, corresponding to target words “Bush”, “with”, and “Sharon”. Definition 4 A node v is said to be consistent with alignment if and only if closure(γ(v))∩δ(v) = ∅.","For example, the closure of the corresponding span of the source node “VP-B5","” is {2-4} and its complement span is {1, 5-6}. As the intersection of the closure and the complement span is an empty set, the source node “VP-B5","” is consistent with the alignment. 560 PP 4 NP-B 7 P 11 NR12 PP 4 P 11 NP-B 7 PP 4 NP-B 7 P 11 NR12 PP 26 IN 20 NP24NNP21 PP 4 P 11 NP-B 7 PP 26 IN 20 NP 24 (a) (b) (c) (d) Figure 2: (a) A frontier tree; (b) a minimal frontier tree; (c) a frontier tree pair; (d) a minimal frontier tree pair. All trees are taken from the example forest pair in Figure 1. Shaded nodes are frontier nodes. Each node is assigned an identity for reference. Definition 5 A node v is said to be a frontier node if and only if: 1. v is consistent;","2. There exists at least one consistent node v′","on another side satisfying:","• closure(γ(v′",")) ⊆ σ(v);","• closure(γ(v)) ⊆ σ(v′","). v′","is said to be a counterpart of v. We use τ (v) to denote the set of counterparts of v.","A frontier node often has multiple counterparts on another side due to the usage of unary rules in parsers. For example, the source node “NP-B6","” has two counterparts on the target side: “NNP16","” and “NP22","”. Conversely, the target node “NNP16","” also has two counterparts counterparts on the source side: “NR9","” and “NP-B6","”.","The node attributes of the example forest pair are listed in Table 2. We use identities to refer to nodes. “cspan” denotes corresponding span and “complement” denotes complement span. In Figure 1, there are 12 frontier nodes (highlighted by shading) on the source side and 12 frontier nodes on the target side. Note that while a consistent node is equal to a frontier node in GHKM, this is not the case in our method because we have a tree on the target side. Frontier nodes play a critical role in forest-based rule extraction because they indicate where to cut the forest pairs to obtain tree-to-tree rules. 3.2 Identifying Minimum Rules Given the frontier nodes, the next step is to identify aligned tree pairs, from which tree-to-tree rules derive. Following Galley et al. (2006), we distinguish between minimal and composed rules. As a composed rule can be decomposed as a sequence of minimal rules, we are particularly interested in how to extract minimal rules. Also, we introduce a number of notions to help identify minimal rules. Definition 6 A frontier tree is a subtree in a forest satisfying: 1. Its root is a frontier node;","2. If the tree contains only one node, it must be a lexicalized frontier node;","3. If the tree contains more than one nodes, its leaves are either non-lexicalized frontier nodes or lexicalized non-frontier nodes.","For example, Figure 2(a) shows a frontier tree in which all nodes are frontier nodes. Definition 7 A minimal frontier tree is a frontier tree such that all nodes other than the root and leaves are non-frontier nodes.","For example, Figure 2(b) shows a minimal frontier tree. Definition 8 A frontier tree pair is a triple ⟨ts, tt, ∼⟩ satisfying: 1. ts is a source frontier tree; 561 2. tt is a target frontier tree; 3. The root of ts is a counterpart of that of tt;","4. There is a one-to-one correspondence ∼ between the frontier leaves of ts and tt.","For example, Figure 2(c) shows a frontier tree pair. Definition 9 A frontier tree pair ⟨ts, tt, ∼⟩ is said to be a subgraph of another frontier tree pair ⟨ts′",", t t ′ , ∼′","⟩ if and only if:","1. root(ts) = root(ts′ );","2. root(tt) = root(tt′ );","3. ts is a subgraph of ts′ ;","4. tt is a subgraph of tt′ .","For example, the frontier tree pair shown in Figure 2(d) is a subgraph of that in Figure 2(c). Definition 10 A frontier tree pair is said to be minimal if and only if it is not a subgraph of any other frontier tree pair that shares with the same root.","For example, Figure 2(d) shows a minimal frontier tree pair.","Our goal is to find the minimal frontier tree pairs, which correspond to minimal tree-to-tree rules. For example, the tree pair shown in Figure 2(d) denotes a minimal rule as follows: PP(x1:P,x2:NP-B) → PP(x1:IN, x2:NP)","Figure 3 shows the algorithm for identifying minimal frontier tree pairs. The input is a source forest Fs, a target forest Ft, and a source frontier node v (line 1). We use a set P to store collected minimal frontier tree pairs (line 2). We first call the procedure FINDTREES(Fs , v) to identify a set of frontier trees rooted at v in Fs (line 3). For example, for the source frontier node “PP4","” in Figure 1, we obtain two frontier trees:","(PP4","(P11 )(NP-B7","))","(PP4","(P11 )(NP-B7","(NR12","))) Then, we try to find the set of corresponding target frontier trees (i.e., Tt). For each counterpart v′","of v (line 5), we call the procedure FIND-TREES(Ft, v′",") to identify a set of frontier trees rooted at v′","in Ft (line 6). For example, the source 1: procedure FINDTREEPAIRS(Fs , Ft, v) 2: P = ∅ 3: Ts ← FINDTREES(Fs , v) 4: Tt ← ∅ 5: for v′","∈ τ (v) do 6: Tt ← Tt∪ FINDTREES(Ft , v′",") 7: end for 8: for ⟨ts, tt⟩ ∈ Ts × Tt do 9: if ts ∼ tt then 10: P ← P ∪ {⟨ts, tt, ∼⟩} 11: end if 12: end for 13: for ⟨ts, tt, ∼⟩ ∈ P do 14: if ∃⟨ts′",", t t ′ , ∼′","⟩ ∈ P : ⟨t s ′ , t t ′ , ∼′","⟩ ⊆","⟨ts, tt, ∼⟩ then 15: P ← P − {⟨ts, tt, ∼⟩} 16: end if 17: end for 18: end procedure Figure 3: Algorithm for identifying minimal frontier tree pairs.","frontier node “PP4","” has two counterparts on the","target side: “NP25","” and “PP26","”. There are four","target frontier trees rooted at the two nodes:","(NP25","(IN20 )(NP24","))","(NP25","(IN20 )(NP24","(NNP21",")))","(PP26","(IN20 )(NP24","))","(PP26","(IN20 )(NP24","(NNP21",")))","Therefore, there are 2 × 4 = 8 pairs of trees. We examine each tree pair ⟨ts, tt⟩ (line 8) to see whether it is a frontier tree pair (line 9) and then update P (line 10). In the above example, all the eight tree pairs are frontier tree pairs.","Finally, we keep only minimal frontier tree pairs in P (lines 13-15). As a result, we obtain the following two minimal frontier tree pairs for the source frontier node “PP4","”:","(PP4","(P11",")(NP-B7 )) ↔ (NP25","(IN20",")(NP24","))","(PP4","(P11",")(NP-B7 )) ↔ (PP26","(IN20",")(NP24","))","To maintain a reasonable rule table size, we restrict that the number of nodes in a tree of an STSG rule is no greater than n, which we refer to as maximal node count.","It seems more efficient to let the procedure FINDTREES(F, v) to search for minimal frontier 562 trees rather than frontier trees. However, a minimal frontier tree pair is not necessarily a pair of minimal frontier trees. On our Chinese-English corpus, we find that 38% of minimal frontier tree pairs are not pairs of minimal frontier trees. As a result, we have to first collect all frontier tree pairs and then decide on the minimal ones.","Table 1 shows some minimal rules extracted from the forest pair shown in Figure 1. 3.3 Inferring Composed Rules After minimal rules are learned, composed rules can be obtained by composing two or more minimal rules. For example, the composition of the second rule and the third rule in Table 1 produces a new rule: NP-B(NR(shalong)) → NP(NNP(Sharon))","While minimal rules derive from minimal frontier tree pairs, composed rules correspond to non-minimal frontier tree pairs. 3.4 Estimating Rule Probabilities We follow Mi and Huang (2008) to estimate the fractional count of a rule extracted from an aligned forest pair. Intuitively, the relative frequency of a subtree that occurs in a forest is the sum of all the trees that traverse the subtree divided by the sum of all trees in the forest. Instead of enumerating all trees explicitly and computing the sum of tree probabilities, we resort to inside and outside probabilities for efficient calculation: c(r) = p(ts) × α(root(ts)) ×","∏ v∈leaves(ts) β(v) β(v̄s) × p(tt) × α(root(tt)) ×","∏ v∈leaves(tt) β(v) β(v̄t) where c(r) is the fractional count of a rule, ts is the source tree in r, tt is the target tree in r, root(·) a function that gets tree root, leaves(·) is a function that gets tree leaves, and α(v) and β(v) are outside and inside probabilities, respectively."]},{"title":"4 Decoding","paragraphs":["Given a source packed forest Fs, our decoder finds the target yield of the single best derivation d that has source yield of Ts(d) ∈ Fs: ê = e (","argmax d s.t. Ts(d)∈Fs p(d) ) (2)","We extend the model in Eq. 1 to a log-linear model (Och and Ney, 2002) that uses the following eight features: relative frequencies in two directions, lexical weights in two directions, number of rules used, language model score, number of target words produced, and the probability of matched source tree (Mi et al., 2008).","Given a source parse forest and an STSG gram-mar G, we first apply the conversion algorithm proposed by Mi et al. (2008) to produce a translation forest. The translation forest has a similar hypergraph structure. While the nodes are the same as those of the parse forest, each hyperedge is associated with an STSG rule. Then, the decoder runs on the translation forest. We use the cube pruning method (Chiang, 2007) to approximately intersect the translation forest with the language model. Traversing the translation forest in a bottom-up order, the decoder tries to build target parses at each node. After the first pass, we use lazy Algorithm 3 (Huang and Chiang, 2005) to generate k-best translations for minimum error rate training."]},{"title":"5 Experiments 5.1 Data Preparation","paragraphs":["We evaluated our model on Chinese-to-English translation. The training corpus contains 840K Chinese words and 950K English words. A trigram language model was trained on the English sentences of the training corpus. We used the 2002 NIST MT Evaluation test set as our development set, and used the 2005 NIST MT Evaluation test set as our test set. We evaluated the translation quality using the BLEU metric, as calculated by mteval-v11b.pl with its default setting except that we used case-insensitive matching of n-grams.","To obtain packed forests, we used the Chinese parser (Xiong et al., 2005) modified by Haitao Mi and the English parser (Charniak and Johnson, 2005) modified by Liang Huang to produce entire parse forests. Then, we ran the Python scripts (Huang, 2008) provided by Liang Huang to output packed forests. To prune the packed forests, Huang (2008) uses inside and outside probabilities to compute the distance of the best derivation that traverses a hyperedge away from the globally best derivation. A hyperedge will be pruned away if the difference is greater than a threshold p. Nodes with all incoming hyperedges pruned are also pruned. The greater the threshold p is, 563 p avg trees # of rules BLEU 0 1 73, 614 0.2021 ± 0.0089 2 238.94 105, 214 0.2165 ± 0.0081 5 5.78 × 106","347, 526 0.2336 ± 0.0078 8 6.59 × 107","573, 738 0.2373 ± 0.0082 10 1.05 × 108","743, 211 0.2385 ± 0.0084 Table 3: Comparison of BLEU scores for tree-based and forest-based tree-to-tree models. 0.04 0.05 0.06 0.07 0.08 0.09 0.10 0 1 2 3 4 5 6 7 8 9 10 11 coverage maximal node count p=0 p=2 p=5 p=8 p=10 Figure 4: Coverage of lexicalized STSG rules on bilingual phrases. the more parses are encoded in a packed forest.","We obtained word alignments of the training data by first running GIZA++ (Och and Ney, 2003) and then applying the refinement rule “grow-diagfinal-and” (Koehn et al., 2003). 5.2 Forests Vs. 1-best Trees Table 3 shows the BLEU scores of tree-based and forest-based tree-to-tree models achieved on the test set over different pruning thresholds. p is the threshold for pruning packed forests, “avg trees” is the average number of trees encoded in one forest on the test set, and “# of rules” is the number of STSG rules used on the test set. We restrict that both source and target trees in a tree-to-tree rule can contain at most 10 nodes (i.e., the maximal node count n = 10). The 95% confidence intervals were computed using Zhang ’s significance tester (Zhang et al., 2004).","We chose five different pruning thresholds in our experiments: p = 0, 2, 5, 8, 10. The forests pruned by p = 0 contained only 1-best tree per sentence. With the increase of p, the average number of trees encoded in one forest rose dramatically. When p was set to 10, there were over 100M parses encoded in one forest on average. p extraction decoding 0 1.26 6.76 2 2.35 8.52 5 6.34 14.87 8 8.51 19.78 10 10.21 25.81 Table 4: Comparison of rule extraction time (seconds/1000 sentence pairs) and decoding time (second/sentence)","Moreover, the more trees are encoded in packed forests, the more rules are made available to forest-based models. The number of rules when p = 10 was almost 10 times of p = 0. With the increase of the number of rules used, the BLEU score increased accordingly. This suggests that packed forests enable tree-to-tree model to learn more useful rules on the training data. However, when a pack forest encodes over 1M parses per sentence, the improvements are less significant, which echoes the results in (Mi et al., 2008).","The forest-based tree-to-tree model outperforms the original model that uses 1-best trees dramatically. The absolute improvement of 3.6 BLEU points (from 0.2021 to 0.2385) is statistically significant at p < 0.01 using the sign-test as described by Collins et al. (2005), with 700(+1), 360(-1), and 15(0). We also ran Moses (Koehn et al., 2007) with its default setting using the same data and obtained a BLEU score of 0.2366, slightly lower than our best result (i.e., 0.2385). But this difference is not statistically significant. 5.3 Effect on Rule Coverage Figure 4 demonstrates the effect of pruning threshold and maximal node count on rule coverage. We extracted phrase pairs from the training data to investigate how many phrase pairs can be captured by lexicalized tree-to-tree rules that contain only terminals. We set the maximal length of phrase pairs to 10. For tree-based tree-to-tree model, the coverage was below 8% even the maximal node count was set to 10. This suggests that conventional tree-to-tree models lose over 92% linguistically unmotivated mappings due to hard syntactic constraints. The absence of such non-syntactic mappings prevents tree-based tree-to-tree models from achieving comparable results to phrase-based models. With more parses included 564 0.090.100.110.120.130.140.150.160.170.180.190.20 0 1 2 3 4 5 6 7 8 9 10 11 BLEU maximal node count Figure 5: Effect of maximal node count on BLEU scores. in packed forests, the rule coverage increased accordingly. When p = 10 and n = 10, the coverage was 9.7%, higher than that of p = 0. As a result, packed forests enable tree-to-tree models to capture more useful source-target mappings and therefore improve translation quality. 2 5.4 Training and Decoding Time Table 4 gives the rule extraction time (seconds/1000 sentence pairs) and decoding time (second/sentence) with varying pruning thresholds. We found that the extraction time grew faster than decoding time with the increase of p. One possible reason is that the number of frontier tree pairs (see Figure 3) rose dramatically when more parses were included in packed forests. 5.5 Effect of Maximal Node Count Figure 5 shows the effect of maximal node count on BLEU scores. With the increase of maximal node count, the BLEU score increased dramatically. This implies that allowing tree-to-tree rules to capture larger contexts will strengthen the expressive power of tree-to-tree model. 5.6 Results on Larger Data We also conducted an experiment on larger data to further examine the effectiveness of our approach. We concatenated the small corpus we used above and the FBIS corpus. After remov-ing the sentences that we failed to obtain forests,","2","Note that even we used packed forests, the rule coverage was still very low. One reason is that we set the maximal phrase length to 10 words, while an STSG rule with 10 nodes in each tree usually cannot subsume 10 words. the new training corpus contained about 260K sentence pairs with 7.39M Chinese words and 9.41M English words. We set the forest pruning threshold p = 5. Moses obtained a BLEU score of 0.3043 and our forest-based tree-to-tree system achieved a BLEU score of 0.3059. The difference is still not significant statistically."]},{"title":"6 Related Work","paragraphs":["In machine translation, the concept of packed forest is first used by Huang and Chiang (2007) to characterize the search space of decoding with language models. The first direct use of packed forest is proposed by Mi et al. (2008). They replace 1-best trees with packed forests both in training and decoding and show superior translation quality over the state-of-the-art hierarchical phrase-based system. We follow the same direction and apply packed forests to tree-to-tree translation.","Zhang et al. (2008) present a tree-to-tree model that uses STSG. To capture non-syntactic phrases, they apply tree-sequence rules (Liu et al., 2007) to tree-to-tree models. Their extraction algorithm first identifies initial rules and then obtains abstract rules. While this method works for 1-best tree pairs, it cannot be applied to packed forest pairs because it is impractical to enumerate all tree pairs over a phrase pair.","While Galley (2004) describes extracting tree-to-string rules from 1-best trees, Mi and Huang et al. (2008) go further by proposing a method for extracting tree-to-string rules from aligned forest-string pairs. We follow their work and focus on identifying tree-tree pairs in a forest pair, which is more difficult than the tree-to-string case."]},{"title":"7 Conclusion","paragraphs":["We have shown how to improve tree-to-tree translation with packed forests, which compactly encode exponentially many parses. To learn STSG rules from aligned forest pairs, we first identify minimal rules and then get composed rules. The decoder finds the best derivation that have the source yield of one source tree in the forest. Experiments show that using packed forests in tree-to-tree translation results in dramatic improvements over using 1-best trees. Our system also achieves comparable performance with the state-of-the-art phrase-based system Moses. 565"]},{"title":"Acknowledgement","paragraphs":["The authors were supported by National Natural Science Foundation of China, Contracts 60603095 and 60736014, and 863 State Key Project No. 2006AA010108. Part of this work was done while Yang Liu was visiting the SMT group led by Stephan Vogel at CMU. We thank the anonymous reviewers for their insightful comments. Many thanks go to Liang Huang, Haitao Mi, and Hao Xiong for their invaluable help in producing packed forests. We are also grateful to Andreas Zollmann, Vamshi Ambati, and Kevin Gimpel for their helpful feedback."]},{"title":"References","paragraphs":["Eugene Charniak and Mark Johnson. 2005. Coarse-to-fine n-best parsing and maxent discriminative reranking. In Proc. of ACL 2005.","David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2).","Brooke Cowan, Ivona Kuc̆erová, and Michael Collins. 2006. A discriminative model for tree-to-tree translation. In Proc. of EMNLP 2006.","Steve DeNeefe, Kevin Knight, Wei Wang, and Daniel Marcu. 2007. What can syntax-based MT learn from phrase-based MT? In Proc. of EMNLP 2007.","Yuan Ding and Martha Palmer. 2005. Machine translation using probabilistic synchronous dependency insertion grammars. In Proc. of ACL 2005.","Jason Eisner. 2003. Learning non-isomorphic tree mappings for machine translation. In Proc. of ACL 2003 (Companion Volume).","Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In Proc. of NAACL/HLT 2004.","Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable inference and training of context-rich syntactic translation models. In Proc. of COLING/ACL 2006.","Liang Huang and David Chiang. 2005. Better k-best parsing. In Proc. of IWPT 2005.","Liang Huang and David Chiang. 2007. Forest rescor-ing: Faster decoding with integrated language models. In Proc. of ACL 2007.","Liang Huang, Kevin Knight, and Aravind Joshi. 2006. Statistical syntax-directed translation with extended domain of locality. In Proc. of AMTA 2006.","Liang Huang. 2008. Forest reranking: Discriminative parsing with non-local features. In Proc. of ACL/HLT 2008.","Phillip Koehn, Franz J. Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proc. of NAACL 2003.","Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proc. of ACL 2007 (demonstration session).","Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-string alignment template for statistical machine translation. In Proc. of COLING/ACL 2006.","Yang Liu, Yun Huang, Qun Liu, and Shouxun Lin. 2007. Forest-to-string statistical translation rules. In Proc. of ACL 2007.","Daniel Marcu, Wei Wang, Abdessamad Echihabi, and Kevin Knight. 2006. Spmt: Statistical machine translation with syntactified target language phrases. In Proc. of EMNLP 2006.","Haitao Mi and Liang Huang. 2008. Forest-based translation rule extraction. In Proc. of EMNLP 2008.","Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-based translation. In Proc. of ACL/HLT 2008.","Franz J. Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In Proc. of ACL 2002.","Franz J. Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1).","Chris Quirk and Simon Corston-Oliver. 2006. The impact of parsing quality on syntactically-informed statistical machine translation. In Proc. of EMNLP 2006.","Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A new string-to-dependency machine translation algorithm with a target dependency language model. In Proc. of ACL/HLT 2008.","Deyi Xiong, Shuanglong Li, Qun Liu, and Shouxun Lin. 2005. Parsing the penn chinese treebank with semantic knowledge. In Proc. of IJCNLP 2005.","Ying Zhang, Stephan Vogel, and Alex Waibel. 2004. Interpreting bleu/nist scores how much improvement do we need to have a better system? In Proc. of LREC 2004.","Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li, Chew Lim Tan, and Sheng Li. 2008. A tree sequence alignment-based tree-to-tree translation model. In Proc. of ACL/HLT 2008. 566"]}],"references":[{"authors":[{"first":"Eugene","last":"Charniak"},{"first":"Mark","last":"Johnson"}],"year":"2005","title":"Coarse-to-fine n-best parsing and maxent discriminative reranking","source":"Eugene Charniak and Mark Johnson. 2005. Coarse-to-fine n-best parsing and maxent discriminative reranking. In Proc. of ACL 2005."},{"authors":[{"first":"David","last":"Chiang"}],"year":"2007","title":"Hierarchical phrase-based translation","source":"David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2)."},{"authors":[{"first":"Brooke","last":"Cowan"},{"first":"Ivona","last":"Kuc̆erová"},{"first":"Michael","last":"Collins"}],"year":"2006","title":"A discriminative model for tree-to-tree translation","source":"Brooke Cowan, Ivona Kuc̆erová, and Michael Collins. 2006. A discriminative model for tree-to-tree translation. In Proc. of EMNLP 2006."},{"authors":[{"first":"Steve","last":"DeNeefe"},{"first":"Kevin","last":"Knight"},{"first":"Wei","last":"Wang"},{"first":"Daniel","last":"Marcu"}],"year":"2007","title":"What can syntax-based MT learn from phrase-based MT? In Proc","source":"Steve DeNeefe, Kevin Knight, Wei Wang, and Daniel Marcu. 2007. What can syntax-based MT learn from phrase-based MT? In Proc. of EMNLP 2007."},{"authors":[{"first":"Yuan","last":"Ding"},{"first":"Martha","last":"Palmer"}],"year":"2005","title":"Machine translation using probabilistic synchronous dependency insertion grammars","source":"Yuan Ding and Martha Palmer. 2005. Machine translation using probabilistic synchronous dependency insertion grammars. In Proc. of ACL 2005."},{"authors":[{"first":"Jason","last":"Eisner"}],"year":"2003","title":"Learning non-isomorphic tree mappings for machine translation","source":"Jason Eisner. 2003. Learning non-isomorphic tree mappings for machine translation. In Proc. of ACL 2003 (Companion Volume)."},{"authors":[{"first":"Michel","last":"Galley"},{"first":"Mark","last":"Hopkins"},{"first":"Kevin","last":"Knight"},{"first":"Daniel","last":"Marcu"}],"year":"2004","title":"What’s in a translation rule? In Proc","source":"Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In Proc. of NAACL/HLT 2004."},{"authors":[{"first":"Michel","last":"Galley"},{"first":"Jonathan","last":"Graehl"},{"first":"Kevin","last":"Knight"},{"first":"Daniel","last":"Marcu"},{"first":"Steve","last":"DeNeefe"},{"first":"Wei","last":"Wang"},{"first":"Ignacio","last":"Thayer"}],"year":"2006","title":"Scalable inference and training of context-rich syntactic translation models","source":"Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable inference and training of context-rich syntactic translation models. In Proc. of COLING/ACL 2006."},{"authors":[{"first":"Liang","last":"Huang"},{"first":"David","last":"Chiang"}],"year":"2005","title":"Better k-best parsing","source":"Liang Huang and David Chiang. 2005. Better k-best parsing. In Proc. of IWPT 2005."},{"authors":[{"first":"Liang","last":"Huang"},{"first":"David","last":"Chiang"}],"year":"2007","title":"Forest rescor-ing: Faster decoding with integrated language models","source":"Liang Huang and David Chiang. 2007. Forest rescor-ing: Faster decoding with integrated language models. In Proc. of ACL 2007."},{"authors":[{"first":"Liang","last":"Huang"},{"first":"Kevin","last":"Knight"},{"first":"Aravind","last":"Joshi"}],"year":"2006","title":"Statistical syntax-directed translation with extended domain of locality","source":"Liang Huang, Kevin Knight, and Aravind Joshi. 2006. Statistical syntax-directed translation with extended domain of locality. In Proc. of AMTA 2006."},{"authors":[{"first":"Liang","last":"Huang"}],"year":"2008","title":"Forest reranking: Discriminative parsing with non-local features","source":"Liang Huang. 2008. Forest reranking: Discriminative parsing with non-local features. In Proc. of ACL/HLT 2008."},{"authors":[{"first":"Phillip","last":"Koehn"},{"first":"Franz","middle":"J.","last":"Och"},{"first":"Daniel","last":"Marcu"}],"year":"2003","title":"Statistical phrase-based translation","source":"Phillip Koehn, Franz J. Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proc. of NAACL 2003."},{"authors":[{"first":"Philipp","last":"Koehn"},{"first":"Hieu","last":"Hoang"},{"first":"Alexandra","last":"Birch"},{"first":"Chris","last":"Callison-Burch"},{"first":"Marcello","last":"Federico"},{"first":"Nicola","last":"Bertoldi"},{"first":"Brooke","last":"Cowan"},{"first":"Wade","last":"Shen"},{"first":"Christine","last":"Moran"},{"first":"Richard","last":"Zens"},{"first":"Chris","last":"Dyer"},{"first":"Ondrej","last":"Bojar"},{"first":"Alexandra","last":"Constantin"},{"first":"Evan","last":"Herbst"}],"year":"2007","title":"Moses: Open source toolkit for statistical machine translation","source":"Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proc. of ACL 2007 (demonstration session)."},{"authors":[{"first":"Yang","last":"Liu"},{"first":"Qun","last":"Liu"},{"first":"Shouxun","last":"Lin"}],"year":"2006","title":"Tree-to-string alignment template for statistical machine translation","source":"Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-string alignment template for statistical machine translation. In Proc. of COLING/ACL 2006."},{"authors":[{"first":"Yang","last":"Liu"},{"first":"Yun","last":"Huang"},{"first":"Qun","last":"Liu"},{"first":"Shouxun","last":"Lin"}],"year":"2007","title":"Forest-to-string statistical translation rules","source":"Yang Liu, Yun Huang, Qun Liu, and Shouxun Lin. 2007. Forest-to-string statistical translation rules. In Proc. of ACL 2007."},{"authors":[{"first":"Daniel","last":"Marcu"},{"first":"Wei","last":"Wang"},{"first":"Abdessamad","last":"Echihabi"},{"first":"Kevin","last":"Knight"}],"year":"2006","title":"Spmt: Statistical machine translation with syntactified target language phrases","source":"Daniel Marcu, Wei Wang, Abdessamad Echihabi, and Kevin Knight. 2006. Spmt: Statistical machine translation with syntactified target language phrases. In Proc. of EMNLP 2006."},{"authors":[{"first":"Haitao","last":"Mi"},{"first":"Liang","last":"Huang"}],"year":"2008","title":"Forest-based translation rule extraction","source":"Haitao Mi and Liang Huang. 2008. Forest-based translation rule extraction. In Proc. of EMNLP 2008."},{"authors":[{"first":"Haitao","last":"Mi"},{"first":"Liang","last":"Huang"},{"first":"Qun","last":"Liu"}],"year":"2008","title":"Forest-based translation","source":"Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-based translation. In Proc. of ACL/HLT 2008."},{"authors":[{"first":"Franz","middle":"J.","last":"Och"},{"first":"Hermann","last":"Ney"}],"year":"2002","title":"Discriminative training and maximum entropy models for statistical machine translation","source":"Franz J. Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In Proc. of ACL 2002."},{"authors":[{"first":"Franz","middle":"J.","last":"Och"},{"first":"Hermann","last":"Ney"}],"year":"2003","title":"A systematic comparison of various statistical alignment models","source":"Franz J. Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1)."},{"authors":[{"first":"Chris","last":"Quirk"},{"first":"Simon","last":"Corston-Oliver"}],"year":"2006","title":"The impact of parsing quality on syntactically-informed statistical machine translation","source":"Chris Quirk and Simon Corston-Oliver. 2006. The impact of parsing quality on syntactically-informed statistical machine translation. In Proc. of EMNLP 2006."},{"authors":[{"first":"Libin","last":"Shen"},{"first":"Jinxi","last":"Xu"},{"first":"Ralph","last":"Weischedel"}],"year":"2008","title":"A new string-to-dependency machine translation algorithm with a target dependency language model","source":"Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A new string-to-dependency machine translation algorithm with a target dependency language model. In Proc. of ACL/HLT 2008."},{"authors":[{"first":"Deyi","last":"Xiong"},{"first":"Shuanglong","last":"Li"},{"first":"Qun","last":"Liu"},{"first":"Shouxun","last":"Lin"}],"year":"2005","title":"Parsing the penn chinese treebank with semantic knowledge","source":"Deyi Xiong, Shuanglong Li, Qun Liu, and Shouxun Lin. 2005. Parsing the penn chinese treebank with semantic knowledge. In Proc. of IJCNLP 2005."},{"authors":[{"first":"Ying","last":"Zhang"},{"first":"Stephan","last":"Vogel"},{"first":"Alex","last":"Waibel"}],"year":"2004","title":"Interpreting bleu/nist scores how much improvement do we need to have a better system? In Proc","source":"Ying Zhang, Stephan Vogel, and Alex Waibel. 2004. Interpreting bleu/nist scores how much improvement do we need to have a better system? In Proc. of LREC 2004."},{"authors":[{"first":"Min","last":"Zhang"},{"first":"Hongfei","last":"Jiang"},{"first":"Aiti","last":"Aw"},{"first":"Haizhou","last":"Li"},{"first":"Chew","middle":"Lim","last":"Tan"},{"first":"Sheng","last":"Li"}],"year":"2008","title":"A tree sequence alignment-based tree-to-tree translation model","source":"Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li, Chew Lim Tan, and Sheng Li. 2008. A tree sequence alignment-based tree-to-tree translation model. In Proc. of ACL/HLT 2008. 566"}],"cites":[{"style":0,"text":"Galley et al., 2006","origin":{"pointer":"/sections/2/paragraphs/0","offset":260,"length":19},"authors":[{"last":"Galley"},{"last":"al."}],"year":"2006","references":["/references/7"]},{"style":0,"text":"Marcu et al., 2006","origin":{"pointer":"/sections/2/paragraphs/0","offset":281,"length":18},"authors":[{"last":"Marcu"},{"last":"al."}],"year":"2006","references":["/references/16"]},{"style":0,"text":"Shen et al., 2008","origin":{"pointer":"/sections/2/paragraphs/0","offset":301,"length":17},"authors":[{"last":"Shen"},{"last":"al."}],"year":"2008","references":["/references/22"]},{"style":0,"text":"Liu et al., 2006","origin":{"pointer":"/sections/2/paragraphs/0","offset":352,"length":16},"authors":[{"last":"Liu"},{"last":"al."}],"year":"2006","references":["/references/14"]},{"style":0,"text":"Huang et al., 2006","origin":{"pointer":"/sections/2/paragraphs/0","offset":370,"length":18},"authors":[{"last":"Huang"},{"last":"al."}],"year":"2006","references":["/references/10"]},{"style":0,"text":"Eisner, 2003","origin":{"pointer":"/sections/2/paragraphs/0","offset":424,"length":12},"authors":[{"last":"Eisner"}],"year":"2003","references":["/references/5"]},{"style":0,"text":"Ding and Palmer, 2005","origin":{"pointer":"/sections/2/paragraphs/0","offset":438,"length":21},"authors":[{"last":"Ding"},{"last":"Palmer"}],"year":"2005","references":["/references/4"]},{"style":0,"text":"Cowan et al., 2006","origin":{"pointer":"/sections/2/paragraphs/0","offset":461,"length":18},"authors":[{"last":"Cowan"},{"last":"al."}],"year":"2006","references":["/references/2"]},{"style":0,"text":"Zhang et al., 2008","origin":{"pointer":"/sections/2/paragraphs/0","offset":481,"length":18},"authors":[{"last":"Zhang"},{"last":"al."}],"year":"2008","references":["/references/25"]},{"style":0,"text":"Quirk and Corston-Oliver, 2006","origin":{"pointer":"/sections/2/paragraphs/1","offset":607,"length":30},"authors":[{"last":"Quirk"},{"last":"Corston-Oliver"}],"year":"2006","references":["/references/21"]},{"style":0,"text":"Marcu et al., 2006","origin":{"pointer":"/sections/2/paragraphs/2","offset":319,"length":18},"authors":[{"last":"Marcu"},{"last":"al."}],"year":"2006","references":["/references/16"]},{"style":0,"text":"Liu et al., 2007","origin":{"pointer":"/sections/2/paragraphs/2","offset":339,"length":16},"authors":[{"last":"Liu"},{"last":"al."}],"year":"2007","references":["/references/15"]},{"style":0,"text":"DeNeefe et al., 2007","origin":{"pointer":"/sections/2/paragraphs/2","offset":357,"length":20},"authors":[{"last":"DeNeefe"},{"last":"al."}],"year":"2007","references":["/references/3"]},{"style":0,"text":"Zhang et al., 2008","origin":{"pointer":"/sections/2/paragraphs/2","offset":379,"length":18},"authors":[{"last":"Zhang"},{"last":"al."}],"year":"2008","references":["/references/25"]},{"style":0,"text":"Mi et al., 2008","origin":{"pointer":"/sections/2/paragraphs/3","offset":130,"length":15},"authors":[{"last":"Mi"},{"last":"al."}],"year":"2008","references":["/references/18"]},{"style":0,"text":"Mi and Huang, 2008","origin":{"pointer":"/sections/2/paragraphs/3","offset":147,"length":18},"authors":[{"last":"Mi"},{"last":"Huang"}],"year":"2008","references":["/references/17"]},{"style":0,"text":"Huang and Chiang (2005)","origin":{"pointer":"/sections/3/paragraphs/13","offset":154,"length":23},"authors":[{"last":"Huang"},{"last":"Chiang"}],"year":"2005","references":["/references/8"]},{"style":0,"text":"Eisner, 2003","origin":{"pointer":"/sections/3/paragraphs/16","offset":70,"length":12},"authors":[{"last":"Eisner"}],"year":"2003","references":["/references/5"]},{"style":0,"text":"Galley et al. (2004)","origin":{"pointer":"/sections/4/paragraphs/0","offset":217,"length":20},"authors":[{"last":"Galley"},{"last":"al."}],"year":"2004","references":["/references/6"]},{"style":0,"text":"Galley et al. (2006)","origin":{"pointer":"/sections/4/paragraphs/24","offset":763,"length":20},"authors":[{"last":"Galley"},{"last":"al."}],"year":"2006","references":["/references/7"]},{"style":0,"text":"Mi and Huang (2008)","origin":{"pointer":"/sections/4/paragraphs/96","offset":167,"length":19},"authors":[{"last":"Mi"},{"last":"Huang"}],"year":"2008","references":["/references/17"]},{"style":0,"text":"Och and Ney, 2002","origin":{"pointer":"/sections/5/paragraphs/2","offset":52,"length":17},"authors":[{"last":"Och"},{"last":"Ney"}],"year":"2002","references":["/references/19"]},{"style":0,"text":"Mi et al., 2008","origin":{"pointer":"/sections/5/paragraphs/2","offset":307,"length":15},"authors":[{"last":"Mi"},{"last":"al."}],"year":"2008","references":["/references/18"]},{"style":0,"text":"Mi et al. (2008)","origin":{"pointer":"/sections/5/paragraphs/3","offset":104,"length":16},"authors":[{"last":"Mi"},{"last":"al."}],"year":"2008","references":["/references/18"]},{"style":0,"text":"Chiang, 2007","origin":{"pointer":"/sections/5/paragraphs/3","offset":402,"length":12},"authors":[{"last":"Chiang"}],"year":"2007","references":["/references/1"]},{"style":0,"text":"Huang and Chiang, 2005","origin":{"pointer":"/sections/5/paragraphs/3","offset":649,"length":22},"authors":[{"last":"Huang"},{"last":"Chiang"}],"year":"2005","references":["/references/8"]},{"style":0,"text":"Xiong et al., 2005","origin":{"pointer":"/sections/6/paragraphs/1","offset":54,"length":18},"authors":[{"last":"Xiong"},{"last":"al."}],"year":"2005","references":["/references/23"]},{"style":0,"text":"Charniak and Johnson, 2005","origin":{"pointer":"/sections/6/paragraphs/1","offset":120,"length":26},"authors":[{"last":"Charniak"},{"last":"Johnson"}],"year":"2005","references":["/references/0"]},{"style":0,"text":"Huang, 2008","origin":{"pointer":"/sections/6/paragraphs/1","offset":238,"length":11},"authors":[{"last":"Huang"}],"year":"2008","references":["/references/11"]},{"style":0,"text":"Huang (2008)","origin":{"pointer":"/sections/6/paragraphs/1","offset":330,"length":12},"authors":[{"last":"Huang"}],"year":"2008","references":["/references/11"]},{"style":0,"text":"Och and Ney, 2003","origin":{"pointer":"/sections/6/paragraphs/5","offset":74,"length":17},"authors":[{"last":"Och"},{"last":"Ney"}],"year":"2003","references":["/references/20"]},{"style":0,"text":"Koehn et al., 2003","origin":{"pointer":"/sections/6/paragraphs/5","offset":153,"length":18},"authors":[{"last":"Koehn"},{"last":"al."}],"year":"2003","references":["/references/12"]},{"style":0,"text":"Zhang et al., 2004","origin":{"pointer":"/sections/6/paragraphs/5","offset":756,"length":18},"authors":[{"last":"Zhang"},{"last":"al."}],"year":"2004","references":["/references/24"]},{"style":0,"text":"Mi et al., 2008","origin":{"pointer":"/sections/6/paragraphs/7","offset":503,"length":15},"authors":[{"last":"Mi"},{"last":"al."}],"year":"2008","references":["/references/18"]},{"style":0,"text":"Collins et al. (2005)","origin":{"pointer":"/sections/6/paragraphs/8","offset":249,"length":21},"authors":[{"last":"Collins"},{"last":"al."}],"year":"2005","references":[]},{"style":0,"text":"Koehn et al., 2007","origin":{"pointer":"/sections/6/paragraphs/8","offset":325,"length":18},"authors":[{"last":"Koehn"},{"last":"al."}],"year":"2007","references":["/references/13"]},{"style":0,"text":"Huang and Chiang (2007)","origin":{"pointer":"/sections/7/paragraphs/0","offset":70,"length":23},"authors":[{"last":"Huang"},{"last":"Chiang"}],"year":"2007","references":["/references/9"]},{"style":0,"text":"Mi et al. (2008)","origin":{"pointer":"/sections/7/paragraphs/0","offset":214,"length":16},"authors":[{"last":"Mi"},{"last":"al."}],"year":"2008","references":["/references/18"]},{"style":0,"text":"Zhang et al. (2008)","origin":{"pointer":"/sections/7/paragraphs/1","offset":0,"length":19},"authors":[{"last":"Zhang"},{"last":"al."}],"year":"2008","references":["/references/25"]},{"style":0,"text":"Liu et al., 2007","origin":{"pointer":"/sections/7/paragraphs/1","offset":131,"length":16},"authors":[{"last":"Liu"},{"last":"al."}],"year":"2007","references":["/references/15"]},{"style":0,"text":"Galley (2004)","origin":{"pointer":"/sections/7/paragraphs/2","offset":6,"length":13},"authors":[{"last":"Galley"}],"year":"2004","references":[]},{"style":0,"text":"Huang et al. (2008)","origin":{"pointer":"/sections/7/paragraphs/2","offset":88,"length":19},"authors":[{"last":"Huang"},{"last":"al."}],"year":"2008","references":[]}]}
