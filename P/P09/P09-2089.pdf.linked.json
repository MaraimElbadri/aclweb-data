{"sections":[{"title":"","paragraphs":["Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 353–356, Suntec, Singapore, 4 August 2009. c⃝2009 ACL and AFNLP"]},{"title":"Updating a Name Tagger Using Contemporary Unlabeled Data Cristina Mota L2F (INESC-ID) & IST & NYU Rua Alves Redol 9 1000-029 Lisboa Portugal cmota@ist.utl.pt Ralph Grishman New York University Computer Science Department New York NY 10003 USA grishman@cs.nyu.edu Abstract","paragraphs":["For many NLP tasks, including named entity tagging, semi-supervised learning has been proposed as a reasonable alternative to methods that require annotating large amounts of training data. In this paper, we address the problem of analyzing new data given a semi-supervised NE tagger trained on data from an earlier time period. We will show that updating the unlabeled data is sufficient to maintain quality over time, and outperforms updating the labeled data. Furthermore, we will also show that augmenting the unlabeled data with older data in most cases does not result in better performance than simply using a smaller amount of current unlabeled data."]},{"title":"1 Introduction","paragraphs":["Brill (2003) observed large gains in performance for different NLP tasks solely by increasing the size of unlabeled data, but stressed that for other NLP tasks, such as named entity recognition (NER), we still need to focus on developing tools that help to increase the size of annotated data.","This problem is particularly crucial when processing languages, such as Portuguese, for which the labeled data is scarce. For instance, in the first NER evaluation for Portuguese, HAREM (Santos and Cardoso, 2007), only two out of the nine participants presented systems based on machine learning, and they both argued they could have achieved significantly better results if they had larger training sets.","Semi-supervised methods are commonly chosen as an alternative to overcome the lack of annotated resources, because they present a good trade-off between amount of labeled data needed and performance achieved. Co-training is one of those methods, and has been extensively studied in NLP (Nigam and Ghani, 2000; Pierce and Cardie, 2001; Ng and Cardie, 2003; Mota and Grishman, 2008). In particular, we showed that the performance of a name tagger based on co-training decays as the time gap between training data (seeds and unlabeled data) and test data increases (Mota and Grishman, 2008). Compared to the original classifier of Collins and Singer (1999) that uses seven seeds, we used substantially larger seed sets (more than 1000), which raises the question of which of the parameters (seeds or unlabeled data) are causing the performance deterioration.","In the present study, we investigated two main questions, from the point of view of a developer who wants to analyze a new data set, given an NE tagger trained with older data. First, we studied whether it was better to update the seeds or the unlabeled data; then, we analyzed whether using a smaller amount of current unlabeled data could be better than increasing the amount of unlabeled data drawn from older sources. The experiments show that using contemporary unlabeled data is the best choice, outperforming most experiments with larger amounts of older unlabeled data and all experiments with contemporary seeds."]},{"title":"2 Contemporary labeled data in NLP","paragraphs":["The speech community has been defending for some time now the idea of having similar temporal data for training and testing automatic speech recognition systems for broadcast news. Most works focus on improving out-of-vocabulary (OOV) rates, to which new names contribute significantly. For instance, Palmer and Ostendorf (2005) aiming at reducing the error rate due to OOV names propose to generate offline name lists from diverse sources, including temporally relevant news texts; Federico and Bertoldi (2004), and Martins et al. (2006) propose to daily adapt the statistical language model of a broadcast 353 news transcription system, exploiting contemporary newswire texts available on the web; Auzanne et al. (2000) proposed a time-adaptive language model, studying its impact over a period of five months on the reduction of OOV rate, word error rate and retrieval accuracy on a spoken document retrieval system.","Concerning variations over longer periods of time, we observed that the performance of a semi-supervised name tagger decays over a period of eight years, which seems to be directly related with the fact that the texts used to train and test the tagger also show a tendency to become less similar over time (Mota and Grishman, 2008); Batista et al. (2008) also observed a decaying tendency in the performance of a system for recovering capitalization over a period of six years, proposing to retrain a MaxEnt model using additional contemporary written texts."]},{"title":"3 Name tagger overview","paragraphs":["We assessed the name tagger described in Mota and Grishman (2008) to recognize names of people, organizations and locations. The tagger is based on the co-training NE classifier proposed by Collins and Singer (1999), and is comprised of several components organized sequentially (cf. Figure 1).                 Figure 1: NE tagger architecture"]},{"title":"4 Data sets","paragraphs":["CETEMPúblico (Rocha and Santos, 2000) is a Portuguese journalistic corpus with 180 million words that spans eight years of news, from 1991 to 1998. The minimum size of epoch (time span of data set) available for analysis is a six-month period, corresponding either to the first half of the year or the second.","The data sets were created using the first 8256 extracts1","within each six-month period of the politics section of the corpus: the first 192 are used to collect seeds, the next 208 extracts are used as test sets and the remaining 7856 are used to collect the unlabeled examples. The seeds correspond to the first 1150 names occurring in those extracts. From the list of unlabeled examples obtained after the NE identification stage, only the first 41226 examples of each epoch were used to bootstrap in the classification stage."]},{"title":"5 Experiments","paragraphs":["We denote by S, U and T , respectively, the seed, unlabeled and test texts, and by (Si, Uj, Tk) a training-test configuration, where 91a ≤ i, j, k ≤ 98b, i.e., epochs i, j and k vary between the first half of 1991 (91a) and the second half of 1998 (98b). For instance, the training-test configuration (Si=91a...98b, Ui=91a...98b, Tj=98b) represents the training-test configuration where the test set was drawn from epoch 98b, and the tagger was trained in turn with seeds and unlabeled data drawn from the same epoch i that varied from 91a to 98b. 5.1 Do we need contemporary labeled data? In order to understand whether it is better to label examples falling within the epoch of the test set or to keep using old labeled data while bootstrapping with contemporary unlabeled data, we fixed the test set to be within the last epoch of the interval (98b), and performed backward experiments, i.e., we varied the epoch of either the seeds or the unlabeled data backwards. The choice of fixing the test within the last epoch of the interval is the one that most approximates a real situation where one has a tagger trained with old data and wants to process a more recent text.","Figure 2 shows the results for both experiments, where (Sj=98b, Ui=91a...98b, Tj=98b) represents the experiment where the test was within the same epoch as the seeds and the unlabeled data were drawn from a single, variable, epoch in turn, and (Si=91a...98b, Uj=98b, Tj=98b) represents the experiment where the test was within the epoch of the","1","Extracts are typically two paragraphs. 354 unlabeled data and the seeds were drawn in turn from each of the epochs; the graphic also shows the baseline backward training (varying the epoch of both the seeds and the unlabeled data together). 0.74 0.76 0.78 0.80 0.82 Training epoch F−measure (i,i,98b) (98b,i,98b) (i,98b,98b)","91a","91b","92a","92b","93a","93b","94a 94b 95a","95b","96a","96b","97a","97b","98a","98b Figure 2: F-measure over time for test set 98b with configurations: (Si=91a...98b, Ui=91a...98b, Tj=98b), (Sj=98b, Ui=91a...98b, Tj=98b), and (Si=91a...98b, Uj=98b, Tj=98b)","As can be seen, there is a small gain in performance by using seeds within the epoch of the test set, but the decay is still observable as we increase the time gap between the unlabeled data and the test set. On the contrary, if we use unlabeled data within the epoch of the test set, we hardly see a degradation trend as the time gap between the epochs of seeds and test set is increased.","An examination of the results shows that, for instance, Sendero Luminoso received the correct classification of organization when the tagger is trained with unlabeled data drawn from the same epoch, but is incorretly classified as person when trained with data that is not contemporary with the test set. Even though that name is not a seed in any of the cases, it occurs twice in good contexts for organization in unlabeled data contemporary with the test set (lı́der do Sendero Luminoso/leader of the Shining Path and acções do Sendero Luminoso/ actions of the Shining Path), while it does not occur in the unlabeled data that is not contemporary. Given that both the name spelling and the context in the test set, o messianismo do peruano Sendero Luminoso/the messianism of the Peruvian Shining Path, are insufficient to assign a correct label, the occurrence of the name in the contemporary unlabeled data contributes to its correct classification in the test set. 5.2 Is more older unlabeled data better? The second question we addressed was whether having more older unlabeled data could result in better performance than less data but within the epoch of the test set. In this case, we conducted two backward experiments, augmenting the unlabeled data backwards with older data than the test set (98b), starting in the previous epoch (98a): in the first experiment, the seeds were within the same epoch as the test set, and in the second experiment the seeds were within the same epoch as the unlabeled set being added. This corresponds to configurations (Sj=98b, U ′ i=91a...98a, Tj=98b) and (Si=91a...98a, U ′ i=91a...98a, Tj=98b), respectively, where U ′ i =","⋃98a","k=i Uk.","In Figure 3, we show the result of these configurations together with the result of the backward experiment corresponding to configuration (Si=91a...98b, Uj=98b, Tj=98b), also represented in Figure 2. We note that, in the case of the former experiments, the size of the unlabeled examples is increasing in the direction 98a to 91a. 0.77 0.78 0.79 0.80 0.81 0.82 0.83 Training epoch F−measure (i,98b,98b) (i,u[i,...,98a],98b) (98b,u[i,...,98a],98b)","91a","91b","92a","92b","93a","93b","94a 94b 95a","95b","96a","96b","97a","97b","98a","98b Figure 3: F-measure for test set 98b with configurations (Si=91a...98b, Uj=98b, Tj=98b), (Sj=98b, U ′ i=91a...98a, Tj=98b) and (Si=91a...98a, U ′ i=91a...98a, Tj=98b), where U ′ i =","⋃98a k=i Uk","As can be observed, increasing the size of the unlabeled data does not necessarily result in better performance: for both choices of seeds, performance sometimes improves, sometimes worsens, as the unlabeled data grows (following the curves 355 from right to left).","Furthermore, the tagger trained with more unlabeled data in most cases did not outperform the tagger trained with less unlabeled data selected from the epoch of the test set."]},{"title":"6 Discussion and future directions","paragraphs":["We conducted experiments varying the epoch of seeds and unlabeled data of a named entity tagger based on co-training. We observed that the performance decay resulting from increasing the time gap between training data (seeds and unlabeled examples) and the test set can be slightly attenuated by using the seeds contemporary with the test set. The gain is larger if one uses older seeds and contemporary unlabeled data, a strategy that, in most of the experiments, results in better performance than using increasing sizes of older unlabeled data.","These results suggest that we may not need to label new data nor train our tagger with increasing sizes of data, as long as we are able to train it with unlabeled data time compatible with the test set.","In the future, one issue that needs clarification is why bootstraping from contemporary labeled data had so little influence on the performance of co-training, and if other semi-supervised approches are also sensitive to this question."]},{"title":"Acknowledgment","paragraphs":["The first author’s research work was funded by Fundação para a Ciência e a Tecnologia through a doctoral scholarship (ref.: SFRH/BD/3237/2000)."]},{"title":"References","paragraphs":["Cédric Auzanne, John S. Garofolo, Jonathan G. Fiscus, and William M. Fisher. 2000. Automatic language model adaptation for spoken document retrieval. In Proceedings of RIAO 2000 Conference on Content-Based Multimedia Information Access.","Fernando Batista, Nuno Mamede, and Isabel Trancoso. 2008. Language dynamics and capitalization using maximum entropy. In Proceedings of ACL-08: HLT, Short Papers, pages 1–4, Columbus, Ohio, June. As-sociation for Computational Linguistics.","Eric Brill. 2003. Processing natural language without natural language processing. In CICLing, pages 360–369.","Michael Collins and Yoram Singer. 1999. Unsupervised models for named entity classification. In Proceedings of the Joint SIGDAT Conference on EMNLP.","Marcello Federico and Nicola Bertoldi. 2004. Broadcast news lm adaptation over time. Computer Speech & Language, 18(4):417–435.","Ciro Martins, António Teixeira, and João Neto. 2006. Dynamic vocabulary adaptation for a daily and real-time broadcast news transcription system. In IEEE/ACL Workshop on Spoken Language Technology, Aruba.","Cristina Mota and Ralph Grishman. 2008. Is this NE tagger getting old? In Proceedings of the Sixth International Language Resources and Evaluation (LREC’08), Marrakech, Morocco, may.","Vincente Ng and Claire Cardie. 2003. Weakly supervised natural language learning without redundant views. In NAACL’03: Proceedings of the 2003 Conference of the North American Chapter of the As-sociation for Computational Linguistics on Human Language Technology, pages 94–101, Morristown, NJ, USA. ACL.","Kamal Nigam and Rayid Ghani. 2000. Analyzing the effectiveness and applicability of co-training. In Proceedings of CIKM, pages 86–93.","David D. Palmer and Mari Ostendorf. 2005. Improving out-of-vocabulary name resolution. Computer Speech & Language, 19(1):107–128.","David Pierce and Claire Cardie. 2001. Limitations of co-training for natural language learning from large datasets. In Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing (EMNLP-2001).","Paulo Rocha and Diana Santos. 2000. Cetempúblico: Um corpus de grandes dimensões de linguagem jornalı́stica portuguesa. In Maria das Graca̧s Volpe Nunes, editor, Actas do V Encontro para o processamento computacional da lı́ngua portuguesa escrita e falada PROPOR 2000, pages 131–140, Atibaia, São Paulo, Brasil.","Diana Santos and Nuno Cardoso, editors. 2007. Reconhecimento de entidades mencionadas em português: Documentação e actas do HAREM, a primeira avaliação conjunta na área. Linguateca, 12 de Novembro. 356"]}],"references":[{"authors":[{"first":"Cédric","last":"Auzanne"},{"first":"John","middle":"S.","last":"Garofolo"},{"first":"Jonathan","middle":"G.","last":"Fiscus"},{"first":"William","middle":"M.","last":"Fisher"}],"year":"2000","title":"Automatic language model adaptation for spoken document retrieval","source":"Cédric Auzanne, John S. Garofolo, Jonathan G. Fiscus, and William M. Fisher. 2000. Automatic language model adaptation for spoken document retrieval. In Proceedings of RIAO 2000 Conference on Content-Based Multimedia Information Access."},{"authors":[{"first":"Fernando","last":"Batista"},{"first":"Nuno","last":"Mamede"},{"first":"Isabel","last":"Trancoso"}],"year":"2008","title":"Language dynamics and capitalization using maximum entropy","source":"Fernando Batista, Nuno Mamede, and Isabel Trancoso. 2008. Language dynamics and capitalization using maximum entropy. In Proceedings of ACL-08: HLT, Short Papers, pages 1–4, Columbus, Ohio, June. As-sociation for Computational Linguistics."},{"authors":[{"first":"Eric","last":"Brill"}],"year":"2003","title":"Processing natural language without natural language processing","source":"Eric Brill. 2003. Processing natural language without natural language processing. In CICLing, pages 360–369."},{"authors":[{"first":"Michael","last":"Collins"},{"first":"Yoram","last":"Singer"}],"year":"1999","title":"Unsupervised models for named entity classification","source":"Michael Collins and Yoram Singer. 1999. Unsupervised models for named entity classification. In Proceedings of the Joint SIGDAT Conference on EMNLP."},{"authors":[{"first":"Marcello","last":"Federico"},{"first":"Nicola","last":"Bertoldi"}],"year":"2004","title":"Broadcast news lm adaptation over time","source":"Marcello Federico and Nicola Bertoldi. 2004. Broadcast news lm adaptation over time. Computer Speech & Language, 18(4):417–435."},{"authors":[{"first":"Ciro","last":"Martins"},{"first":"António","last":"Teixeira"},{"first":"João","last":"Neto"}],"year":"2006","title":"Dynamic vocabulary adaptation for a daily and real-time broadcast news transcription system","source":"Ciro Martins, António Teixeira, and João Neto. 2006. Dynamic vocabulary adaptation for a daily and real-time broadcast news transcription system. In IEEE/ACL Workshop on Spoken Language Technology, Aruba."},{"authors":[{"first":"Cristina","last":"Mota"},{"first":"Ralph","last":"Grishman"}],"year":"2008","title":"Is this NE tagger getting old? In Proceedings of the Sixth International Language Resources and Evaluation (LREC’08), Marrakech, Morocco, may","source":"Cristina Mota and Ralph Grishman. 2008. Is this NE tagger getting old? In Proceedings of the Sixth International Language Resources and Evaluation (LREC’08), Marrakech, Morocco, may."},{"authors":[{"first":"Vincente","last":"Ng"},{"first":"Claire","last":"Cardie"}],"year":"2003","title":"Weakly supervised natural language learning without redundant views","source":"Vincente Ng and Claire Cardie. 2003. Weakly supervised natural language learning without redundant views. In NAACL’03: Proceedings of the 2003 Conference of the North American Chapter of the As-sociation for Computational Linguistics on Human Language Technology, pages 94–101, Morristown, NJ, USA. ACL."},{"authors":[{"first":"Kamal","last":"Nigam"},{"first":"Rayid","last":"Ghani"}],"year":"2000","title":"Analyzing the effectiveness and applicability of co-training","source":"Kamal Nigam and Rayid Ghani. 2000. Analyzing the effectiveness and applicability of co-training. In Proceedings of CIKM, pages 86–93."},{"authors":[{"first":"David","middle":"D.","last":"Palmer"},{"first":"Mari","last":"Ostendorf"}],"year":"2005","title":"Improving out-of-vocabulary name resolution","source":"David D. Palmer and Mari Ostendorf. 2005. Improving out-of-vocabulary name resolution. Computer Speech & Language, 19(1):107–128."},{"authors":[{"first":"David","last":"Pierce"},{"first":"Claire","last":"Cardie"}],"year":"2001","title":"Limitations of co-training for natural language learning from large datasets","source":"David Pierce and Claire Cardie. 2001. Limitations of co-training for natural language learning from large datasets. In Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing (EMNLP-2001)."},{"authors":[{"first":"Paulo","last":"Rocha"},{"first":"Diana","last":"Santos"}],"year":"2000","title":"Cetempúblico: Um corpus de grandes dimensões de linguagem jornalı́stica portuguesa","source":"Paulo Rocha and Diana Santos. 2000. Cetempúblico: Um corpus de grandes dimensões de linguagem jornalı́stica portuguesa. In Maria das Graca̧s Volpe Nunes, editor, Actas do V Encontro para o processamento computacional da lı́ngua portuguesa escrita e falada PROPOR 2000, pages 131–140, Atibaia, São Paulo, Brasil."},{"authors":[{"first":"Diana","last":"Santos"},{"first":"Nuno","last":"Cardoso"},{"last":"editors"}],"year":"2007","title":"Reconhecimento de entidades mencionadas em português: Documentação e actas do HAREM, a primeira avaliação conjunta na área","source":"Diana Santos and Nuno Cardoso, editors. 2007. Reconhecimento de entidades mencionadas em português: Documentação e actas do HAREM, a primeira avaliação conjunta na área. Linguateca, 12 de Novembro. 356"}],"cites":[{"style":0,"text":"Brill (2003)","origin":{"pointer":"/sections/2/paragraphs/0","offset":0,"length":12},"authors":[{"last":"Brill"}],"year":"2003","references":["/references/2"]},{"style":0,"text":"Santos and Cardoso, 2007","origin":{"pointer":"/sections/2/paragraphs/1","offset":187,"length":24},"authors":[{"last":"Santos"},{"last":"Cardoso"}],"year":"2007","references":[]},{"style":0,"text":"Nigam and Ghani, 2000","origin":{"pointer":"/sections/2/paragraphs/2","offset":287,"length":21},"authors":[{"last":"Nigam"},{"last":"Ghani"}],"year":"2000","references":["/references/8"]},{"style":0,"text":"Pierce and Cardie, 2001","origin":{"pointer":"/sections/2/paragraphs/2","offset":310,"length":23},"authors":[{"last":"Pierce"},{"last":"Cardie"}],"year":"2001","references":["/references/10"]},{"style":0,"text":"Ng and Cardie, 2003","origin":{"pointer":"/sections/2/paragraphs/2","offset":335,"length":19},"authors":[{"last":"Ng"},{"last":"Cardie"}],"year":"2003","references":["/references/7"]},{"style":0,"text":"Mota and Grishman, 2008","origin":{"pointer":"/sections/2/paragraphs/2","offset":356,"length":23},"authors":[{"last":"Mota"},{"last":"Grishman"}],"year":"2008","references":["/references/6"]},{"style":0,"text":"Mota and Grishman, 2008","origin":{"pointer":"/sections/2/paragraphs/2","offset":563,"length":23},"authors":[{"last":"Mota"},{"last":"Grishman"}],"year":"2008","references":["/references/6"]},{"style":0,"text":"Collins and Singer (1999)","origin":{"pointer":"/sections/2/paragraphs/2","offset":628,"length":25},"authors":[{"last":"Collins"},{"last":"Singer"}],"year":"1999","references":["/references/3"]},{"style":0,"text":"Palmer and Ostendorf (2005)","origin":{"pointer":"/sections/3/paragraphs/0","offset":301,"length":27},"authors":[{"last":"Palmer"},{"last":"Ostendorf"}],"year":"2005","references":["/references/9"]},{"style":0,"text":"Federico and Bertoldi (2004)","origin":{"pointer":"/sections/3/paragraphs/0","offset":483,"length":28},"authors":[{"last":"Federico"},{"last":"Bertoldi"}],"year":"2004","references":["/references/4"]},{"style":0,"text":"Martins et al. (2006)","origin":{"pointer":"/sections/3/paragraphs/0","offset":517,"length":21},"authors":[{"last":"Martins"},{"last":"al."}],"year":"2006","references":["/references/5"]},{"style":0,"text":"Auzanne et al. (2000)","origin":{"pointer":"/sections/3/paragraphs/0","offset":700,"length":21},"authors":[{"last":"Auzanne"},{"last":"al."}],"year":"2000","references":["/references/0"]},{"style":0,"text":"Mota and Grishman, 2008","origin":{"pointer":"/sections/3/paragraphs/1","offset":307,"length":23},"authors":[{"last":"Mota"},{"last":"Grishman"}],"year":"2008","references":["/references/6"]},{"style":0,"text":"Batista et al. (2008)","origin":{"pointer":"/sections/3/paragraphs/1","offset":333,"length":21},"authors":[{"last":"Batista"},{"last":"al."}],"year":"2008","references":["/references/1"]},{"style":0,"text":"Mota and Grishman (2008)","origin":{"pointer":"/sections/4/paragraphs/0","offset":41,"length":24},"authors":[{"last":"Mota"},{"last":"Grishman"}],"year":"2008","references":["/references/6"]},{"style":0,"text":"Collins and Singer (1999)","origin":{"pointer":"/sections/4/paragraphs/0","offset":190,"length":25},"authors":[{"last":"Collins"},{"last":"Singer"}],"year":"1999","references":["/references/3"]},{"style":0,"text":"Rocha and Santos, 2000","origin":{"pointer":"/sections/5/paragraphs/0","offset":14,"length":22},"authors":[{"last":"Rocha"},{"last":"Santos"}],"year":"2000","references":["/references/11"]}]}
