{"sections":[{"title":"","paragraphs":["Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 305–308, Suntec, Singapore, 4 August 2009. c⃝2009 ACL and AFNLP"]},{"title":"Extending a Surface Realizer to Generate Coherent Discourse Eva Banik The Open University Milton Keynes, UK e.banik@open.ac.uk Abstract","paragraphs":["We present a discourse-level Tree Adjoining Grammar which tightly integrates syntax and discourse levels, including a representation for discourse entities. We show that this technique makes it possible to extend an optimisation algorithm used in natural language generation (polarity filtering) to the discourse level. We implemented the grammar in a surface realizer and show that this technique can be used to reduce the search space by filtering out referentially incoherent solutions."]},{"title":"1 Introduction","paragraphs":["A fundamental problem that microplanners and surface realizers face in natural language generation is how to restrict the search space of possible solutions. A traditional solution to this computational complexity problem is to divide the generation process into tractable sub-problems, each represented as a module in a pipeline, where every decision made by a module restricts the number of options available to others further down the line. Though such pipeline architectures are computationally efficient, they severely restrict the flexibility of the system and the quality of the generated output. Most systems with pipeline architectures generate relatively simple, domain-specific out-put. Systems that produce more complex linguistic constructions typically achieve this by adding more modules to the pipeline (e.g. a revision module (Robin, 1994) or aggregation (Shaw, 2002)). Since complex linguistic constructions often require interaction between modules, adding them to the repertoire of pipelined NLG systems becomes an engineering and programming task.","Integrated NLG systems have a simpler architecture because they do not need to model in-teractions between modules. However, they still face the problem of computational complexity that was originally solved by the pipeline model. Strategies that have been introduced to reduce the search space in integrated systems include greedy/incremental search algorithms (Stone et al., 2003), constructing a dependency graph for a flat semantic input and converting it into a deriva-tion tree (Koller and Striegnitz, 2002), using planning algorithms (Appelt, 1985; Koller and Stone, 2007), polarity filtering (Kow, 2007) and using underspecified g-derivation trees (G-TAG, Danlos (2000)). Despite all these efforts, most systems still don’t attempt to go above the sentence level or generate very complex sentences. In this paper we present a new technique for designing an integrated grammar for natural language generation. Using this technique it is possible to use linguistic constraints on referential coherence to automatically reduce the search space — which in turn makes it possible to generate longer and more coherent texts.","First we extend the grammar of a surface realizer to produce complex, multi-sentential output. Then we add a representation for discourse referents to the grammar, inspired by Centering Theory’s notion of a backward looking center and preferred center. Having done this, we show that by integrating discourse-level representations into a syntactic grammar we can extend an optimization technique — polarity filtering (Kow, 2007; Gardent and Kow, 2006) — from syntactic realization to the discourse level."]},{"title":"2 The Problem of Referential Coherence","paragraphs":["Referential coherence is the phenomenon which is responsible for the contrast in (1), in the sense that the example in (1b) is perceived to be more coherent than (1a).","(1) a Elixir is approved by the FDA. Viral skin disorders are relieved by 305 Aliprosan. Elixir is a white cream. Aliprosan is an ingredient of Elixir.","b Elixir is a white cream. Elixir is approved by the FDA. Elixir contains Aliprosan. Aliprosan relieves viral skin disorders.","Centering Theory (Grosz et al., 1995) is a frequently used framework for modeling referential coherence in discourse. It is based on the notion that for each utterance in a discourse there is a set of entities which are the centers of attention and which serve to link that utterance to other utterances in the same discourse segment. Entities mentioned by an utterance (the set of forward looking centers) form a partially ordered list called the Cf list where roughly, subjects are ranked highest, followed by objects, indirect objects and other arguments or adjuncts. The backward looking center of Un is said to be the most highly ranked element on the Cf list of Un-1 mentioned in the previous utterance.","Centering Theory has been adapted to NLG by Kibble (1999; 2001), and implemented in Kibble and Power (2004). Rather than using the notion of centering transitions as defined by Grosz et al. (1995), in these papers centering theory is re-defined as constraints on salience and cohesion. These constraints state that there is a preference for consecutive utterances to keep the same center and that there is a preference for the center of Un to be realized as the highest ranked entity on the Cf list of Un. Kibble and Power (2004) show how these constraints can be used to drive text planning, sentence planning and pronominalization in an integrated fashion. Our approach is similar to Kibble and Power (2004) in that we don’t use the concept of centering transitions. However, our method is more efficient in that Kibble and Power (2004) use centering transitions to rank the set of generated solutions (some of which are incoherent), whereas we encode centering constraints in elementary trees to reduce the search space of possible solutions before we start computing them."]},{"title":"3 GenI and Polarity Filtering","paragraphs":["The grammar described in the next section was implemented in the GenI surface realizer (Kow, 2007), which uses a lexicalized feature-based Tree Adjoining Grammar to generate all possible paraphrases for a given flat semantic input. GenI implements an optimization technique called polar-","h1:white-cream(e)","Dc  ","S "," NP↓","[idx:e] VP  V is NP cream [idx:e] Punct .","h2:contain(e,a) Dc [c:e]   Dc ↓ [c:e]","Dc   S",""," NP↓ [idx:e] VP "," V contains NP↓ [idx:a] Punct . Figure 1: Elementary syntax/discourse trees ity filtering to constrain the effects of lexical ambiguity. The basic idea of polarity filtering is to associate elementary trees with a set of polarities. When these polarities don’t ‘cancel each other out’, it means that it is not possible to combine the set of trees selected for a given input. This is a quick way to check whether the number of argument slots is the same as the number of potential arguments. For example, if the lexical selec-tion consists of two trees for a given input, one of which provides an NP (-NP) and one of which expects two NPs (-2NP) then the sum of polarities will be -NP and therefore the generator will not attempt to combine the trees.","Values for polarities are defined as follows: every initial tree is assigned a -cat polarity for each substitution node of category cat and a +cat polarity if its root node is of category cat. Auxiliary trees are assigned a -cat polarity for each substitution node only.","Polarity filtering is a very powerful optimization technique, because it allows the generator to reduce the search space early on in the process, before it attempts to combine any trees."]},{"title":"4 An Integrated Syntax-Discourse Grammar","paragraphs":["In order to generate mutisentential text, we first define a discourse-level Tree Adjoining Grammar. The trees in the grammar tightly integrate syntax and discourse representations in the sense that sentence-level elementary trees include one or more discourse-level nodes. The elementary trees in Fig. 1 illustrate what we mean by this: every lexical item that would normally project a sentence in a syntactic grammar (i.e., an S-rooted 306 +e +a -v +e -e +a -e Dc [c:e] "," S"," ","NP↓","[arg:e] VP "," V approved by NP↓ [arg:f]"]},{"title":".","paragraphs":["Dc [c:a]",""," Dc ↓ [c:v]","S "," NP↓","[arg:v] VP "," V relieved by NP↓ [arg:a]"]},{"title":".","paragraphs":["Dc [c:e]   Dc ↓ [c:e]","S "," NP↓","[arg:e] VP "," V is NP a cream"]},{"title":".","paragraphs":["Dc [c:a]   Dc ↓ [c:e]","S   NP↓ [arg:a] VP "," V","is ingredient of NP↓ [arg:e]"]},{"title":".","paragraphs":["h3:approve(f,e) h6:relieve(a,v) h0:cream(e) h4:contain(e,a)"]},{"title":"+2a -v","paragraphs":["Elixir is approved by the FDA. Viral skin disorders are relieved by Aliprosan. Elixir is a white cream. Aliprosan is an ingredient of Elixir. Figure 2: Discourse-level polarities for (1a) sum up to +2a -v +e -e +a -a +e +a -e Dc [c:e] "," Dc ↓ [c:e]","S "," NP↓","[arg:e] VP "," V approved by NP↓ [arg:f]"]},{"title":".","paragraphs":["Dc [c:a]",""," Dc ↓ [c:a]","S "," NP↓","[arg:a] VP "," V relieves NP↓ [arg:v]"]},{"title":".","paragraphs":["Dc [c:e] "," S","","","NP↓","[arg:e] VP "," V is NP a cream"]},{"title":".","paragraphs":["Dc [c:a]",""," Dc ↓ [c:e]","S   NP↓ [arg:e] VP "," V contains NP↓ [arg:a]"]},{"title":".","paragraphs":["h3:approve(f,e) h6:relieve(a,v) h0:cream(e) h4:contain(e,a)"]},{"title":"+a","paragraphs":["Elixir is a white cream. Elixir is approved by the FDA. Elixir contains Aliprosan. Aliprosan relieves viral skin disorders. Figure 3: Discourse-level polarities for (1b) sum up to +a tree) here projects a discourse clause (i.e., a Dc rooted tree). Every predicate that projects a discourse clause is assigned two kinds of elementary trees: a discourse initial tree (Fig. 1a) and a discourse continuing tree (Fig. 1b), which takes the preceding discourse clause as an argument.","We model referential coherence by associating a discourse entity with every root- and substitution node of category Dc. A discourse entity on a root node is “exported” by the elementary tree to be the center of attention in the next sentence. This roughly corresponds to Centering Theory’s notion of a forward looking center. A discourse entity on a substitution node is the entity expected by the sentence to have been the center of attention in the previous utterance, roughly corresponding to the notion of backward looking center in Centering Theory.","For example, the tree on the left in Fig. 1. exports the discourse entity representing its subject (‘e’) as its “forward looking center”. The tree on the right in Fig. 1. is looking for a discourse entity called ‘e’ as its “backward looking center” and exports the same discourse entity as its “forward looking center”. The combination of these two trees therefore yields a coherent discourse, which is expected to be continued with an utterance centered on ‘e’."]},{"title":"5 Polarity Filtering on Discourse Entities","paragraphs":["By treating discourse entities on Dc nodes as an additional polarity key we can apply the polarity filtering technique on the discourse level. This means we can filter out lexical selections that wouldn’t lead to a coherent discourse the same way as those lexical selections are filtered out which won’t lead to a syntactically well formed sentence. To give an example, given the semantic representation in Figure 4 potential realizations by a generator which is not aware of discourse coherence would include both of the examples in (1).","As an experiment, we generated the above example using the same input but two different grammars. In the first case we used a grammar which consists of discourse-level trees but no an-notations for discourse entities. The realizer pro-307 h0:white cream(e) h1:elixir(e) h2:fda(f) h3:approve(f e) h4:contain(e a) h5:aliprosan(a) h6:relieve(a v) h7:viral skin disorders(v) Figure 4: Input for the sentences in (1) duced 192 solutions, including many incoherent ones such as (1a). In the second case, we used a grammar with the same trees, but annotated with discourse referents. In this case the realizer produced only 16 solutions, all of which maintained referential coherence. In the first case, the grammar provided 128 ways to associate trees with the input (tree sets), and the 192 solutions included all possible sentence orders. Since for most trees in the grammar there are more than one ways to annotate them with discourse referents, in the second case the grammar contained more trees (differing only in their discourse referent asignments). In this case there were 1536 tree sets selected for the same input. Of these, 1320 were discarded by polarity filtering on discourse entities. Of the remaining 216 tree sets 200 were ruled out by feature unification when the trees were combined.","Figures 2 and 3 illustrate two sets of trees that were selected by the realizer, corresponding to the examples in (1). Discourse-level polarity filtering in this example (for the input in (4)) discards all tree sets whose polarities don’t sum up to one of the discourse entities, i.e., +e, +a, +f or +v. The polarity of the tree set in Fig.2 is +2a -v so the tree set is discarded. For the tree set in Fig.3 the polarities sum up to +e and the realizer attempts to combine the trees, which in this case leads to a referentially coherent solution (1b).","The search space of the realizer can be further restricted by only allowing tree sets whose polarities sum up to a specific discourse entity. In this case the realizer will produce paragraphs where the center of attention in the last sentence is the discourse entity used for polarity filtering."]},{"title":"6 Conclusions","paragraphs":["We have described a discourse-level extension of Tree Adjoining Grammar which tightly integrates syntax with discourse and includes a representation of discourse entities. We have shown that including discourse entities in the grammar of a surface realizer improves the coherence of the generated text and that these variables can also be used in a very efficient optimization technique, polarity filtering, to filter out referentially incoherent solutions."]},{"title":"References","paragraphs":["D.E. Appelt. 1985. Planning English sentences. Cambridge University Press, Cambridge.","L. Danlos. 2000. G-TAG: A lexicalized formalism for text generation inspired by Tree Adjoining Grammar. In A. Abeille and O. Rambow, editors, Tree Adjoining Grammars: Formalisms, linguistic analysis and processing, pages 343–370. CSLI, Stanford, CA.","C. Gardent and E. Kow. 2006. Three reasons to adopt TAG-based surface realisation. In Proceedings of TAG+8), Sydney/Australia.","B.J. Grosz, A.K. Joshi, and S Weinstein. 1995. Centering: a framework for modelling the local coherence of discourse. Computational Linguistics, 21(2):203–225.","R. Kibble and R. Power. 2004. Optimizing referential coherence in text generation. Computational Linguistics, 30(4):401–416.","R. Kibble. 1999. Cb or not Cb? centering theory applied to NLG. In ACL workshop on Discourse and Reference Structure, pages 72–81.","R. Kibble. 2001. A reformulation of rule 2 of centering theory. Comput. Linguist., 27(4):579–587.","A. Koller and M. Stone. 2007. Sentence generation as planning. In Proceedings of ACL.","A. Koller and K. Striegnitz. 2002. Generation as dependency parsing. In Proceedings of ACL.","E. Kow. 2007. Surface realisation: ambiguity and determinism. Ph.D. thesis, Universite de Henri Poincare - Nancy 1.","J. Robin. 1994. Revision-based generation of Natural Language Summaries providing historical Background. Ph.D. thesis, Columbia University.","J. Shaw. 2002. Clause Aggregation: An approach to generating concise text. Ph.D. thesis, Columbia University.","M. Stone, C. Doran, B. Webber, T. Bleam, and M. Palmer. 2003. Microplanning with communicative intentions: The SPUD system. Computational Intelligence, 19(4):311–381. 308"]}],"references":[{"authors":[{"first":"D.","middle":"E.","last":"Appelt"}],"year":"1985","title":"Planning English sentences","source":"D.E. Appelt. 1985. Planning English sentences. Cambridge University Press, Cambridge."},{"authors":[{"first":"L.","last":"Danlos"}],"year":"2000","title":"G-TAG: A lexicalized formalism for text generation inspired by Tree Adjoining Grammar","source":"L. Danlos. 2000. G-TAG: A lexicalized formalism for text generation inspired by Tree Adjoining Grammar. In A. Abeille and O. Rambow, editors, Tree Adjoining Grammars: Formalisms, linguistic analysis and processing, pages 343–370. CSLI, Stanford, CA."},{"authors":[{"first":"C.","last":"Gardent"},{"first":"E.","last":"Kow"}],"year":"2006","title":"Three reasons to adopt TAG-based surface realisation","source":"C. Gardent and E. Kow. 2006. Three reasons to adopt TAG-based surface realisation. In Proceedings of TAG+8), Sydney/Australia."},{"authors":[{"first":"B.","middle":"J.","last":"Grosz"},{"first":"A.","middle":"K.","last":"Joshi"},{"first":"S","last":"Weinstein"}],"year":"1995","title":"Centering: a framework for modelling the local coherence of discourse","source":"B.J. Grosz, A.K. Joshi, and S Weinstein. 1995. Centering: a framework for modelling the local coherence of discourse. Computational Linguistics, 21(2):203–225."},{"authors":[{"first":"R.","last":"Kibble"},{"first":"R.","last":"Power"}],"year":"2004","title":"Optimizing referential coherence in text generation","source":"R. Kibble and R. Power. 2004. Optimizing referential coherence in text generation. Computational Linguistics, 30(4):401–416."},{"authors":[{"first":"R.","last":"Kibble"}],"year":"1999","title":"Cb or not Cb? centering theory applied to NLG","source":"R. Kibble. 1999. Cb or not Cb? centering theory applied to NLG. In ACL workshop on Discourse and Reference Structure, pages 72–81."},{"authors":[{"first":"R.","last":"Kibble"}],"year":"2001","title":"A reformulation of rule 2 of centering theory","source":"R. Kibble. 2001. A reformulation of rule 2 of centering theory. Comput. Linguist., 27(4):579–587."},{"authors":[{"first":"A.","last":"Koller"},{"first":"M.","last":"Stone"}],"year":"2007","title":"Sentence generation as planning","source":"A. Koller and M. Stone. 2007. Sentence generation as planning. In Proceedings of ACL."},{"authors":[{"first":"A.","last":"Koller"},{"first":"K.","last":"Striegnitz"}],"year":"2002","title":"Generation as dependency parsing","source":"A. Koller and K. Striegnitz. 2002. Generation as dependency parsing. In Proceedings of ACL."},{"authors":[{"first":"E.","last":"Kow"}],"year":"2007","title":"Surface realisation: ambiguity and determinism","source":"E. Kow. 2007. Surface realisation: ambiguity and determinism. Ph.D. thesis, Universite de Henri Poincare - Nancy 1."},{"authors":[{"first":"J.","last":"Robin"}],"year":"1994","title":"Revision-based generation of Natural Language Summaries providing historical Background","source":"J. Robin. 1994. Revision-based generation of Natural Language Summaries providing historical Background. Ph.D. thesis, Columbia University."},{"authors":[{"first":"J.","last":"Shaw"}],"year":"2002","title":"Clause Aggregation: An approach to generating concise text","source":"J. Shaw. 2002. Clause Aggregation: An approach to generating concise text. Ph.D. thesis, Columbia University."},{"authors":[{"first":"M.","last":"Stone"},{"first":"C.","last":"Doran"},{"first":"B.","last":"Webber"},{"first":"T.","last":"Bleam"},{"first":"M.","last":"Palmer"}],"year":"2003","title":"Microplanning with communicative intentions: The SPUD system","source":"M. Stone, C. Doran, B. Webber, T. Bleam, and M. Palmer. 2003. Microplanning with communicative intentions: The SPUD system. Computational Intelligence, 19(4):311–381. 308"}],"cites":[{"style":0,"text":"Robin, 1994","origin":{"pointer":"/sections/2/paragraphs/0","offset":844,"length":11},"authors":[{"last":"Robin"}],"year":"1994","references":["/references/10"]},{"style":0,"text":"Shaw, 2002","origin":{"pointer":"/sections/2/paragraphs/0","offset":873,"length":10},"authors":[{"last":"Shaw"}],"year":"2002","references":["/references/11"]},{"style":0,"text":"Stone et al., 2003","origin":{"pointer":"/sections/2/paragraphs/1","offset":363,"length":18},"authors":[{"last":"Stone"},{"last":"al."}],"year":"2003","references":["/references/12"]},{"style":0,"text":"Koller and Striegnitz, 2002","origin":{"pointer":"/sections/2/paragraphs/1","offset":485,"length":27},"authors":[{"last":"Koller"},{"last":"Striegnitz"}],"year":"2002","references":["/references/8"]},{"style":0,"text":"Appelt, 1985","origin":{"pointer":"/sections/2/paragraphs/1","offset":542,"length":12},"authors":[{"last":"Appelt"}],"year":"1985","references":["/references/0"]},{"style":0,"text":"Koller and Stone, 2007","origin":{"pointer":"/sections/2/paragraphs/1","offset":556,"length":22},"authors":[{"last":"Koller"},{"last":"Stone"}],"year":"2007","references":["/references/7"]},{"style":0,"text":"Kow, 2007","origin":{"pointer":"/sections/2/paragraphs/1","offset":601,"length":9},"authors":[{"last":"Kow"}],"year":"2007","references":["/references/9"]},{"style":0,"text":"Danlos (2000)","origin":{"pointer":"/sections/2/paragraphs/1","offset":664,"length":13},"authors":[{"last":"Danlos"}],"year":"2000","references":["/references/1"]},{"style":0,"text":"Kow, 2007","origin":{"pointer":"/sections/2/paragraphs/2","offset":418,"length":9},"authors":[{"last":"Kow"}],"year":"2007","references":["/references/9"]},{"style":0,"text":"Gardent and Kow, 2006","origin":{"pointer":"/sections/2/paragraphs/2","offset":429,"length":21},"authors":[{"last":"Gardent"},{"last":"Kow"}],"year":"2006","references":["/references/2"]},{"style":0,"text":"Grosz et al., 1995","origin":{"pointer":"/sections/3/paragraphs/3","offset":18,"length":18},"authors":[{"last":"Grosz"},{"last":"al."}],"year":"1995","references":["/references/3"]},{"style":0,"text":"Kibble and Power (2004)","origin":{"pointer":"/sections/3/paragraphs/4","offset":84,"length":23},"authors":[{"last":"Kibble"},{"last":"Power"}],"year":"2004","references":["/references/4"]},{"style":0,"text":"Grosz et al. (1995)","origin":{"pointer":"/sections/3/paragraphs/4","offset":177,"length":19},"authors":[{"last":"Grosz"},{"last":"al."}],"year":"1995","references":["/references/3"]},{"style":0,"text":"Kibble and Power (2004)","origin":{"pointer":"/sections/3/paragraphs/4","offset":506,"length":23},"authors":[{"last":"Kibble"},{"last":"Power"}],"year":"2004","references":["/references/4"]},{"style":0,"text":"Kibble and Power (2004)","origin":{"pointer":"/sections/3/paragraphs/4","offset":686,"length":23},"authors":[{"last":"Kibble"},{"last":"Power"}],"year":"2004","references":["/references/4"]},{"style":0,"text":"Kibble and Power (2004)","origin":{"pointer":"/sections/3/paragraphs/4","offset":815,"length":23},"authors":[{"last":"Kibble"},{"last":"Power"}],"year":"2004","references":["/references/4"]},{"style":0,"text":"Kow, 2007","origin":{"pointer":"/sections/4/paragraphs/0","offset":88,"length":9},"authors":[{"last":"Kow"}],"year":"2007","references":["/references/9"]}]}
