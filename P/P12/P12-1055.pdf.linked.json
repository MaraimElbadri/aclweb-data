{"sections":[{"title":"","paragraphs":["Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 526–535, Jeju, Republic of Korea, 8-14 July 2012. c⃝2012 Association for Computational Linguistics"]},{"title":"Joint Inference of Named Entity Recognition and Normalization for Tweets Xiaohua Liu","paragraphs":["‡ †"]},{"title":", Ming Zhou","paragraphs":["†"]},{"title":", Furu Wei","paragraphs":["†"]},{"title":", Zhongyang Fu","paragraphs":["§"]},{"title":", Xiangyang Zhou","paragraphs":["♯ ‡"]},{"title":"School of Computer Science and Technology Harbin Institute of Technology, Harbin, 150001, China","paragraphs":["§"]},{"title":"Department of Computer Science and Engineering Shanghai Jiao Tong University, Shanghai, 200240, China","paragraphs":["♯"]},{"title":"School of Computer Science and Technology Shandong University, Jinan, 250100, China","paragraphs":["†"]},{"title":"Microsoft Research Asia Beijing, 100190, China","paragraphs":["†"]},{"title":"{xiaoliu, fuwei, mingzhou}@microsoft.com","paragraphs":["§"]},{"title":"zhongyang.fu@gmail.com","paragraphs":["♯"]},{"title":"v-xzho@microsoft.com Abstract","paragraphs":["Tweets represent a critical source of fresh information, in which named entities occur frequently with rich variations. We study the problem of named entity normalization (NEN) for tweets. Two main challenges are the errors propagated from named entity recognition (NER) and the dearth of information in a single tweet. We propose a novel graphical model to simultaneously conduct NER and NEN on multiple tweets to address these challenges. Particularly, our model introduces a binary random variable for each pair of words with the same lemma across similar tweets, whose value indicates whether the two related words are mentions of the same entity. We evaluate our method on a manually annotated data set, and show that our method outperforms the baseline that handles these two tasks separately, boosting the F1 from 80.2% to 83.6% for NER, and the Accuracy from 79.4% to 82.6% for NEN, respectively."]},{"title":"1 Introduction","paragraphs":["Tweets, short messages of less than 140 characters shared through the Twitter service 1",", have become an important source of fresh information. As a result, the task of named entity recognition (NER) for tweets, which aims to identify mentions of rigid designators from tweets belonging to named-entity types such as persons, organizations and locations (2007), has attracted increasing research interest. For example, Ritter et al. (2011) develop a system that exploits a CRF model to segment named","1","http://www.twitter.com entities and then uses a distantly supervised approach based on LabeledLDA to classify named entities. Liu et al. (2011) combine a classifier based on the k-nearest neighbors algorithm with a CRF-based model to leverage cross tweets information, and adopt the semi-supervised learning to leverage unlabeled tweets.","However, named entity normalization (NEN) for tweets, which transforms named entities mentioned in tweets to their unambiguous canonical forms, has not been well studied. Owing to the informal nature of tweets, there are rich variations of named entities in them. According to our investigation on the data set provided by Liu et al. (2011), every named entity in tweets has an average of 3.3 variations 2",". As an illustrative example, we show “Anneke Gronloh”, which may occur as “Mw.,Gronloh”, “Anneke Kronloh” or “Mevrouw G”. We thus propose NEN for tweets, which plays an important role in entity retrieval, trend detection, and event and entity track-ing. For example, Khalid et al. (2008) show that even a simple normalization method leads to improvements of early precision, for both document and passage retrieval, and better normalization results in better retrieval performance.","Traditionally, NEN is regarded as a septated task, which takes the output of NER as its input (Li et al., 2002; Cohen, 2005; Jijkoun et al., 2008; Dai et al., 2011). One limitation of this cascaded approach is that errors propagate from NER to NEN and there is no feedback from NEN to NER. As demonstrated by Khalid et al. (2008), most NEN errors are caused 2 This data set consists of 12,245 randomly sampled tweets","within five days. 526 by recognition errors. Another challenge of NEN is the dearth of information in a single tweet, due to the short and noise-prone nature of tweets. Reportedly, the accuracy of a baseline NEN system based on Wikipedia drops considerably from 94% on edited news to 77% on news comments, a kind of user generated content (UGC) with similar style to tweets (Jijkoun et al., 2008).","We propose jointly conducting NER and NEN on multiple tweets using a graphical model, to address these challenges. Intuitively, improving the performance of NER boosts the performance of NEN. For example, consider the following two tweets: “· · · Alex’s jokes. Justin’s smartness. Max’s randomnes· · · ” and “· · · Alex Russo was like the best character on Disney Channel· · · ”. Identify-ing “Alex” and “Alex Russo” as PERSON will encourage NEN systems to normalize “Alex” into “Alex Russo”. On the other hand, NEN can guide NER. For instance, consider the following two tweets: “· · · she knew Burger King when he was a Prince!· · · ” and “· · · I’m craving all sorts of food: mcdonalds, burger king, pizza, chinese· · · ”. Suppose the NEN system believes that “burger king” cannot be mapped to “Burger King” since these two tweets are not similar in content. This will help NER to assign them different types of labels. Our method optimizes these two tasks simultaneously by enabling them to interact with each other. This largely differentiates our method from existing work.","Furthermore, considering multiple tweets simultaneously allows us to exploit the redundancy in tweets, as suggested by Liu et al. (2011). For example, consider the following two tweets: “· · · Bobby Shaw you don’t invite the wind · · · ” and “· · · I own yah ! Loool bobby shaw· · · ”. Recognizing “Bobby Shaw” in the first tweet as a PERSON is easy owing to its capitalization and the following word “you”, which in turn helps to identify “bobby shaw” in the second tweet as a PERSON.","We adopt a factor graph as our graphical model, which is constructed in the following manner. We first introduce a random variable for each word in every tweet, which represents the BILOU (Beginning, the Inside and the Last tokens of multi-token entities as well as Unit-length entities) label of the corresponding word. Then we add a factor to connect two neighboring variables, forming a conventional linear chain CRFs. Hereafter, we use tm to denote the mth","tweet ,ti","m and yi","m to denote the ith word of of tm and its BILOU label, respectively, and f i m to denote the factor related to yi−1","m and yi","m. Next, for each word pair with the same lemma, denoted by ti m and t j n, we introduce a binary random variable,","denoted by z ij mn, whose value indicates whether ti","m and t j n belong to two mentions of the same entity. Fi-","nally, for any z ij mn we add a factor, denoted by f ij mn,","to connect yi m, y j n and z ij mn. Factors in the same group ({f ij mn} or {f i","m}) share the same set of fea-","ture templates. Figure 1 illustrates an example of","our factor graph for two tweets. Figure 1: A factor graph that jointly conducts NER and NEN on multiple tweets. Blue and green circles represent NE type (y-serials) and normalization variables (z-serials), respectively; filled circles indicate observed random variables; blue rectangles represent the factors connecting neighboring y-serial variables while red rectangles stand for the factors connecting distant y-serial and z-serial variables.","It is worth noting that our factor graph is different from the skip-chain CRFs (Galley, 2006) in the sense that any skip-chain factor of our model consists not only of two NE type variables (yi","m and y","j","n),","which is the case for skip-chain CRFs, but also a nor-","malization variable (z","ij","mn). It is these normalization variables that enable us to conduct NER and NEN jointly.","We manually add normalization information to the data set shared by Liu et al. (2011), to evaluate our method. Experimental results show that our method achieves 83.6% F1 for NER and 82.6% Accuracy for NEN, outperforming the baseline with 80.2%F1 for NER and 79.4% Accuracy for NEN.","We summarize our contributions as follows.","1. We introduce the task of NEN for tweets, and propose jointly conducting NER and NEN for 527 multiple tweets using a factor graph, which leverages redundancy in tweets to make up for the dearth of information in a single tweet and allows these two tasks to inform each other.","2. We evaluate our method on a human annotated data set, and show that our method compares favorably with the baseline, achieving better performance in both tasks.","Our paper is organized as follows. In the next sec-tion, we introduce related work. In Section 3 and 4, we formally define the task and present our method. In Section 5, we evaluate our method. And finally we conclude our work in Section 6."]},{"title":"2 Related Work","paragraphs":["Related work can be divided into two categories: NER and NEN. 2.1 NER NER has been well studied and its solutions can be divided into three categories: 1) Rule-based (Krupka and Hausman, 1998); 2) machine learning based (Finkel and Manning, 2009; Singh et al., 2010); and 3) hybrid methods (Jansche and Abney, 2002). Owing to the availability of annotated corpora, such as ACE05, Enron (Minkov et al., 2005) and CoNLL03 (Tjong Kim Sang and De Meulder, 2003), data driven methods are now dominant.","Current studies of NER mainly focus on formal text such as news articles (Mccallum and Li, 2003; Etzioni et al., 2005). A representative work is that of Ratinov and Roth (2009), in which they system-atically study the challenges of NER, compare several solutions, and show some interesting findings. For example, they show that the BILOU encoding scheme significantly outperforms the BIO schema (Beginning, the Inside and Outside of a chunk).","A handful of work on other genres of texts exists. For example, Yoshida and Tsujii build a biomedical NER system (2007) using lexical features, orthographic features, semantic features and syntactic features, such as part-of-speech (POS) and shallow parsing; Downey et al. (2007) employ capitalization cues and n-gram statistics to locate names of a variety of classes in web text; Wang (2009) introduces NER to clinical notes. A linear CRF model is trained on a manually annotated data set, which achieves an F1 of 81.48% on the test data set; Chiticariu et al. (2010) design and implement a highlevel language NERL which simplifies the process of building, understanding, and customizing complex rule-based named-entity annotators for different domains.","Recently, NER for Tweets attracts growing interest. Finin et al. (2010) use Amazons Mechani-cal Turk service 3","and CrowdFlower 4","to annotate named entities in tweets and train a CRF model to evaluate the effectiveness of human labeling. Ritter et al. (2011) re-build the NLP pipeline for tweets beginning with POS tagging, through chunk-ing, to NER, which first exploits a CRF model to segment named entities and then uses a distantly supervised approach based on LabeledLDA to classify named entities. Unlike this work, our work de-tects the boundary and type of a named entity simultaneously using sequential labeling techniques. Liu et al. (2011) combine a classifier based on the k-nearest neighbors algorithm with a CRF-based model to leverage cross tweets information, and adopt the semi-supervised learning to leverage unlabeled tweets. Our method leverages redundance in similar tweets, using a factor graph rather than a two-stage labeling strategy. One advantage of our method is that local and global information can interact with each other. 2.2 NEN There is a large body of studies into normalizing various types of entities for formally written texts. For instance, Cohen (2005) normalizes gene/protein names using dictionaries automatically extracted from gene databases; Magdy et al. (2007) address cross-document Arabic name normalization using a machine learning approach, a dictionary of person names and frequency information for names in a collection; Cucerzan (2007) demostrates a large-scale system for the recognition and semantic disambiguation of named entities based on information extracted from a large encyclopedic collection and Web search results; Dai et al. (2011) employ a Markov logic network to model interweaved con-3 https://www.mturk.com/mturk/ 4 http://crowdflower.com/ 528 straints in a setting of gene mention normalization.","Jijkoun et al. (2008) study NEN for UGC. They report that the accuracy of a baseline NEN system based on Wikipedia drops considerably from 94% on edited news to 77% on UGC. They identify three main error sources, i.e., entity recognition errors, multiple ways of referring to the same entity and ambiguous references, and exploit hand-crafted rules to improve the baseline NEN system.","We introduce the task of NEN for tweets, a new genre of texts with rich entity variations. In contrast to existing NEN systems, which take the output of NER systems as their input, our method conducts NER and NEN at the same time, allowing them to reinforce each other, as demonstrated by the experimental results."]},{"title":"3 Task Definition","paragraphs":["A tweet is a short text message with no more than 140 characters. Here is an example of a tweet: “mycraftingworld: #Win Microsoft Office 2010 Home and Student #Contest from @office http://bit.ly/ · · · ”, where “mycraftingworld” is the name of the user who published this tweet. Words beginning with “#” like “”#Win” are hash tags; words starting with “@” like “@office” represent user names; and “http://bit.ly/” is a shortened link.","Given a set of tweets, e.g., tweets within some period or related to some query, our task is: 1) To recognize each mention of entities of predefined types for each tweet; and 2) to restore each entity mention into its unambiguous canonical form. Following Liu et al. (2011), we focus on four types of entities, i.e., PERSON, ORGANIZATION, PRODUCT, and LOCATION, and constrain our scope to English tweets. Note that the NEN sub-task can be transformed as follows. Given each pair of entity mentions, decide whether they denote the same entity. Once this is achieved, we can link all the mentions of the same entity, and choose a representative mention, e.g., the longest mention, as their canonical form.","As an illustrative example, consider the following three tweets: “· · · Gaga’s Christmas dinner with her family. Awwwwn· · · ”, “· · · Lady Gaaaaga with her family on Christmas· · · ” and “· · · Buying a magazine just because Lady Gaga’s on the cover · · · ”. It is expected that “Gaga”, “Lady Gaaaaga” and “Lady Gaga” are all labeled as PERSON, and can be restored as “Lady Gaga”."]},{"title":"4 Our Method","paragraphs":["In contrast to existing work, our method jointly conducts NER and NEN for multiple tweets. We first give an overview of our method, then detail its model and features. 4.1 Overview Given a set of tweets as input, our method recognizes predefined types of named entities and for each entity outputs its unambiguous canonical form.","To resolve NER, we assign a label to each word in a tweet, indicating both the boundary and entity type. Following Ratinov and Roth (2009), we use the BILOU schema. For example, consider the tweet “· · · without you is like an iphone without apps; Lady gaga without her telephone· · · ”, the labeled sequence using the BILOU schema is: “· · · withoutO youO isO likeO anO iphoneU−P RODUCT withoutO appsO; LadyB−P ERSON gagaL−P ERSON withoutO herO telephoneO· · · ” , where “iphoneU−P RODUCT ” indicates that “iphone” is a product name of unit length; “LadyB−P ERSON ” means “Lady” is the beginning of a person name while “gagaL−P ERSON ” suggests that “gaga” is the last token of a person name.","To resolve NEN, we assign a binary value label z ij mn to each pair of words ti","m and t j n which share the same lemma. z ij mn = 1 or -1, indicating whether ti","m and t j n belong to two mentions of the same entity 5",".","For example, consider the three tweets presented in","Section 3. “Gaga1","1” 6","and “Gaga1","3” will be assigned","a “1” label, since they are part of two mentions of the","same entity “Lady Gaga”; similarly, “Lady1","2” and","“Lady1","3” are connected with a “1” label. Note that","there are no NEN labels for pairs like “her1","1” and","“her1 2” or “with1","1 and “with1","2”, since words like “her”","and “with” are stop words. With NE type and normalization labels obtained,","we judge two mentions, denoted by ti1···ik","m and 5 Stop words have no normalization labels. The stop words","are mainly from http://www.textfixer.com/resources/common-","english-words.txt. 6 We use wi","m to denote word w’s ith","appearance in the mth","tweet. For example, “Gaga1","1” denotes the first occurance of “Gaga” in the first tweet. 529 t j1···jl n , respectively, refer to the same entity if and only if: 1) The two mentions share the same entity type; 2) ti1···ik","m is a sub-string of t j1···jl n or vise versa; and 3) z ij mn = 1, i = i1, · · · , ik and j = j1, · · · , jl, if z ij mn exists. Still take the three tweets presented","in Section 3 for example. Suppose “Gaga1","1” and “Lady Gaga1","3” are labeled as PERSON, and there is only one related NE normalization label, which is associated with “‘Gaga 1","1” and “Gaga1","3” and has 1 as its value. We then consider that these two mentions can be normalized into the same entity; in a similar way, we can align “Lady1","2 Gaaaaga” with","“Lady1 3 Gaga”. Combining these pieces informa-","tion together, we can infer that “‘Gaga 1","1”, “Lady1","2 Gaaaaga” and “Lady1","3 Gaga” are three mentions of the same entity. Finally, we can select ‘Lady 1","3 Gaga” as the representative, and output ‘Lady Gaga” as their canonical form. We choose the mention with the maximum number of words as the representative. In case of a tie, we prefer the mention with an Wikipedia entry 7",".","The central problem with our method is inferring all the NE type (y-serial) and normalization (z-serial) variables. To achieve this, we construct a factor graph according to the input tweets, which can evaluate the probability of every possible assignment of y-serials and z-serials, by checking the characteristics of the assignment. Each characteristic is called a feature. In this way, we can select the assignment with the highest probability. Next we will introduce our model in detail, including its training and inference procedure and features. 4.2 Model We adopt a factor graph as our model. One advantage of our model is that it allows y-serials and z-serials variables to interact with each other to jointly optimize NER and NEN.","Given a set of tweets T = {tm}N","m=1, we can build a factor graph G = (Y, Z, F, E), where: Y and Z denote y-serials and z-serials variables, respectively; F represents factor vertices, consisting of {f i","m} and {f","ij","mn}, f i","m = f i","m(yi−1","m , yi","m) and f ij mn = f ij mn(yi","m, y j n, z ij mn); E stands for edges, which de-","pends on F , and consists of edges between yi−1","m and yi m, and those between yi","m,y j n and f","ij","mn. 7 If it still ends up as a draw, we will randomly choose one","from the best.","G = (Y, Z, F, E) defines a probability distribu-tion according to Formula 1. ln P (Y, Z|G, T ) ∝ ∑ m,i","ln f i","m(yi−1","m , yi","m)+ ∑ m,n,i,j","δij","mn · ln f ij","mn(yi","m, yj","n, zij","mn) (1) where δ ij mn = 1 if and only if ti","m and t","j","n have the same lemma and are not stop words, otherwise zero. A factor factorizes according to a set of features, so that:","ln f i","m(yi−1","m , yi","m) = ∑ k λ(1) k φ","(1)","k (yi−1","m , yi","m)","ln f ij","mn(yi m, yj","n, zij","mn) = ∑ k λ(2) k φ","(2)","k (yi m, yj","n, zij","mn) (2) {φ (1) k }K1","k=1 and {φ (2) k }K2","k=1 are two feature sets. Θ = {λ (1) k }K1","k=1","⋃","{λ(2)","k }K2","k=1 is called the feature weight","set or parameter set of G. Each feature has a real","value as its weight.","Training Θ is learnt from annotated tweets T , by","maximizing the data likelihood, i.e.,","Θ∗ = arg max","Θ ln P (Y, Z|Θ, T ) (3) To solve this optimization problem, we first calculate its gradient:","∂ ln P (Y, Z|T ; Θ) ∂λ1","k = ∑ m,i φ","(1)","k (yi−1","m , yi","m) − ∑ m,i ∑ yi−1 m ,yi","m","p(yi−1 m , yi","m|T ; Θ)φ","(1)","k (yi−1","m , yi","m) (4)","∂ ln P (Y, Z|T ; Θ) ∂λ2","k = ∑ m,n,i,j δij mn · φ","(2)","k (yi m, yj","n, zij","mn) − ∑ m,n,i,j δij mn ∑","yi m,yj n,zij","mn","p(yi","m, yj","n, zij","mn|T ; Θ) ·φ (2) k (yi","m, yj","n, zij","mn)","(5)","Here, the two marginal probabilities","p(yi−1","m , yi","m|T ; Θ) and p(yi","m, y j n, z","ij","mn|T ; Θ) are computed using loopy belief propagation (Murphy et al., 1999). Once we have computed the gradient, Θ∗","can be worked out by standard techniques such as steepest descent, conjugate gradient and the 530 limited-memory BFGS algorithm (L-BFGS). We choose L-BFGS because it is particularly well suited for optimization problems with a large number of variables. Inference Supposing the parameters Θ have been set to Θ∗",", the inference problem is: Given a set of testing tweets T , output the most probable assignment of Y and Z, i.e.,","(Y, Z)∗ = arg max","(Y,Z)","ln P (Y, Z|Θ∗ , T ) (6) We adopt the max-product algorithm to solve this inference problem. The max-product algorithm is nearly identical to the loopy belief propagation algorithm, with the sums replaced by maxima in the definitions. Note that in both the training and testing stage, the factor graph is constructed in the same way as described in Section 1. Efficiency We take several actions to improve our model’s efficiency. Firstly, we manually compile a comprehensive named entity dictionary from various sources including Wikipedia, Freebase 8",", news articles and the gazetteers shared by Ratinov and Roth (2009). In total this dictionary contains 350 million entries 9",". By looking up this dictionary 10",", we generate the possible BILOU labels, denoted by Y i m hereafter, for each word ti","m. For instance, consider “· · · Good Morning new1","1 york1","1· · · ”. Suppose “New York City” and “New York Times” are in our dictionary, then “new1","1 york1","1” is the matched string with two corresponding entities. As a result, “B-LOCATION” and “B-ORGANIZATION” will be added to Ynew1","1 , and “I-LOCATION” and","“I-ORGANIZATION” will be added to Yyork1","1 . If Y i m ̸= ∅, we enforce the constraint for training and testing that yi","m ∈ Y i","m , to reduce the search space. Secondly, in the testing phase, we introduce three rules related to z ij mn: 1) z","ij","mm = 1, which says two","words sharing the same lemma in the same tweet","denote the same entity; 2) set z","ij","mn to 1, if the similarity between tm and tn is above a threshold (0.8 in our work), or tm and tn share one hash tag; and 3)zmnij = −1, if the similarity between tm and tn is below a threshold (0.3 in work). To compute 8 http://freebase.com/view/military 9 One phrase refereing to L entities has L entries. 10 We use case-insensitive leftmost longest match. the similarity, each tweet is represented as a bag-of-words vector with the stop words removed, and the cosine similarity is adopted, as defined in Formula 7. These rules pre-label a significant part of z-serial variables (accounting for 22.5%), with an accuracy of 93.5%. sim(tm, tn) = ⃗tm · ⃗tn |⃗tm||⃗tn| (7) Note that in our experiments, these measures reduce the training and testing time by 36.2% and 62.8%, respectively, while no obvious performance drop is observed. 4.3 Features A feature in {φ","(1)","k }K1","k=1 involves a pair of neighbor-","ing NE-type labels, i.e., yi−1","m and yi","m, while a feature in {φ","(2)","k }K2","k=1 concerns a pair of distant NE-type","labels and its associated normalization label, i.e.,","yi","m,y j n and z ij mn. Details are given below. 4.3.1 Feature Set One: {φ","(1)","k }K1","k=1","We adopts features similar to Wang (2009), and","Ratinov and Roth (2009), i.e., orthographic features,","lexical features and gazetteer-related features. These","features are defined on the observation. Combining","them with yi−1","m and yi","m constitutes {φ (1) k }K1","k=1. Orthographic features: Whether ti","m is capitalized or upper case; whether it is alphanumeric or contains any slashes; wether it is a stop word; word prefixes and suffixes. Lexical features: Lemma of ti","m, ti−1","m and ti+1","m ,","respectively; whether ti m is an out-of-vocabulary","(OOV) word 11","; POS of ti","m, ti−1","m and ti+1","m , respec-","tively; whether ti m is a hash tag, a link, or a user account. Gazetteer-related features: Whether Y i","m is empty; the dominating label/entity type in Y i","m. Which one is dominant is decided by majority voting of the entities in our dictionary. In case of a tie, we randomly choose one from the best. 4.3.2 Feature Set Two: {φ","(2)","k }K2","k=1 Similarly, we define orthographic, lexical features","and gazetteer-related features on the observation, yi","m 11 We first conduct a simple dictionary-lookup based normal-","ization with the incorrect/correct word pair list provided by Han","et al. (2011) to correct common ill-formed words. Then we call","an online dictionary service to judge whether a word is OOV. 531 and y j n; and then we combine these features with z ij mn, forming {φ (2) k }K2","k=1.","Orthographic features: Whether ti","m / t j n is capital-","ized or upper case; whether ti m / t j n is alphanumeric","or contains any slashes; prefixes and suffixes of ti","m.","Lexical features: Lemma of ti m; whether ti","m is","OOV; whether ti","m / ti+1","m / ti−1","m and t j n / t j+1 n / t j−1 n","have the same POS; whether yi m and y j n have the same label/entity type. Gazetteer-related features: Whether Y i","m ⋂ Y j n / Y i+1 m ⋂ Y j+1 n / Y i−1","m ⋂ Y j−1 n is empty; whether the","dominating label/entity type in Y i","m is the same as that in Y j n ."]},{"title":"5 Experiments","paragraphs":["We manually annotate a data set to evaluate our method. We show that our method outperforms the baseline, a cascaded system that conducts NER and NEN individually. 5.1 Data Preparation We use the data set provided by Liu et al. (2011), which consists of 12,245 tweets with four types of entities annotated: PERSON, LOCATION, ORGANIZATION and PRODUCT. We enrich this data set by adding entity normalization information. Two annotators 12","are involved. For any entity mention, two annotators independently annotate its canonical form. The inter-rater agreement measured by kappa is 0.72. Any inconsistent case is discussed by the two annotators till a consensus is reached. 2, 245 tweets are used for development, and the remainder are used for 5-fold cross validation. 5.2 Evaluation Metrics We adopt the widely-used Precision, Recall and F1 to measure the performance of NER for a particular type of entity, and the average Precision, Recall and F1 to measure the overall performance of NER (Liu et al., 2011; Ritter et al., 2011). As for NEN, we adopt the widely-used Accuracy, i.e., to what percentage the outputted canonical forms are correct (Jijkoun et al., 2008; Cucerzan, 2007; Li et al., 2002). 12 Two native English speakers. 5.3 Baseline We develop a cascaded system as the baseline, which conducts NER and NEN sequentially. Its NER module, denoted by SBR, is based on the state-of-the-art method introduced by Liu et al. (2011); and its NEN model , denoted by SBN , follows the NEN system for user-generated news comments proposed by Jijkoun et al. (2008), which uses handcrafted rules to improve a typical NEN system that normalizes surface forms to Wikipedia page ti-tles. We use the POS tagger developed by Ritter et al. (2011) to extract POS related features, and the OpenNLP toolkit to get lemma related features. 5.4 Results Tables 1- 2 show the overall performance of the baseline and ours (denoted by SRN ). It can be seen that, our method yields a significantly higher F1 (with p < 0.01) than SBR, and a moderate improvement of accuracy as compared with SBN (with p < 0.05). As a case study, we show that our system successfully identified “jaxon1","1” as a PERSON in the tweet “· · · come to see jaxon1","1 someday· · · ”, which is mistakenly labeled as a LOCATION by SBR. This is largely owing to the fact that our system aligns “jaxon1","1” with “Jaxson1","2” in the tweet “· · · I","love Jaxson1 2,Hes like my little brother· · · ”, in which “Jaxson1","2” is identified as a PERSON. As a result, this encourages our system to consider “jaxon1","1” as a PERSON. We also find cases where our system works but SBN fails. For example, “Goldman1","1” in the tweet “· · · Goldman sees massive upside risk in oil prices· · · ” is normalized into “Albert Goldman” by SBR, because it is mistakenly identified as a PERSON by SBS; in contrast, our system recognizes “Goldman1","2 Sachs” as an ORGANIZATION, and successfully links ‘Goldman 1","2” to “Goldman1","1”, resulting that “Goldman1","1” is identified as an ORGANIZATION and normalized into “Goldman Sachs”.","Table 3 reports the NER performance of our method for each entity type, from which we see that our system consistently yields better F1 on all entity types than SBR. We also see that our system boosts the F1 for ORGANIZATION most significantly, reflecting the fact that a large number of organizations that are incorrectly labeled as PERSON by SBR, are now correctly recognized by our method. 532 System Pre Rec F1 SRN 84.7 82.5 83.6 SBR 81.6 78.8 80.2 Table 1: Overall performance (%) of NER. System Accuracy SRN 82.6 SBN 79.4 Table 2: Overall Accuracy (%) of NEN . System PER PRO LOC ORG SRN 84.2 80.5 82.1 85.2 SBR 83.9 78.7 81.3 79.8 Table 3: F1 (%) of NER on different entity types. Features NER (F1) NEN (Accuracy)","Fo 59.2 61.3 Fo + Fl 65.8 68.7 Fo + Fg 80.1 77.2","Fo + Fl + Fg 83.6 82.6 Table 4: Overall F1 (%) of NER and Accuracy (%) of NEN with different feature sets.","Table 4 shows the overall performance of our method with various feature set combinations, where Fo, Fl and Fg denote the orthographic features, the lexical features, and the gazetteer-related features, respectively. From Table 4 we see that gazetteer-related features significantly boost the F1 for NER and Accuracy for NEN, suggesting the importance of external knowledge for this task. 5.5 Discussion One main error source for NER and NEN, which accounts for more than half of all the errors, is slang expressions and informal abbreviations. For instance, our method recognizes “California1","1” in the tweet “· · · And Now, He Lives All The Way In California1","1· · · ” as a LOCATION, however, it mistakenly identifies “Cali1","2” in the tweet “· · · i love Cali so much· · · ” as a PERSON. One reason is our system does not generate any z-serial variable for “California1","1” and “Cali1","2” since they have different lemmas. A more complicated case is “BS1","1” in the tweet “· · · I, bobby shaw, am gonna put BS1","1 on everything· · · ”, in which “BS1","1” is the abbreviation of “bobby shaw”. Our method fails to recognize “BS1","1” as an entity. There are two possible ways to fix these errors: 1) Extending the scope of z-serial variables to each word pairs with a common prefix; and 2) developing advanced normalization components to restore such slang expressions and informal abbreviations into their canonical forms.","Our method does not directly exploit Wikipedia for NEN. This explains the cases where our system correctly links multiple entity mentions but fails to generate canonical forms. Take the following two tweets for example: “· · · nitip link win71","1 sp1· · · ” and “· · · Hit the 3TB wall on SRT installing fresh Win71","2· · · ”. Our system recognizes “win71","1” and “Win71","2” as two mentions of the same product, but cannot output their canonical forms “Windows 7”. One possible solution is to exploit Wikipedia to compile a dictionary consisting of entities and their variations."]},{"title":"6 Conclusions and Future work","paragraphs":["We study the task of NEN for tweets, a new genre of texts that are short and prone to noise. Two challenges of this task are the dearth of information in a single tweet and errors propagated from the NER component. We propose jointly conducting NER and NEN for multiple tweets using a factor graph, to address these challenges. One unique characteristic of our model is that a NE normalization variable is introduced to indicate whether a word pair belongs to the mentions of the same entity. We evaluate our method on a manually annotated data set. Experimental results show our method yields better F1 for NER and Accuracy for NEN than the state-of-the-art baseline that conducts two tasks sequentially.","In the future, we plan to explore two directions to improve our method. First, we are going to develop advanced tweet normalization technologies to resolve slang expressions and informal abbreviations. Second, we are interested in incorporating knowledge mined from Wikipedia into our factor graph."]},{"title":"Acknowledgments","paragraphs":["We thank Yunbo Cao, Dongdong Zhang, and Mu Li for helpful discussions, and the anonymous reviewers for their valuable comments. 533"]},{"title":"References","paragraphs":["Laura Chiticariu, Rajasekar Krishnamurthy, Yunyao Li, Frederick Reiss, and Shivakumar Vaithyanathan. 2010. Domain adaptation of rule-based annotators for named-entity recognition tasks. In EMNLP, pages 1002–1012.","Aaron Cohen. 2005. Unsupervised gene/protein named entity normalization using automatically extracted dictionaries. In Proceedings of the ACL-ISMB Workshop on Linking Biological Literature, Ontologies and Databases: Mining Biological Semantics, pages 17– 24, Detroit, June. Association for Computational Linguistics.","Silviu Cucerzan. 2007. Large-scale named entity disambiguation based on wikipedia data. In In Proc. 2007 Joint Conference on EMNLP and CNLL, pages 708– 716.","Hong-Jie Dai, Richard Tzong-Han Tsai, and Wen-Lian Hsu. 2011. Entity disambiguation using a markovlogic network. In Proceedings of 5th International Joint Conference on Natural Language Processing, pages 846–855, Chiang Mai, Thailand, November. Asian Federation of Natural Language Processing.","Doug Downey, Matthew Broadhead, and Oren Etzioni. 2007. Locating Complex Named Entities in Web Text. In IJCAI.","Oren Etzioni, Michael Cafarella, Doug Downey, Ana-Maria Popescu, Tal Shaked, Stephen Soderland, Daniel S. Weld, and Alexander Yates. 2005. Unsupervised named-entity extraction from the web: an experimental study. Artif. Intell., 165(1):91–134.","Tim Finin, Will Murnane, Anand Karandikar, Nicholas Keller, Justin Martineau, and Mark Dredze. 2010. Annotating named entities in twitter data with crowdsourcing. In CSLDAMT, pages 80–88.","Jenny Rose Finkel and Christopher D. Manning. 2009. Nested named entity recognition. In EMNLP, pages 141–150.","Michel Galley. 2006. A skip-chain conditional random field for ranking meeting utterances by importance. In Association for Computational Linguistics, pages 364–372.","Bo Han and Timothy Baldwin. 2011. Lexical normalisa-tion of short text messages: Makn sens a #twitter. In ACL HLT.","Martin Jansche and Steven P. Abney. 2002. Informa-tion extraction from voicemail transcripts. In EMNLP, pages 320–327.","Valentin Jijkoun, Mahboob Alam Khalid, Maarten Marx, and Maarten de Rijke. 2008. Named entity normalization in user generated content. In Proceedings of the second workshop on Analytics for noisy unstructured text data, AND ’08, pages 23–30, New York, NY, USA. ACM.","Mahboob Khalid, Valentin Jijkoun, and Maarten de Rijke. 2008. The impact of named entity normalization on information retrieval for question answering. In Craig Macdonald, Iadh Ounis, Vassilis Plachouras, Ian Ruthven, and Ryen White, editors, Advances in In-formation Retrieval, volume 4956 of Lecture Notes in Computer Science, pages 705–710. Springer Berlin / Heidelberg.","George R. Krupka and Kevin Hausman. 1998. Isoquest: Description of the netowlT M","extractor system as used in muc-7. In MUC-7.","Huifeng Li, Rohini K. Srihari, Cheng Niu, and Wei Li. 2002. Location normalization for information extraction. In COLING.","Xiaohua Liu, Shaodian Zhang, Furu Wei, and Ming Zhou. 2011. Recognizing named entities in tweets. In ACL.","Walid Magdy, Kareem Darwish, Ossama Emam, and Hany Hassan. 2007. Arabic cross-document person name normalization. In In CASL Workshop 07, pages 25–32.","Andrew Mccallum and Wei Li. 2003. Early results for named entity recognition with conditional random fields, feature induction and web-enhanced lexicons. In HLT-NAACL, pages 188–191.","Einat Minkov, Richard C. Wang, and William W. Cohen. 2005. Extracting personal names from email: apply-ing named entity recognition to informal text. In HLT, pages 443–450.","Kevin P. Murphy, Yair Weiss, and Michael I. Jordan. 1999. Loopy belief propagation for approximate inference: An empirical study. In In Proceedings of Uncertainty in AI, pages 467–475.","David Nadeau and Satoshi Sekine. 2007. A survey of named entity recognition and classification. Linguisticae Investigationes, 30:3–26.","Lev Ratinov and Dan Roth. 2009. Design challenges and misconceptions in named entity recognition. In CoNLL, pages 147–155.","Alan Ritter, Sam Clark, Mausam, and Oren Etzioni. 2011. Named entity recognition in tweets: An experimental study. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1524–1534, Edinburgh, Scotland, UK., July. Association for Computational Linguistics.","Sameer Singh, Dustin Hillard, and Chris Leggetter. 2010. Minimally-supervised extraction of entities from text advertisements. In HLT-NAACL, pages 73–81.","Erik F. Tjong Kim Sang and Fien De Meulder. 2003. In-troduction to the CoNLL-2003 shared task: languageindependent named entity recognition. In HLT-NAACL, pages 142–147. 534","Yefeng Wang. 2009. Annotating and recognising named entities in clinical notes. In ACL-IJCNLP, pages 18– 26.","Kazuhiro Yoshida and Jun’ichi Tsujii. 2007. Reranking for biomedical named-entity recognition. In BioNLP, pages 209–216. 535"]}],"references":[{"authors":[{"first":"Laura","last":"Chiticariu"},{"first":"Rajasekar","last":"Krishnamurthy"},{"first":"Yunyao","last":"Li"},{"first":"Frederick","last":"Reiss"},{"first":"Shivakumar","last":"Vaithyanathan"}],"year":"2010","title":"Domain adaptation of rule-based annotators for named-entity recognition tasks","source":"Laura Chiticariu, Rajasekar Krishnamurthy, Yunyao Li, Frederick Reiss, and Shivakumar Vaithyanathan. 2010. Domain adaptation of rule-based annotators for named-entity recognition tasks. In EMNLP, pages 1002–1012."},{"authors":[{"first":"Aaron","last":"Cohen"}],"year":"2005","title":"Unsupervised gene/protein named entity normalization using automatically extracted dictionaries","source":"Aaron Cohen. 2005. Unsupervised gene/protein named entity normalization using automatically extracted dictionaries. In Proceedings of the ACL-ISMB Workshop on Linking Biological Literature, Ontologies and Databases: Mining Biological Semantics, pages 17– 24, Detroit, June. Association for Computational Linguistics."},{"authors":[{"first":"Silviu","last":"Cucerzan"}],"year":"2007","title":"Large-scale named entity disambiguation based on wikipedia data","source":"Silviu Cucerzan. 2007. Large-scale named entity disambiguation based on wikipedia data. In In Proc. 2007 Joint Conference on EMNLP and CNLL, pages 708– 716."},{"authors":[{"first":"Hong-Jie","last":"Dai"},{"first":"Richard","middle":"Tzong-Han","last":"Tsai"},{"first":"Wen-Lian","last":"Hsu"}],"year":"2011","title":"Entity disambiguation using a markovlogic network","source":"Hong-Jie Dai, Richard Tzong-Han Tsai, and Wen-Lian Hsu. 2011. Entity disambiguation using a markovlogic network. In Proceedings of 5th International Joint Conference on Natural Language Processing, pages 846–855, Chiang Mai, Thailand, November. Asian Federation of Natural Language Processing."},{"authors":[{"first":"Doug","last":"Downey"},{"first":"Matthew","last":"Broadhead"},{"first":"Oren","last":"Etzioni"}],"year":"2007","title":"Locating Complex Named Entities in Web Text","source":"Doug Downey, Matthew Broadhead, and Oren Etzioni. 2007. Locating Complex Named Entities in Web Text. In IJCAI."},{"authors":[{"first":"Oren","last":"Etzioni"},{"first":"Michael","last":"Cafarella"},{"first":"Doug","last":"Downey"},{"first":"Ana-Maria","last":"Popescu"},{"first":"Tal","last":"Shaked"},{"first":"Stephen","last":"Soderland"},{"first":"Daniel","middle":"S.","last":"Weld"},{"first":"Alexander","last":"Yates"}],"year":"2005","title":"Unsupervised named-entity extraction from the web: an experimental study","source":"Oren Etzioni, Michael Cafarella, Doug Downey, Ana-Maria Popescu, Tal Shaked, Stephen Soderland, Daniel S. Weld, and Alexander Yates. 2005. Unsupervised named-entity extraction from the web: an experimental study. Artif. Intell., 165(1):91–134."},{"authors":[{"first":"Tim","last":"Finin"},{"first":"Will","last":"Murnane"},{"first":"Anand","last":"Karandikar"},{"first":"Nicholas","last":"Keller"},{"first":"Justin","last":"Martineau"},{"first":"Mark","last":"Dredze"}],"year":"2010","title":"Annotating named entities in twitter data with crowdsourcing","source":"Tim Finin, Will Murnane, Anand Karandikar, Nicholas Keller, Justin Martineau, and Mark Dredze. 2010. Annotating named entities in twitter data with crowdsourcing. In CSLDAMT, pages 80–88."},{"authors":[{"first":"Jenny","middle":"Rose","last":"Finkel"},{"first":"Christopher","middle":"D.","last":"Manning"}],"year":"2009","title":"Nested named entity recognition","source":"Jenny Rose Finkel and Christopher D. Manning. 2009. Nested named entity recognition. In EMNLP, pages 141–150."},{"authors":[{"first":"Michel","last":"Galley"}],"year":"2006","title":"A skip-chain conditional random field for ranking meeting utterances by importance","source":"Michel Galley. 2006. A skip-chain conditional random field for ranking meeting utterances by importance. In Association for Computational Linguistics, pages 364–372."},{"authors":[{"first":"Bo","last":"Han"},{"first":"Timothy","last":"Baldwin"}],"year":"2011","title":"Lexical normalisa-tion of short text messages: Makn sens a #twitter","source":"Bo Han and Timothy Baldwin. 2011. Lexical normalisa-tion of short text messages: Makn sens a #twitter. In ACL HLT."},{"authors":[{"first":"Martin","last":"Jansche"},{"first":"Steven","middle":"P.","last":"Abney"}],"year":"2002","title":"Informa-tion extraction from voicemail transcripts","source":"Martin Jansche and Steven P. Abney. 2002. Informa-tion extraction from voicemail transcripts. In EMNLP, pages 320–327."},{"authors":[{"first":"Valentin","last":"Jijkoun"},{"first":"Mahboob","middle":"Alam","last":"Khalid"},{"first":"Maarten","last":"Marx"},{"first":"Maarten","last":"de Rijke"}],"year":"2008","title":"Named entity normalization in user generated content","source":"Valentin Jijkoun, Mahboob Alam Khalid, Maarten Marx, and Maarten de Rijke. 2008. Named entity normalization in user generated content. In Proceedings of the second workshop on Analytics for noisy unstructured text data, AND ’08, pages 23–30, New York, NY, USA. ACM."},{"authors":[{"first":"Mahboob","last":"Khalid"},{"first":"Valentin","last":"Jijkoun"},{"first":"Maarten","last":"de Rijke"}],"year":"2008","title":"The impact of named entity normalization on information retrieval for question answering","source":"Mahboob Khalid, Valentin Jijkoun, and Maarten de Rijke. 2008. The impact of named entity normalization on information retrieval for question answering. In Craig Macdonald, Iadh Ounis, Vassilis Plachouras, Ian Ruthven, and Ryen White, editors, Advances in In-formation Retrieval, volume 4956 of Lecture Notes in Computer Science, pages 705–710. Springer Berlin / Heidelberg."},{"authors":[],"source":"George R. Krupka and Kevin Hausman. 1998. Isoquest: Description of the netowlT M"},{"authors":[],"source":"extractor system as used in muc-7. In MUC-7."},{"authors":[{"first":"Huifeng","last":"Li"},{"first":"Rohini","middle":"K.","last":"Srihari"},{"first":"Cheng","last":"Niu"},{"first":"Wei","last":"Li"}],"year":"2002","title":"Location normalization for information extraction","source":"Huifeng Li, Rohini K. Srihari, Cheng Niu, and Wei Li. 2002. Location normalization for information extraction. In COLING."},{"authors":[{"first":"Xiaohua","last":"Liu"},{"first":"Shaodian","last":"Zhang"},{"first":"Furu","last":"Wei"},{"first":"Ming","last":"Zhou"}],"year":"2011","title":"Recognizing named entities in tweets","source":"Xiaohua Liu, Shaodian Zhang, Furu Wei, and Ming Zhou. 2011. Recognizing named entities in tweets. In ACL."},{"authors":[{"first":"Walid","last":"Magdy"},{"first":"Kareem","last":"Darwish"},{"first":"Ossama","last":"Emam"},{"first":"Hany","last":"Hassan"}],"year":"2007","title":"Arabic cross-document person name normalization","source":"Walid Magdy, Kareem Darwish, Ossama Emam, and Hany Hassan. 2007. Arabic cross-document person name normalization. In In CASL Workshop 07, pages 25–32."},{"authors":[{"first":"Andrew","last":"Mccallum"},{"first":"Wei","last":"Li"}],"year":"2003","title":"Early results for named entity recognition with conditional random fields, feature induction and web-enhanced lexicons","source":"Andrew Mccallum and Wei Li. 2003. Early results for named entity recognition with conditional random fields, feature induction and web-enhanced lexicons. In HLT-NAACL, pages 188–191."},{"authors":[{"first":"Einat","last":"Minkov"},{"first":"Richard","middle":"C.","last":"Wang"},{"first":"William","middle":"W.","last":"Cohen"}],"year":"2005","title":"Extracting personal names from email: apply-ing named entity recognition to informal text","source":"Einat Minkov, Richard C. Wang, and William W. Cohen. 2005. Extracting personal names from email: apply-ing named entity recognition to informal text. In HLT, pages 443–450."},{"authors":[{"first":"Kevin","middle":"P.","last":"Murphy"},{"first":"Yair","last":"Weiss"},{"first":"Michael I","middle":".","last":"Jordan"}],"year":"1999","title":"Loopy belief propagation for approximate inference: An empirical study","source":"Kevin P. Murphy, Yair Weiss, and Michael I. Jordan. 1999. Loopy belief propagation for approximate inference: An empirical study. In In Proceedings of Uncertainty in AI, pages 467–475."},{"authors":[{"first":"David","last":"Nadeau"},{"first":"Satoshi","last":"Sekine"}],"year":"2007","title":"A survey of named entity recognition and classification","source":"David Nadeau and Satoshi Sekine. 2007. A survey of named entity recognition and classification. Linguisticae Investigationes, 30:3–26."},{"authors":[{"first":"Lev","last":"Ratinov"},{"first":"Dan","last":"Roth"}],"year":"2009","title":"Design challenges and misconceptions in named entity recognition","source":"Lev Ratinov and Dan Roth. 2009. Design challenges and misconceptions in named entity recognition. In CoNLL, pages 147–155."},{"authors":[{"first":"Alan","last":"Ritter"},{"first":"Sam","last":"Clark"},{"last":"Mausam"},{"first":"Oren","last":"Etzioni"}],"year":"2011","title":"Named entity recognition in tweets: An experimental study","source":"Alan Ritter, Sam Clark, Mausam, and Oren Etzioni. 2011. Named entity recognition in tweets: An experimental study. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1524–1534, Edinburgh, Scotland, UK., July. Association for Computational Linguistics."},{"authors":[{"first":"Sameer","last":"Singh"},{"first":"Dustin","last":"Hillard"},{"first":"Chris","last":"Leggetter"}],"year":"2010","title":"Minimally-supervised extraction of entities from text advertisements","source":"Sameer Singh, Dustin Hillard, and Chris Leggetter. 2010. Minimally-supervised extraction of entities from text advertisements. In HLT-NAACL, pages 73–81."},{"authors":[{"first":"Erik","middle":"F. Tjong Kim","last":"Sang"},{"first":"Fien","last":"De Meulder"}],"year":"2003","title":"In-troduction to the CoNLL-2003 shared task: languageindependent named entity recognition","source":"Erik F. Tjong Kim Sang and Fien De Meulder. 2003. In-troduction to the CoNLL-2003 shared task: languageindependent named entity recognition. In HLT-NAACL, pages 142–147. 534"},{"authors":[{"first":"Yefeng","last":"Wang"}],"year":"2009","title":"Annotating and recognising named entities in clinical notes","source":"Yefeng Wang. 2009. Annotating and recognising named entities in clinical notes. In ACL-IJCNLP, pages 18– 26."},{"authors":[{"first":"Kazuhiro","last":"Yoshida"},{"first":"Jun’ichi","last":"Tsujii"}],"year":"2007","title":"Reranking for biomedical named-entity recognition","source":"Kazuhiro Yoshida and Jun’ichi Tsujii. 2007. Reranking for biomedical named-entity recognition. In BioNLP, pages 209–216. 535"}],"cites":[{"style":0,"text":"Ritter et al. (2011)","origin":{"pointer":"/sections/13/paragraphs/1","offset":331,"length":20},"authors":[{"last":"Ritter"},{"last":"al."}],"year":"2011","references":["/references/23"]},{"style":0,"text":"Liu et al. (2011)","origin":{"pointer":"/sections/13/paragraphs/3","offset":126,"length":17},"authors":[{"last":"Liu"},{"last":"al."}],"year":"2011","references":["/references/16"]},{"style":0,"text":"Liu et al. (2011)","origin":{"pointer":"/sections/13/paragraphs/4","offset":323,"length":17},"authors":[{"last":"Liu"},{"last":"al."}],"year":"2011","references":["/references/16"]},{"style":0,"text":"Khalid et al. (2008)","origin":{"pointer":"/sections/13/paragraphs/5","offset":268,"length":20},"authors":[{"last":"Khalid"},{"last":"al."}],"year":"2008","references":["/references/12"]},{"style":0,"text":"Li et al., 2002","origin":{"pointer":"/sections/13/paragraphs/6","offset":95,"length":15},"authors":[{"last":"Li"},{"last":"al."}],"year":"2002","references":["/references/15"]},{"style":0,"text":"Cohen, 2005","origin":{"pointer":"/sections/13/paragraphs/6","offset":112,"length":11},"authors":[{"last":"Cohen"}],"year":"2005","references":["/references/1"]},{"style":0,"text":"Jijkoun et al., 2008","origin":{"pointer":"/sections/13/paragraphs/6","offset":125,"length":20},"authors":[{"last":"Jijkoun"},{"last":"al."}],"year":"2008","references":["/references/11"]},{"style":0,"text":"Dai et al., 2011","origin":{"pointer":"/sections/13/paragraphs/6","offset":147,"length":16},"authors":[{"last":"Dai"},{"last":"al."}],"year":"2011","references":["/references/3"]},{"style":0,"text":"Khalid et al. (2008)","origin":{"pointer":"/sections/13/paragraphs/6","offset":309,"length":20},"authors":[{"last":"Khalid"},{"last":"al."}],"year":"2008","references":["/references/12"]},{"style":0,"text":"Jijkoun et al., 2008","origin":{"pointer":"/sections/13/paragraphs/7","offset":375,"length":20},"authors":[{"last":"Jijkoun"},{"last":"al."}],"year":"2008","references":["/references/11"]},{"style":0,"text":"Liu et al. (2011)","origin":{"pointer":"/sections/13/paragraphs/9","offset":119,"length":17},"authors":[{"last":"Liu"},{"last":"al."}],"year":"2011","references":["/references/16"]},{"style":0,"text":"Galley, 2006","origin":{"pointer":"/sections/13/paragraphs/23","offset":80,"length":12},"authors":[{"last":"Galley"}],"year":"2006","references":["/references/8"]},{"style":0,"text":"Liu et al. (2011)","origin":{"pointer":"/sections/13/paragraphs/31","offset":68,"length":17},"authors":[{"last":"Liu"},{"last":"al."}],"year":"2011","references":["/references/16"]},{"style":0,"text":"Krupka and Hausman, 1998","origin":{"pointer":"/sections/14/paragraphs/0","offset":167,"length":24},"authors":[{"last":"Krupka"},{"last":"Hausman"}],"year":"1998","references":[]},{"style":0,"text":"Finkel and Manning, 2009","origin":{"pointer":"/sections/14/paragraphs/0","offset":221,"length":24},"authors":[{"last":"Finkel"},{"last":"Manning"}],"year":"2009","references":["/references/7"]},{"style":0,"text":"Singh et al., 2010","origin":{"pointer":"/sections/14/paragraphs/0","offset":247,"length":18},"authors":[{"last":"Singh"},{"last":"al."}],"year":"2010","references":["/references/24"]},{"style":0,"text":"Jansche and Abney, 2002","origin":{"pointer":"/sections/14/paragraphs/0","offset":291,"length":23},"authors":[{"last":"Jansche"},{"last":"Abney"}],"year":"2002","references":["/references/10"]},{"style":0,"text":"Minkov et al., 2005","origin":{"pointer":"/sections/14/paragraphs/0","offset":387,"length":19},"authors":[{"last":"Minkov"},{"last":"al."}],"year":"2005","references":["/references/19"]},{"style":0,"text":"Meulder, 2003","origin":{"pointer":"/sections/14/paragraphs/0","offset":443,"length":13},"authors":[{"last":"Meulder"}],"year":"2003","references":[]},{"style":0,"text":"Mccallum and Li, 2003","origin":{"pointer":"/sections/14/paragraphs/1","offset":74,"length":21},"authors":[{"last":"Mccallum"},{"last":"Li"}],"year":"2003","references":["/references/18"]},{"style":0,"text":"Etzioni et al., 2005","origin":{"pointer":"/sections/14/paragraphs/1","offset":97,"length":20},"authors":[{"last":"Etzioni"},{"last":"al."}],"year":"2005","references":["/references/5"]},{"style":0,"text":"Ratinov and Roth (2009)","origin":{"pointer":"/sections/14/paragraphs/1","offset":153,"length":23},"authors":[{"last":"Ratinov"},{"last":"Roth"}],"year":"2009","references":["/references/22"]},{"style":0,"text":"Downey et al. (2007)","origin":{"pointer":"/sections/14/paragraphs/2","offset":259,"length":20},"authors":[{"last":"Downey"},{"last":"al."}],"year":"2007","references":["/references/4"]},{"style":0,"text":"Wang (2009)","origin":{"pointer":"/sections/14/paragraphs/2","offset":382,"length":11},"authors":[{"last":"Wang"}],"year":"2009","references":["/references/26"]},{"style":0,"text":"Chiticariu et al. (2010)","origin":{"pointer":"/sections/14/paragraphs/2","offset":545,"length":24},"authors":[{"last":"Chiticariu"},{"last":"al."}],"year":"2010","references":["/references/0"]},{"style":0,"text":"Finin et al. (2010)","origin":{"pointer":"/sections/14/paragraphs/3","offset":52,"length":19},"authors":[{"last":"Finin"},{"last":"al."}],"year":"2010","references":["/references/6"]},{"style":0,"text":"Ritter et al. (2011)","origin":{"pointer":"/sections/14/paragraphs/5","offset":108,"length":20},"authors":[{"last":"Ritter"},{"last":"al."}],"year":"2011","references":["/references/23"]},{"style":0,"text":"Liu et al. (2011)","origin":{"pointer":"/sections/14/paragraphs/5","offset":503,"length":17},"authors":[{"last":"Liu"},{"last":"al."}],"year":"2011","references":["/references/16"]},{"style":0,"text":"Cohen (2005)","origin":{"pointer":"/sections/14/paragraphs/5","offset":1051,"length":12},"authors":[{"last":"Cohen"}],"year":"2005","references":["/references/1"]},{"style":0,"text":"Magdy et al. (2007)","origin":{"pointer":"/sections/14/paragraphs/5","offset":1158,"length":19},"authors":[{"last":"Magdy"},{"last":"al."}],"year":"2007","references":["/references/17"]},{"style":0,"text":"Cucerzan (2007)","origin":{"pointer":"/sections/14/paragraphs/5","offset":1344,"length":15},"authors":[{"last":"Cucerzan"}],"year":"2007","references":["/references/2"]},{"style":0,"text":"Dai et al. (2011)","origin":{"pointer":"/sections/14/paragraphs/5","offset":1551,"length":17},"authors":[{"last":"Dai"},{"last":"al."}],"year":"2011","references":["/references/3"]},{"style":0,"text":"Jijkoun et al. (2008)","origin":{"pointer":"/sections/14/paragraphs/6","offset":0,"length":21},"authors":[{"last":"Jijkoun"},{"last":"al."}],"year":"2008","references":["/references/11"]},{"style":0,"text":"Liu et al. (2011)","origin":{"pointer":"/sections/15/paragraphs/1","offset":256,"length":17},"authors":[{"last":"Liu"},{"last":"al."}],"year":"2011","references":["/references/16"]},{"style":0,"text":"Ratinov and Roth (2009)","origin":{"pointer":"/sections/16/paragraphs/1","offset":115,"length":23},"authors":[{"last":"Ratinov"},{"last":"Roth"}],"year":"2009","references":["/references/22"]},{"style":0,"text":"Murphy et al., 1999","origin":{"pointer":"/sections/16/paragraphs/139","offset":55,"length":19},"authors":[{"last":"Murphy"},{"last":"al."}],"year":"1999","references":["/references/20"]},{"style":0,"text":"Ratinov and Roth (2009)","origin":{"pointer":"/sections/16/paragraphs/145","offset":45,"length":23},"authors":[{"last":"Ratinov"},{"last":"Roth"}],"year":"2009","references":["/references/22"]},{"style":0,"text":"Wang (2009)","origin":{"pointer":"/sections/16/paragraphs/179","offset":30,"length":11},"authors":[{"last":"Wang"}],"year":"2009","references":["/references/26"]},{"style":0,"text":"Ratinov and Roth (2009)","origin":{"pointer":"/sections/16/paragraphs/180","offset":0,"length":23},"authors":[{"last":"Ratinov"},{"last":"Roth"}],"year":"2009","references":["/references/22"]},{"style":0,"text":"Liu et al. (2011)","origin":{"pointer":"/sections/17/paragraphs/0","offset":217,"length":17},"authors":[{"last":"Liu"},{"last":"al."}],"year":"2011","references":["/references/16"]},{"style":0,"text":"Liu et al., 2011","origin":{"pointer":"/sections/17/paragraphs/1","offset":555,"length":16},"authors":[{"last":"Liu"},{"last":"al."}],"year":"2011","references":["/references/16"]},{"style":0,"text":"Ritter et al., 2011","origin":{"pointer":"/sections/17/paragraphs/1","offset":573,"length":19},"authors":[{"last":"Ritter"},{"last":"al."}],"year":"2011","references":["/references/23"]},{"style":0,"text":"Jijkoun et al., 2008","origin":{"pointer":"/sections/17/paragraphs/1","offset":710,"length":20},"authors":[{"last":"Jijkoun"},{"last":"al."}],"year":"2008","references":["/references/11"]},{"style":0,"text":"Cucerzan, 2007","origin":{"pointer":"/sections/17/paragraphs/1","offset":732,"length":14},"authors":[{"last":"Cucerzan"}],"year":"2007","references":["/references/2"]},{"style":0,"text":"Li et al., 2002","origin":{"pointer":"/sections/17/paragraphs/1","offset":748,"length":15},"authors":[{"last":"Li"},{"last":"al."}],"year":"2002","references":["/references/15"]},{"style":0,"text":"Liu et al. (2011)","origin":{"pointer":"/sections/17/paragraphs/1","offset":984,"length":17},"authors":[{"last":"Liu"},{"last":"al."}],"year":"2011","references":["/references/16"]},{"style":0,"text":"Jijkoun et al. (2008)","origin":{"pointer":"/sections/17/paragraphs/1","offset":1108,"length":21},"authors":[{"last":"Jijkoun"},{"last":"al."}],"year":"2008","references":["/references/11"]},{"style":0,"text":"Ritter et al. (2011)","origin":{"pointer":"/sections/17/paragraphs/1","offset":1284,"length":20},"authors":[{"last":"Ritter"},{"last":"al."}],"year":"2011","references":["/references/23"]}]}
