{"sections":[{"title":"","paragraphs":["Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 100–104, Jeju, Republic of Korea, 8-14 July 2012. c⃝2012 Association for Computational Linguistics"]},{"title":"A Comparative Study of Target Dependency Structures for Statistical Machine Translation Xianchao Wu","paragraphs":["∗"]},{"title":", Katsuhito Sudoh, Kevin Duh","paragraphs":["†"]},{"title":", Hajime Tsukada, Masaaki Nagata NTT Communication Science Laboratories, NTT Corporation 2-4 Hikaridai Seika-cho, Soraku-gun Kyoto 619-0237 Japan","paragraphs":["wuxianchao@gmail.com,sudoh.katsuhito@lab.ntt.co.jp, kevinduh@is.naist.jp,{tsukada.hajime,nagata.masaaki}@lab.ntt.co.jp"]},{"title":"Abstract","paragraphs":["This paper presents a comparative study of target dependency structures yielded by several state-of-the-art linguistic parsers. Our approach is to measure the impact of these non-isomorphic dependency structures to be used for string-to-dependency translation. Besides using traditional dependency parsers, we also use the dependency structures transformed from PCFG trees and predicate-argument structures (PASs) which are generated by an HPSG parser and a CCG parser. The experiments on Chinese-to-English translation show that the HPSG parser’s PASs achieved the best dependency and translation accuracies."]},{"title":"1 Introduction","paragraphs":["Target language side dependency structures have been successfully used in statistical machine translation (SMT) by Shen et al. (2008) and achieved state-of-the-art results as reported in the NIST 2008 Open MT Evaluation workshop and the NTCIR-9 Chinese-to-English patent translation task (Goto et al., 2011; Ma and Matsoukas, 2011). A primary advantage of dependency representations is that they have a natural mechanism for representing discontinuous constructions, which arise due to longdistance dependencies or in languages where grammatical relations are often signaled by morphology instead of word order (McDonald and Nivre, 2011).","It is known that dependency-style structures can be transformed from a number of linguistic struc-","∗","Now at Baidu Inc.","†","Now at Nara Institute of Science & Technology (NAIST) tures. For example, using the constituent-to-dependency conversion approach proposed by Johansson and Nugues (2007), we can easily yield dependency trees from PCFG style trees. A semantic dependency representation of a whole sentence, predicate-argument structures (PASs), are also included in the output trees of (1) a state-of-the-art head-driven phrase structure grammar (HPSG) (Pollard and Sag, 1994; Sag et al., 2003) parser, Enju1 (Miyao and Tsujii, 2008) and (2) a state-of-the-art CCG parser2","(Clark and Curran, 2007). The motivation of this paper is to investigate the impact of these non-isomorphic dependency structures to be used for SMT. That is, we would like to provide a comparative evaluation of these dependencies in a string-to-dependency decoder (Shen et al., 2008)."]},{"title":"2 Gaining Dependency Structures 2.1 Dependency tree","paragraphs":["We follow the definition of dependency graph and dependency tree as given in (McDonald and Nivre, 2011). A dependency graph G for sentence s is called a dependency tree when it satisfies, (1) the nodes cover all the words in s besides the ROOT; (2) one node can have one and only one head (word) with a determined syntactic role; and (3) the ROOT of the graph is reachable from all other nodes.","For extracting string-to-dependency transfer rules, we use well-formed dependency structures, either fixed or floating, as defined in (Shen et al., 2008). Similarly, we ignore the syntactic roles 1 http://www-tsujii.is.s.u-tokyo.ac.jp/enju/index.html 2 http://groups.inf.ed.ac.uk/ccg/software.html 100  when the fluid pressure cylinder 31 is used , fluid is gradually applied . t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 c2 c5 c7 c9 c11 c12 c14 c15 c17 c20 c22 c24 c25 c3 c4 c6 c8 c10 c13 c18 c19 c21 c23 c16 c1 c0 conj_ arg12 det_ arg1 adj_ arg1 noun_ arg1 noun_ arg0 adj_ arg1 aux_ arg12 verb_ arg12 punct_ arg1 noun_ arg0 aux_ arg12 adj_ arg1 verb_ arg12       ","     Figure 1: HPSG tree of an example sentence. ‘*’/ ‘+’=syntactic/semantic heads. Arrows in red (upper)= PASs, orange (bottom)=word-level dependencies generated from PASs, blue=newly appended dependencies. both during rule extracting and target dependency language model (LM) training. 2.2 Dependency parsing Graph-based and transition-based are two predominant paradigms for data-driven dependency parsing. The MST parser (McDonald et al., 2005) and the Malt parser (Nivre, 2003) stand for two typical parsers, respectively. Parsing accuracy comparison and error analysis under the CoNLL-X dependency shared task data (Buchholz and Marsi, 2006) have been performed by McDonald and Nivre (2011). Here, we compare them on the SMT tasks through parsing the real-world SMT data. 2.3 PCFG parsing For PCFG parsing, we select the Berkeley parser (Petrov and Klein, 2007). In order to generate word-level dependency trees from the PCFG tree, we use the LTH constituent-to-dependency conversion tool3 written by Johansson and Nugues (2007). The head finding rules4","are according to Magerman (1995) and Collins (1997). Similar approach has been originally used by Shen et al. (2008). 2.4 HPSG parsing In the Enju English HPSG grammar (Miyao et al., 2003) used in this paper, the semantic content of 3 http://nlp.cs.lth.se/software/treebank converter/ 4 http://www.cs.columbia.edu/ mcollins/papers/heads a sentence/phrase is represented by a PAS. In an HPSG tree, each leaf node generally introduces a predicate, which is represented by the pair made up of the lexical entry feature and predicate type feature. The arguments of a predicate are designated by the arrows from the argument features in a leaf node to non-terminal nodes (e.g., t0→c3, t0→c16).","Since the PASs use the non-terminal nodes in the HPSG tree (Figure 1), this prevents their direct usage in a string-to-dependency decoder. We thus need an algorithm to transform these phrasal predicate-argument dependencies into a word-to-word dependency tree. Our algorithm (refer to Figure 1 for an example) for changing PASs into word-based dependency trees is as follows:","1. finding, i.e., find the syntactic/semantic head word of each argument node through a bottomup traversal of the tree;","2. mapping, i.e., determine the arc directions (among a predicate word and the syntactic/semantic head words of the argument nodes) for each predicate type according to Table 1. Then, a dependency graph will be generated;","3. checking, i.e., post modifying the dependency graph according to the definition of dependency tree (Section 2.1).","Table 1 lists the mapping from HPSG’s PAS types to word-level dependency arcs. Since a non-terminal node in an HPSG tree has two kinds of heads, syntactic or semantic, we will generate two dependency graphs after mapping. We use “PAS+syn” to represent the dependency trees generated from the HPSG PASs guided by the syntactic heads. For semantic heads, we use “PAS+sem”.","For example, refer to t0 = when in Figure 1. Its arg1 = c16 (with syntactic head t10), arg2 = c3 (with syntactic head t6), and PAS type = conj arg12. In Table 1, this PAS type corresponds to arg2→pred→arg1, then the result word-level dependency is t6(is)→t0(when)→t10(is).","We need to post modify the dependency graph after applying the mapping, since it is not guaranteed to be a dependency tree. Referring to the definition of dependency tree (Section 2.1), we need the strategy for (1) selecting only one head from multiple 101 PAS Type Dependency Relation adj arg1[2] [arg2 →] pred → arg1 adj mod arg1[2] [arg2 →] pred → arg1 → mod aux[ mod] arg12 arg1/pred → arg2 [→ mod] conj arg1[2[3]] [arg2[/arg3]] → pred → arg1 comp arg1[2] pred → arg1 [→ arg2] comp mod arg1 arg1 → pred → mod noun arg1 pred → arg1 noun arg[1]2 arg2 → pred [→ arg1] poss arg[1]2 pred → arg2 [→ arg1] prep arg12[3] arg2[/arg3] → pred → arg1 prep mod arg12[3] arg2[/arg3] → pred → arg1 → mod quote arg[1]2 [arg1 →] pred → arg2 quote arg[1]23 [arg1/]arg3 → pred → arg2 lparen arg123 pred/arg2 → arg3 → arg1 relative arg1[2] [arg2 →] pred → arg1 verb arg1[2[3[4]]] arg1[/arg2[/arg3[/arg4]]] → pred verb mod arg1[2[3[4]]] arg1[/arg2[/arg3[/arg4]]]→pred→mod app arg12,coord arg12 arg2/pred → arg1 det arg1,it arg1,punct arg1 pred → arg1 dtv arg2 pred → arg2 lgs arg2 arg2 → pred Table 1: Mapping from HPSG’s PAS types to dependency relations. Dependent(s) → head(s), / = and, [] = optional. heads and (2) appending dependency relations for those words/punctuation that do not have any head. When one word has multiple heads, we only keep one. The selection strategy is that, if this arc was deleted, it will cause the biggest number of words that can not reach to the root word anymore. In case of a tie, we greedily pack the arc that connect two words wi and wj where |i − j| is the biggest. For all the words and punctuation that do not have a head, we greedily take the root word of the sentence as their heads. In order to fully use the training data, if there are directed cycles in the result dependency graph, we still use the graph in our experiments, where only partial dependency arcs, i.e., those target flat/hierarchical phrases attached with well-formed dependency structures, can be used during translation rule extraction. 2.5 CCG parsing We also use the predicate-argument dependencies generated by the CCG parser developed by Clark and Curran (2007). The algorithm for generating word-level dependency tree is easier than processing the PASs included in the HPSG trees, since the word level predicate-argument relations have already been included in the output of CCG parser. The mapping from predicate types to the gold-standard grammatical relations can be found in Table 13 in (Clark and Curran, 2007). The post-processing is like that described for HPSG parsing, except we greedily use the MST’s sentence root when we can not determine it based on the CCG parser’s PASs."]},{"title":"3 Experiments 3.1 Setup","paragraphs":["We re-implemented the string-to-dependency decoder described in (Shen et al., 2008). Dependency structures from non-isomorphic syntactic/semantic parsers are separately used to train the transfer rules as well as target dependency LMs. For intuitive comparison, an outside SMT system is Moses (Koehn et al., 2007).","For Chinese-to-English translation, we use the parallel data from NIST Open Machine Translation Evaluation tasks. The training data contains 353,796 sentence pairs, 8.7M Chinese words and 10.4M English words. The NIST 2003 and 2005 test data are respectively taken as the development and test set. We performed GIZA++ (Och and Ney, 2003) and the grow-diag-final-and symmetrizing strategy (Koehn et al., 2007) to obtain word alignments. The Berkeley Language Modeling Toolkit, berkeleylm-1.0b35","(Pauls and Klein, 2011), was employed to train (1) a five-gram LM on the Xinhua portion of LDC English Gigaword corpus v3 (LDC2007T07) and (2) a tri-gram dependency LM on the English dependency structures of the training data. We report the translation quality using the case-insensitive BLEU-4 metric (Papineni et al., 2002). 3.2 Statistics of dependencies We compare the similarity of the dependencies with each other, as shown in Table 2. Basically, we investigate (1) if two dependency graphs of one sentence share the same root word and (2) if the head of one word in one sentence are identical in two dependency graphs. In terms of root word comparison, we observe that MST and CCG share 87.3% of identical root words, caused by borrowing roots from MST to CCG. Then, it is interesting that Berkeley and PAS+syn share 74.8% of identical root words. Note that the Berkeley parser is trained on the Penn treebank (Marcus et al., 1994) yet the HPSG parser is trained on the HPSG treebank (Miyao and Tsujii,","5","http://code.google.com/p/berkeleylm/ 102 Dependency Precision Recall BLEU-Dev BLEU-Test # phrases # hier rules # illegal dep trees # directed cycles Moses-1 - - 0.3349 0.3207 5.4M - - - Moses-2 - - 0.3445 0.3262 0.7M 4.5M - - MST 0.744 0.750 0.3520 0.3291 2.4M 2.1M 251 0 Malt 0.732 0.738 0.3423 0.3203 1.5M 1.3M 130,960 0 Berkeley 0.800 0.806 0.3475 0.3312 2.4M 2.2M 282 0 PAS+syn 0.818 0.824 0.3499 0.3376 2.2M 1.9M 10,411 5,853 PAS+sem 0.777 0.782 0.3484 0.3343 2.1M 1.6M 14,271 9,747 CCG 0.701 0.705 0.3442 0.3283 1.7M 1.3M 61,015 49,955 Table 3: Comparison of dependency and translation accuracies. Moses-1 = phrasal, Moses-2 = hierarchical.","Malt Berkeley PAS PAS CCG","+syn +sem MST 70.5 62.5 69.2 53.3 87.3","(77.3) (64.6) (58.5) (58.1) (61.7) Malt 66.2 73.0 46.8 62.9","(63.2) (57.7) (56.6) (58.1) Berkeley 74.8 44.2 56.5","(64.3) (56.0) (59.2) PAS+ 59.3 62.9 syn (79.1) (61.0) PAS+ 60.0 sem (58.8) Table 2: Comparison of the dependencies of the English sentences in the training data. Without () = % of similar root words; with () = % of similar head words. 2008). In terms of head word comparison, PAS+syn and PAS+sem share 79.1% of identical head words. This is basically due to that we used the similar PASs of the HPSG trees. Interestingly, there are only 59.3% identical root words shared by PAS+syn and PAS+sem. This reflects the significant difference be-tween syntactic and semantic heads.","We also manually created the golden dependency trees for the first 200 English sentences in the training data. The precision/recall (P/R) are shown in Table 3. We observe that (1) the translation accuracies approximately follow the P/R scores yet are not that sensitive to their large variances, and (2) it is still tough for domain-adapting from the treebank-trained parsers to parse the real-world SMT data. PAS+syn performed the best by avoiding the errors of missing of arguments for a predicate, wrongly identified head words for a linguistic phrase, and in-consistency dependencies inside relatively long coordinate structures. These errors significantly influence the number of extractable translation rules and the final translation accuracies.","Note that, these P/R scores on the first 200 sentences (all from less than 20 newswire documents) shall only be taken as an approximation of the total training data and not necessarily exactly follow the tendency of the final BLEU scores. For example, CCG is worse than Malt in terms of P/R yet with a higher BLEU score. We argue this is mainly due to that the number of illegal dependency trees generated by Malt is the highest. Consequently, the number of flat/hierarchical rules generated by using Malt trees is the lowest. Also, PAS+sem has a lower P/R than Berkeley, yet their final BLEU scores are not statistically different. 3.3 Results Table 3 also shows the BLEU scores, the number of flat phrases and hierarchical rules (both integrated with target dependency structures), and the number of illegal dependency trees generated by each parser. From the table, we have the following observations: (1) all the dependency structures (except Malt) achieved a significant better BLEU score than the phrasal Moses; (2) PAS+syn performed the best in the test set (0.3376), and it is significantly better than phrasal/hierarchical Moses (p < 0.01), MST (p < 0.05), Malt (p < 0.01), Berkeley (p < 0.05), and CCG (p < 0.05); and (3) CCG performed as well as MST and Berkeley. These results lead us to argue that the robustness of deep syntactic parsers can be advantageous in SMT compared with traditional dependency parsers."]},{"title":"4 Conclusion","paragraphs":["We have constructed a string-to-dependency translation platform for comparing non-isomorphic target dependency structures. Specially, we proposed an algorithm for generating word-based dependency trees from PASs which are generated by a state-of-the-art HPSG parser. We found that dependency trees transformed from these HPSG PASs achieved the best dependency/translation accuracies. 103"]},{"title":"Acknowledgments","paragraphs":["We thank the anonymous reviewers for their constructive comments and suggestions."]},{"title":"References","paragraphs":["Sabine Buchholz and Erwin Marsi. 2006. Conll-x shared task on multilingual dependency parsing. In Proceedings of the Tenth Conference on Computational Natural Language Learning (CoNLL-X), pages 149–164, New York City, June. Association for Computational Linguistics.","Stephen Clark and James R. Curran. 2007. Widecoverage efficient statistical parsing with ccg and loglinear models. Computational Linguistics, 33(4):493– 552.","Michael Collins. 1997. Three generative, lexicalised models for statistical parsing. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics, pages 16–23, Madrid, Spain, July. Association for Computational Linguistics.","Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and Benjamin K. Tsou. 2011. Overview of the patent machine translation task at the ntcir-9 workshop. In Proceedings of NTCIR-9, pages 559–578.","Richard Johansson and Pierre Nugues. 2007. Extended constituent-to-dependency conversion for english. In In Proceedings of NODALIDA, Tartu, Estonia, April.","Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondřej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the ACL 2007 Demo and Poster Sessions, pages 177–180.","Jeff Ma and Spyros Matsoukas. 2011. Bbn’s systems for the chinese-english sub-task of the ntcir-9 patentmt evaluation. In Proceedings of NTCIR-9, pages 579– 584.","David Magerman. 1995. Statistical decision-tree models for parsing. In In Proceedings of of the 33rd Annual Meeting of the Association for Computational Linguistics, pages 276–283.","Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann Bies, Mark Ferguson, Karen Katz, and Britta Schasberger. 1994. The penn treebank: Annotating predicate argument structure. In Proceedings of the Workshop on HLT, pages 114–119, Plainsboro.","Ryan McDonald and Joakim Nivre. 2011. Analyzing and integrating dependency parsers. Computational Linguistics, 37(1):197–230.","Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency parsers. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 91–98, Ann Arbor, Michigan, June. Association for Computational Linguistics.","Yusuke Miyao and Jun’ichi Tsujii. 2008. Feature forest models for probabilistic hpsg parsing. Computational Lingustics, 34(1):35–80.","Yusuke Miyao, Takashi Ninomiya, and Jun’ichi Tsujii. 2003. Probabilistic modeling of argument structures including non-local dependencies. In Proceedings of the International Conference on Recent Advances in Natural Language Processing, pages 285– 291, Borovets.","Joakim Nivre. 2003. An efficient algorithm for projective dependency parsing. In Proceedings of the 8th International Workshop on Parsing Technologies (IWPT, pages 149–160.","Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.","Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of ACL, pages 311–318.","Adam Pauls and Dan Klein. 2011. Faster and smaller n-gram language models. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 258–267, Portland, Oregon, USA, June. Association for Computational Linguistics.","Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In Human Language Technologies 2007: The Conference of the North Ameri-can Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 404–411, Rochester, New York, April. Association for Computational Linguistics.","Carl Pollard and Ivan A. Sag. 1994. Head-Driven Phrase Structure Grammar. University of Chicago Press.","Ivan A. Sag, Thomas Wasow, and Emily M. Bender. 2003. Syntactic Theory: A Formal Introduction. Number 152 in CSLI Lecture Notes. CSLI Publications.","Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A new string-to-dependency machine translation algorithm with a target dependency language model. In Proceedings of ACL-08:HLT, pages 577–585, Columbus, Ohio. 104"]}],"references":[{"authors":[{"first":"Sabine","last":"Buchholz"},{"first":"Erwin","last":"Marsi"}],"year":"2006","title":"Conll-x shared task on multilingual dependency parsing","source":"Sabine Buchholz and Erwin Marsi. 2006. Conll-x shared task on multilingual dependency parsing. In Proceedings of the Tenth Conference on Computational Natural Language Learning (CoNLL-X), pages 149–164, New York City, June. Association for Computational Linguistics."},{"authors":[{"first":"Stephen","last":"Clark"},{"first":"James","middle":"R.","last":"Curran"}],"year":"2007","title":"Widecoverage efficient statistical parsing with ccg and loglinear models","source":"Stephen Clark and James R. Curran. 2007. Widecoverage efficient statistical parsing with ccg and loglinear models. Computational Linguistics, 33(4):493– 552."},{"authors":[{"first":"Michael","last":"Collins"}],"year":"1997","title":"Three generative, lexicalised models for statistical parsing","source":"Michael Collins. 1997. Three generative, lexicalised models for statistical parsing. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics, pages 16–23, Madrid, Spain, July. Association for Computational Linguistics."},{"authors":[{"first":"Isao","last":"Goto"},{"first":"Bin","last":"Lu"},{"first":"Ka","middle":"Po","last":"Chow"},{"first":"Eiichiro","last":"Sumita"},{"first":"Benjamin","middle":"K.","last":"Tsou"}],"year":"2011","title":"Overview of the patent machine translation task at the ntcir-9 workshop","source":"Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and Benjamin K. Tsou. 2011. Overview of the patent machine translation task at the ntcir-9 workshop. In Proceedings of NTCIR-9, pages 559–578."},{"authors":[{"first":"Richard","last":"Johansson"},{"first":"Pierre","last":"Nugues"}],"year":"2007","title":"Extended constituent-to-dependency conversion for english","source":"Richard Johansson and Pierre Nugues. 2007. Extended constituent-to-dependency conversion for english. In In Proceedings of NODALIDA, Tartu, Estonia, April."},{"authors":[{"first":"Philipp","last":"Koehn"},{"first":"Hieu","last":"Hoang"},{"first":"Alexandra","last":"Birch"},{"first":"Chris","last":"Callison-Burch"},{"first":"Marcello","last":"Federico"},{"first":"Nicola","last":"Bertoldi"},{"first":"Brooke","last":"Cowan"},{"first":"Wade","last":"Shen"},{"first":"Christine","last":"Moran"},{"first":"Richard","last":"Zens"},{"first":"Chris","last":"Dyer"},{"first":"Ondřej","last":"Bojar"},{"first":"Alexandra","last":"Constantin"},{"first":"Evan","last":"Herbst"}],"year":"2007","title":"Moses: Open source toolkit for statistical machine translation","source":"Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondřej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the ACL 2007 Demo and Poster Sessions, pages 177–180."},{"authors":[{"first":"Jeff","last":"Ma"},{"first":"Spyros","last":"Matsoukas"}],"year":"2011","title":"Bbn’s systems for the chinese-english sub-task of the ntcir-9 patentmt evaluation","source":"Jeff Ma and Spyros Matsoukas. 2011. Bbn’s systems for the chinese-english sub-task of the ntcir-9 patentmt evaluation. In Proceedings of NTCIR-9, pages 579– 584."},{"authors":[{"first":"David","last":"Magerman"}],"year":"1995","title":"Statistical decision-tree models for parsing","source":"David Magerman. 1995. Statistical decision-tree models for parsing. In In Proceedings of of the 33rd Annual Meeting of the Association for Computational Linguistics, pages 276–283."},{"authors":[{"first":"Mitchell","last":"Marcus"},{"first":"Grace","last":"Kim"},{"first":"Mary","middle":"Ann","last":"Marcinkiewicz"},{"first":"Robert","last":"MacIntyre"},{"first":"Ann","last":"Bies"},{"first":"Mark","last":"Ferguson"},{"first":"Karen","last":"Katz"},{"first":"Britta","last":"Schasberger"}],"year":"1994","title":"The penn treebank: Annotating predicate argument structure","source":"Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann Bies, Mark Ferguson, Karen Katz, and Britta Schasberger. 1994. The penn treebank: Annotating predicate argument structure. In Proceedings of the Workshop on HLT, pages 114–119, Plainsboro."},{"authors":[{"first":"Ryan","last":"McDonald"},{"first":"Joakim","last":"Nivre"}],"year":"2011","title":"Analyzing and integrating dependency parsers","source":"Ryan McDonald and Joakim Nivre. 2011. Analyzing and integrating dependency parsers. Computational Linguistics, 37(1):197–230."},{"authors":[{"first":"Ryan","last":"McDonald"},{"first":"Koby","last":"Crammer"},{"first":"Fernando","last":"Pereira"}],"year":"2005","title":"Online large-margin training of dependency parsers","source":"Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency parsers. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 91–98, Ann Arbor, Michigan, June. Association for Computational Linguistics."},{"authors":[{"first":"Yusuke","last":"Miyao"},{"first":"Jun’ichi","last":"Tsujii"}],"year":"2008","title":"Feature forest models for probabilistic hpsg parsing","source":"Yusuke Miyao and Jun’ichi Tsujii. 2008. Feature forest models for probabilistic hpsg parsing. Computational Lingustics, 34(1):35–80."},{"authors":[{"first":"Yusuke","last":"Miyao"},{"first":"Takashi","last":"Ninomiya"},{"first":"Jun’ichi","last":"Tsujii"}],"year":"2003","title":"Probabilistic modeling of argument structures including non-local dependencies","source":"Yusuke Miyao, Takashi Ninomiya, and Jun’ichi Tsujii. 2003. Probabilistic modeling of argument structures including non-local dependencies. In Proceedings of the International Conference on Recent Advances in Natural Language Processing, pages 285– 291, Borovets."},{"authors":[{"first":"Joakim","last":"Nivre"}],"year":"2003","title":"An efficient algorithm for projective dependency parsing","source":"Joakim Nivre. 2003. An efficient algorithm for projective dependency parsing. In Proceedings of the 8th International Workshop on Parsing Technologies (IWPT, pages 149–160."},{"authors":[{"first":"Franz","middle":"Josef","last":"Och"},{"first":"Hermann","last":"Ney"}],"year":"2003","title":"A systematic comparison of various statistical alignment models","source":"Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51."},{"authors":[{"first":"Kishore","last":"Papineni"},{"first":"Salim","last":"Roukos"},{"first":"Todd","last":"Ward"},{"first":"Wei-Jing","last":"Zhu"}],"year":"2002","title":"Bleu: a method for automatic evaluation of machine translation","source":"Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of ACL, pages 311–318."},{"authors":[{"first":"Adam","last":"Pauls"},{"first":"Dan","last":"Klein"}],"year":"2011","title":"Faster and smaller n-gram language models","source":"Adam Pauls and Dan Klein. 2011. Faster and smaller n-gram language models. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 258–267, Portland, Oregon, USA, June. Association for Computational Linguistics."},{"authors":[{"first":"Slav","last":"Petrov"},{"first":"Dan","last":"Klein"}],"year":"2007","title":"Improved inference for unlexicalized parsing","source":"Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In Human Language Technologies 2007: The Conference of the North Ameri-can Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 404–411, Rochester, New York, April. Association for Computational Linguistics."},{"authors":[{"first":"Carl","last":"Pollard"},{"first":"Ivan","middle":"A.","last":"Sag"}],"year":"1994","title":"Head-Driven Phrase Structure Grammar","source":"Carl Pollard and Ivan A. Sag. 1994. Head-Driven Phrase Structure Grammar. University of Chicago Press."},{"authors":[{"first":"Ivan","middle":"A.","last":"Sag"},{"first":"Thomas","last":"Wasow"},{"first":"Emily","middle":"M.","last":"Bender"}],"year":"2003","title":"Syntactic Theory: A Formal Introduction","source":"Ivan A. Sag, Thomas Wasow, and Emily M. Bender. 2003. Syntactic Theory: A Formal Introduction. Number 152 in CSLI Lecture Notes. CSLI Publications."},{"authors":[{"first":"Libin","last":"Shen"},{"first":"Jinxi","last":"Xu"},{"first":"Ralph","last":"Weischedel"}],"year":"2008","title":"A new string-to-dependency machine translation algorithm with a target dependency language model","source":"Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A new string-to-dependency machine translation algorithm with a target dependency language model. In Proceedings of ACL-08:HLT, pages 577–585, Columbus, Ohio. 104"}],"cites":[{"style":0,"text":"Shen et al. (2008)","origin":{"pointer":"/sections/5/paragraphs/0","offset":115,"length":18},"authors":[{"last":"Shen"},{"last":"al."}],"year":"2008","references":["/references/20"]},{"style":0,"text":"Goto et al., 2011","origin":{"pointer":"/sections/5/paragraphs/0","offset":289,"length":17},"authors":[{"last":"Goto"},{"last":"al."}],"year":"2011","references":["/references/3"]},{"style":0,"text":"Ma and Matsoukas, 2011","origin":{"pointer":"/sections/5/paragraphs/0","offset":308,"length":22},"authors":[{"last":"Ma"},{"last":"Matsoukas"}],"year":"2011","references":["/references/6"]},{"style":0,"text":"McDonald and Nivre, 2011","origin":{"pointer":"/sections/5/paragraphs/0","offset":612,"length":24},"authors":[{"last":"McDonald"},{"last":"Nivre"}],"year":"2011","references":["/references/9"]},{"style":0,"text":"Johansson and Nugues (2007)","origin":{"pointer":"/sections/5/paragraphs/5","offset":142,"length":27},"authors":[{"last":"Johansson"},{"last":"Nugues"}],"year":"2007","references":["/references/4"]},{"style":0,"text":"Pollard and Sag, 1994","origin":{"pointer":"/sections/5/paragraphs/5","offset":436,"length":21},"authors":[{"last":"Pollard"},{"last":"Sag"}],"year":"1994","references":["/references/18"]},{"style":0,"text":"Sag et al., 2003","origin":{"pointer":"/sections/5/paragraphs/5","offset":459,"length":16},"authors":[{"last":"Sag"},{"last":"al."}],"year":"2003","references":["/references/19"]},{"style":0,"text":"Miyao and Tsujii, 2008","origin":{"pointer":"/sections/5/paragraphs/5","offset":492,"length":22},"authors":[{"last":"Miyao"},{"last":"Tsujii"}],"year":"2008","references":["/references/11"]},{"style":0,"text":"Clark and Curran, 2007","origin":{"pointer":"/sections/5/paragraphs/6","offset":1,"length":22},"authors":[{"last":"Clark"},{"last":"Curran"}],"year":"2007","references":["/references/1"]},{"style":0,"text":"Shen et al., 2008","origin":{"pointer":"/sections/5/paragraphs/6","offset":266,"length":17},"authors":[{"last":"Shen"},{"last":"al."}],"year":"2008","references":["/references/20"]},{"style":0,"text":"McDonald and Nivre, 2011","origin":{"pointer":"/sections/6/paragraphs/0","offset":78,"length":24},"authors":[{"last":"McDonald"},{"last":"Nivre"}],"year":"2011","references":["/references/9"]},{"style":0,"text":"Shen et al., 2008","origin":{"pointer":"/sections/6/paragraphs/1","offset":135,"length":17},"authors":[{"last":"Shen"},{"last":"al."}],"year":"2008","references":["/references/20"]},{"style":0,"text":"McDonald et al., 2005","origin":{"pointer":"/sections/6/paragraphs/2","offset":426,"length":21},"authors":[{"last":"McDonald"},{"last":"al."}],"year":"2005","references":["/references/10"]},{"style":0,"text":"Nivre, 2003","origin":{"pointer":"/sections/6/paragraphs/2","offset":470,"length":11},"authors":[{"last":"Nivre"}],"year":"2003","references":["/references/13"]},{"style":0,"text":"Buchholz and Marsi, 2006","origin":{"pointer":"/sections/6/paragraphs/2","offset":622,"length":24},"authors":[{"last":"Buchholz"},{"last":"Marsi"}],"year":"2006","references":["/references/0"]},{"style":0,"text":"McDonald and Nivre (2011)","origin":{"pointer":"/sections/6/paragraphs/2","offset":671,"length":25},"authors":[{"last":"McDonald"},{"last":"Nivre"}],"year":"2011","references":["/references/9"]},{"style":0,"text":"Petrov and Klein, 2007","origin":{"pointer":"/sections/6/paragraphs/2","offset":844,"length":22},"authors":[{"last":"Petrov"},{"last":"Klein"}],"year":"2007","references":["/references/17"]},{"style":0,"text":"Johansson and Nugues (2007)","origin":{"pointer":"/sections/6/paragraphs/2","offset":1007,"length":27},"authors":[{"last":"Johansson"},{"last":"Nugues"}],"year":"2007","references":["/references/4"]},{"style":0,"text":"Magerman (1995)","origin":{"pointer":"/sections/6/paragraphs/3","offset":17,"length":15},"authors":[{"last":"Magerman"}],"year":"1995","references":["/references/7"]},{"style":0,"text":"Collins (1997)","origin":{"pointer":"/sections/6/paragraphs/3","offset":37,"length":14},"authors":[{"last":"Collins"}],"year":"1997","references":["/references/2"]},{"style":0,"text":"Shen et al. (2008)","origin":{"pointer":"/sections/6/paragraphs/3","offset":98,"length":18},"authors":[{"last":"Shen"},{"last":"al."}],"year":"2008","references":["/references/20"]},{"style":0,"text":"Miyao et al., 2003","origin":{"pointer":"/sections/6/paragraphs/3","offset":169,"length":18},"authors":[{"last":"Miyao"},{"last":"al."}],"year":"2003","references":["/references/12"]},{"style":0,"text":"Clark and Curran (2007)","origin":{"pointer":"/sections/6/paragraphs/10","offset":2140,"length":23},"authors":[{"last":"Clark"},{"last":"Curran"}],"year":"2007","references":["/references/1"]},{"style":0,"text":"Clark and Curran, 2007","origin":{"pointer":"/sections/6/paragraphs/10","offset":2495,"length":22},"authors":[{"last":"Clark"},{"last":"Curran"}],"year":"2007","references":["/references/1"]},{"style":0,"text":"Shen et al., 2008","origin":{"pointer":"/sections/7/paragraphs/0","offset":65,"length":17},"authors":[{"last":"Shen"},{"last":"al."}],"year":"2008","references":["/references/20"]},{"style":0,"text":"Koehn et al., 2007","origin":{"pointer":"/sections/7/paragraphs/0","offset":294,"length":18},"authors":[{"last":"Koehn"},{"last":"al."}],"year":"2007","references":["/references/5"]},{"style":0,"text":"Och and Ney, 2003","origin":{"pointer":"/sections/7/paragraphs/1","offset":319,"length":17},"authors":[{"last":"Och"},{"last":"Ney"}],"year":"2003","references":["/references/14"]},{"style":0,"text":"Koehn et al., 2007","origin":{"pointer":"/sections/7/paragraphs/1","offset":389,"length":18},"authors":[{"last":"Koehn"},{"last":"al."}],"year":"2007","references":["/references/5"]},{"style":0,"text":"Pauls and Klein, 2011","origin":{"pointer":"/sections/7/paragraphs/2","offset":1,"length":21},"authors":[{"last":"Pauls"},{"last":"Klein"}],"year":"2011","references":["/references/16"]},{"style":0,"text":"Papineni et al., 2002","origin":{"pointer":"/sections/7/paragraphs/2","offset":303,"length":21},"authors":[{"last":"Papineni"},{"last":"al."}],"year":"2002","references":["/references/15"]},{"style":0,"text":"Marcus et al., 1994","origin":{"pointer":"/sections/7/paragraphs/2","offset":918,"length":19},"authors":[{"last":"Marcus"},{"last":"al."}],"year":"1994","references":["/references/8"]}]}
