{"sections":[{"title":"","paragraphs":["Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 440–448, Jeju, Republic of Korea, 8-14 July 2012. c⃝2012 Association for Computational Linguistics"]},{"title":"Bayesian Symbol-Refined Tree Substitution Grammars for Syntactic Parsing Hiroyuki Shindo","paragraphs":["†"]},{"title":"Yusuke Miyao","paragraphs":["‡"]},{"title":"Akinori Fujino","paragraphs":["†"]},{"title":"Masaaki Nagata","paragraphs":["† †"]},{"title":"NTT Communication Science Laboratories, NTT Corporation 2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto, Japan {shindo.hiroyuki,fujino.akinori,nagata.masaaki}@lab.ntt.co.jp","paragraphs":["‡"]},{"title":"National Institute of Informatics 2-1-2 Hitotsubashi, Chiyoda-ku, Tokyo, Japan yusuke@nii.ac.jp Abstract","paragraphs":["We propose Symbol-Refined Tree Substitution Grammars (SR-TSGs) for syntactic parsing. An SR-TSG is an extension of the conventional TSG model where each nonterminal symbol can be refined (subcategorized) to fit the training data. We aim to provide a unified model where TSG rules and symbol refinement are learned from training data in a fully automatic and consistent fashion. We present a novel probabilistic SR-TSG model based on the hierarchical Pitman-Yor Process to encode backoff smoothing from a fine-grained SR-TSG to simpler CFG rules, and develop an efficient training method based on Markov Chain Monte Carlo (MCMC) sampling. Our SR-TSG parser achieves an F1 score of 92.4% in the Wall Street Journal (WSJ) English Penn Treebank parsing task, which is a 7.7 point improvement over a conventional Bayesian TSG parser, and better than state-of-the-art discriminative reranking parsers."]},{"title":"1 Introduction","paragraphs":["Syntactic parsing has played a central role in natural language processing. The resulting syntactic analysis can be used for various applications such as machine translation (Galley et al., 2004; DeNeefe and Knight, 2009), sentence compression (Cohn and Lapata, 2009; Yamangil and Shieber, 2010), and ques-tion answering (Wang et al., 2007). Probabilistic context-free grammar (PCFG) underlies many statistical parsers, however, it is well known that the PCFG rules extracted from treebank data via maximum likelihood estimation do not perform well due to unrealistic context freedom assumptions (Klein and Manning, 2003).","In recent years, there has been an increasing interest in tree substitution grammar (TSG) as an alternative to CFG for modeling syntax trees (Post and Gildea, 2009; Tenenbaum et al., 2009; Cohn et al., 2010). TSG is a natural extension of CFG in which nonterminal symbols can be rewritten (substituted) with arbitrarily large tree fragments. These tree fragments have great advantages over tiny CFG rules since they can capture non-local contexts explicitly such as predicate-argument structures, idioms and grammatical agreements (Cohn et al., 2010). Previous work on TSG parsing (Cohn et al., 2010; Post and Gildea, 2009; Bansal and Klein, 2010) has consistently shown that a probabilistic TSG (PTSG) parser is significantly more accurate than a PCFG parser, but is still inferior to state-of-the-art parsers (e.g., the Berkeley parser (Petrov et al., 2006) and the Charniak parser (Charniak and Johnson, 2005)). One major drawback of TSG is that the context freedom assumptions still remain at substitution sites, that is, TSG tree fragments are generated that are conditionally independent of all others given root nonterminal symbols. Furthermore, when a sentence is unparsable with large tree fragments, the PTSG parser usually uses naive CFG rules derived from its backoff model, which diminishes the benefits obtained from large tree fragments.","On the other hand, current state-of-the-art parsers use symbol refinement techniques (Johnson, 1998; Collins, 2003; Matsuzaki et al., 2005). Symbol refinement is a successful approach for weaken-ing context freedom assumptions by dividing coarse treebank symbols (e.g. NP and VP) into subcategories, rather than extracting large tree fragments. As shown in several studies on TSG parsing (Zuidema, 2007; Bansal and Klein, 2010), large 440 tree fragments and symbol refinement work complementarily for syntactic parsing. For example, Bansal and Klein (2010) have reported that deterministic symbol refinement with heuristics helps improve the accuracy of a TSG parser.","In this paper, we propose Symbol-Refined Tree Substitution Grammars (SR-TSGs) for syntactic parsing. SR-TSG is an extension of the conventional TSG model where each nonterminal symbol can be refined (subcategorized) to fit the training data. Our work differs from previous studies in that we focus on a unified model where TSG rules and symbol refinement are learned from training data in a fully automatic and consistent fashion. We also propose a novel probabilistic SR-TSG model with the hierarchical Pitman-Yor Process (Pitman and Yor, 1997), namely a sort of nonparametric Bayesian model, to encode backoff smoothing from a fine-grained SR-TSG to simpler CFG rules, and develop an efficient training method based on blocked MCMC sampling.","Our SR-TSG parser achieves an F1 score of 92.4% in the WSJ English Penn Treebank parsing task, which is a 7.7 point improvement over a conventional Bayesian TSG parser, and superior to state-of-the-art discriminative reranking parsers."]},{"title":"2 Background and Related Work","paragraphs":["Our SR-TSG work is built upon recent work on Bayesian TSG induction from parse trees (Post and Gildea, 2009; Cohn et al., 2010). We firstly review the Bayesian TSG model used in that work, and then present related work on TSGs and symbol refinement.","A TSG consists of a 4-tuple, G = (T, N, S, R), where T is a set of terminal symbols, N is a set of nonterminal symbols, S ∈ N is the distinguished start nonterminal symbol and R is a set of productions (a.k.a. rules). The productions take the form of elementary trees i.e., tree fragments of height ≥ 1. The root and internal nodes of the elementary trees are labeled with nonterminal symbols, and leaf nodes are labeled with either terminal or nonterminal symbols. Nonterminal leaves are referred to as frontier nonterminals, and form the substitution sites to be combined with other elementary trees.","A derivation is a process of forming a parse tree. It starts with a root symbol and rewrites (substitutes) nonterminal symbols with elementary trees until there are no remaining frontier nonterminals. Figure 1a shows an example parse tree and Figure 1b shows its example TSG derivation. Since different derivations may produce the same parse tree, recent work on TSG induction (Post and Gildea, 2009; Cohn et al., 2010) employs a probabilistic model of a TSG and predicts derivations from observed parse trees in an unsupervised way.","A Probabilistic Tree Substitution Grammar (PTSG) assigns a probability to each rule in the grammar. The probability of a derivation is defined as the product of the probabilities of its component elementary trees as follows. p (e) = ∏ x→e∈e p (e |x ) ,","where e = (e1, e2, . . .) is a sequence of elementary trees used for the derivation, x = root (e) is the root symbol of e, and p (e |x ) is the probability of generating e given its root symbol x. As in a PCFG, e is generated conditionally independent of all others given x.","The posterior distribution over elementary trees given a parse tree t can be computed by using the Bayes’ rule: p (e |t ) ∝ p (t |e ) p (e) .","where p (t |e ) is either equal to 1 (when t and e are consistent) or 0 (otherwise). Therefore, the task of TSG induction from parse trees turns out to consist of modeling the prior distribution p (e). Recent work on TSG induction defines p (e) as a nonparametric Bayesian model such as the Dirichlet Process (Ferguson, 1973) or the Pitman-Yor Process to encourage sparse and compact grammars.","Several studies have combined TSG induction and symbol refinement. An adaptor grammar (Johnson et al., 2007a) is a sort of nonparametric Bayesian TSG model with symbol refinement, and is thus closely related to our SR-TSG model. However, an adaptor grammar differs from ours in that all its rules are complete: all leaf nodes must be terminal symbols, while our model permits nonterminal symbols as leaf nodes. Furthermore, adaptor grammars have largely been applied to the task of unsupervised structural induction from raw texts such as 441 (a) (b) (c) Figure 1: (a) Example parse tree. (b) Example TSG derivation of (a). (c) Example SR-TSG derivation of (a). The refinement annotation is hyphenated with a nonterminal symbol. morphology analysis, word segmentation (Johnson and Goldwater, 2009), and dependency grammar induction (Cohen et al., 2010), rather than constituent syntax parsing.","An all-fragments grammar (Bansal and Klein, 2010) is another variant of TSG that aims to utilize all possible subtrees as rules. It maps a TSG to an implicit representation to make the grammar tractable and practical for large-scale parsing. The manual symbol refinement described in (Klein and Manning, 2003) was applied to an all-fragments grammar and this improved accuracy in the English WSJ parsing task. As mentioned in the introduc-tion, our model focuses on the automatic learning of a TSG and symbol refinement without heuristics."]},{"title":"3 Symbol-Refined Tree Substitution Grammars","paragraphs":["In this section, we propose Symbol-Refined Tree Substitution Grammars (SR-TSGs) for syntactic parsing. Our SR-TSG model is an extension of the conventional TSG model where every symbol of the elementary trees can be refined to fit the training data. Figure 1c shows an example of SR-TSG derivation. As with previous work on TSG induction, our task is the induction of SR-TSG derivations from a corpus of parse trees in an unsupervised fashion. That is, we wish to infer the symbol subcategories of every node and substitution site (i.e., nodes where substitution occurs) from parse trees. Extracted rules and their probabilities can be used to parse new raw sentences. 3.1 Probabilistic Model We define a probabilistic model of an SR-TSG based on the Pitman-Yor Process (PYP) (Pitman and Yor, 1997), namely a sort of nonparametric Bayesian model. The PYP produces power-law distributions, which have been shown to be well-suited for such uses as language modeling (Teh, 2006b), and TSG induction (Cohn et al., 2010). One major issue as regards modeling an SR-TSG is that the space of the grammar rules will be very sparse since SR-TSG allows for arbitrarily large tree fragments and also an arbitrarily large set of symbol subcategories. To address the sparseness problem, we employ a hierarchical PYP to encode a backoff scheme from the SR-TSG rules to simpler CFG rules, inspired by recent work on dependency parsing (Blunsom and Cohn, 2010).","Our model consists of a three-level hierarchy. Table 1 shows an example of the SR-TSG rule and its backoff tree fragments as an illustration of this three-level hierarchy. The topmost level of our model is a distribution over the SR-TSG rules as follows. e |xk ∼ Gxk Gxk ∼ PYP ( dxk , θxk , P sr-tsg","(· |x","k )) ,","where xk is a refined root symbol of an elementary tree e, while x is a raw nonterminal symbol in the corpus and k = 0, 1, . . . is an index of the symbol subcategory. Suppose x is NP and its symbol subcategory is 0, then xk is NP0. The PYP has three parameters: (dxk , θxk , P sr-tsg","). P sr-tsg","(· |x k ) 442 SR-TSG SR-CFG RU-CFG Table 1: Example three-level backoff. is a base distribution over infinite space of symbol-refined elementary trees rooted with xk, which provides the backoff probability of e. The remaining parameters dxk and θxk control the strength of the base distribution.","The backoff probability P sr-tsg","(e |x","k ) is given by the product of symbol-refined CFG (SR-CFG) rules that e contains as follows.","P sr-tsg (e |x k ) = ∏ f∈F (e) scf × ∏ i∈I(e) (1 − sci)","× H (cfg-rules (e |xk )) α |xk ∼ Hxk Hxk ∼ PYP ( dx, θx, P sr-cfg","(· |x","k )) ,","where F (e) is a set of frontier nonterminal nodes and I (e) is a set of internal nodes in e. cf and ci are nonterminal symbols of nodes f and i, respectively. sc is the probability of stopping the expansion of a node labeled with c. SR-CFG rules are CFG rules where every symbol is refined, as shown in Table 1. The function cfg-rules (e |xk ) returns the SR-CFG rules that e contains, which take the form of xk → α. Each SR-CFG rule α rooted with xk is drawn from the backoff distribution Hxk , and Hxk is produced by the PYP with parameters:( dx, θx, P sr-cfg)",". This distribution over the SR-CFG rules forms the second level hierarchy of our model.","The backoff probability of the SR-CFG rule, P sr-cfg","(α |x","k ), is given by the root-unrefined CFG (RU-CFG) rule as follows,","P sr-cfg (α |x k ) = I (root-unrefine (α |xk )) α |x ∼ Ix Ix ∼ PYP","(","d′ x, θ′ x, P ru-cfg","(· |x ))",",","where the function root-unrefine (α |xk ) returns the RU-CFG rule of α, which takes the form of x → α. The RU-CFG rule is a CFG rule where the root symbol is unrefined and all leaf nonterminal symbols are refined, as shown in Table 1. Each RU-CFG rule α rooted with x is drawn from the backoff distribution Ix, and Ix is produced by a PYP. This distribution over the RU-CFG rules forms the third level hierarchy of our model. Finally, we set the backoff probability of the RU-CFG rule, P ru-cfg","(α |x ), so that it is uniform as follows.","P ru-cfg","(α |x ) = 1","|x → ·| .","where |x → ·| is the number of RU-CFG rules rooted with x. Overall, our hierarchical model encodes backoff smoothing consistently from the SR-TSG rules to the SR-CFG rules, and from the SR-CFG rules to the RU-CFG rules. As shown in (Blunsom and Cohn, 2010; Cohen et al., 2010), the parsing accuracy of the TSG model is strongly affected by its backoff model. The effects of our hierarchical backoff model on parsing performance are evaluated in Section 5."]},{"title":"4 Inference","paragraphs":["We use Markov Chain Monte Carlo (MCMC) sampling to infer the SR-TSG derivations from parse trees. MCMC sampling is a widely used approach for obtaining random samples from a probability distribution. In our case, we wish to obtain derivation samples of an SR-TSG from the posterior distribution, p (e |t, d, θ, s ).","The inference of the SR-TSG derivations corresponds to inferring two kinds of latent variables: latent symbol subcategories and latent substitution 443 sites. We first infer latent symbol subcategories for every symbol in the parse trees, and then infer latent substitution sites stepwise. During the inference of symbol subcategories, every internal node is fixed as a substitution site. After that, we unfix that assump-tion and infer latent substitution sites given symbol-refined parse trees. This stepwise learning is simple and efficient in practice, but we believe that the joint learning of both latent variables is possible, and we will deal with this in future work. Here we describe each inference algorithm in detail. 4.1 Inference of Symbol Subcategories For the inference of latent symbol subcategories, we adopt split and merge training (Petrov et al., 2006) as follows. In each split-merge step, each symbol is split into at most two subcategories. For example, every NP symbol in the training data is split into either NP0 or NP1 to maximize the posterior probability. After convergence, we measure the loss of each split symbol in terms of the likelihood incurred when removing it, then the smallest 50% of the newly split symbols as regards that loss are merged to avoid overfitting. The split-merge algorithm terminates when the total number of steps reaches the user-specified value.","In each splitting step, we use two types of blocked MCMC algorithm: the sentence-level blocked Metroporil-Hastings (MH) sampler and the tree-level blocked Gibbs sampler, while (Petrov et al., 2006) use a different MLE-based model and the EM algorithm. Our sampler iterates sentence-level sampling and tree-level sampling alternately.","The sentence-level MH sampler is a recently proposed algorithm for grammar induction (Johnson et al., 2007b; Cohn et al., 2010). In this work, we apply it to the training of symbol splitting. The MH sampler consists of the following three steps: for each sentence, 1) calculate the inside probability (Lari and Young, 1991) in a bottom-up manner, 2) sample a derivation tree in a top-down manner, and 3) accept or reject the derivation sample by using the MH test. See (Cohn et al., 2010) for details. This sampler simultaneously updates blocks of latent variables associated with a sentence, thus it can find MAP solutions efficiently.","The tree-level blocked Gibbs sampler focuses on the type of SR-TSG rules and simultaneously updates all root and child nodes that are annotated with the same SR-TSG rule. For example, the sampler collects all nodes that are annotated with S0 → NP1VP2, then updates those nodes to another subcategory such as S0 → NP2VP0 according to the posterior distribution. This sampler is similar to table label resampling (Johnson and Goldwater, 2009), but differs in that our sampler can update multiple table labels simultaneously when multiple tables are labeled with the same elementary tree. The tree-level sampler also simultaneously updates blocks of latent variables associated with the type of SR-TSG rules, thus it can find MAP solutions efficiently. 4.2 Inference of Substitution Sites After the inference of symbol subcategories, we use Gibbs sampling to infer the substitution sites of parse trees as described in (Cohn and Lapata, 2009; Post and Gildea, 2009). We assign a binary variable to each internal node in the training data, which in-dicates whether that node is a substitution site or not. For each iteration, the Gibbs sampler works by sampling the value of each binary variable in random order. See (Cohn et al., 2010) for details.","During the inference, our sampler ignores the symbol subcategories of internal nodes of elementary trees since they do not affect the derivation of the SR-TSG. For example, the elementary trees “(S0 (NP0 NNP0) VP0)” and “(S0 (NP1 NNP0) VP0)” are regarded as being the same when we calculate the generation probabilities according to our model. This heuristics is helpful for finding large tree fragments and learning compact grammars. 4.3 Hyperparameter Estimation We treat hyperparameters {d, θ} as random variables and update their values for every MCMC it-eration. We place a prior on the hyperparameters as follows: d ∼ Beta (1, 1), θ ∼ Gamma (1, 1). The values of d and θ are optimized with the auxiliary variable technique (Teh, 2006a). 444"]},{"title":"5 Experiment 5.1 Settings 5.1.1 Data Preparation","paragraphs":["We ran experiments on the Wall Street Journal (WSJ) portion of the English Penn Treebank data set (Marcus et al., 1993), using a standard data split (sections 2–21 for training, 22 for development and 23 for testing). We also used section 2 as a small training set for evaluating the performance of our model under low-resource conditions. Henceforth, we distinguish the small training set (section 2) from the full training set (sections 2-21). The treebank data is right-binarized (Matsuzaki et al., 2005) to construct grammars with only unary and binary productions. We replace lexical words with count ≤ 5 in the training data with one of 50 unknown words using lexical features, following (Petrov et al., 2006). We also split off all the function tags and eliminated empty nodes from the data set, following (Johnson, 1998). 5.1.2 Training and Parsing","For the inference of symbol subcategories, we trained our model with the MCMC sampler by using 6 split-merge steps for the full training set and 3 split-merge steps for the small training set. Therefore, each symbol can be subdivided into a maximum of 26","= 64 and 23","= 8 subcategories, respectively. In each split-merge step, we initialized the sampler by randomly splitting every symbol in two subcategories and ran the MCMC sampler for 1000 iterations. After that, to infer the substitution sites, we initialized the model with the final sample from a run on the small training set, and used the Gibbs sampler for 2000 iterations. We estimated the optimal values of the stopping probabilities s by using the development set.","We obtained the parsing results with the MAX-RULE-PRODUCT algorithm (Petrov et al., 2006) by using the SR-TSG rules extracted from our model. We evaluated the accuracy of our parser by bracketing F1 score of predicted parse trees. We used EVALB1","to compute the F1 score. In all our experiments, we conducted ten independent runs to train our model, and selected the one that performed best on the development set in terms of parsing accuracy.","1","http://nlp.cs.nyu.edu/evalb/ Model F1 (small) F1 (full) CFG 61.9 63.6","*TSG 77.1 85.0","SR-TSG (P sr-tsg ) 73.0 86.4","SR-TSG (P sr-tsg , P sr-cfg",") 79.4 89.7","SR-TSG (P sr-tsg , P sr-cfg",", P ru-cfg",") 81.7 91.1 Table 2: Comparison of parsing accuracy with the small and full training sets. *Our reimplementation of (Cohn et al., 2010). Figure 2: Histogram of SR-TSG and TSG rule sizes on the small training set. The size is defined as the number of CFG rules that the elementary tree contains. 5.2 Results and Discussion 5.2.1 Comparison of SR-TSG with TSG","We compared the SR-TSG model with the CFG and TSG models as regards parsing accuracy. We also tested our model with three backoff hierarchy settings to evaluate the effects of backoff smoothing on parsing accuracy. Table 2 shows the F1 scores of the CFG, TSG and SR-TSG parsers for small and full training sets. In Table 2, SR-TSG (P sr-tsg",") denotes that we used only the topmost level of the hierarchy. Similary, SR-TSG (P sr-tsg",", P sr-cfg",") denotes that we used only the P sr-tsg","and P sr-cfg","backoff models.","Our best model, SR-TSG (P sr-tsg",", P sr-cfg",", P ru-cfg","), outperformed both the CFG and TSG models on both the small and large training sets. This result suggests that the conventional TSG model trained from the vanilla treebank is insufficient to resolve 445 Model F1 (≤ 40) F1 (all) TSG (no symbol refinement) Post and Gildea (2009) 82.6 - Cohn et al. (2010) 85.4 84.7 TSG with Symbol Refinement Zuidema (2007) - *83.8 Bansal et al. (2010) 88.7 88.1 SR-TSG (single) 91.6 91.1 SR-TSG (multiple) 92.9 92.4 CFG with Symbol Refinement Collins (1999) 88.6 88.2 Petrov and Klein (2007) 90.6 90.1 Petrov (2010) - 91.8 Discriminative Carreras et al. (2008) - 91.1 Charniak and Johnson (2005) 92.0 91.4 Huang (2008) 92.3 91.7 Table 3: Our parsing performance for the testing set compared with those of other parsers. *Results for the development set (≤ 100). structural ambiguities caused by coarse symbol an-notations in a training corpus. As we expected, symbol refinement can be helpful with the TSG model for further fitting the training set and improving the parsing accuracy.","The performance of the SR-TSG parser was strongly affected by its backoff models. For example, the simplest model, P sr-tsg",", performed poorly compared with our best model. This result suggests that the SR-TSG rules extracted from the training set are very sparse and cannot cover the space of unknown syntax patterns in the testing set. Therefore, sophisticated backoff modeling is essential for the SR-TSG parser. Our hierarchical PYP modeling technique is a successful way to achieve backoff smoothing from sparse SR-TSG rules to simpler CFG rules, and offers the advantage of automatically estimating the optimal backoff probabilities from the training set.","We compared the rule sizes and frequencies of SR-TSG with those of TSG. The rule sizes of SR-TSG and TSG are defined as the number of CFG rules that the elementary tree contains. Figure 2 shows a histogram of the SR-TSG and TSG rule sizes (by unrefined token) on the small training set. For example, SR-TSG rules: S1 → NP0VP1 and S0 → NP1VP2 were considered to be the same to-ken. In Figure 2, we can see that there are almost the same number of SR-TSG rules and TSG rules with size = 1. However, there are more SR-TSG rules than TSG rules with size ≥ 2. This shows that an SR-TSG can use various large tree fragments depending on the context, which is specified by the symbol subcategories.","5.2.2 Comparison of SR-TSG with Other Models","We compared the accuracy of the SR-TSG parser with that of conventional high-performance parsers. Table 3 shows the F1 scores of an SR-TSG and conventional parsers with the full training set. In Table 3, SR-TSG (single) is a standard SR-TSG parser, 446 and SR-TSG (multiple) is a combination of sixteen independently trained SR-TSG models, following the work of (Petrov, 2010).","Our SR-TSG (single) parser achieved an F1 score of 91.1%, which is a 6.4 point improvement over the conventional Bayesian TSG parser reported by (Cohn et al., 2010). Our model can be viewed as an extension of Cohn’s work by the incorporation of symbol refinement. Therefore, this result confirms that a TSG and symbol refinement work complementarily in improving parsing accuracy. Compared with a symbol-refined CFG model such as the Berkeley parser (Petrov et al., 2006), the SR-TSG model can use large tree fragments, which strengthens the probability of frequent syntax patterns in the training set. Indeed, the few very large rules of our model memorized full parse trees of sentences, which were repeated in the training set.","The SR-TSG (single) is a pure generative model of syntax trees but it achieved results comparable to those of discriminative parsers. It should be noted that discriminative reranking parsers such as (Charniak and Johnson, 2005) and (Huang, 2008) are constructed on a generative parser. The reranking parser takes the k-best lists of candidate trees or a packed forest produced by a baseline parser (usually a generative model), and then reranks the candidates using arbitrary features. Hence, we can expect that combining our SR-TSG model with a discriminative reranking parser would provide better performance than SR-TSG alone.","Recently, (Petrov, 2010) has reported that combining multiple grammars trained independently gives significantly improved performance over a single grammar alone. We applied his method (referred to as a TREE-LEVEL inference) to the SR-TSG model as follows. We first trained sixteen SR-TSG models independently and produced a 100-best list of the derivations for each model. Then, we erased the subcategory information of parse trees and selected the best tree that achieved the highest likelihood under the product of sixteen models. The combination model, SR-TSG (multiple), achieved an F1 score of 92.4%, which is a state-of-the-art result for the WSJ parsing task. Compared with discriminative reranking parsers, combining multiple grammars by using the product model provides the advantage that it does not require any additional training. Several studies (Fossum and Knight, 2009; Zhang et al., 2009) have proposed different approaches that involve combining k-best lists of candidate trees. We will deal with those methods in future work.","Let us note the relation between SR-CFG, TSG and SR-TSG. TSG is weakly equivalent to CFG and generates the same set of strings. For example, the TSG rule “S → (NP NNP) VP” with probability p can be converted to the equivalent CFG rules as follows: “S → NPNNP","VP ” with probability p and “NPNNP","→ NNP” with probability 1. From this viewpoint, TSG utilizes surrounding symbols (NNP of NPNNP","in the above example) as latent variables with which to capture context information. The search space of learning a TSG given a parse tree is O (2n",") where n is the number of internal nodes of the parse tree. On the other hand, an SR-CFG utilizes an arbitrary index such as 0, 1, . . . as latent variables and the search space is larger than that of a TSG when the symbol refinement model allows for more than two subcategories for each symbol. Our experimental results comfirm that jointly modeling both latent variables using our SR-TSG assists accurate parsing."]},{"title":"6 Conclusion","paragraphs":["We have presented an SR-TSG, which is an extension of the conventional TSG model where each symbol of tree fragments can be automatically subcategorized to address the problem of the conditional independence assumptions of a TSG. We proposed a novel backoff modeling of an SR-TSG based on the hierarchical Pitman-Yor Process and sentence-level and tree-level blocked MCMC sampling for training our model. Our best model significantly outperformed the conventional TSG and achieved state-of-the-art result in a WSJ parsing task. Future work will involve examining the SR-TSG model for different languages and for unsupervised grammar induction."]},{"title":"Acknowledgements","paragraphs":["We would like to thank Liang Huang for helpful comments and the three anonymous reviewers for thoughtful suggestions. We would also like to thank Slav Petrov and Hui Zhang for answering our ques-tions about their parsers. 447"]},{"title":"References","paragraphs":["Mohit Bansal and Dan Klein. 2010. Simple, Accurate Parsing with an All-Fragments Grammar. In In Proc. of ACL, pages 1098–1107.","Phil Blunsom and Trevor Cohn. 2010. Unsupervised Induction of Tree Substitution Grammars for Dependency Parsing. In Proc. of EMNLP, pages 1204–1213.","Eugene Charniak and Mark Johnson. 2005. Coarse-to-Fine n-Best Parsing and MaxEnt Discriminative Reranking. In Proc. of ACL, 1:173–180.","Shay B Cohen, David M Blei, and Noah A Smith. 2010. Variational Inference for Adaptor Grammars. In In Proc. of HLT-NAACL, pages 564–572.","Trevor Cohn and Mirella Lapata. 2009. Sentence Compression as Tree Transduction. Journal of Artificial Intelligence Research, 34:637–674.","Trevor Cohn, Phil Blunsom, and Sharon Goldwater. 2010. Inducing Tree-Substitution Grammars. Journal of Machine Learning Research, 11:3053–3096.","Michael Collins. 2003. Head-Driven Statistical Models for Natural Language Parsing. Computational Linguistics, 29:589–637.","Steve DeNeefe and Kevin Knight. 2009. Synchronous Tree Adjoining Machine Translation. In Proc. of EMNLP, page 727.","Thomas S Ferguson. 1973. A Bayesian Analysis of Some Nonparametric Problems. Annals of Statistics, 1:209–230.","Victoria Fossum and Kevin Knight. 2009. Combining Constituent Parsers. In Proc. of HLT-NAACL, pages 253–256.","Michel Galley, Mark Hopkins, Kevin Knight, Daniel Marcu, Los Angeles, and Marina Del Rey. 2004. What’s in a Translation Rule? Information Sciences, pages 273–280.","Liang Huang. 2008. Forest Reranking : Discriminative Parsing with Non-Local Features. In Proc. of ACL, 19104:0.","Mark Johnson and Sharon Goldwater. 2009. Improving nonparameteric Bayesian inference: experiments on unsupervised word segmentation with adaptor grammars. In In Proc. of HLT-NAACL, pages 317–325.","Mark Johnson, Thomas L Griffiths, and Sharon Goldwater. 2007a. Adaptor Grammars : A Frame-work for Specifying Compositional Nonparametric Bayesian Models. Advances in Neural Information Processing Systems 19, 19:641–648.","Mark Johnson, Thomas L Griffiths, and Sharon Goldwater. 2007b. Bayesian Inference for PCFGs via Markov chain Monte Carlo. In In Proc. of HLT-NAACL, pages 139–146.","Mark Johnson. 1998. PCFG Models of Linguistic Tree Representations. Computational Linguistics, 24:613– 632.","Dan Klein and Christopher D Manning. 2003. Accurate Unlexicalized Parsing. In Proc. of ACL, 1:423–430.","K Lari and S J Young. 1991. Applications of Stochastic Context-Free Grammars Using the Inside–Outside Algorithm. Computer Speech and Language, 5:237– 257.","Mitchell P Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a Large Annotated Corpus of English: The Penn Treebank. Computational Linguistics, 19:313–330.","Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsujii. 2005. Probabilistic CFG with latent annotations. In Proc. of ACL, pages 75–82.","Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning Accurate, Compact, and In-terpretable Tree Annotation. In Proc. of ACL, pages 433–440.","Slav Petrov. 2010. Products of Random Latent Variable Grammars. In Proc. of HLT-NAACL, pages 19–27.","Jim Pitman and Marc Yor. 1997. The two-parameter Poisson-Dirichlet distribution derived from a stable subordinator. The Annals of Probability, 25:855–900.","Matt Post and Daniel Gildea. 2009. Bayesian Learning of a Tree Substitution Grammar. In In Proc. of ACL-IJCNLP, pages 45–48.","Yee Whye Teh. 2006a. A Bayesian Interpretation of Interpolated Kneser-Ney. NUS School of Computing Technical Report TRA2/06.","YW Teh. 2006b. A Hierarchical Bayesian Language Model based on Pitman-Yor Processes. In Proc. of ACL, 44:985–992.","J Tenenbaum, TJ O’Donnell, and ND Goodman. 2009. Fragment Grammars: Exploring Computation and Reuse in Language. MIT Computer Science and Artificial Intelligence Laboratory Technical Report Series.","Mengqiu Wang, Noah A Smith, and Teruko Mitamura. 2007. What is the Jeopardy Model ? A Quasi-Synchronous Grammar for QA. In Proc. of EMNLP-CoNLL, pages 22–32.","Elif Yamangil and Stuart M Shieber. 2010. Bayesian Synchronous Tree-Substitution Grammar Induction and Its Application to Sentence Compression. In In Proc. of ACL, pages 937–947.","Hui Zhang, Min Zhang, Chew Lim Tan, and Haizhou Li. 2009. K-Best Combination of Syntactic Parsers. In Proc. of EMNLP, pages 1552–1560.","Willem Zuidema. 2007. Parsimonious Data-Oriented Parsing. In Proc. of EMNLP-CoNLL, pages 551–560. 448"]}],"references":[{"authors":[{"first":"Mohit","last":"Bansal"},{"first":"Dan","last":"Klein"}],"year":"2010","title":"Simple, Accurate Parsing with an All-Fragments Grammar","source":"Mohit Bansal and Dan Klein. 2010. Simple, Accurate Parsing with an All-Fragments Grammar. In In Proc. of ACL, pages 1098–1107."},{"authors":[{"first":"Phil","last":"Blunsom"},{"first":"Trevor","last":"Cohn"}],"year":"2010","title":"Unsupervised Induction of Tree Substitution Grammars for Dependency Parsing","source":"Phil Blunsom and Trevor Cohn. 2010. Unsupervised Induction of Tree Substitution Grammars for Dependency Parsing. In Proc. of EMNLP, pages 1204–1213."},{"authors":[{"first":"Eugene","last":"Charniak"},{"first":"Mark","last":"Johnson"}],"year":"2005","title":"Coarse-to-Fine n-Best Parsing and MaxEnt Discriminative Reranking","source":"Eugene Charniak and Mark Johnson. 2005. Coarse-to-Fine n-Best Parsing and MaxEnt Discriminative Reranking. In Proc. of ACL, 1:173–180."},{"authors":[{"first":"Shay","middle":"B","last":"Cohen"},{"first":"David","middle":"M","last":"Blei"},{"first":"Noah","middle":"A","last":"Smith"}],"year":"2010","title":"Variational Inference for Adaptor Grammars","source":"Shay B Cohen, David M Blei, and Noah A Smith. 2010. Variational Inference for Adaptor Grammars. In In Proc. of HLT-NAACL, pages 564–572."},{"authors":[{"first":"Trevor","last":"Cohn"},{"first":"Mirella","last":"Lapata"}],"year":"2009","title":"Sentence Compression as Tree Transduction","source":"Trevor Cohn and Mirella Lapata. 2009. Sentence Compression as Tree Transduction. Journal of Artificial Intelligence Research, 34:637–674."},{"authors":[{"first":"Trevor","last":"Cohn"},{"first":"Phil","last":"Blunsom"},{"first":"Sharon","last":"Goldwater"}],"year":"2010","title":"Inducing Tree-Substitution Grammars","source":"Trevor Cohn, Phil Blunsom, and Sharon Goldwater. 2010. Inducing Tree-Substitution Grammars. Journal of Machine Learning Research, 11:3053–3096."},{"authors":[{"first":"Michael","last":"Collins"}],"year":"2003","title":"Head-Driven Statistical Models for Natural Language Parsing","source":"Michael Collins. 2003. Head-Driven Statistical Models for Natural Language Parsing. Computational Linguistics, 29:589–637."},{"authors":[{"first":"Steve","last":"DeNeefe"},{"first":"Kevin","last":"Knight"}],"year":"2009","title":"Synchronous Tree Adjoining Machine Translation","source":"Steve DeNeefe and Kevin Knight. 2009. Synchronous Tree Adjoining Machine Translation. In Proc. of EMNLP, page 727."},{"authors":[{"first":"Thomas","middle":"S","last":"Ferguson"}],"year":"1973","title":"A Bayesian Analysis of Some Nonparametric Problems","source":"Thomas S Ferguson. 1973. A Bayesian Analysis of Some Nonparametric Problems. Annals of Statistics, 1:209–230."},{"authors":[{"first":"Victoria","last":"Fossum"},{"first":"Kevin","last":"Knight"}],"year":"2009","title":"Combining Constituent Parsers","source":"Victoria Fossum and Kevin Knight. 2009. Combining Constituent Parsers. In Proc. of HLT-NAACL, pages 253–256."},{"authors":[{"first":"Michel","last":"Galley"},{"first":"Mark","last":"Hopkins"},{"first":"Kevin","last":"Knight"},{"first":"Daniel","last":"Marcu"},{"first":"Los","last":"Angeles"},{"first":"Marina","middle":"Del","last":"Rey"}],"year":"2004","title":"What’s in a Translation Rule? Information Sciences, pages 273–280","source":"Michel Galley, Mark Hopkins, Kevin Knight, Daniel Marcu, Los Angeles, and Marina Del Rey. 2004. What’s in a Translation Rule? Information Sciences, pages 273–280."},{"authors":[{"first":"Liang","last":"Huang"}],"year":"2008","title":"Forest Reranking : Discriminative Parsing with Non-Local Features","source":"Liang Huang. 2008. Forest Reranking : Discriminative Parsing with Non-Local Features. In Proc. of ACL, 19104:0."},{"authors":[{"first":"Mark","last":"Johnson"},{"first":"Sharon","last":"Goldwater"}],"year":"2009","title":"Improving nonparameteric Bayesian inference: experiments on unsupervised word segmentation with adaptor grammars","source":"Mark Johnson and Sharon Goldwater. 2009. Improving nonparameteric Bayesian inference: experiments on unsupervised word segmentation with adaptor grammars. In In Proc. of HLT-NAACL, pages 317–325."},{"authors":[{"first":"Mark","last":"Johnson"},{"first":"Thomas","middle":"L","last":"Griffiths"},{"first":"Sharon","last":"Goldwater"}],"year":"2007a","title":"Adaptor Grammars : A Frame-work for Specifying Compositional Nonparametric Bayesian Models","source":"Mark Johnson, Thomas L Griffiths, and Sharon Goldwater. 2007a. Adaptor Grammars : A Frame-work for Specifying Compositional Nonparametric Bayesian Models. Advances in Neural Information Processing Systems 19, 19:641–648."},{"authors":[{"first":"Mark","last":"Johnson"},{"first":"Thomas","middle":"L","last":"Griffiths"},{"first":"Sharon","last":"Goldwater"}],"year":"2007b","title":"Bayesian Inference for PCFGs via Markov chain Monte Carlo","source":"Mark Johnson, Thomas L Griffiths, and Sharon Goldwater. 2007b. Bayesian Inference for PCFGs via Markov chain Monte Carlo. In In Proc. of HLT-NAACL, pages 139–146."},{"authors":[{"first":"Mark","last":"Johnson"}],"year":"1998","title":"PCFG Models of Linguistic Tree Representations","source":"Mark Johnson. 1998. PCFG Models of Linguistic Tree Representations. Computational Linguistics, 24:613– 632."},{"authors":[{"first":"Dan","last":"Klein"},{"first":"Christopher","middle":"D","last":"Manning"}],"year":"2003","title":"Accurate Unlexicalized Parsing","source":"Dan Klein and Christopher D Manning. 2003. Accurate Unlexicalized Parsing. In Proc. of ACL, 1:423–430."},{"authors":[{"first":"K","last":"Lari"},{"first":"S","middle":"J","last":"Young"}],"year":"1991","title":"Applications of Stochastic Context-Free Grammars Using the Inside–Outside Algorithm","source":"K Lari and S J Young. 1991. Applications of Stochastic Context-Free Grammars Using the Inside–Outside Algorithm. Computer Speech and Language, 5:237– 257."},{"authors":[{"first":"Mitchell","middle":"P","last":"Marcus"},{"first":"Beatrice","last":"Santorini"},{"first":"Mary","middle":"Ann","last":"Marcinkiewicz"}],"year":"1993","title":"Building a Large Annotated Corpus of English: The Penn Treebank","source":"Mitchell P Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a Large Annotated Corpus of English: The Penn Treebank. Computational Linguistics, 19:313–330."},{"authors":[{"first":"Takuya","last":"Matsuzaki"},{"first":"Yusuke","last":"Miyao"},{"first":"Jun’ichi","last":"Tsujii"}],"year":"2005","title":"Probabilistic CFG with latent annotations","source":"Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsujii. 2005. Probabilistic CFG with latent annotations. In Proc. of ACL, pages 75–82."},{"authors":[{"first":"Slav","last":"Petrov"},{"first":"Leon","last":"Barrett"},{"first":"Romain","last":"Thibaux"},{"first":"Dan","last":"Klein"}],"year":"2006","title":"Learning Accurate, Compact, and In-terpretable Tree Annotation","source":"Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning Accurate, Compact, and In-terpretable Tree Annotation. In Proc. of ACL, pages 433–440."},{"authors":[{"first":"Slav","last":"Petrov"}],"year":"2010","title":"Products of Random Latent Variable Grammars","source":"Slav Petrov. 2010. Products of Random Latent Variable Grammars. In Proc. of HLT-NAACL, pages 19–27."},{"authors":[{"first":"Jim","last":"Pitman"},{"first":"Marc","last":"Yor"}],"year":"1997","title":"The two-parameter Poisson-Dirichlet distribution derived from a stable subordinator","source":"Jim Pitman and Marc Yor. 1997. The two-parameter Poisson-Dirichlet distribution derived from a stable subordinator. The Annals of Probability, 25:855–900."},{"authors":[{"first":"Matt","last":"Post"},{"first":"Daniel","last":"Gildea"}],"year":"2009","title":"Bayesian Learning of a Tree Substitution Grammar","source":"Matt Post and Daniel Gildea. 2009. Bayesian Learning of a Tree Substitution Grammar. In In Proc. of ACL-IJCNLP, pages 45–48."},{"authors":[{"first":"Yee","middle":"Whye","last":"Teh"}],"year":"2006a","title":"A Bayesian Interpretation of Interpolated Kneser-Ney","source":"Yee Whye Teh. 2006a. A Bayesian Interpretation of Interpolated Kneser-Ney. NUS School of Computing Technical Report TRA2/06."},{"authors":[{"first":"YW","last":"Teh"}],"year":"2006b","title":"A Hierarchical Bayesian Language Model based on Pitman-Yor Processes","source":"YW Teh. 2006b. A Hierarchical Bayesian Language Model based on Pitman-Yor Processes. In Proc. of ACL, 44:985–992."},{"authors":[{"first":"J","last":"Tenenbaum"},{"first":"TJ","last":"O’Donnell"},{"first":"ND","last":"Goodman"}],"year":"2009","title":"Fragment Grammars: Exploring Computation and Reuse in Language","source":"J Tenenbaum, TJ O’Donnell, and ND Goodman. 2009. Fragment Grammars: Exploring Computation and Reuse in Language. MIT Computer Science and Artificial Intelligence Laboratory Technical Report Series."},{"authors":[{"first":"Mengqiu","last":"Wang"},{"first":"Noah","middle":"A","last":"Smith"},{"first":"Teruko","last":"Mitamura"}],"year":"2007","title":"What is the Jeopardy Model ? A Quasi-Synchronous Grammar for QA","source":"Mengqiu Wang, Noah A Smith, and Teruko Mitamura. 2007. What is the Jeopardy Model ? A Quasi-Synchronous Grammar for QA. In Proc. of EMNLP-CoNLL, pages 22–32."},{"authors":[{"first":"Elif","last":"Yamangil"},{"first":"Stuart","middle":"M","last":"Shieber"}],"year":"2010","title":"Bayesian Synchronous Tree-Substitution Grammar Induction and Its Application to Sentence Compression","source":"Elif Yamangil and Stuart M Shieber. 2010. Bayesian Synchronous Tree-Substitution Grammar Induction and Its Application to Sentence Compression. In In Proc. of ACL, pages 937–947."},{"authors":[{"first":"Hui","last":"Zhang"},{"first":"Min","last":"Zhang"},{"first":"Chew","middle":"Lim","last":"Tan"},{"first":"Haizhou","last":"Li"}],"year":"2009","title":"K-Best Combination of Syntactic Parsers","source":"Hui Zhang, Min Zhang, Chew Lim Tan, and Haizhou Li. 2009. K-Best Combination of Syntactic Parsers. In Proc. of EMNLP, pages 1552–1560."},{"authors":[{"first":"Willem","last":"Zuidema"}],"year":"2007","title":"Parsimonious Data-Oriented Parsing","source":"Willem Zuidema. 2007. Parsimonious Data-Oriented Parsing. In Proc. of EMNLP-CoNLL, pages 551–560. 448"}],"cites":[{"style":0,"text":"Galley et al., 2004","origin":{"pointer":"/sections/7/paragraphs/0","offset":175,"length":19},"authors":[{"last":"Galley"},{"last":"al."}],"year":"2004","references":["/references/10"]},{"style":0,"text":"DeNeefe and Knight, 2009","origin":{"pointer":"/sections/7/paragraphs/0","offset":196,"length":24},"authors":[{"last":"DeNeefe"},{"last":"Knight"}],"year":"2009","references":["/references/7"]},{"style":0,"text":"Cohn and Lapata, 2009","origin":{"pointer":"/sections/7/paragraphs/0","offset":245,"length":21},"authors":[{"last":"Cohn"},{"last":"Lapata"}],"year":"2009","references":["/references/4"]},{"style":0,"text":"Yamangil and Shieber, 2010","origin":{"pointer":"/sections/7/paragraphs/0","offset":268,"length":26},"authors":[{"last":"Yamangil"},{"last":"Shieber"}],"year":"2010","references":["/references/28"]},{"style":0,"text":"Wang et al., 2007","origin":{"pointer":"/sections/7/paragraphs/0","offset":322,"length":17},"authors":[{"last":"Wang"},{"last":"al."}],"year":"2007","references":["/references/27"]},{"style":0,"text":"Klein and Manning, 2003","origin":{"pointer":"/sections/7/paragraphs/0","offset":597,"length":23},"authors":[{"last":"Klein"},{"last":"Manning"}],"year":"2003","references":["/references/16"]},{"style":0,"text":"Post and Gildea, 2009","origin":{"pointer":"/sections/7/paragraphs/1","offset":142,"length":21},"authors":[{"last":"Post"},{"last":"Gildea"}],"year":"2009","references":["/references/23"]},{"style":0,"text":"Tenenbaum et al., 2009","origin":{"pointer":"/sections/7/paragraphs/1","offset":165,"length":22},"authors":[{"last":"Tenenbaum"},{"last":"al."}],"year":"2009","references":["/references/26"]},{"style":0,"text":"Cohn et al., 2010","origin":{"pointer":"/sections/7/paragraphs/1","offset":189,"length":17},"authors":[{"last":"Cohn"},{"last":"al."}],"year":"2010","references":["/references/5"]},{"style":0,"text":"Cohn et al., 2010","origin":{"pointer":"/sections/7/paragraphs/1","offset":532,"length":17},"authors":[{"last":"Cohn"},{"last":"al."}],"year":"2010","references":["/references/5"]},{"style":0,"text":"Cohn et al., 2010","origin":{"pointer":"/sections/7/paragraphs/1","offset":582,"length":17},"authors":[{"last":"Cohn"},{"last":"al."}],"year":"2010","references":["/references/5"]},{"style":0,"text":"Post and Gildea, 2009","origin":{"pointer":"/sections/7/paragraphs/1","offset":601,"length":21},"authors":[{"last":"Post"},{"last":"Gildea"}],"year":"2009","references":["/references/23"]},{"style":0,"text":"Bansal and Klein, 2010","origin":{"pointer":"/sections/7/paragraphs/1","offset":624,"length":22},"authors":[{"last":"Bansal"},{"last":"Klein"}],"year":"2010","references":["/references/0"]},{"style":0,"text":"Petrov et al., 2006","origin":{"pointer":"/sections/7/paragraphs/1","offset":839,"length":19},"authors":[{"last":"Petrov"},{"last":"al."}],"year":"2006","references":["/references/20"]},{"style":0,"text":"Charniak and Johnson, 2005","origin":{"pointer":"/sections/7/paragraphs/1","offset":885,"length":26},"authors":[{"last":"Charniak"},{"last":"Johnson"}],"year":"2005","references":["/references/2"]},{"style":0,"text":"Johnson, 1998","origin":{"pointer":"/sections/7/paragraphs/2","offset":86,"length":13},"authors":[{"last":"Johnson"}],"year":"1998","references":["/references/15"]},{"style":0,"text":"Collins, 2003","origin":{"pointer":"/sections/7/paragraphs/2","offset":101,"length":13},"authors":[{"last":"Collins"}],"year":"2003","references":["/references/6"]},{"style":0,"text":"Matsuzaki et al., 2005","origin":{"pointer":"/sections/7/paragraphs/2","offset":116,"length":22},"authors":[{"last":"Matsuzaki"},{"last":"al."}],"year":"2005","references":["/references/19"]},{"style":0,"text":"Zuidema, 2007","origin":{"pointer":"/sections/7/paragraphs/2","offset":389,"length":13},"authors":[{"last":"Zuidema"}],"year":"2007","references":["/references/30"]},{"style":0,"text":"Bansal and Klein, 2010","origin":{"pointer":"/sections/7/paragraphs/2","offset":404,"length":22},"authors":[{"last":"Bansal"},{"last":"Klein"}],"year":"2010","references":["/references/0"]},{"style":0,"text":"Bansal and Klein (2010)","origin":{"pointer":"/sections/7/paragraphs/2","offset":533,"length":23},"authors":[{"last":"Bansal"},{"last":"Klein"}],"year":"2010","references":["/references/0"]},{"style":0,"text":"Pitman and Yor, 1997","origin":{"pointer":"/sections/7/paragraphs/3","offset":524,"length":20},"authors":[{"last":"Pitman"},{"last":"Yor"}],"year":"1997","references":["/references/22"]},{"style":0,"text":"Post and Gildea, 2009","origin":{"pointer":"/sections/8/paragraphs/0","offset":86,"length":21},"authors":[{"last":"Post"},{"last":"Gildea"}],"year":"2009","references":["/references/23"]},{"style":0,"text":"Cohn et al., 2010","origin":{"pointer":"/sections/8/paragraphs/0","offset":109,"length":17},"authors":[{"last":"Cohn"},{"last":"al."}],"year":"2010","references":["/references/5"]},{"style":0,"text":"Post and Gildea, 2009","origin":{"pointer":"/sections/8/paragraphs/2","offset":378,"length":21},"authors":[{"last":"Post"},{"last":"Gildea"}],"year":"2009","references":["/references/23"]},{"style":0,"text":"Cohn et al., 2010","origin":{"pointer":"/sections/8/paragraphs/2","offset":401,"length":17},"authors":[{"last":"Cohn"},{"last":"al."}],"year":"2010","references":["/references/5"]},{"style":0,"text":"Ferguson, 1973","origin":{"pointer":"/sections/8/paragraphs/6","offset":310,"length":14},"authors":[{"last":"Ferguson"}],"year":"1973","references":["/references/8"]},{"style":0,"text":"Johnson et al., 2007a","origin":{"pointer":"/sections/8/paragraphs/7","offset":87,"length":21},"authors":[{"last":"Johnson"},{"last":"al."}],"year":"2007a","references":["/references/13"]},{"style":0,"text":"Johnson and Goldwater, 2009","origin":{"pointer":"/sections/8/paragraphs/7","offset":769,"length":27},"authors":[{"last":"Johnson"},{"last":"Goldwater"}],"year":"2009","references":["/references/12"]},{"style":0,"text":"Cohen et al., 2010","origin":{"pointer":"/sections/8/paragraphs/7","offset":833,"length":18},"authors":[{"last":"Cohen"},{"last":"al."}],"year":"2010","references":["/references/3"]},{"style":0,"text":"Bansal and Klein, 2010","origin":{"pointer":"/sections/8/paragraphs/8","offset":26,"length":22},"authors":[{"last":"Bansal"},{"last":"Klein"}],"year":"2010","references":["/references/0"]},{"style":0,"text":"Klein and Manning, 2003","origin":{"pointer":"/sections/8/paragraphs/8","offset":285,"length":23},"authors":[{"last":"Klein"},{"last":"Manning"}],"year":"2003","references":["/references/16"]},{"style":0,"text":"Pitman and Yor, 1997","origin":{"pointer":"/sections/9/paragraphs/0","offset":777,"length":20},"authors":[{"last":"Pitman"},{"last":"Yor"}],"year":"1997","references":["/references/22"]},{"style":0,"text":"Teh, 2006b","origin":{"pointer":"/sections/9/paragraphs/0","offset":965,"length":10},"authors":[{"last":"Teh"}],"year":"2006b","references":["/references/25"]},{"style":0,"text":"Cohn et al., 2010","origin":{"pointer":"/sections/9/paragraphs/0","offset":997,"length":17},"authors":[{"last":"Cohn"},{"last":"al."}],"year":"2010","references":["/references/5"]},{"style":0,"text":"Blunsom and Cohn, 2010","origin":{"pointer":"/sections/9/paragraphs/0","offset":1420,"length":22},"authors":[{"last":"Blunsom"},{"last":"Cohn"}],"year":"2010","references":["/references/1"]},{"style":0,"text":"Blunsom and Cohn, 2010","origin":{"pointer":"/sections/9/paragraphs/29","offset":233,"length":22},"authors":[{"last":"Blunsom"},{"last":"Cohn"}],"year":"2010","references":["/references/1"]},{"style":0,"text":"Cohen et al., 2010","origin":{"pointer":"/sections/9/paragraphs/29","offset":257,"length":18},"authors":[{"last":"Cohen"},{"last":"al."}],"year":"2010","references":["/references/3"]},{"style":0,"text":"Petrov et al., 2006","origin":{"pointer":"/sections/10/paragraphs/1","offset":853,"length":19},"authors":[{"last":"Petrov"},{"last":"al."}],"year":"2006","references":["/references/20"]},{"style":0,"text":"Petrov et al., 2006","origin":{"pointer":"/sections/10/paragraphs/2","offset":177,"length":19},"authors":[{"last":"Petrov"},{"last":"al."}],"year":"2006","references":["/references/20"]},{"style":0,"text":"Johnson et al., 2007b","origin":{"pointer":"/sections/10/paragraphs/3","offset":86,"length":21},"authors":[{"last":"Johnson"},{"last":"al."}],"year":"2007b","references":["/references/14"]},{"style":0,"text":"Cohn et al., 2010","origin":{"pointer":"/sections/10/paragraphs/3","offset":109,"length":17},"authors":[{"last":"Cohn"},{"last":"al."}],"year":"2010","references":["/references/5"]},{"style":0,"text":"Lari and Young, 1991","origin":{"pointer":"/sections/10/paragraphs/3","offset":302,"length":20},"authors":[{"last":"Lari"},{"last":"Young"}],"year":"1991","references":["/references/17"]},{"style":0,"text":"Cohn et al., 2010","origin":{"pointer":"/sections/10/paragraphs/3","offset":470,"length":17},"authors":[{"last":"Cohn"},{"last":"al."}],"year":"2010","references":["/references/5"]},{"style":0,"text":"Johnson and Goldwater, 2009","origin":{"pointer":"/sections/10/paragraphs/4","offset":412,"length":27},"authors":[{"last":"Johnson"},{"last":"Goldwater"}],"year":"2009","references":["/references/12"]},{"style":0,"text":"Cohn and Lapata, 2009","origin":{"pointer":"/sections/10/paragraphs/4","offset":917,"length":21},"authors":[{"last":"Cohn"},{"last":"Lapata"}],"year":"2009","references":["/references/4"]},{"style":0,"text":"Post and Gildea, 2009","origin":{"pointer":"/sections/10/paragraphs/4","offset":940,"length":21},"authors":[{"last":"Post"},{"last":"Gildea"}],"year":"2009","references":["/references/23"]},{"style":0,"text":"Cohn et al., 2010","origin":{"pointer":"/sections/10/paragraphs/4","offset":1214,"length":17},"authors":[{"last":"Cohn"},{"last":"al."}],"year":"2010","references":["/references/5"]},{"style":0,"text":"Teh, 2006a","origin":{"pointer":"/sections/10/paragraphs/5","offset":730,"length":10},"authors":[{"last":"Teh"}],"year":"2006a","references":["/references/24"]},{"style":0,"text":"Marcus et al., 1993","origin":{"pointer":"/sections/11/paragraphs/0","offset":99,"length":19},"authors":[{"last":"Marcus"},{"last":"al."}],"year":"1993","references":["/references/18"]},{"style":0,"text":"Matsuzaki et al., 2005","origin":{"pointer":"/sections/11/paragraphs/0","offset":484,"length":22},"authors":[{"last":"Matsuzaki"},{"last":"al."}],"year":"2005","references":["/references/19"]},{"style":0,"text":"Petrov et al., 2006","origin":{"pointer":"/sections/11/paragraphs/0","offset":695,"length":19},"authors":[{"last":"Petrov"},{"last":"al."}],"year":"2006","references":["/references/20"]},{"style":0,"text":"Johnson, 1998","origin":{"pointer":"/sections/11/paragraphs/0","offset":814,"length":13},"authors":[{"last":"Johnson"}],"year":"1998","references":["/references/15"]},{"style":0,"text":"Petrov et al., 2006","origin":{"pointer":"/sections/11/paragraphs/4","offset":69,"length":19},"authors":[{"last":"Petrov"},{"last":"al."}],"year":"2006","references":["/references/20"]},{"style":0,"text":"Cohn et al., 2010","origin":{"pointer":"/sections/11/paragraphs/14","offset":117,"length":17},"authors":[{"last":"Cohn"},{"last":"al."}],"year":"2010","references":["/references/5"]},{"style":0,"text":"Post and Gildea (2009)","origin":{"pointer":"/sections/11/paragraphs/24","offset":257,"length":22},"authors":[{"last":"Post"},{"last":"Gildea"}],"year":"2009","references":["/references/23"]},{"style":0,"text":"Cohn et al. (2010)","origin":{"pointer":"/sections/11/paragraphs/24","offset":287,"length":18},"authors":[{"last":"Cohn"},{"last":"al."}],"year":"2010","references":["/references/5"]},{"style":0,"text":"Zuidema (2007)","origin":{"pointer":"/sections/11/paragraphs/24","offset":343,"length":14},"authors":[{"last":"Zuidema"}],"year":"2007","references":["/references/30"]},{"style":0,"text":"Bansal et al. (2010)","origin":{"pointer":"/sections/11/paragraphs/24","offset":366,"length":20},"authors":[{"last":"Bansal"},{"last":"al."}],"year":"2010","references":[]},{"style":0,"text":"Collins (1999)","origin":{"pointer":"/sections/11/paragraphs/24","offset":478,"length":14},"authors":[{"last":"Collins"}],"year":"1999","references":[]},{"style":0,"text":"Petrov and Klein (2007)","origin":{"pointer":"/sections/11/paragraphs/24","offset":503,"length":23},"authors":[{"last":"Petrov"},{"last":"Klein"}],"year":"2007","references":[]},{"style":0,"text":"Petrov (2010)","origin":{"pointer":"/sections/11/paragraphs/24","offset":537,"length":13},"authors":[{"last":"Petrov"}],"year":"2010","references":["/references/21"]},{"style":0,"text":"Carreras et al. (2008)","origin":{"pointer":"/sections/11/paragraphs/24","offset":573,"length":22},"authors":[{"last":"Carreras"},{"last":"al."}],"year":"2008","references":[]},{"style":0,"text":"Charniak and Johnson (2005)","origin":{"pointer":"/sections/11/paragraphs/24","offset":603,"length":27},"authors":[{"last":"Charniak"},{"last":"Johnson"}],"year":"2005","references":["/references/2"]},{"style":0,"text":"Huang (2008)","origin":{"pointer":"/sections/11/paragraphs/24","offset":641,"length":12},"authors":[{"last":"Huang"}],"year":"2008","references":["/references/11"]},{"style":0,"text":"Petrov, 2010","origin":{"pointer":"/sections/11/paragraphs/29","offset":363,"length":12},"authors":[{"last":"Petrov"}],"year":"2010","references":["/references/21"]},{"style":0,"text":"Cohn et al., 2010","origin":{"pointer":"/sections/11/paragraphs/30","offset":146,"length":17},"authors":[{"last":"Cohn"},{"last":"al."}],"year":"2010","references":["/references/5"]},{"style":0,"text":"Petrov et al., 2006","origin":{"pointer":"/sections/11/paragraphs/30","offset":451,"length":19},"authors":[{"last":"Petrov"},{"last":"al."}],"year":"2006","references":["/references/20"]},{"style":0,"text":"Charniak and Johnson, 2005","origin":{"pointer":"/sections/11/paragraphs/31","offset":200,"length":26},"authors":[{"last":"Charniak"},{"last":"Johnson"}],"year":"2005","references":["/references/2"]},{"style":0,"text":"Huang, 2008","origin":{"pointer":"/sections/11/paragraphs/31","offset":233,"length":11},"authors":[{"last":"Huang"}],"year":"2008","references":["/references/11"]},{"style":0,"text":"Petrov, 2010","origin":{"pointer":"/sections/11/paragraphs/32","offset":11,"length":12},"authors":[{"last":"Petrov"}],"year":"2010","references":["/references/21"]},{"style":0,"text":"Fossum and Knight, 2009","origin":{"pointer":"/sections/11/paragraphs/32","offset":861,"length":23},"authors":[{"last":"Fossum"},{"last":"Knight"}],"year":"2009","references":["/references/9"]},{"style":0,"text":"Zhang et al., 2009","origin":{"pointer":"/sections/11/paragraphs/32","offset":886,"length":18},"authors":[{"last":"Zhang"},{"last":"al."}],"year":"2009","references":["/references/29"]}]}
