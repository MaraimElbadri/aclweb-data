{"sections":[{"title":"","paragraphs":["Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 699–709, Baltimore, Maryland, USA, June 23-25 2014. c⃝2014 Association for Computational Linguistics"]},{"title":"Learning Continuous Phrase Representations for Translation Modeling  Jianfeng Gao Xiaodong He Wen-tau Yih Li Deng","paragraphs":["Microsoft Research","One Microsoft Way","Redmond, WA 98052, USA","{jfgao,xiaohe,scottyih,deng}@microsoft.com   "]},{"title":"Abstract","paragraphs":["This paper tackles the sparsity problem in estimating phrase translation probabilities by learning continuous phrase representations, whose distributed nature enables the sharing of related phrases in their representations. A pair of source and target phrases are projected into continuous-valued vector representations in a low-dimensional latent space, where their translation score is computed by the distance between the pair in this new space. The projection is performed by a neural network whose weights are learned on parallel training data. Experimental evaluation has been performed on two WMT translation tasks. Our best result improves the performance of a state-of-the-art phrase-based statistical machine translation system trained on WMT 2012 French-English data by up to 1.3 BLEU points."]},{"title":"1 Introduction","paragraphs":["The phrase translation model, also known as the phrase table, is one of the core components of phrase-based statistical machine translation (SMT) systems. The most common method of construct-ing the phrase table takes a two-phase approach (Koehn et al. 2003). First, the bilingual phrase pairs are extracted heuristically from an automatically word-aligned training data. The second phase, which is the focus of this paper, is parameter estimation where each phrase pair is assigned with some scores that are estimated based on counting these phrases or their words using the same word-aligned training data.","Phrase-based SMT systems have achieved state-of-the-art performance largely due to the fact that long phrases, rather than single words, are used as translation units so that useful context information can be captured in selecting translations. However, longer phrases occur less often in training data, leading to a severe data sparseness problem in parameter estimation. There has been a plethora of research reported in the literature on improving parameter estimation for the phrase translation model (e.g., DeNero et al. 2006; Wuebker et al. 2010; He and Deng 2012; Gao and He 2013).","This paper revisits the problem of scoring a phrase translation pair by developing a Continuous-space Phrase Translation Model (CPTM). The translation score of a phrase pair in this model is computed as follows. First, we represent each phrase as a bag-of-words vector, called word vector henceforth. We then project the word vector, in either the source language or the target language, into a respective continuous feature vector in a common low-dimensional space that is language independent. The projection is performed by a multi-layer neural network. The projected feature vector forms the continuous representation of a phrase. Finally, the translation score of a source-target phrase pair is computed by the distance between their feature vectors.","The main motivation behind the CPTM is to alleviate the data sparseness problem associated with the traditional counting-based methods by grouping phrases with a similar meaning across different languages. This style of grouping is made possible because of the distributed nature of the continuous-space representations for phrases. No such sharing was possible in the original symbolic space for representing words or phrases. In this model, semantically or grammatically related phrases, in both the source and the target languages, would tend to have similar (close) feature vectors in the continuous space, guided by the training objective. Since the translation score is a smooth function of these feature vectors, a small 699 change in the features should only lead to a small change in the translation score.","The primary research task in developing the CPTM is learning the continuous representation of a phrase that is effective for SMT. Motivated by recent studies on continuous-space language models (e.g., Bengio et al. 2003; Mikolov et al. 2011; Schwenk et al., 2012), we use a neural network to project a word vector to a feature vector. Ideally, the projection would discover those latent features that are useful to differentiate good translations from bad ones, for a given source phrase. However, there is no training data with explicit annotation on the quality of phrase translations. The phrase translation pairs are hidden in the parallel source-target sentence pairs, which are used to train the traditional translation models. The quality of a phrase translation can only be judged implicitly through the translation quality of the sentences, as measured by BLEU, which contain the phrase pair. In order to overcome this challenge and let the BLEU metric guide the projection learning, we propose a new method to learn the parameters of a neural network. This new method, via the choice of an appropriate objective function in training, automatically forces the feature vector of a source phrase to be closer to the feature vectors of its candidate translations. As a result, the BLEU score is improved when these translations are selected by an SMT decoder to produce final, sentence-level translations. The new learning method makes use of the L-BFGS algorithm and the expected BLEU as the objective function defined on N-best lists.","To the best of our knowledge, the CPTM proposed in this paper is the first continuous-space phrase translation model that makes use of joint representations of a phrase in the source language and its translation in the target language (to be detailed in Section 4) and that is shown to lead to significant improvement over a standard phrase-based SMT system (to be detailed in Section 6).","Like the traditional phrase translation model, the translation score of each bilingual phrase pair is modeled explicitly in our model. However, in-stead of estimating the phrase translation score on aligned parallel data, our model intends to capture the grammatical and semantic similarity between a source phrase and its paired target phrase by projecting them into a common, continuous space that is language independent.  1 Niehues et al. (2011) use different translation units in order to integrate the n-gram translation model into the phrase-based approach. However, it is not clear how a continuous","The rest of the paper is organized as follows. Section 2 reviews previous work. Section 3 reviews the log-linear model for phrase-based SMT and Sections 4 presents the CPTM. Section 5 describes the way the model parameters are estimated, followed by the experimental results in Section 6. Finally, Section 7 concludes the paper."]},{"title":"2 Related Work","paragraphs":["Representations of words or documents as continuous vectors have a long history. Most of the earlier latent semantic models for learning such vectors are designed for information retrieval (Deerwester et al. 1990; Hofmann 1999; Blei et al. 2003). In contrast, recent work on continuous space language models, which estimate the probability of a word sequence in a continuous space (Bengio et al. 2003; Mikolov et al. 2010), have advanced the state of the art in language modeling, outperforming the traditional n-gram model on speech recognition (Mikolov et al. 2012; Sundermeyer et al. 2013) and machine translation (Mikolov 2012; Auli et al. 2013).","Because these models are developed for monolingual settings, word embedding from these models is not directly applicable to translation. As a result, variants of such models for cross-lingual scenarios have been proposed so that words in different languages are projected into the shared latent vector space (Dumais et al. 1997; Platt et al. 2010; Vinokourov et al. 2002; Yih et al. 2011; Gao et al. 2011; Huang et al. 2013; Zou et al. 2013). In principle, a phrase table can be derived using any of these cross-lingual models, although decoupling the derivation from the SMT training often results in suboptimal performance (e.g., measured in BLEU), as we will show in Section 6.","Recently, there is growing interest in applying continuous-space models for translation. The most related to this study is the work of continuous space n-gram translation models (Schwenk et al. 2007; Schwenk 2012; Son et al. 2012), where the feed-forward neural network language model is extended to represent translation probabilities. However, these earlier studies focused on the n-gram translation models, where the translation probability of a phrase or a sentence is decomposed as a product of n-gram probabilities as in a standard n-gram language model. Therefore, it is not clear how their approaches can be applied to the phrase translation model1",", which is much more version of such a model can be trained efficiently because the factor models used by Son et al. cannot be applied directly. 700 widely used in modern SMT systems. In contrast, our model learns jointly the representations of a phrase in the source language as well as its translation in the target language. The recurrent continuous translation models proposed by Kalchbrenner and Blunsom (2013) also adopt the recurrent language model (Mikolov et al. 2010). But unlike the n-gram translation models above, they make no Markov assumptions about the dependency of the words in the target sentence. Continuous space models have also been used for generating translations for new words (Mikolov et al. 2013a) and ITG reordering (Li et al. 2013).","There has been a lot of research on improving the phrase table in phrase-based SMT (Marcu and Wong 2002; Lamber and Banchs 2005; Denero et al. 2006; Wuebker et al. 2010; Zhang et al., 2011; He and Deng 2012; Gao and He 2013). Among them, (Gao and He 2013) is most relevant to the work described in this paper. They estimate phrase translation probabilities using a discriminative training method under the N-best reranking framework of SMT. In this study we use the same objective function to learn the continuous representations of phrases, integrating the strengths associated with these earlier studies."]},{"title":"3 The Log-Linear Model for SMT","paragraphs":["Phrase-based SMT is based on a log-linear model","which requires learning a mapping between input","F ∈ F to output E ∈ E. We are given"," Training samples (Fi, Ei) for i = 1 ... N, where each source sentence Fi is paired with a reference translation in target language Ei;"," A procedure GEN to generate a list of N-best candidates GEN(Fi) for an input Fi , where GEN in this study is the baseline phrase-based SMT system, i.e., an in-house implementation of the Moses system (Koehn et al. 2007) that does not use the CPTM, and each E ∈ GEN(Fi) is labeled by the sentence-level BLEU score (He and Deng 2012), denoted by sBleu(Ei, E) , which measures the quality of E with respect to its reference translation Ei;"," A vector of features h ∈ RM","that maps each (Fi, E) to a vector of feature values2","; and"," A parameter vector λ ∈ RM",", which assigns a real-valued weight to each feature.  2 Our baseline system uses a set of standard features suggested in Koehn et al. (2007), which is also detailed in Section 6.","The components GEN(. ), h and λ define a log-","linear model that maps Fi to an output sentence as","follows:","E∗","= argmax","(E,A)∈GEN(Fi)","λT h(Fi, E, A) (1) which states that given λ and h, argmax returns the highest scoring translation E∗",", maximizing over correspondences A. In phrase-based SMT, A consists of a segmentation of the source and target sentences into phrases and an alignment between source and target phrases. Since computing the argmax exactly is intractable, it is commonly performed approximatedly by beam search (Och and Ney 2004). Following Liang et al. (2006), we assume that every translation candidate is always coupled with a corresponding A, called the Viterbi derivation, generated by (1)."]},{"title":"4 A Continuous-Space Phrase Translation Model (CPTM)","paragraphs":["The architecture of the CPTM is shown in Figures 1 and 2, where for each pair of source and target phrases (fi, ej) in a source-target sentence pair, we first project them into feature vectors yfi and yej in a latent, continuous space via a neural network with one hidden layer (as shown in Figure 2), and then compute the translation score, score(fi, ej), by the distance of their feature vectors in that space.","We start with a bag-of-words representation of a phrase x ∈ Rd",", where x is a word vector and d is the size of the vocabulary consisting of words in both source and target languages, which is set to 200K in our experiments. We then learn to project x to a low-dimensional continuous space Rk",":","φ(x): Rd","→ Rk"," The projection is performed using a fully connected neural network with one hidden layer and tanh activation functions. Let W1 be the projection matrix from the input layer to the hidden layer and W2 the projection matrix from the hidden layer to the output layer, we have","y ≡ φ(x) = tanh (W2T (tanh(W1 T x))) (2)   701"," Figure 2. A neural network model for phrases giving rise to their continuous representations. The model with the same form is used for both source and target languages.   The translation score of a source phrase f and a target phrase e can be measured as the similarity (or distance) between their feature vectors. We choose the dot product as the similarity function3",":","score(f, e) ≡ simθ(xf, xe) = yfT ye (3) According to (2), we see that the value of the scoring function is determined by the projection matrices θ = {W1, W2}.","The CPTM of (2) and (3) can be incorporated into the log-linear model for SMT (1) by  3 In our experiments, we compare dot product and the cosine similarity functions and find that the former works better for nonlinear multi-layer neural networks, and the latter works better for linear neural networks. For the sake of clarity, we choose dot product when we describe the CPTM and its training in Sections 4 and 5, respectively. 4 The baseline SMT needs to be reasonably good in the sense that the oracle BLEU score on the generated n-best introducing a new feature hM+1 and a new feature weight λM+1. The new feature is defined as hM+1(Fi, E, A) = ∑ simθ(xf, xe)(f,e )∈A (4) Thus, the phrase-based SMT system, into which the CPTM is incorporated, is parameterized by (λ, θ), where λ is a vector of a handful of parameters used in the log-linear model of (1), with one weight for each feature; and θ is the projection matrices used in the CPTM defined by (2) and (3). In our experiments we take three steps to learn (λ, θ):","1. We use a baseline phrase-based SMT system to generate for each source sentence in training data an N-best list of translation hypotheses4",".","2. We set λ to that of the baseline system and let λM+1 = 1, and optimize θ w.r.t. a loss function on training data5",".","3. We fix θ , and optimize λ using MERT (Och 2003) to maximize BLEU on dev data. In the next section, we will describe Step 2 in detail as it is directly related to the CPTM training.  lists needs to be significantly higher than that of the top-1 translations so that the CPTM can be effectively trained. 5 The initial value of λM+1 can also be tuned using the dev set. However, we find in a pilot study that it is good enough to set it to 1 when the values of all the baseline feature weights, used in the log-linear model of (1), are properly normalized, such as by setting λm = λm/C for m = 1 ... M , where C is the unnormalized weight value of the target language model.  Figure 1. The architecture of the CPTM, where the mapping from a phrase to its continuous representation is shown in Figure 2.   200K (d) 100 100 (k) (w1 ... wn) Word vector Neural network Feature vector W1 W2 x y Raw phrase e or f ... (the process of) (machine translation) (consists of). . . ... (le processus de) (traduction automatique) (consiste en). . . ej−1 ej ej+1 fi−1 fi fi+1 yej  yfi  yf(k)","score(fi, ej) = yfiT yej  Target phrases Continuous representations of target phrases Source phrases  Continuous representations of source phrases Translation score as dot product of feature vectors in the continuous space 702"]},{"title":"5 Training CPTM","paragraphs":["This section describes the loss function we employ with the CPTM and the algorithm to train the neural network weights.","We define the loss function L(θ) as the nega-tive of the N-best list based expected BLEU, denoted by xBleu(θ). In the reranking framework of SMT outlined in Section 3, xBleu(θ) over one training sample (Fi, Ei) is defined as xBleu(θ) = ∑ P(E|Fi)sBleu(Ei, E)E∈GEN(Fi) (5) where sBleu(Ei, E) is the sentence-level BLEU score, and P(E|Fi) is the translation probability from Fi to E computed using softmax as P(E|Fi) =","exp(γλT","h(Fi,E,A))","∑ exp(γλT","h(Fi,E′",",A))E′","∈GEN(F i) (6) where λT","h is the log-linear model of (1), which also includes the feature derived from the CPTM as defined by (4), and γ is a tuned smoothing factor.","Let L(θ) be a loss function which is differentiable w.r.t. the parameters of the CPTM, θ. We can compute the gradient of the loss and learn θ using gradient-based numerical optimization algorithms, such as L-BFGS or stochastic gradient descent (SGD). 5.1"]},{"title":"Computing the Gradient","paragraphs":["Since the loss does not explicitly depend on θ, we use the chain rule for differentiation: ∂L(θ) ∂θ = ∑","∂L(θ) ∂simθ(xf, xe)","∂simθ(xf, xe) ∂θ (f,e )  = ∑ −δ(f,e)","∂simθ(xf, xe) ∂θ (f,e ) (7) which takes the form of summation over all phrase pairs occurring either in a training sample (stochastic mode) or in the entire training data (batch mode). δ(f,e) in (7) is known as the error term of the phrase pair (f, e), and is defined as δ(f,e) = −","∂L(θ) ∂simθ(xf,xe) (8) It describes how the overall loss changes with the translation score of the phrase pair (f, e). We will leave the derivation of δ(f,e) to Section 5.1.2, and will first describe how the gradient of simθ(xf, xe) w.r.t. θ is computed."]},{"title":"5.1.1 Computing ∂sim","paragraphs":["θ"]},{"title":"(x","paragraphs":["f"]},{"title":", x","paragraphs":["e"]},{"title":")/∂θ","paragraphs":["Without loss of generality, we use the following","notations to describe a neural network:  Wl is the projection matrix for the l-th layer","of the neural network;  x is the input word vector of a phrase;  zl","is the sum vector of the l-th layer; and  yl","= σ(zl",") is the output vector of the l-th","layer, where σ is an activation function; Thus, the CPTM defined by (2) and (3) can be represented as","z1 = W1","T","x","y1 = σ(z1",")","z2 = W2 T y1","","y2","= σ(z2 )","simθ(xf, xe) = (yf2",")T","ye2  The gradient of the matrix W2 which projects the hidden vector to the output vector is computed as:","∂simθ(xf, xe) ∂W2 = ∂(yf2",")T ∂W2","ye2","+ (yf2",")T ∂ye2","∂W2 ","= yf1","(ye2 ∘ σ′","(zf2","))T","+ ye1 (yf2","∘ σ′(ze2))T (9) where ∘ is the element-wise multiplication (Hadamard product). Applying the back propagation principle, the gradient of the projection matrix mapping the input vector to the hidden vector W1 is computed as","∂simθ(xf, xe) ∂W1 ","= xf (W2 (ye2","∘ σ′ (zf2 )) ∘ σ′","(zf1","))T ","+xe (W2 (yf2 ∘ σ′(ze2)) ∘ σ′(ze1))T (10) The derivation can be easily extended to a neural network with multiple hidden layers."]},{"title":"5.1.2 Computing δ","paragraphs":["(f,e) To simplify the notation, we rewrite our loss function of (5) and (6) over one training sample as 703 L(θ) = −xBleu(θ) = − G(θ) Z(θ) (11) where G(θ) = ∑ sBleu(E, Ei) exp(λT","h(Fi, E, A))E Z(θ) = ∑ exp(λT","h(Fi, E, A))E Combining (8) and (11), we have δ(f,e) = ∂xBleu(θ) ∂simθ(xf, xe) (12) = 1 Z(θ) (","∂G(θ) ∂simθ(xf, xe) − ∂Z(θ) ∂simθ(xf, xe) xBleu(θ)) Because θ is only relevant to hM+1 which is defined in (4), we have ∂λT","h(Fi, E, A) ∂simθ(xf, xe) = λM+1 ∂hM+1(Fi, E, A) ∂simθ(xf, xe)  = λM+1N(f, e; A) (13) where N(f, e; A) is the number of times the phrase pair (f, e) occurs in A . Combining (12) and (13), we end up with the following equation δ(f,e)","= ∑ U(θ, E)P(E|Fi)λM+1N(f, e; A) (E,A)∈GEN(Fi)  where (14) U(θ, E) = sBleu(Ei, E) − xBleu(θ). 5.2"]},{"title":"The Training Algorithm","paragraphs":["In our experiments we train the parameters of the CPTM, θ, using the L-BFGS optimizer described in Andrew and Gao (2007), together with the loss function described in (5). The gradient is computed as described in Sections 5.1. Although SGD has been advocated for neural network training due to its simplicity and its robustness to local minima (Bengio 2009), we find that in our task that the L-BFGS minimizes the loss in a desirable fashion empirically when iterating over the complete training data (batch mode). For example, the convergence of the algorithm was found to be smooth, despite the non-convexity in our loss. An-other merit of batch training is that the gradient over all training data can be computed efficiently. As shown in Section 5.1, computing ∂simθ(xf, xe)/∂θ requires large-scale matrix multiplications, and is expensive for multi-layer neural networks. Eq. (7) suggests that ∂simθ(xf, xe)/∂θ and δ(f,e) can be computed separately, thus making the computation cost of the former term only depends on the number of phrase pairs in the phrase table, but not the size of training data. Therefore, the training method described here can be used on larger amounts of training data with little difficulty.","As described in Section 4, we take three steps to learn the parameters for both the log-linear model of SMT and the CPTM. While steps 1 and 3 can be easily parallelized on a computer cluster, the CPTM training is performed on a single machine. For example, given a phrase table containing 16M pairs and a 1M-sentence training set, it takes a couple of hours to generate the N-best lists on a cluster, and about 10 hours to train the CPTM on a Xeon E5-2670 2.60GHz machine.","For a non-convex problem, model initialization is important. In our experiments we always initialize W1 using a bilingual topic model trained on parallel data (see detail in Section 6.2), and W2 as an identity matrix. In principle, the loss function of (5) can be further regularized (e.g. by adding a term of L2 norm) to deal with overfitting. However, we did not find clear empirical advantage over the simpler early stop approach in a pilot study, which is adopted in the experiments in this paper."]},{"title":"6 Experiments","paragraphs":["This section evaluates the CPTM presented on two translation tasks using WMT data sets. We first describe the data sets and baseline setup. Then we present experiments where we compare different versions of the CPTM and previous models. 6.1"]},{"title":"Experimental Setup Baseline.","paragraphs":["We experiment with an in-house phrase-based system similar to Moses (Koehn et al. 2007), where the translation candidates are scored by a set of common features including maximum likelihood estimates of source given target phrase mappings PMLE(e|f) and vice versa PMLE(f|e), as well as lexical weighting estimates PLW(e|f) and PLW(f|e), word and phrase penalties, a linear distortion feature, and a lexicalized reordering feature. The baseline includes a standard 5-gram modified Kneser-Ney language model trained on the target side of the parallel corpora described below. Log-linear weights are estimated with the MERT algorithm (Och 2003). 704 Evaluation. We test our models on two different data sets. First, we train an English to French system based on the data of WMT 2006 shared task (Koehn and Monz 2006). The parallel corpus includes 688K sentence pairs of parliamentary proceedings for training. The development set contains 2000 sentences, and the test set contains other 2000 sentences, all from the official WMT 2006 shared task.","Second, we experiment with a French to English system developed using 2.1M sentence pairs of training data, which amounts to 102M words, from the WMT 2012 campaign. The majority of the training data set is parliamentary proceedings except for 5M words which are newswire. We use the 2009 newswire data set, comprising 2525 sentences, as the development set. We evaluate on four newswire domain test sets from 2008, 2010 and 2011 as well as the 2010 system combination test set, containing 2034 to 3003 sentences.","In this study we perform a detailed empirical comparison using the WMT 2006 data set, and verify our best models and results using the larger WMT 2012 data set.","The metric used for evaluation is case insensi-tive BLEU score (Papineni et al. 2002). We also perform a significance test using the Wilcoxon signed rank test. Differences are considered statistically significant when the p-value is less than 0.05. 6.2"]},{"title":"Results of the CPTM","paragraphs":["Table 1 shows the results measured in BLEU evaluated on the WMT 2006 data set, where Row 1 is the baseline system. Rows 2 to 4 are the systems enhanced by integrating different versions of the CPTM. Rows 5 to 7 present the results of previous models. Row 8 is our best system. Table 2 shows the main results on the WMT 2012 data set.","CPTM is the model described in Sections 4. As illustrated in Figure 2, the number of the nodes in the input layer is the vocabulary size d. Both the hidden layer and the output layer have 100 nodes6",". That is, W1","is a d × 100 matrix and W2"," a 100 × 100 matrix. The result shows that CPTM leads to a substantial improvement over the baseline system with a statistically significant margin of 1.0 BLEU points as in Table 1.","We have developed a set of variants of CPTM to investigate two design choices we made in developing the CPTM: (1) whether to use a linear  6 We can achieve slightly better results using more nodes in the hidden and output layers, say 500 nodes. But the model projection or a multi-layer nonlinear projection; and (2) whether to compute the phrase similarity using word-word similarities as suggested by e.g., the lexical weighting model (Koehn et al. 2003). We compare these variants on the WMT 2006 data set, as shown in Table 1.","CPTML (Row 3 in Table 1) uses a linear neural network to project a word vector of a phrase x to a feature vector y: y ≡ φ(x) = WT","x, where W is a d × 100 projection matrix. The translation score of a source phrase f and a target phrase e is measured as the similarity of their feature vectors. We choose cosine similarity because it works better than dot product for linear projection. CPTMW (Row 4 in Table 1) computes the phrase similarity using word-word similarity scores. This follows the common smoothing strategy of addressing the data sparseness problem in modeling phrase translations, such as the lexical weighting model (Koehn et al. 2003) and the word factored n-gram translation model (Son et al. 2012). Let w denote a word, and f and e the source and target phrases, respectively. We define sim(f, e) = 1 |f| ∑ simτ(w, e) +w∈f 1 |e| ∑ simτ(w, f)w∈e where simτ(w, e) (or simτ(w, f) ) is the word-phrase similarity, and is defined as a smooth approximation of the maximum function simτ(w, e) =","∑ sim(w, w′) exp(τsim(w, w′))w′ ∈e ∑ exp(τsim(w, w′))w′","∈e   training is too slow to perform a detailed study within a reasonable time. Therefore, all the models reported in this paper use 100 nodes. # Systems WMT test2006 1 Baseline 33.06 2 CPTM 34.10α"," 3 CPTML 33.60αβ"," 4 CPTMW 33.25β"," 5 BLTMPR 33.15β"," 6 DPM 33.29β"," 7 MRFP 33.91α"," 8 Comb (2 + 7) 34.39αβ"," Table 1: BLEU results for the English to French task using translation models and systems built on the WMT 2006 data set. The superscripts α and β indicate statistically significant difference (p < 0.05) from Baseline and CPTM, respectively.  705 where simτ(w, e) (or simτ(w, f) ) is the word-phrase similarity, and is defined as a smooth approximation of the maximum function where τ is the tuned smoothing parameter.","Similar to CPTM, CPTMW also uses a nonlinear projection to map each word (not a phrase vector as in CPTM) to a feature vector.","Two observations can be made by comparing CPTM in Row 2 to its variants in Table 1. First of all, it is more effective to model the phrase translation directly than decomposing it into word-word translations in the CPTMs. Second, we see that the nonlinear projection is able to generate more effective features, leading to better results than the linear projection. We also compare the best version of the CPTM i.e., CPTM, with three related models proposed previously. We start the discussion with the results on the WMT 2006 data set in Table 1.","Rows 5 and 6 in Table 1 are two state-of-the-art latent semantic models that are originally trained on clicked query-document pairs (i.e., clickthrough data extracted from search logs) for query-document matching (Gao et al. 2011). To adopt these models for SMT, we view source-target sentence pairs as clicked query-document pairs, and trained both models using the same methods as in Gao et al. (2011) on the parallel bilingual training data described earlier. Specifically, BTLMPR is an extension to PLSA, and is the best performer among different versions of the Bi-Lingual Topic Model (BLTM) described in Gao et al. (2011). BLTM with Posterior Regularization (BLTMPR) is trained on parallel training data using the EM algorithm with a constraint enforcing a source sentence and its paralleled target sentence to not only share the same prior topic distribution, but to also have similar fractions of words assigned to each topic. We incorporated the model into the log-linear model for SMT (1) as  7 Gao and He (2013) reported results of MRF models with different feature sets. We picked the MRF using phrase features only (MRFP) for comparison since we are mainly interested in phrase representation. follows. First of all, the topic distribution of a source sentence Fi , denoted by P(z|Fi) , is in-duced from the learned topic-word distributions using EM. Then, each translation candidate E in the N-best list GEN(Fi) is scored as P(E|Fi) = ∏ ∑ P(w|z)P(z|Fi)zw∈E P(Fi|E) can be similarly computed. Finally, the logarithms of the two probabilities are incorporated into the log-linear model of (1) as two additional features. DPM is the Discriminative Projec-tion Model described in Gao et al. (2011), which is an extension of LSA. DPM uses a matrix to project a word vector of a sentence to a feature vector. The projection matrix is learned on parallel training data using the S2Net algorithm (Yih et al. 2011). DPM can be incorporated into the log-linear model for SMT (1) by introducing a new feature hM+1 for each phrase pair, which is defined as the cosine similarity of the phrases in the project space.","As we see from Table 1, both latent semantic models, although leading to some slight improvement over Baseline, are much less effective than CPTM.","Finally, we compare the CPTM with the Markov Random Field model using phrase features (MRFP in Tables 1 and 2), proposed by Gao and He (2013)7",", on both the WMT 2006 and WMT 2012 datasets. MRFp is a state-of-the-art large scale discriminative training model that uses the same expected BLEU training criterion, which has proven to give superior performance across a range of MT tasks recently (He and Deng 2012, Setiawan and Zhou 2013, Gao and He 2013).","Unlike CPTM, MRFp is a linear model that simply treats each phrase pair as a single feature. Therefore, although both are trained using the","# Systems dev news2011 news2010 news2008 newssyscomb2010","1 Baseline 23.58 25.24 24.35 20.36 24.14","2 MRFP 24.07α 26.00α","24.90 20.84α","25.05α","","3 CPTM 24.12α 26.25α","25.05α","21.15αβ","24.91α","","4 Comb (2 + 3) 24.46αβ 26.56αβ","25.52αβ","21.64αβ","25.22α","","Table 2: BLEU results for the French to English task using translation models and systems built on","the WMT 2012 data set. The superscripts α and β indicate statistically significant difference (p <","0.05) from Baseline and MRFp, respectively.",""," 706 same expected BLEU based objective function, CPTM and MRFp model the translation relationship between two phrases from different angles. MRFp estimates one translation score for each phrase pair explicitly without parameter sharing, while in CPTM, all phrases share the same neural network that projects raw phrases to the continuous space, providing a more smoothed estimation of the translation score for each phrase pair.","The results in Tables 1 and 2 show that CPTM outperforms MRFP on most of the test sets across the two WMT data sets, but the difference between them is often not significant. Our interpretation is that although CPTM provides a better smoothed estimation for low-frequent phrase pairs, which otherwise suffer the data sparsity is-sue, MRFp provides a more precise estimation for those high-frequent phrase pairs. That is, CPTM and MRFp capture complementary information for translation. We thus combine CPTM and MRFP (Comb in Tables 1 and 2) by incorporating two features, each for one model, into the log-linear model of SMT (1). We observe that for both translation tasks, accuracy improves by up to 0.8 BLEU over MRFP alone (e.g., on the news2008 test set in Table 2). The results confirm that CPTM captures complementary translation information to MRFp. Overall, we improve accuracy by up to 1.3 BLEU over the baseline on both WMT data sets."]},{"title":"7 Conclusions","paragraphs":["The work presented in this paper makes two major contributions. First, we develop a novel phrase translation model for SMT, where joint representations are exploited of a phrase in the source language and of its translation in the target language, and where the translation score of the pair of source-target phrases are represented as the distance between their feature vectors in a low-dimensional, continuous space. The space is derived from the representations generated using a multi-layer neural network. Second, we present a new learning method to train the weights in the multi-layer neural network for the end-to-end BLEU metric directly. The training method is based on L-BFGS. We describe in detail how the gradient in closed form, as required for efficient optimization, is derived. The objective function, which takes the form of the expected BLEU computed from N-best lists, is very different from the usual objective functions used in most existing architectures of neural networks, e.g., cross entropy (Hinton et al. 2012) or mean square error (Deng et al. 2012). We hence have provided details in the derivation of the gradient, which can serve as an example to guide the derivation of neural network learning with other non-standard objective functions in the future.","Our evaluation on two WMT data sets show that incorporating the continuous-space phrase translation model into the log-linear framework significantly improves the accuracy of a state-of-the-art phrase-based SMT system, leading to a gain up to 1.3 BLEU. Careful implementation of the L-BFGS optimization based on the BLEU-centric objective function, together with the associated closed-form gradient, is a key to the success.","A natural extension of this work is to expand the model and learning algorithm from shallow to deep neural networks. The deep models are expected to produce more powerful and flexible semantic representations (e.g., Tur et al., 2012), and thus greater performance gain than what is presented in this paper."]},{"title":"8 Acknowledgements","paragraphs":["We thank Michael Auli for providing a dataset and for helpful discussions. We also thank the four anonymous reviewers for their comments."]},{"title":"References","paragraphs":["Andrew, G. and Gao, J. 2007. Scalable training of L1-regularized log-linear models. In ICML.","Auli, M., Galley, M., Quirk, C. and Zweig, G. 2013 Joint language and translation modeling with recurrent neural networks. In EMNLP.","Bengio, Y. 2009. Learning deep architectures for AI. Fundamental Trends Machine Learning, vol. 2, no. 1, pp. 1–127.","Bengio, Y., Duharme, R., Vincent, P., and Janvin, C. 2003. A neural probabilistic language model. JMLR, 3:1137-1155.","Blei, D. M., Ng, A. Y., and Jordan, M. J. 2003. Latent Dirichlet allocation. Journal of Machine Learning Research, 3: 993-1022.","Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., and Kuksa, P. 2011. Natural language processing (almost) from scratch. Journal of Machine Learning Research, vol. 12. 707","Deerwester, S., Dumais, S. T., Furnas, G. W., Landauer, T., and Harshman, R. 1990. Index-ing by latent semantic analysis. Journal of the American Society for Information Science, 41(6): 391-407","DeNero, J., Gillick, D., Zhang, J., and Klein, D. 2006. Why generative phrase models underper-form surface heuristics. In Workshop on Statistical Machine Translation, pp. 31-38.","Deng, L., Yu, D., and Platt, J. 2012. Scalable stacking and learning for building deep architectures. In ICASSP.","Diamantaras, K. I., and Kung, S. Y. 1996. Principle Component Neural Networks: Theory and Applications. Wiley-Interscience.","Dumais S., Letsche T., Littman M. and Landauer T. 1997. Automatic cross-language retrieval using latent semantic indexing. In AAAI-97 Spring Symposium Series: Cross-Language Text and Speech Retrieval.","Ganchev, K., Graca, J., Gillenwater, J., and Taskar, B. 2010. Posterior regularization for structured latent variable models. Journal of Machine Learning Research, 11 (2010): 2001-2049.","Gao, J., and He, X. 2013. Training MRF-based translation models using gradient ascent. In NAACL-HLT, pp. 450-459.","Gao, J., Toutanova, K., Yih., W-T. 2011. Click-through-based latent semantic models for web search. In SIGIR, pp. 675-684.","He, X., and Deng, L. 2012. Maximum expected bleu training of phrase and lexicon translation models. In ACL, pp. 292-301.","Hinton, G., and Salakhutdinov, R., 2010. Discovering Binary Codes for Documents by Learning Deep Generative Models. Topics in Cogni-tive Science, pp. 1-18.","Hinton, G., Deng, L., Yu, D., Dahl, G., Mohamed, A., Jaitly, N., Senior, A., Vanhoucke, V., Nguyen, P., Sainath, T., and Kingsbury, B., 2012. Deep neural networks for acoustic modeling in speech recognition. IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82-97.","Hofmann, T. 1999. Probabilistic latent semantic indexing. In SIGIR, pp. 50-57.","Huang, P-S., He, X., Gao, J., Deng, L., Acero, A. and Heck, L. 2013. Learning deep structured semantic models for web search using clickthrough data. In CIKM.","Kalchbrenner, N. and Blunsom, P. 2013. Recurrent continuous translation models. In EMNLP.","Koehn, P., Hoang, H., Birch, A., Callison-Burch, C., Federico, M., Bertoldi, N., Cowan, B., Shen, W., Moran, C., Zens, R., Dyer, C., Bojar, O., Constantin, A., and Herbst, E. 2007. Moses: open source toolkit for statistical machine translation. In ACL 2007, demonstration session.","Koehn, P. and Monz, C. 2006. Manual and automatic evaluation of machine translation between European languages. In Workshop on Statistical Machine Translation, pp. 102-121.","Koehn, P., Och, F., and Marcu, D. 2003. Statistical phrase-based translation. In HLT-NAACL, pp. 127-133.","Lambert, P. and Banchs, R. E. 2005. Data inferred multi-word expressions for statistical machine translation. In MT Summit X, Phuket, Thailand.","Li, P., Liu, Y., and Sun, M. 2013. Recursive autoencoders for ITG-based translation. In EMNLP.","Liang,P., Bouchard-Cote,A., Klein, D. and Taskar, B. 2006. An end-to-end discriminative approach to machine translation. In COLING-ACL.","Marcu, D., and Wong, W. 2002. A phrase-based, joint probability model for statistical machine translation. In EMNLP.","Mikolov, T., Karafiat, M., Burget, L., Cernocky, J., and Khudanpur, S. 2010. Recurrent neural network based language model. In INTER-SPEECH, pp. 1045-1048.","Mikolov, T., Kombrink, S., Burget, L., Cernocky, J., and Khudanpur, S. 2011. Extensions of recurrent neural network language model. In ICASSP, pp. 5528-5531.","Mikolov, T. 2012. Statistical Language Model based on Neural Networks. Ph.D. thesis, Brno University of Technology.","Mikolov, T., Le, Q. V., and Sutskever, H. 2013a. Exploiting similarities among languages for machine translation. CoRR. 2013; abs/1309.4148.","Mikolov, T., Yih, W. and Zweig, G. 2013b. Linguistic Regularities in Continuous Space Word Representations. In NAACL-HLT.","Mimno, D., Wallach, H., Naradowsky, J., Smith, D. and McCallum, A. 2009. Polylingual topic models. In EMNLP. 708","Niehues J., Herrmann, T., Vogel, S., and Waibel, A. 2011. Wider context by using bilingual language models in machine translation.","Och, F. 2003. Minimum error rate training in statistical machine translation. In ACL, pp. 160-167.","Och, F., and Ney, H. 2004. The alignment tem-plate approach to statistical machine translation. Computational Linguistics, 29(1): 19-51.","Papineni, K., Roukos, S., Ward, T., and Zhu W-J. 2002. BLEU: a method for automatic evaluation of machine translation. In ACL.","Platt, J., Toutanova, K., and Yih, W. 2010. Translingual Document Representations from Discriminative Projections. In EMNLP.","Rosti, A-V., Hang, B., Matsoukas, S., and Schwartz, R. S. 2011. Expected BLEU training for graphs: bbn system description for WMT system combination task. In Workshop on Statistical Machine Translation.","Schwenk, H., Costa-Jussa, M. R. and Fonollosa, J. A. R. 2007. Smooth bilingual n-gram translation. In EMNLP-CoNLL, pp. 430-438.","Schwenk, H. 2012. Continuous space translation models for phrase-based statistical machine translation. In COLING.","Schwenk, H., Rousseau, A., and Mohammed A. 2012. Large, pruned or continuous space language models on a GPU for statistical machine translation. In NAACL-HLT Workshop on the future of language modeling for HLT, pp. 11-19.","Setiawan, H. and Zhou, B., 2013. Discriminative training of 150 million translation parameters and its application to pruning. In NAACL.","Socher, R., Huval, B., Manning, C., Ng, A., 2012. Semantic Compositionality through Recursive Matrix-Vector Spaces. In EMNLP.","Socher, R., Lin, C., Ng, A. Y., and Manning, C. D. 2011. Parsing natural scenes and natural language with recursive neural networks. In ICML.","Son, L. H., Allauzen, A., and Yvon, F. 2012. Continuous space translation models with neural networks. In NAACL-HLT, pp. 29-48.","Sundermeyer, M., Oparin, I., Gauvain, J-L. Freiberg, B., Schluter, R. and Ney, H. 2013. Comparison of feed forward and recurrent neural network language models. In ICASSP, pp. 8430–8434.","Tur, G, Deng, L., Hakkani-Tur, D., and He, X., 2012. Towards deeper understanding: deep convex networks for semantic utterance classifica-tion. In ICASSP.","Vinokourov,A., Shawe-Taylor,J. and Cristianini,N. 2002. Inferring a semantic representation of text via cross-language correlation analysis. In NIPS.","Weston, J., Bengio, S., and Usunier, N. 2011. Large scale image annotation: learning to rank with joint word-image embeddings. In IJCAI.","Wuebker, J., Mauser, A., and Ney, H. 2010. Training phrase translation models with leaving-one-out. In ACL, pp. 475-484.","Yih, W., Toutanova, K., Platt, J., and Meek, C. 2011. Learning discriminative projections for text similarity measures. In CoNLL.","Zhang, Y., Deng, L., He, X., and Acero, A. 2011. A novel decision function and the associated decision-feedback learning for speech translation. In ICASSP.","Zhila, A., Yih, W., Meek, C., Zweig, G. and Mikolov, T. 2013. Combining heterogeneous models for measuring relational similarity. In NAACL-HLT.","Zou, W. Y., Socher, R., Cer, D., and Manning, C. D. 2013. Bilingual word embeddings for phrase-based machine translation. In EMNLP. 709"]}],"references":[{"authors":[{"first":"G.","last":"Andrew"},{"last":"Gao"},{"last":"J"}],"year":"2007","title":"Scalable training of L1-regularized log-linear models","source":"Andrew, G. and Gao, J. 2007. Scalable training of L1-regularized log-linear models. In ICML."},{"authors":[],"source":"Auli, M., Galley, M., Quirk, C. and Zweig, G. 2013 Joint language and translation modeling with recurrent neural networks. In EMNLP."},{"authors":[{"last":"Bengio"},{"last":"Y"}],"year":"2009","title":"Learning deep architectures for AI","source":"Bengio, Y. 2009. Learning deep architectures for AI. Fundamental Trends Machine Learning, vol. 2, no. 1, pp. 1–127."},{"authors":[{"first":"Y.","last":"Bengio"},{"first":"R.","last":"Duharme"},{"first":"P.","last":"Vincent"},{"last":"Janvin"},{"last":"C"}],"year":"2003","title":"A neural probabilistic language model","source":"Bengio, Y., Duharme, R., Vincent, P., and Janvin, C. 2003. A neural probabilistic language model. JMLR, 3:1137-1155."},{"authors":[{"first":"D.","middle":"M.","last":"Blei"},{"first":"A.","middle":"Y.","last":"Ng"},{"first":"M.","last":"Jordan"},{"last":"J"}],"year":"2003","title":"Latent Dirichlet allocation","source":"Blei, D. M., Ng, A. Y., and Jordan, M. J. 2003. Latent Dirichlet allocation. Journal of Machine Learning Research, 3: 993-1022."},{"authors":[{"first":"R.","last":"Collobert"},{"first":"J.","last":"Weston"},{"first":"L.","last":"Bottou"},{"first":"M.","last":"Karlen"},{"first":"K.","last":"Kavukcuoglu"},{"last":"Kuksa"},{"last":"P"}],"year":"2011","title":"Natural language processing (almost) from scratch","source":"Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., and Kuksa, P. 2011. Natural language processing (almost) from scratch. Journal of Machine Learning Research, vol. 12. 707"},{"authors":[{"first":"S.","last":"Deerwester"},{"first":"S.","middle":"T.","last":"Dumais"},{"first":"G.","middle":"W.","last":"Furnas"},{"first":"T.","last":"Landauer"},{"last":"Harshman"},{"last":"R"}],"year":"1990","title":"Index-ing by latent semantic analysis","source":"Deerwester, S., Dumais, S. T., Furnas, G. W., Landauer, T., and Harshman, R. 1990. Index-ing by latent semantic analysis. Journal of the American Society for Information Science, 41(6): 391-407"},{"authors":[{"first":"J.","last":"DeNero"},{"first":"D.","last":"Gillick"},{"first":"J.","last":"Zhang"},{"last":"Klein"},{"last":"D"}],"year":"2006","title":"Why generative phrase models underper-form surface heuristics","source":"DeNero, J., Gillick, D., Zhang, J., and Klein, D. 2006. Why generative phrase models underper-form surface heuristics. In Workshop on Statistical Machine Translation, pp. 31-38."},{"authors":[{"first":"L.","last":"Deng"},{"first":"D.","last":"Yu"},{"last":"Platt"},{"last":"J"}],"year":"2012","title":"Scalable stacking and learning for building deep architectures","source":"Deng, L., Yu, D., and Platt, J. 2012. Scalable stacking and learning for building deep architectures. In ICASSP."},{"authors":[{"first":"K.","middle":"I.","last":"Diamantaras"},{"first":"S.","last":"Kung"},{"last":"Y"}],"year":"1996","title":"Principle Component Neural Networks: Theory and Applications","source":"Diamantaras, K. I., and Kung, S. Y. 1996. Principle Component Neural Networks: Theory and Applications. Wiley-Interscience."},{"authors":[{"first":"Dumais","last":"S."},{"first":"Letsche","last":"T."},{"first":"Littman","last":"M."},{"first":"Landauer","last":"T"}],"year":"1997","title":"Automatic cross-language retrieval using latent semantic indexing","source":"Dumais S., Letsche T., Littman M. and Landauer T. 1997. Automatic cross-language retrieval using latent semantic indexing. In AAAI-97 Spring Symposium Series: Cross-Language Text and Speech Retrieval."},{"authors":[{"first":"K.","last":"Ganchev"},{"first":"J.","last":"Graca"},{"first":"J.","last":"Gillenwater"},{"last":"Taskar"},{"last":"B"}],"year":"2010","title":"Posterior regularization for structured latent variable models","source":"Ganchev, K., Graca, J., Gillenwater, J., and Taskar, B. 2010. Posterior regularization for structured latent variable models. Journal of Machine Learning Research, 11 (2010): 2001-2049."},{"authors":[{"first":"J.","last":"Gao"},{"last":"He"},{"last":"X"}],"year":"2013","title":"Training MRF-based translation models using gradient ascent","source":"Gao, J., and He, X. 2013. Training MRF-based translation models using gradient ascent. In NAACL-HLT, pp. 450-459."},{"authors":[{"first":"J.","last":"Gao"},{"first":"K.","last":"Toutanova"},{"last":"Yih."},{"last":"W-T"}],"year":"2011","title":"Click-through-based latent semantic models for web search","source":"Gao, J., Toutanova, K., Yih., W-T. 2011. Click-through-based latent semantic models for web search. In SIGIR, pp. 675-684."},{"authors":[{"first":"X.","last":"He"},{"last":"Deng"},{"last":"L"}],"year":"2012","title":"Maximum expected bleu training of phrase and lexicon translation models","source":"He, X., and Deng, L. 2012. Maximum expected bleu training of phrase and lexicon translation models. In ACL, pp. 292-301."},{"authors":[{"first":"G.","last":"Hinton"},{"first":"R.","last":"Salakhutdinov"}],"year":"2010","title":"Discovering Binary Codes for Documents by Learning Deep Generative Models","source":"Hinton, G., and Salakhutdinov, R., 2010. Discovering Binary Codes for Documents by Learning Deep Generative Models. Topics in Cogni-tive Science, pp. 1-18."},{"authors":[{"first":"G.","last":"Hinton"},{"first":"L.","last":"Deng"},{"first":"D.","last":"Yu"},{"first":"G.","last":"Dahl"},{"first":"A.","last":"Mohamed"},{"first":"N.","last":"Jaitly"},{"first":"A.","last":"Senior"},{"first":"V.","last":"Vanhoucke"},{"first":"P.","last":"Nguyen"},{"first":"T.","last":"Sainath"},{"first":"B.","last":"Kingsbury"}],"year":"2012","title":"Deep neural networks for acoustic modeling in speech recognition","source":"Hinton, G., Deng, L., Yu, D., Dahl, G., Mohamed, A., Jaitly, N., Senior, A., Vanhoucke, V., Nguyen, P., Sainath, T., and Kingsbury, B., 2012. Deep neural networks for acoustic modeling in speech recognition. IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82-97."},{"authors":[{"last":"Hofmann"},{"last":"T"}],"year":"1999","title":"Probabilistic latent semantic indexing","source":"Hofmann, T. 1999. Probabilistic latent semantic indexing. In SIGIR, pp. 50-57."},{"authors":[{"last":"Huang"},{"last":"P-S."},{"first":"X.","last":"He"},{"first":"J.","last":"Gao"},{"first":"L.","last":"Deng"},{"first":"A.","last":"Acero"},{"last":"Heck"},{"last":"L"}],"year":"2013","title":"Learning deep structured semantic models for web search using clickthrough data","source":"Huang, P-S., He, X., Gao, J., Deng, L., Acero, A. and Heck, L. 2013. Learning deep structured semantic models for web search using clickthrough data. In CIKM."},{"authors":[{"first":"N.","last":"Kalchbrenner"},{"last":"Blunsom"},{"last":"P"}],"year":"2013","title":"Recurrent continuous translation models","source":"Kalchbrenner, N. and Blunsom, P. 2013. Recurrent continuous translation models. In EMNLP."},{"authors":[{"first":"P.","last":"Koehn"},{"first":"H.","last":"Hoang"},{"first":"A.","last":"Birch"},{"first":"C.","last":"Callison-Burch"},{"first":"M.","last":"Federico"},{"first":"N.","last":"Bertoldi"},{"first":"B.","last":"Cowan"},{"first":"W.","last":"Shen"},{"first":"C.","last":"Moran"},{"first":"R.","last":"Zens"},{"first":"C.","last":"Dyer"},{"first":"O.","last":"Bojar"},{"first":"A.","last":"Constantin"},{"last":"Herbst"},{"last":"E"}],"year":"2007","title":"Moses: open source toolkit for statistical machine translation","source":"Koehn, P., Hoang, H., Birch, A., Callison-Burch, C., Federico, M., Bertoldi, N., Cowan, B., Shen, W., Moran, C., Zens, R., Dyer, C., Bojar, O., Constantin, A., and Herbst, E. 2007. Moses: open source toolkit for statistical machine translation. In ACL 2007, demonstration session."},{"authors":[{"first":"P.","last":"Koehn"},{"last":"Monz"},{"last":"C"}],"year":"2006","title":"Manual and automatic evaluation of machine translation between European languages","source":"Koehn, P. and Monz, C. 2006. Manual and automatic evaluation of machine translation between European languages. In Workshop on Statistical Machine Translation, pp. 102-121."},{"authors":[{"first":"P.","last":"Koehn"},{"first":"F.","last":"Och"},{"last":"Marcu"},{"last":"D"}],"year":"2003","title":"Statistical phrase-based translation","source":"Koehn, P., Och, F., and Marcu, D. 2003. Statistical phrase-based translation. In HLT-NAACL, pp. 127-133."},{"authors":[{"first":"P.","last":"Lambert"},{"first":"R.","last":"Banchs"},{"last":"E"}],"year":"2005","title":"Data inferred multi-word expressions for statistical machine translation","source":"Lambert, P. and Banchs, R. E. 2005. Data inferred multi-word expressions for statistical machine translation. In MT Summit X, Phuket, Thailand."},{"authors":[{"first":"P.","last":"Li"},{"first":"Y.","last":"Liu"},{"last":"Sun"},{"last":"M"}],"year":"2013","title":"Recursive autoencoders for ITG-based translation","source":"Li, P., Liu, Y., and Sun, M. 2013. Recursive autoencoders for ITG-based translation. In EMNLP."},{"authors":[{"first":"P.","last":"Liang"},{"first":"A.","last":"Bouchard-Cote"},{"first":"D.","last":"Klein"},{"last":"Taskar"},{"last":"B"}],"year":"2006","title":"An end-to-end discriminative approach to machine translation","source":"Liang,P., Bouchard-Cote,A., Klein, D. and Taskar, B. 2006. An end-to-end discriminative approach to machine translation. In COLING-ACL."},{"authors":[{"first":"D.","last":"Marcu"},{"last":"Wong"},{"last":"W"}],"year":"2002","title":"A phrase-based, joint probability model for statistical machine translation","source":"Marcu, D., and Wong, W. 2002. A phrase-based, joint probability model for statistical machine translation. In EMNLP."},{"authors":[{"first":"T.","last":"Mikolov"},{"first":"M.","last":"Karafiat"},{"first":"L.","last":"Burget"},{"first":"J.","last":"Cernocky"},{"last":"Khudanpur"},{"last":"S"}],"year":"2010","title":"Recurrent neural network based language model","source":"Mikolov, T., Karafiat, M., Burget, L., Cernocky, J., and Khudanpur, S. 2010. Recurrent neural network based language model. In INTER-SPEECH, pp. 1045-1048."},{"authors":[{"first":"T.","last":"Mikolov"},{"first":"S.","last":"Kombrink"},{"first":"L.","last":"Burget"},{"first":"J.","last":"Cernocky"},{"last":"Khudanpur"},{"last":"S"}],"year":"2011","title":"Extensions of recurrent neural network language model","source":"Mikolov, T., Kombrink, S., Burget, L., Cernocky, J., and Khudanpur, S. 2011. Extensions of recurrent neural network language model. In ICASSP, pp. 5528-5531."},{"authors":[{"last":"Mikolov"},{"last":"T"}],"year":"2012","title":"Statistical Language Model based on Neural Networks","source":"Mikolov, T. 2012. Statistical Language Model based on Neural Networks. Ph.D. thesis, Brno University of Technology."},{"authors":[{"first":"T.","last":"Mikolov"},{"first":"Q.","middle":"V.","last":"Le"},{"last":"Sutskever"},{"last":"H"}],"year":"2013a","title":"Exploiting similarities among languages for machine translation","source":"Mikolov, T., Le, Q. V., and Sutskever, H. 2013a. Exploiting similarities among languages for machine translation. CoRR. 2013; abs/1309.4148."},{"authors":[{"first":"T.","last":"Mikolov"},{"first":"W.","last":"Yih"},{"last":"Zweig"},{"last":"G"}],"year":"2013b","title":"Linguistic Regularities in Continuous Space Word Representations","source":"Mikolov, T., Yih, W. and Zweig, G. 2013b. Linguistic Regularities in Continuous Space Word Representations. In NAACL-HLT."},{"authors":[{"first":"D.","last":"Mimno"},{"first":"H.","last":"Wallach"},{"first":"J.","last":"Naradowsky"},{"first":"D.","last":"Smith"},{"last":"McCallum"},{"last":"A"}],"year":"2009","title":"Polylingual topic models","source":"Mimno, D., Wallach, H., Naradowsky, J., Smith, D. and McCallum, A. 2009. Polylingual topic models. In EMNLP. 708"},{"authors":[{"first":"Niehues","last":"J."},{"first":"T.","last":"Herrmann"},{"first":"S.","last":"Vogel"},{"last":"Waibel"},{"last":"A"}],"year":"2011","title":"Wider context by using bilingual language models in machine translation","source":"Niehues J., Herrmann, T., Vogel, S., and Waibel, A. 2011. Wider context by using bilingual language models in machine translation."},{"authors":[{"last":"Och"},{"last":"F"}],"year":"2003","title":"Minimum error rate training in statistical machine translation","source":"Och, F. 2003. Minimum error rate training in statistical machine translation. In ACL, pp. 160-167."},{"authors":[{"first":"F.","last":"Och"},{"last":"Ney"},{"last":"H"}],"year":"2004","title":"The alignment tem-plate approach to statistical machine translation","source":"Och, F., and Ney, H. 2004. The alignment tem-plate approach to statistical machine translation. Computational Linguistics, 29(1): 19-51."},{"authors":[{"first":"K.","last":"Papineni"},{"first":"S.","last":"Roukos"},{"first":"T.","last":"Ward"},{"first":"Zhu","last":"W-J"}],"year":"2002","title":"BLEU: a method for automatic evaluation of machine translation","source":"Papineni, K., Roukos, S., Ward, T., and Zhu W-J. 2002. BLEU: a method for automatic evaluation of machine translation. In ACL."},{"authors":[{"first":"J.","last":"Platt"},{"first":"K.","last":"Toutanova"},{"last":"Yih"},{"last":"W"}],"year":"2010","title":"Translingual Document Representations from Discriminative Projections","source":"Platt, J., Toutanova, K., and Yih, W. 2010. Translingual Document Representations from Discriminative Projections. In EMNLP."},{"authors":[{"last":"Rosti"},{"last":"A-V."},{"first":"B.","last":"Hang"},{"first":"S.","last":"Matsoukas"},{"first":"R.","last":"Schwartz"},{"last":"S"}],"year":"2011","title":"Expected BLEU training for graphs: bbn system description for WMT system combination task","source":"Rosti, A-V., Hang, B., Matsoukas, S., and Schwartz, R. S. 2011. Expected BLEU training for graphs: bbn system description for WMT system combination task. In Workshop on Statistical Machine Translation."},{"authors":[{"first":"H.","last":"Schwenk"},{"first":"M.","middle":"R.","last":"Costa-Jussa"},{"first":"J.","middle":"A.","last":"Fonollosa"},{"last":"R"}],"year":"2007","title":"Smooth bilingual n-gram translation","source":"Schwenk, H., Costa-Jussa, M. R. and Fonollosa, J. A. R. 2007. Smooth bilingual n-gram translation. In EMNLP-CoNLL, pp. 430-438."},{"authors":[{"last":"Schwenk"},{"last":"H"}],"year":"2012","title":"Continuous space translation models for phrase-based statistical machine translation","source":"Schwenk, H. 2012. Continuous space translation models for phrase-based statistical machine translation. In COLING."},{"authors":[{"first":"H.","last":"Schwenk"},{"first":"A.","last":"Rousseau"},{"first":"Mohammed","last":"A"}],"year":"2012","title":"Large, pruned or continuous space language models on a GPU for statistical machine translation","source":"Schwenk, H., Rousseau, A., and Mohammed A. 2012. Large, pruned or continuous space language models on a GPU for statistical machine translation. In NAACL-HLT Workshop on the future of language modeling for HLT, pp. 11-19."},{"authors":[{"first":"H.","last":"Setiawan"},{"first":"B.","last":"Zhou"}],"year":"2013","title":"Discriminative training of 150 million translation parameters and its application to pruning","source":"Setiawan, H. and Zhou, B., 2013. Discriminative training of 150 million translation parameters and its application to pruning. In NAACL."},{"authors":[{"first":"R.","last":"Socher"},{"first":"B.","last":"Huval"},{"first":"C.","last":"Manning"},{"first":"A.","last":"Ng"}],"year":"2012","title":"Semantic Compositionality through Recursive Matrix-Vector Spaces","source":"Socher, R., Huval, B., Manning, C., Ng, A., 2012. Semantic Compositionality through Recursive Matrix-Vector Spaces. In EMNLP."},{"authors":[{"first":"R.","last":"Socher"},{"first":"C.","last":"Lin"},{"first":"A.","middle":"Y.","last":"Ng"},{"first":"C.","last":"Manning"},{"last":"D"}],"year":"2011","title":"Parsing natural scenes and natural language with recursive neural networks","source":"Socher, R., Lin, C., Ng, A. Y., and Manning, C. D. 2011. Parsing natural scenes and natural language with recursive neural networks. In ICML."},{"authors":[{"first":"L.","middle":"H.","last":"Son"},{"first":"A.","last":"Allauzen"},{"last":"Yvon"},{"last":"F"}],"year":"2012","title":"Continuous space translation models with neural networks","source":"Son, L. H., Allauzen, A., and Yvon, F. 2012. Continuous space translation models with neural networks. In NAACL-HLT, pp. 29-48."},{"authors":[{"first":"M.","last":"Sundermeyer"},{"first":"I.","last":"Oparin"},{"last":"Gauvain"},{"first":"J-L.","last":"Freiberg"},{"last":"B."},{"first":"R.","last":"Schluter"},{"last":"Ney"},{"last":"H"}],"year":"2013","title":"Comparison of feed forward and recurrent neural network language models","source":"Sundermeyer, M., Oparin, I., Gauvain, J-L. Freiberg, B., Schluter, R. and Ney, H. 2013. Comparison of feed forward and recurrent neural network language models. In ICASSP, pp. 8430–8434."},{"authors":[{"last":"Tur"},{"last":"G"},{"first":"L.","last":"Deng"},{"first":"D.","last":"Hakkani-Tur"},{"first":"X.","last":"He"}],"year":"2012","title":"Towards deeper understanding: deep convex networks for semantic utterance classifica-tion","source":"Tur, G, Deng, L., Hakkani-Tur, D., and He, X., 2012. Towards deeper understanding: deep convex networks for semantic utterance classifica-tion. In ICASSP."},{"authors":[{"first":"A.","last":"Vinokourov"},{"first":"J.","last":"Shawe-Taylor"},{"last":"Cristianini"},{"last":"N"}],"year":"2002","title":"Inferring a semantic representation of text via cross-language correlation analysis","source":"Vinokourov,A., Shawe-Taylor,J. and Cristianini,N. 2002. Inferring a semantic representation of text via cross-language correlation analysis. In NIPS."},{"authors":[{"first":"J.","last":"Weston"},{"first":"S.","last":"Bengio"},{"last":"Usunier"},{"last":"N"}],"year":"2011","title":"Large scale image annotation: learning to rank with joint word-image embeddings","source":"Weston, J., Bengio, S., and Usunier, N. 2011. Large scale image annotation: learning to rank with joint word-image embeddings. In IJCAI."},{"authors":[{"first":"J.","last":"Wuebker"},{"first":"A.","last":"Mauser"},{"last":"Ney"},{"last":"H"}],"year":"2010","title":"Training phrase translation models with leaving-one-out","source":"Wuebker, J., Mauser, A., and Ney, H. 2010. Training phrase translation models with leaving-one-out. In ACL, pp. 475-484."},{"authors":[{"first":"W.","last":"Yih"},{"first":"K.","last":"Toutanova"},{"first":"J.","last":"Platt"},{"last":"Meek"},{"last":"C"}],"year":"2011","title":"Learning discriminative projections for text similarity measures","source":"Yih, W., Toutanova, K., Platt, J., and Meek, C. 2011. Learning discriminative projections for text similarity measures. In CoNLL."},{"authors":[{"first":"Y.","last":"Zhang"},{"first":"L.","last":"Deng"},{"first":"X.","last":"He"},{"last":"Acero"},{"last":"A"}],"year":"2011","title":"A novel decision function and the associated decision-feedback learning for speech translation","source":"Zhang, Y., Deng, L., He, X., and Acero, A. 2011. A novel decision function and the associated decision-feedback learning for speech translation. In ICASSP."},{"authors":[{"first":"A.","last":"Zhila"},{"first":"W.","last":"Yih"},{"first":"C.","last":"Meek"},{"first":"G.","last":"Zweig"},{"last":"Mikolov"},{"last":"T"}],"year":"2013","title":"Combining heterogeneous models for measuring relational similarity","source":"Zhila, A., Yih, W., Meek, C., Zweig, G. and Mikolov, T. 2013. Combining heterogeneous models for measuring relational similarity. In NAACL-HLT."},{"authors":[{"first":"W.","middle":"Y.","last":"Zou"},{"first":"R.","last":"Socher"},{"first":"D.","last":"Cer"},{"first":"C.","last":"Manning"},{"last":"D"}],"year":"2013","title":"Bilingual word embeddings for phrase-based machine translation","source":"Zou, W. Y., Socher, R., Cer, D., and Manning, C. D. 2013. Bilingual word embeddings for phrase-based machine translation. In EMNLP. 709"}],"cites":[{"style":0,"text":"Schwenk et al., 2012","origin":{"pointer":"/sections/3/paragraphs/4","offset":242,"length":20},"authors":[{"last":"Schwenk"},{"last":"al."}],"year":"2012","references":["/references/41"]},{"style":0,"text":"Niehues et al. (2011)","origin":{"pointer":"/sections/3/paragraphs/6","offset":428,"length":21},"authors":[{"last":"Niehues"},{"last":"al."}],"year":"2011","references":[]},{"style":0,"text":"Kalchbrenner and Blunsom (2013)","origin":{"pointer":"/sections/4/paragraphs/3","offset":384,"length":31},"authors":[{"last":"Kalchbrenner"},{"last":"Blunsom"}],"year":"2013","references":[]},{"style":0,"text":"Zhang et al., 2011","origin":{"pointer":"/sections/4/paragraphs/4","offset":170,"length":18},"authors":[{"last":"Zhang"},{"last":"al."}],"year":"2011","references":["/references/52"]},{"style":0,"text":"Koehn et al. (2007)","origin":{"pointer":"/sections/5/paragraphs/9","offset":122,"length":19},"authors":[{"last":"Koehn"},{"last":"al."}],"year":"2007","references":["/references/20"]},{"style":0,"text":"Liang et al. (2006)","origin":{"pointer":"/sections/5/paragraphs/17","offset":323,"length":19},"authors":[{"last":"Liang"},{"last":"al."}],"year":"2006","references":["/references/25"]},{"style":0,"text":"Andrew and Gao (2007)","origin":{"pointer":"/sections/14/paragraphs/0","offset":99,"length":21},"authors":[{"last":"Andrew"},{"last":"Gao"}],"year":"2007","references":[]},{"style":0,"text":"Gao et al. (2011)","origin":{"pointer":"/sections/17/paragraphs/19","offset":386,"length":17},"authors":[{"last":"Gao"},{"last":"al."}],"year":"2011","references":["/references/13"]},{"style":0,"text":"Gao et al. (2011)","origin":{"pointer":"/sections/17/paragraphs/19","offset":610,"length":17},"authors":[{"last":"Gao"},{"last":"al."}],"year":"2011","references":["/references/13"]},{"style":0,"text":"Gao and He (2013)","origin":{"pointer":"/sections/17/paragraphs/19","offset":1005,"length":17},"authors":[{"last":"Gao"},{"last":"He"}],"year":"2013","references":[]},{"style":0,"text":"Gao et al. (2011)","origin":{"pointer":"/sections/17/paragraphs/19","offset":1690,"length":17},"authors":[{"last":"Gao"},{"last":"al."}],"year":"2011","references":["/references/13"]},{"style":0,"text":"Gao and He (2013)","origin":{"pointer":"/sections/17/paragraphs/21","offset":124,"length":17},"authors":[{"last":"Gao"},{"last":"He"}],"year":"2013","references":[]},{"style":0,"text":"Tur et al., 2012","origin":{"pointer":"/sections/18/paragraphs/2","offset":216,"length":16},"authors":[{"last":"Tur"},{"last":"al."}],"year":"2012","references":["/references/47"]}]}
