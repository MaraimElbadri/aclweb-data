{"sections":[{"title":"","paragraphs":["Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 293–303, Baltimore, Maryland, USA, June 23-25 2014. c⃝2014 Association for Computational Linguistics"]},{"title":"Max-Margin Tensor Neural Network for Chinese Word Segmentation Wenzhe Pei Tao Ge Baobao Chang","paragraphs":["∗"]},{"title":"Key Laboratory of Computational Linguistics, Ministry of Education School of Electronics Engineering and Computer Science, Peking University Beijing, P.R.China, 100871 {peiwenzhe,getao,chbb}@pku.edu.cn Abstract","paragraphs":["Recently, neural network models for natural language processing tasks have been increasingly focused on for their ability to alleviate the burden of manual feature engineering. In this paper, we propose a novel neural network model for Chinese word segmentation called Max-Margin Tensor Neural Network (MMTNN). By exploiting tag embeddings and tensor-based transformation, MMTNN has the ability to model complicated interactions between tags and context characters. Furthermore, a new tensor factorization approach is proposed to speed up the model and avoid overfitting. Experiments on the benchmark dataset show that our model achieves better performances than previous neural network models and that our model can achieve a competitive performance with minimal feature engineering. Despite Chinese word segmentation being a specific case, MMTNN can be easily generalized and applied to other sequence labeling tasks."]},{"title":"1 Introduction","paragraphs":["Unlike English and other western languages, Chinese do not delimit words by white-space. Therefore, word segmentation is a preliminary and important pre-process for Chinese language processing. Most previous systems address this problem by treating this task as a sequence labeling problem where each character is assigned a tag indicating its position in the word. These systems are effective because researchers can incorporate a large body of handcrafted features into the models. However, the ability of these models is restricted ∗ Corresponding author by the design of features and the number of features could be so large that the result models are too large for practical use and prone to overfit on training corpus.","Recently, neural network models have been increasingly focused on for their ability to minimize the effort in feature engineering. Collobert et al. (2011) developed the SENNA system that approaches or surpasses the state-of-the-art systems on a variety of sequence labeling tasks for English. Zheng et al. (2013) applied the architecture of Collobert et al. (2011) to Chinese word segmentation and POS tagging and proposed a perceptron-style algorithm to speed up the training process with negligible loss in performance.","Workable as previous neural network models seem, a limitation of them to be pointed out is that the tag-tag interaction, tag-character interaction and character-character interaction are not well modeled. In conventional feature-based linear (log-linear) models, these interactions are explicitly modeled as features. Take phrase “ 打篮 球(play basketball)” as an example, assuming we are labeling character C0=“ 篮”, possible features could be: f1 =","{ 1 C−1=“ 打” and C1=“ 球” and y0=“B” 0 else f2 =","{ 1 C0=“ 篮” and y0=“B” and y−1=“S” 0 else To capture more interactions, researchers have designed a large number of features based on linguistic intuition and statistical information. In previous neural network models, however, hardly can such interactional effects be fully captured rely-ing only on the simple transition score and the single non-linear transformation (See section 2). In order to address this problem, we propose a new model called Max-Margin Tensor Neural Network (MMTNN) that explicitly models the interactions 293 between tags and context characters by exploiting tag embeddings and tensor-based transformation. Moreover, we propose a tensor factorization approach that effectively improves the model efficiency and prevents from overfitting. We evaluate the performance of Chinese word segmentation on the PKU and MSRA benchmark datasets in the second International Chinese Word Segmentation Bakeoff (Emerson, 2005) which are commonly used for evaluation of Chinese word segmentation. Experiment results show that our model outperforms other neural network models.","Although we focus on the question that how far we can go without using feature engineering in this paper, the study of deep learning for NLP tasks is still a new area in which it is currently challenging to surpass the state-of-the-art without additional features. Following Mansur et al. (2013), we wonder how well our model can perform with minimal feature engineering. Therefore, we integrate additional simple character bigram features into our model and the result shows that our model can achieve a competitive performance that other systems hardly achieve unless they use more complex task-specific features.","The main contributions of our work are as follows:","• We propose a Max-Margin Tensor Neural Network for Chinese word segmentation without feature engineering. The test results on the benchmark dataset show that our model outperforms previous neural network models.","• We propose a new tensor factorization approach that models each tensor slice as the product of two low-rank matrices. Not only does this approach improve the efficiency of our model but also it avoids the risk of overfitting.","• Compared with previous works that use a large number of handcrafted features, our model can achieve a competitive performance with minimal feature engineering.","• Despite Chinese word segmentation being a specific case, our approach can be easily generalized to other sequence labeling tasks. The remaining part of this paper is organized as follows. Section 2 describes the details of conventional neural network architecture. Section 3 Figure 1: Conventional Neural Network describes the details of our model. Experiment results are reported in Section 4. Section 5 reviews the related work. The conclusions are given in Section 6."]},{"title":"2 Conventional Neural Network 2.1 Lookup Table","paragraphs":["The idea of distributed representation for symbolic data is one of the most important reasons why the neural network works. It was proposed by Hinton (1986) and has been a research hot spot for more than twenty years (Bengio et al., 2003; Collobert et al., 2011; Schwenk et al., 2012; Mikolov et al., 2013a). Formally, in the Chinese word segmentation task, we have a character dictionary D of size |D|. Unless otherwise specified, the character dictionary is extracted from the training set and unknown characters are mapped to a special symbol that is not used elsewhere. Each character c ∈ D is represented as a real-valued vector (character embedding) Embed(c) ∈ Rd","where d is the dimensionality of the vector space. The character embeddings are then stacked into a embedding matrix M ∈ Rd×|D|",". For a character c ∈ D that has an associated index k, the corresponding character embedding Embed(c) ∈ Rd","is retrieved by the Lookup Table layer as shown in Figure 1: Embed(c) = M ek (1) Here ek ∈ R|D|","is a binary vector which is zero in all positions except at k-th index. The Lookup Table layer can be seen as a simple projection layer where the character embedding for each context 294 character is achieved by table lookup operation according to their indices. The embedding matrix M is initialized with small random numbers and trained by back-propagation. We will analyze in more detail about the effect of character embeddings in Section 4. 2.2 Tag Scoring The most common tagging approach is the window approach. The window approach assumes that the tag of a character largely depends on its neighboring characters. Given an input sentence c[1:n], a window of size w slides over the sentence from character c1 to cn. We set w = 5 in all experiments. As shown in Figure 1, at position ci, 1 ≤ i ≤ n, the context characters are fed into the Lookup Table layer. The characters exceeding the sentence boundaries are mapped to one of two special symbols, namely “start” and “end” symbols. The character embeddings extracted by the Lookup Table layer are then concatenated into a single vector a ∈ RH1",", where H1 = w · d is the size of Layer 1. Then a is fed into the next layer which performs linear transformation followed by an element-wise activation function g such as tanh, which is used in our experiments: h = g(W1a + b1) (2) where W1 ∈ RH2×H1",", b1 ∈ RH2×1",", h ∈ RH2",". H2 is a hyper-parameter which is the number of hidden units in Layer 2. Given a set of tags T of size |T |, a similar linear transformation is performed except that no non-linear function is followed: f (t|c[i−2:i+2]) = W2h + b2 (3) where W2 ∈ R|T |×H2",", b2 ∈ R|T |×1",". f (t|c[i−2:i+2]) ∈ R|T |","is the score vector for each possible tag. In Chinese word segmentation, the most prevalent tag set T is BMES tag set, which uses 4 tags to carry word boundary information. It uses B, M, E and S to denote the Beginning, the Middle, the End of a word and a Single character forming a word respectively. We use this tag set in our method. 2.3 Model Training and Inference Despite sharing commonalities mentioned above, previous work models the segmentation task differently and therefore uses different training and inference procedure. Mansur et al. (2013) modeled Chinese word segmentation as a series of classification task at each position of the sentence in which the tag score is transformed into probability using softmax function: p(ti|c[i−2:i+2]) = exp(f (ti|c[i−2:i+2]))","∑ t′ exp(f (t′","|c[i−2:i+2])) The model is then trained in MLE-style which maximizes the log-likelihood of the tagged data. Obviously, it is a local model which cannot capture the dependency between tags and does not support to infer the tag sequence globally.","To model the tag dependency, previous neural network models (Collobert et al., 2011; Zheng et al., 2013) introduce a transition score Aij for jumping from tag i ∈ T to tag j ∈ T . For a input sentence c[1:n] with a tag sequence t[1:n], a sentence-level score is then given by the sum of transition and network scores: s(c[1:n], t[1:n], θ) = n ∑ i=1 (Ati−1ti +fθ(ti|c[i−2:i+2]))","(4) where fθ(ti|c[i−2:i+2]) indicates the score output for tag ti at the i-th character by the network with parameters θ = (M, A, W1, b1, W2, b2). Given the sentence-level score, Zheng et al. (2013) proposed a perceptron-style training algorithm in-spired by the work of Collins (2002). Compared with Mansur et al. (2013), their model is a global one where the training and inference is performed at sentence-level.","Workable as these methods seem, one of the limitations of them is that the tag-tag interaction and the neural network are modeled seperately. The simple tag-tag transition neglects the impact of context characters and thus limits the ability to capture flexible interactions between tags and context characters. Moreover, the simple non-linear transformation in equation (2) is also poor to model the complex interactional effects in Chinese word segmentation."]},{"title":"3 Max-Margin Tensor Neural Network 3.1 Tag Embedding","paragraphs":["To better model the tag-tag interaction given the context characters, distributed representation for tags instead of traditional discrete symbolic representation is used in our model. Similar to character embeddings, given a fixed-sized tag setT , the tag embeddings for tags are stored in a tag embedding matrix L ∈ Rd×|T |",", where d is the dimensionality 295 Figure 2: Max-Margin Tensor Neural Network of the vector space (same with character embeddings). Then the tag embedding Embed(t) ∈ Rd for tag t ∈ T with index k can be retrieved by the lookup operation: Embed(t) = Lek (5) where ek ∈ R|T |×1","is a binary vector which is zero in all positions except at k-th index. The tag embeddings start from a random initialization and can be automatically trained by back-propagation. Figure 2 shows the new Lookup Table layer with tag embeddings. Assuming we are at the i-th character of a sentence, besides the character embeddings, the tag embeddings of the previous tags are also considered1",". For a fast tag inference, only the previous tag ti−1 is used in our model even though a longer history of tags can be considered. The concatenation operation in Layer 1 then concatenates the character embeddings and tag embedding together into a long vector a. In this way, the tag representation can be directly incorporated in the neural network so that the tag-tag interaction and tag-character interaction can be explicitly modeled in deeper layers (See Section 3.2). Moreover, the transition score in equation (4) is not necessary in our model, because, by incorporating tag embedding into the neural network, the effect of tag-tag interaction and tag-character interaction are covered uniformly in one same model. Now 1","We also tried the architecture in which the tag embedding of current tag is also considered, but this did not bring much improvement and runs slower Figure 3: The tensor-based transformation in Layer 2. a is the input from Layer 1. V is the tensor parameter. Each dashed box represents one of the H2-many tensor slices, which defines the bilinear form on vector a. equation (4) can be rewritten as follows: s(c[1:n], t[1:n], θ) = n ∑ i=1 fθ(ti|c[i−2:i+2], ti−1)","(6) where fθ(ti|c[i−2:i+2], ti−1) is the score output for tag ti at the i-th character by the network with parameters θ. Like Collobert et al. (2011) and Zheng et al. (2013), our model is also trained at sentence-level and carries out inference globally. 3.2 Tensor Neural Network A tensor is a geometric object that describes relations between vectors, scalars, and other tensors. It can be represented as a multi-dimensional array of numerical values. An advantage of the tensor is that it can explicitly model multiple interactions in data. As a result, tensor-based model have been widely used in a variety of tasks (Salakhutdinov et al., 2007; Krizhevsky et al., 2010; Socher et al., 2013b).","In Chinese word segmentation, a proper modeling of the tag-tag interaction, tag-character interaction and character-character interaction is very important. In linear models, these kinds of interactions are usually modeled as features. In conventional neural network models, however, the input embeddings only implicitly interact through the non-linear function which can hardly model the complexity of the interactions. Given the advantage of tensors, we apply a tensor-based transformation to the input vector. Formally, we use a 3-way tensor V [1:H2]","∈ RH2×H1×H1","to directly model the interactions, where H2 is the size of 296 Layer 2 and H1 = (w + 1) · d is the size of concatenated vector a in Layer 1 as shown in Figure 2. Figure 3 gives an example of the tensor-based transformation2",". The output of a tensor product is a vector z ∈ RH2","where each dimension z","i is the","result of the bilinear form defined by each tensor","slice V [i] ∈ RH1×H1",":","z = aT V [1:H2]","a; z","i = aT V [i]","a = ∑ j,k V [i] jk ajak","(7) Since vector a is the concatenation of character embeddings and the tag embedding, equation (7) can be written in the following form: zi = ∑ p,q ∑ j,k V [i] (p,q,j,k)E [p] j E [q] k where E [p] j is the j-th element of the p-th embedding in Lookup Table layer and V [i] (p,q,j,k) is the corresponding coefficient for E [p] j and E","[q]","k in V [i]",". As we can see, in each tensor slice i, the embeddings are explicitly related in a bilinear form which captures the interactions between characters and tags. The multiplicative operations between tag embeddings and character embeddings can somehow be seen as “feature combination”, which are hand-designed in feature-based models. Our model learns the information automatically and encodes them in tensor parameters and embeddings. Intuitively, we can interpret each slice of the tensor as capturing a specific type of tag-character interaction and character-character interaction.","Combining the tensor product with linear transformation, the tensor-based transformation in Layer 2 is defined as:","h = g(aT","V [1:H2]","a + W 1a + b1) (8)","where W1 ∈ RH2×H1",", b1 ∈ RH2×1",", h ∈ RH2",".","In fact, equation (2) used in previous work is a","special case of equation (8) when V is set to 0. 3.3 Tensor Factorization Despite tensor-based transformation being effective for capturing the interactions, introducing tensor-based transformation into neural network models to solve sequence labeling task is time prohibitive since the tensor product operation drastically slows down the model. Without consider-ing matrix optimization algorithms, the complexity of the non-linear transformation in equation (2)","2","The bias term is omitted in Figure 3 for simplicity Figure 4: Tensor product with tensor factorization is O(H1H2) while the tensor operation complexity in equation (8) is O(H2","1 H2). The tensor-based transformation is H1 times slower. Moreover, the additional tensor could bring millions of parameters to the model which makes the model suffer from the risk of overfitting. To remedy this, we propose a tensor factorization approach that factorizes each tensor slice as the product of two low-rank matrices. Formally, each tensor slice V [i]","∈ RH1×H1","is factorized into two low rank matrix P [i]","∈ RH1×r","and Q[i]","∈ Rr×H1",":","V [i]","= P [i]","Q[i]",", 1 ≤ i ≤ H 2 (9) where r ≪ H1 is the number of factors. Substituting equation (9) into equation (8), we get the factorized tensor function:","h = g(aT","P [1:H2]","Q[1:H2]","a + W 1a + b1) (10) Figure 4 illustrates the operation in each slice of the factorized tensor. First, vector a is projected into two r-dimension vectors f1 and f2. Then the output zi for each tensor slice i is the dot-product of f1 and f2. The complexity of the tensor operation is now O(rH1H2). As long as r is small enough, the factorized tensor operation would be much faster than the un-factorized one and the number of free parameters would also be much smaller, which prevent the model from overfitting. 3.4 Max-Margin Training We use the Max-Margin criterion to train our model. Intuitively, the Max-Margin criterion provides an alternative to probabilistic, likelihood-based estimation methods by concentrating directly on the robustness of the decision boundary of a model (Taskar et al., 2005). We use Y (xi) to denote the set of all possible tag sequences for 297 a given sentence xi and the correct tag sequence for xi is yi. The parameters of our model are θ = {W1, b1, W2, b2, M, L, P [1:H2]",", Q[1:H2]","}. We first define a structured margin loss△(yi, ŷ) for predicting a tag sequence ŷ for a given correct tag sequence yi: △(yi, ŷ) = n ∑ j κ1{yi,j ̸= ŷj} (11) where n is the length of sentence xi and κ is a discount parameter. The loss is proportional to the number of characters with an incorrect tag in the proposed tag sequence, which increases the more incorrect the proposed tag sequence is. For a given training instance (xi, yi), we search for the tag sequence with the highest score:","y∗ = arg max","ŷ∈Y (x) s(xi, ŷ, θ) (12) where the tag sequence is found and scored by the Tensor Neural Network via the function s in equation (6). The object of Max-Margin training is that the highest scoring tag sequence is the correct one: y∗","= yi and its score will be larger up to a margin to other possible tag sequences ŷ ∈ Y (xi): s(x, yi, θ) ≥ s(x, ŷ, θ) + △(yi, ŷ) This leads to the regularized objective function for m training examples: J (θ) = 1 m m ∑ i=1 li(θ) + λ 2 ||θ||2","li(θ) = max ŷ∈Y (xi)(s(xi, ŷ, θ) + △(yi, ŷ)) −s(xi, yi, θ)) (13) By minimizing this object, the score of the correct tag sequence yi is increased and score of the highest scoring incorrect tag sequence ŷ is decreased.","The objective function is not differentiable due to the hinge loss. We use a generalization of gradient descent called subgradient method (Ratliff et al., 2007) which computes a gradient-like direc-tion. The subgradient of equation (13) is: ∂J ∂θ = 1 m ∑ i (","∂s(xi, ŷmax, θ) ∂θ −","∂s(xi, yi, θ) ∂θ )+λθ where ŷmax is the tag sequence with the highest score in equation (13). Following Socher et al. (2013a), we use the diagonal variant of AdaGrad PKU MSRA","Identical words 5.5 × 104","8.8 × 104","Total words 1.1 × 106","2.4 × 106","Identical characters 5 × 103","5 × 103","Total characters 1.8 × 106","4.1 × 106 Table 1: Details of the PKU and MSRA datasets Window size w = 5 Character(tag) embedding size d = 25 Hidden unit number H2 = 50 Number of factors r = 10 Initial learning rate α = 0.2 Margin loss discount κ = 0.2 Regularization λ = 10−4 Table 2: Hyperparameters of our model (Duchi et al., 2011) with minibatchs to minimize the objective. The parameter update for the i-th parameter θt,i at time step t is as follows: θt,i = θt−1,i − α","√∑t τ=1 g2","τ,i gt,i (14) where α is the initial learning rate and gτ ∈ R|θi| is the subgradient at time step τ for parameter θi."]},{"title":"4 Experiment 4.1 Data and Model Selection","paragraphs":["We use the PKU and MSRA data provided by the second International Chinese Word Segmentation Bakeoff (Emerson, 2005) to test our model. They are commonly used by previous state-of-the-art models and neural network models. Details of the data are listed in Table 1. For evaluation, we use the standard bake-off scoring program to calculate precision, recall, F1-score and out-of-vocabulary (OOV) word recall.","For model selection, we use the first 90% sentences in the training data for training and the rest 10% sentences as development data. The minibatch size is set to 20. Generally, the number of hidden units has a limited impact on the performance as long as it is large enough. We found that 50 is a good trade-off between speed and model performance. The dimensionality of character (tag) embedding is set to 25 which achieved the best performance and faster than 50- or 100dimensional ones. We also validated on the number of factors for tensor factorization. The performance is not boosted and the training time in-298","P R F OOV CRF 87.8 85.7 86.7 57.1 NN 92.4 92.2 92.3 60.0 NN+Tag Embed 93.0 92.7 92.9 61.0 MMTNN 93.7 93.4 93.5 64.2 Table 3: Test results with different configurations. NN stands for the conventional neural network. NN+Tag Embed stands for the neural network with tag embeddings. creases drastically when the number of factors is larger than 10. We hypothesize that larger factor size results in too many parameters to train and hence perform worse. The final hyperparameters of our model are set as in Table 2. 4.2 Experiment Results We first perform a close test3","on the PKU dataset to show the effect of different model configurations. We also compare our model with the CRF model (Lafferty et al., 2001), which is a widely used log-linear model for Chinese word segmentation. The input feature to the CRF model is simply the context characters (unigram feature) without any additional feature engineering. We use an open source toolkit CRF++4","to train the CRF model. All the neural networks are trained using the Max-Margin approach described in Section 3.4. Table 3 summarizes the test results. As we can see, by using Tag embedding, the F-score is improved by +0.6% and OOV recall is improved by +1.0%, which shows that tag embeddings succeed in modeling the tag-tag interaction and tag-character interaction. Model performance is further boosted after using tensor-based transformation. The F-score is improved by +0.6% while OOV recall is improved by +3.2%, which denotes that tensor-based transformation captures more interactional information than simple non-linear transformation.","Another important result in Table 3 is that our neural network models perform much better than CRF-based model when only unigram features are used. Compared with CRF, there are two differences in neural network models. First, the discrete feature vector is replaced with dense character embeddings. Second, the non-linear transformation","3","No other material or knowledge except the training data is allowed","4","http://crfpp.googlecode.com/svn/ trunk/doc/index.html?source=navbar 一一一(one) 李李李(Li) 。。。(period) 二(two) 赵(Zhao) ,(comma) 三(three) 蒋(Jiang) :(colon) 四(four) 孔(Kong) ?(question mark) 五(five) 冯(Feng) “(quotation mark) 六(six) 吴(Wu) 、(Chinese comma) Table 4: Examples of character embeddings is used to discover higher level representation. In fact, CRF can be regarded as a special neural network without non-linear function (Wang and Manning, 2013). Wang and Manning (2013) conduct an empirical study on the effect of non-linearity and the results suggest that non-linear models are highly effective only when distributed representation is used. To explain why distributed representation captures more information than discrete features, we show in Table 4 the effect of character embeddings which are obtained from the lookup table of MMTNN after training. The first row lists three characters we are interested in. In each column, we list the top 5 characters that are nearest (measured by Euclidean distance) to the corresponding character in the first row according to their embeddings. As we can see, characters in the first column are all Chinese number characters and characters in the second column and the third column are all Chinese family names and Chinese punctuations respectively. Therefore, compared with discrete feature representations, distributed representation can capture the syntactic and semantic similarity between characters. As a result, the model can still perform well even if some words do not appear in the training cases.","We further compare our model with previous neural network models on both PKU and MSRA datasets. Since Zheng et al. (2013) did not report the results on the these datasets, we reimplemented their model and tested it on the test data. The results are listed in the first three rows of Table 5, which shows that our model achieved higher F-score than the previous neural network models. 4.3 Unsupervised Pre-training Previous work found that the performance can be improved by pre-training the character embeddings on large unlabeled data and using the obtained embeddings to initialize the character lookup table instead of random initialization 299","Models PKU MSRA","P R F OOV P R F OOV (Mansur et al., 2013) 87.1 87.9 87.5 48.9 92.3 92.2 92.2 53.7 (Zheng et al., 2013) 92.8 92.0 92.4 63.3 92.9 93.6 93.3 55.7 MMTNN 93.7 93.4 93.5 64.2 94.6 94.2 94.4 61.4 (Mansur et al., 2013) + Pre-training 91.2 92.7 92.0 68.8 93.1 93.1 93.1 59.7 (Zheng et al., 2013) + Pre-training 93.5 92.2 92.8 69.0 94.2 93.7 93.9 64.1 MMTNN + Pre-training 94.4 93.6 94.0 69.0 95.2 94.6 94.9 64.8 Table 5: Comparison with previous neural network models (Mansur et al., 2013; Zheng et al., 2013). Mikolov et al. (2013b) show that pre-trained embeddings can capture interesting semantic and syntactic information such as king−man+woman ≈ queen on English data. There are several ways to learn the embeddings on unlabeled data. Mansur et al. (2013) used the model proposed by Bengio et al. (2003) which learns the embeddings based on neural language model. Zheng et al. (2013) followed the model proposed by Collobert et al. (2008). They constructed a neural network that outputs high scores for windows that occur in the corpus and low scores for windows where one character is replaced by a random one. Mikolov et al. (2013a) proposed a faster skip-gram model word2vec5","which tries to maximize classification of a word based on another word in the same sentence. In this paper, we use word2vec because preliminary experiments did not show differences between performances of these models but word2vec is much faster to train. We pre-train the embeddings on the Chinese Giga-word corpus (Graff and Chen, 2005). As shown in Table 5 (last three rows), both the F-score and OOV recall of our model boost by using pre-training. Our model still outperforms other models after pre-training. 4.4 Minimal Feature Engineering Although we focus on the question that how far we can go without using feature engineering in this paper, the study of deep learning for NLP tasks is still a new area in which it is currently challenging to surpass the state-of-the-art without additional features. To incorporate features into the neural network, Mansur et al. (2013) proposed the feature-based neural network where each context feature is represented as feature embeddings. The idea of feature embeddings is similar to that of character embeddings described in section 2.1. 5 https://code.google.com/p/word2vec/","Model PKU MSRA Best05(Chen et al., 2005) 95.0 96.0 Best05(Tseng et al., 2005) 95.0 96.4 (Zhang et al., 2006) 95.1 97.1 (Zhang and Clark, 2007) 94.5 97.2 (Sun et al., 2009) 95.2 97.3 (Sun et al., 2012) 95.4 97.4 (Zhang et al., 2013) 96.1 97.4 MMTNN 94.0 94.9 MMTNN + bigram 95.2 97.2 Table 6: Comparison with state-of-the-art systems Formally, we assume the extracted features form a feature dictionary Df . Then each feature f ∈ Df is represented by a d-dimensional vector which is called feature embedding. Following their idea, we try to find out how well our model can perform with minimal feature engineering.","A very common feature in Chinese word segmentation is the character bigram feature. For-mally, at the i-th character of a sentence c[1:n], the bigram features are ckck+1(i − 3 < k < i + 2). In our model, the bigram features are extracted in the window context and then the corresponding bigram embeddings are concatenated with character embeddings in Layer 1 and fed into Layer 2. In Mansur et al. (2013), the bigram embeddings are pre-trained on unlabeled data with character embeddings, which significantly improves the model performance. Given the long time for pre-training bigram embeddings, we only pre-train the character embeddings and the bigram embeddings are initialized as the average of character embeddings of ck and ck+1. Further improvement could be obtained if the bigram embeddings are also pre-trained. Table 6 lists the segmentation performances of our model as well as previous state-of-the-art systems. When bigram features are added, the F-score of our model improves 300 from 94.0% to 95.2% on PKU dataset and from 94.9% to 97.2% on MSRA dataset. It is a competitive result given that our model only use simple bigram features while other models use more complex features. For example, Sun et al. (2012) uses additional word-based features. Zhang et al. (2013) uses eight types of features such as Mutual Information and Accessor Variety and they extract dynamic statistical features from both an in-domain corpus and an out-of-domain corpus using co-training. Since feature engineering is not the main focus of this paper, we did not experiment with more features."]},{"title":"5 Related Work","paragraphs":["Chinese word segmentation has been studied with considerable efforts in the NLP community. The most popular approach treats word segmentation as a sequence labeling problem which was first proposed in Xue (2003). Most previous systems address this task by using linear statistical models with carefully designed features such as bigram features, punctuation information (Li and Sun, 2009) and statistical information (Sun and Xu, 2011). Recently, researchers have tended to explore new approaches for word segmentation which circumvent the feature engineering by automatically learning features with neural network models (Mansur et al., 2013; Zheng et al., 2013). Our study is consistent with this line of research, however, our model explicitly models the interactions between tags and context characters and accordingly captures more semantic information.","Tensor-based transformation was also used in other neural network models for its ability to capture multiple interactions in data. For example, Socher et al. (2013b) exploited tensor-based function in the task of Sentiment Analysis to capture more semantic information from constituents. However, given the small size of their tensor matrix, they do not have the problem of high time cost and overfitting problem as we faced in modeling a sequence labeling task like Chinese word segmentation. That’s why we propose to decrease computational cost and avoid overfitting with tensor factorization.","Various tensor factorization (decomposition) methods have been proposed recently for tensor-based dimension reduction (Cohen et al., 2013; Van de Cruys et al., 2013; Chang et al., 2013). For example, Chang et al. (2013) proposed the Multi-Relational Latent Semantic Analysis. Similar to LSA, a low rank approximation of the tensor is derived using a tensor decomposition approch. Similar ideas were also used for collaborative filtering (Salakhutdinov et al., 2007) and object recognition (Ranzato et al., 2010). Our tensor factorization is related to these work but uses a different tensor factorization approach. By introducing tensor factorization into the neural network model for sequence labeling tasks, the model training and inference are speeded up and overfitting is prevented."]},{"title":"6 Conclusion","paragraphs":["In this paper, we propose a new model called Max-Margin Tensor Neural Network that explicitly models the interactions between tags and context characters. Moreover, we propose a tensor factorization approach that effectively improves the model efficiency and avoids the risk of overfitting. Experiments on the benchmark datasets show that our model achieve better results than previous neural network models and that our model can achieve a competitive result with minimal feature engineering. In the future, we plan to further extend our model and apply it to other structure predic-tion problems."]},{"title":"Acknowledgments","paragraphs":["This work is supported by National Natural Science Foundation of China under Grant No. 61273318 and National Key Basic Research Pro-gram of China 2014CB340504."]},{"title":"References","paragraphs":["Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. A neural probabilistic language model. Journal of Machine Learning Research, 3:1137–1155.","Kai-Wei Chang, Wen-tau Yih, and Christopher Meek. 2013. Multi-relational latent semantic analysis. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1602–1612, Seattle, Washington, USA, October. Association for Computational Linguistics.","Aitao Chen, Yiping Zhou, Anne Zhang, and Gordon Sun. 2005. Unigram language model for chinese word segmentation. In Proceedings of the 4th SIGHAN Workshop on Chinese Language Process-ing, pages 138–141. Association for Computational Linguistics Jeju Island, Korea. 301","Shay B Cohen, Giorgio Satta, and Michael Collins. 2013. Approximate pcfg parsing using tensor decomposition. In Proceedings of NAACL-HLT, pages 487–496.","Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10, pages 1–8. Association for Computational Linguistics.","Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th international conference on Machine learning, pages 160–167. ACM.","Ronan Collobert, Jason Weston, Léon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493–2537.","John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. The Journal of Machine Learning Research, 999999:2121–2159.","Thomas Emerson. 2005. The second international chinese word segmentation bakeoff. In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing, volume 133.","David Graff and Ke Chen. 2005. Chinese gigaword. LDC Catalog No.: LDC2003T09, ISBN, 1:58563– 230.","Geoffrey E Hinton. 1986. Learning distributed representations of concepts. In Proceedings of the eighth annual conference of the cognitive science society, pages 1–12. Amherst, MA.","Alex Krizhevsky, Geoffrey E Hinton, et al. 2010. Factored 3-way restricted boltzmann machines for modeling natural images. In International Conference on Artificial Intelligence and Statistics, pages 621– 628.","John Lafferty, Andrew McCallum, and Fernando CN Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data.","Zhongguo Li and Maosong Sun. 2009. Punctuation as implicit annotations for chinese word segmentation. Computational Linguistics, 35(4):505–512.","Mairgup Mansur, Wenzhe Pei, and Baobao Chang. 2013. Feature-based neural language model and chinese word segmentation. In Proceedings of the Sixth International Joint Conference on Natural Language Processing.","Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.","Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013b. Linguistic regularities in continuous space word representations. In Proceedings of NAACL-HLT, pages 746–751.","Marc’Aurelio Ranzato, Alex Krizhevsky, and Geoffrey E Hinton. 2010. Factored 3-way restricted boltzmann machines for modeling natural images.","Nathan D Ratliff, J Andrew Bagnell, and Martin A Zinkevich. 2007. (online) subgradient methods for structured prediction.","Ruslan Salakhutdinov, Andriy Mnih, and Geoffrey Hinton. 2007. Restricted boltzmann machines for collaborative filtering. InProceedings of the 24th international conference on Machine learning, pages 791–798. ACM.","Holger Schwenk, Anthony Rousseau, and Mohammed Attik. 2012. Large, pruned or continuous space language models on a gpu for statistical machine translation. In Proceedings of the NAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-gram Model? On the Future of Language Modeling for HLT, pages 11–19. Association for Computational Linguistics.","Richard Socher, John Bauer, Christopher D Manning, and Andrew Y Ng. 2013a. Parsing with composi-tional vector grammars. In Annual Meeting of the Association for Computational Linguistics (ACL).","Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. 2013b. Recursive deep models for semantic compositionality over a sentiment treebank. EMNLP.","Weiwei Sun and Jia Xu. 2011. Enhancing chinese word segmentation using unlabeled data. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 970–979. Association for Computational Linguistics.","Xu Sun, Yaozhong Zhang, Takuya Matsuzaki, Yoshimasa Tsuruoka, and Jun’ichi Tsujii. 2009. A discriminative latent variable chinese segmenter with hybrid word/character information. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 56–64. Association for Computational Linguistics.","Xu Sun, Houfeng Wang, and Wenjie Li. 2012. Fast online training with frequency-adaptive learning rates for chinese word segmentation and new word detec-tion. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 253–262, Jeju Island, 302 Korea, July. Association for Computational Linguistics.","Ben Taskar, Vassil Chatalbashev, Daphne Koller, and Carlos Guestrin. 2005. Learning structured predic-tion models: A large margin approach. In Proceedings of the 22nd international conference on Machine learning, pages 896–903. ACM.","Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel Jurafsky, and Christopher Manning. 2005. A condi-tional random field word segmenter for sighan bake-off 2005. In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing, volume 171.","Tim Van de Cruys, Thierry Poibeau, and Anna Korhonen. 2013. A tensor-based factorization model of semantic compositionality. In Proceedings of NAACL-HLT, pages 1142–1151.","Mengqiu Wang and Christopher D Manning. 2013. Effect of non-linear deep architecture in sequence labeling. In Proceedings of the Sixth International Joint Conference on Natural Language Processing.","Nianwen Xue. 2003. Chinese word segmentation as character tagging. Computational Linguistics and Chinese Language Processing, 8(1):29–48.","Yue Zhang and Stephen Clark. 2007. Chinese segmentation with a word-based perceptron algorithm. In ANNUAL MEETING-ASSOCIATION FOR COM-PUTATIONAL LINGUISTICS, volume 45, page 840.","Ruiqiang Zhang, Genichiro Kikui, and Eiichiro Sumita. 2006. Subword-based tagging by condi-tional random fields for chinese word segmentation. In Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers, pages 193–196. Association for Computational Linguistics.","Longkai Zhang, Houfeng Wang, Xu Sun, and Mairgup Mansur. 2013. Exploring representations from unlabeled data with co-training for Chinese word segmentation. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 311–321, Seattle, Washington, USA, October. Association for Computational Linguistics.","Xiaoqing Zheng, Hanyang Chen, and Tianyu Xu. 2013. Deep learning for Chinese word segmentation and POS tagging. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 647–657, Seattle, Washington, USA, October. Association for Computational Linguistics. 303"]}],"references":[{"authors":[{"first":"Yoshua","last":"Bengio"},{"first":"Réjean","last":"Ducharme"},{"first":"Pascal","last":"Vincent"},{"first":"Christian","last":"Jauvin"}],"year":"2003","title":"A neural probabilistic language model","source":"Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. A neural probabilistic language model. Journal of Machine Learning Research, 3:1137–1155."},{"authors":[{"first":"Kai-Wei","last":"Chang"},{"first":"Wen-tau","last":"Yih"},{"first":"Christopher","last":"Meek"}],"year":"2013","title":"Multi-relational latent semantic analysis","source":"Kai-Wei Chang, Wen-tau Yih, and Christopher Meek. 2013. Multi-relational latent semantic analysis. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1602–1612, Seattle, Washington, USA, October. Association for Computational Linguistics."},{"authors":[{"first":"Aitao","last":"Chen"},{"first":"Yiping","last":"Zhou"},{"first":"Anne","last":"Zhang"},{"first":"Gordon","last":"Sun"}],"year":"2005","title":"Unigram language model for chinese word segmentation","source":"Aitao Chen, Yiping Zhou, Anne Zhang, and Gordon Sun. 2005. Unigram language model for chinese word segmentation. In Proceedings of the 4th SIGHAN Workshop on Chinese Language Process-ing, pages 138–141. Association for Computational Linguistics Jeju Island, Korea. 301"},{"authors":[{"first":"Shay","middle":"B","last":"Cohen"},{"first":"Giorgio","last":"Satta"},{"first":"Michael","last":"Collins"}],"year":"2013","title":"Approximate pcfg parsing using tensor decomposition","source":"Shay B Cohen, Giorgio Satta, and Michael Collins. 2013. Approximate pcfg parsing using tensor decomposition. In Proceedings of NAACL-HLT, pages 487–496."},{"authors":[{"first":"Michael","last":"Collins"}],"year":"2002","title":"Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms","source":"Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10, pages 1–8. Association for Computational Linguistics."},{"authors":[{"first":"Ronan","last":"Collobert"},{"first":"Jason","last":"Weston"}],"year":"2008","title":"A unified architecture for natural language processing: Deep neural networks with multitask learning","source":"Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th international conference on Machine learning, pages 160–167. ACM."},{"authors":[{"first":"Ronan","last":"Collobert"},{"first":"Jason","last":"Weston"},{"first":"Léon","last":"Bottou"},{"first":"Michael","last":"Karlen"},{"first":"Koray","last":"Kavukcuoglu"},{"first":"Pavel","last":"Kuksa"}],"year":"2011","title":"Natural language processing (almost) from scratch","source":"Ronan Collobert, Jason Weston, Léon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493–2537."},{"authors":[{"first":"John","last":"Duchi"},{"first":"Elad","last":"Hazan"},{"first":"Yoram","last":"Singer"}],"year":"2011","title":"Adaptive subgradient methods for online learning and stochastic optimization","source":"John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. The Journal of Machine Learning Research, 999999:2121–2159."},{"authors":[{"first":"Thomas","last":"Emerson"}],"year":"2005","title":"The second international chinese word segmentation bakeoff","source":"Thomas Emerson. 2005. The second international chinese word segmentation bakeoff. In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing, volume 133."},{"authors":[{"first":"David","last":"Graff"},{"first":"Ke","last":"Chen"}],"year":"2005","title":"Chinese gigaword","source":"David Graff and Ke Chen. 2005. Chinese gigaword. LDC Catalog No.: LDC2003T09, ISBN, 1:58563– 230."},{"authors":[{"first":"Geoffrey","middle":"E","last":"Hinton"}],"year":"1986","title":"Learning distributed representations of concepts","source":"Geoffrey E Hinton. 1986. Learning distributed representations of concepts. In Proceedings of the eighth annual conference of the cognitive science society, pages 1–12. Amherst, MA."},{"authors":[{"first":"Alex","last":"Krizhevsky"},{"first":"Geoffrey","middle":"E","last":"Hinton"},{"last":"al"}],"year":"2010","title":"Factored 3-way restricted boltzmann machines for modeling natural images","source":"Alex Krizhevsky, Geoffrey E Hinton, et al. 2010. Factored 3-way restricted boltzmann machines for modeling natural images. In International Conference on Artificial Intelligence and Statistics, pages 621– 628."},{"authors":[{"first":"John","last":"Lafferty"},{"first":"Andrew","last":"McCallum"},{"first":"Fernando","middle":"CN","last":"Pereira"}],"year":"2001","title":"Conditional random fields: Probabilistic models for segmenting and labeling sequence data","source":"John Lafferty, Andrew McCallum, and Fernando CN Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data."},{"authors":[{"first":"Zhongguo","last":"Li"},{"first":"Maosong","last":"Sun"}],"year":"2009","title":"Punctuation as implicit annotations for chinese word segmentation","source":"Zhongguo Li and Maosong Sun. 2009. Punctuation as implicit annotations for chinese word segmentation. Computational Linguistics, 35(4):505–512."},{"authors":[{"first":"Mairgup","last":"Mansur"},{"first":"Wenzhe","last":"Pei"},{"first":"Baobao","last":"Chang"}],"year":"2013","title":"Feature-based neural language model and chinese word segmentation","source":"Mairgup Mansur, Wenzhe Pei, and Baobao Chang. 2013. Feature-based neural language model and chinese word segmentation. In Proceedings of the Sixth International Joint Conference on Natural Language Processing."},{"authors":[{"first":"Tomas","last":"Mikolov"},{"first":"Kai","last":"Chen"},{"first":"Greg","last":"Corrado"},{"first":"Jeffrey","last":"Dean"}],"year":"2013a","title":"Efficient estimation of word representations in vector space","source":"Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781."},{"authors":[{"first":"Tomas","last":"Mikolov"},{"first":"Wen-tau","last":"Yih"},{"first":"Geoffrey","last":"Zweig"}],"year":"2013b","title":"Linguistic regularities in continuous space word representations","source":"Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013b. Linguistic regularities in continuous space word representations. In Proceedings of NAACL-HLT, pages 746–751."},{"authors":[{"first":"Marc’Aurelio","last":"Ranzato"},{"first":"Alex","last":"Krizhevsky"},{"first":"Geoffrey","middle":"E","last":"Hinton"}],"year":"2010","title":"Factored 3-way restricted boltzmann machines for modeling natural images","source":"Marc’Aurelio Ranzato, Alex Krizhevsky, and Geoffrey E Hinton. 2010. Factored 3-way restricted boltzmann machines for modeling natural images."},{"authors":[{"first":"Nathan","middle":"D","last":"Ratliff"},{"first":"J","middle":"Andrew","last":"Bagnell"},{"first":"Martin","middle":"A","last":"Zinkevich"}],"year":"2007","title":"(online) subgradient methods for structured prediction","source":"Nathan D Ratliff, J Andrew Bagnell, and Martin A Zinkevich. 2007. (online) subgradient methods for structured prediction."},{"authors":[{"first":"Ruslan","last":"Salakhutdinov"},{"first":"Andriy","last":"Mnih"},{"first":"Geoffrey","last":"Hinton"}],"year":"2007","title":"Restricted boltzmann machines for collaborative filtering","source":"Ruslan Salakhutdinov, Andriy Mnih, and Geoffrey Hinton. 2007. Restricted boltzmann machines for collaborative filtering. InProceedings of the 24th international conference on Machine learning, pages 791–798. ACM."},{"authors":[{"first":"Holger","last":"Schwenk"},{"first":"Anthony","last":"Rousseau"},{"first":"Mohammed","last":"Attik"}],"year":"2012","title":"Large, pruned or continuous space language models on a gpu for statistical machine translation","source":"Holger Schwenk, Anthony Rousseau, and Mohammed Attik. 2012. Large, pruned or continuous space language models on a gpu for statistical machine translation. In Proceedings of the NAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-gram Model? On the Future of Language Modeling for HLT, pages 11–19. Association for Computational Linguistics."},{"authors":[{"first":"Richard","last":"Socher"},{"first":"John","last":"Bauer"},{"first":"Christopher","middle":"D","last":"Manning"},{"first":"Andrew","middle":"Y","last":"Ng"}],"year":"2013a","title":"Parsing with composi-tional vector grammars","source":"Richard Socher, John Bauer, Christopher D Manning, and Andrew Y Ng. 2013a. Parsing with composi-tional vector grammars. In Annual Meeting of the Association for Computational Linguistics (ACL)."},{"authors":[{"first":"Richard","last":"Socher"},{"first":"Alex","last":"Perelygin"},{"first":"Jean","middle":"Y","last":"Wu"},{"first":"Jason","last":"Chuang"},{"first":"Christopher","middle":"D","last":"Manning"},{"first":"Andrew","middle":"Y","last":"Ng"},{"first":"Christopher","last":"Potts"}],"year":"2013b","title":"Recursive deep models for semantic compositionality over a sentiment treebank","source":"Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. 2013b. Recursive deep models for semantic compositionality over a sentiment treebank. EMNLP."},{"authors":[{"first":"Weiwei","last":"Sun"},{"first":"Jia","last":"Xu"}],"year":"2011","title":"Enhancing chinese word segmentation using unlabeled data","source":"Weiwei Sun and Jia Xu. 2011. Enhancing chinese word segmentation using unlabeled data. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 970–979. Association for Computational Linguistics."},{"authors":[{"first":"Xu","last":"Sun"},{"first":"Yaozhong","last":"Zhang"},{"first":"Takuya","last":"Matsuzaki"},{"first":"Yoshimasa","last":"Tsuruoka"},{"first":"Jun’ichi","last":"Tsujii"}],"year":"2009","title":"A discriminative latent variable chinese segmenter with hybrid word/character information","source":"Xu Sun, Yaozhong Zhang, Takuya Matsuzaki, Yoshimasa Tsuruoka, and Jun’ichi Tsujii. 2009. A discriminative latent variable chinese segmenter with hybrid word/character information. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 56–64. Association for Computational Linguistics."},{"authors":[{"first":"Xu","last":"Sun"},{"first":"Houfeng","last":"Wang"},{"first":"Wenjie","last":"Li"}],"year":"2012","title":"Fast online training with frequency-adaptive learning rates for chinese word segmentation and new word detec-tion","source":"Xu Sun, Houfeng Wang, and Wenjie Li. 2012. Fast online training with frequency-adaptive learning rates for chinese word segmentation and new word detec-tion. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 253–262, Jeju Island, 302 Korea, July. Association for Computational Linguistics."},{"authors":[{"first":"Ben","last":"Taskar"},{"first":"Vassil","last":"Chatalbashev"},{"first":"Daphne","last":"Koller"},{"first":"Carlos","last":"Guestrin"}],"year":"2005","title":"Learning structured predic-tion models: A large margin approach","source":"Ben Taskar, Vassil Chatalbashev, Daphne Koller, and Carlos Guestrin. 2005. Learning structured predic-tion models: A large margin approach. In Proceedings of the 22nd international conference on Machine learning, pages 896–903. ACM."},{"authors":[{"first":"Huihsin","last":"Tseng"},{"first":"Pichuan","last":"Chang"},{"first":"Galen","last":"Andrew"},{"first":"Daniel","last":"Jurafsky"},{"first":"Christopher","last":"Manning"}],"year":"2005","title":"A condi-tional random field word segmenter for sighan bake-off 2005","source":"Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel Jurafsky, and Christopher Manning. 2005. A condi-tional random field word segmenter for sighan bake-off 2005. In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing, volume 171."},{"authors":[{"first":"Tim","middle":"Van de","last":"Cruys"},{"first":"Thierry","last":"Poibeau"},{"first":"Anna","last":"Korhonen"}],"year":"2013","title":"A tensor-based factorization model of semantic compositionality","source":"Tim Van de Cruys, Thierry Poibeau, and Anna Korhonen. 2013. A tensor-based factorization model of semantic compositionality. In Proceedings of NAACL-HLT, pages 1142–1151."},{"authors":[{"first":"Mengqiu","last":"Wang"},{"first":"Christopher","middle":"D","last":"Manning"}],"year":"2013","title":"Effect of non-linear deep architecture in sequence labeling","source":"Mengqiu Wang and Christopher D Manning. 2013. Effect of non-linear deep architecture in sequence labeling. In Proceedings of the Sixth International Joint Conference on Natural Language Processing."},{"authors":[{"first":"Nianwen","last":"Xue"}],"year":"2003","title":"Chinese word segmentation as character tagging","source":"Nianwen Xue. 2003. Chinese word segmentation as character tagging. Computational Linguistics and Chinese Language Processing, 8(1):29–48."},{"authors":[{"first":"Yue","last":"Zhang"},{"first":"Stephen","last":"Clark"}],"year":"2007","title":"Chinese segmentation with a word-based perceptron algorithm","source":"Yue Zhang and Stephen Clark. 2007. Chinese segmentation with a word-based perceptron algorithm. In ANNUAL MEETING-ASSOCIATION FOR COM-PUTATIONAL LINGUISTICS, volume 45, page 840."},{"authors":[{"first":"Ruiqiang","last":"Zhang"},{"first":"Genichiro","last":"Kikui"},{"first":"Eiichiro","last":"Sumita"}],"year":"2006","title":"Subword-based tagging by condi-tional random fields for chinese word segmentation","source":"Ruiqiang Zhang, Genichiro Kikui, and Eiichiro Sumita. 2006. Subword-based tagging by condi-tional random fields for chinese word segmentation. In Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers, pages 193–196. Association for Computational Linguistics."},{"authors":[{"first":"Longkai","last":"Zhang"},{"first":"Houfeng","last":"Wang"},{"first":"Xu","last":"Sun"},{"first":"Mairgup","last":"Mansur"}],"year":"2013","title":"Exploring representations from unlabeled data with co-training for Chinese word segmentation","source":"Longkai Zhang, Houfeng Wang, Xu Sun, and Mairgup Mansur. 2013. Exploring representations from unlabeled data with co-training for Chinese word segmentation. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 311–321, Seattle, Washington, USA, October. Association for Computational Linguistics."},{"authors":[{"first":"Xiaoqing","last":"Zheng"},{"first":"Hanyang","last":"Chen"},{"first":"Tianyu","last":"Xu"}],"year":"2013","title":"Deep learning for Chinese word segmentation and POS tagging","source":"Xiaoqing Zheng, Hanyang Chen, and Tianyu Xu. 2013. Deep learning for Chinese word segmentation and POS tagging. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 647–657, Seattle, Washington, USA, October. Association for Computational Linguistics. 303"}],"cites":[{"style":0,"text":"Collobert et al. (2011)","origin":{"pointer":"/sections/3/paragraphs/1","offset":131,"length":23},"authors":[{"last":"Collobert"},{"last":"al."}],"year":"2011","references":["/references/6"]},{"style":0,"text":"Zheng et al. (2013)","origin":{"pointer":"/sections/3/paragraphs/1","offset":293,"length":19},"authors":[{"last":"Zheng"},{"last":"al."}],"year":"2013","references":["/references/34"]},{"style":0,"text":"Collobert et al. (2011)","origin":{"pointer":"/sections/3/paragraphs/1","offset":341,"length":23},"authors":[{"last":"Collobert"},{"last":"al."}],"year":"2011","references":["/references/6"]},{"style":0,"text":"Emerson, 2005","origin":{"pointer":"/sections/3/paragraphs/4","offset":924,"length":13},"authors":[{"last":"Emerson"}],"year":"2005","references":["/references/8"]},{"style":0,"text":"Mansur et al. (2013)","origin":{"pointer":"/sections/3/paragraphs/5","offset":275,"length":20},"authors":[{"last":"Mansur"},{"last":"al."}],"year":"2013","references":["/references/14"]},{"style":0,"text":"Hinton (1986)","origin":{"pointer":"/sections/4/paragraphs/0","offset":143,"length":13},"authors":[{"last":"Hinton"}],"year":"1986","references":["/references/10"]},{"style":0,"text":"Bengio et al., 2003","origin":{"pointer":"/sections/4/paragraphs/0","offset":218,"length":19},"authors":[{"last":"Bengio"},{"last":"al."}],"year":"2003","references":["/references/0"]},{"style":0,"text":"Collobert et al., 2011","origin":{"pointer":"/sections/4/paragraphs/0","offset":239,"length":22},"authors":[{"last":"Collobert"},{"last":"al."}],"year":"2011","references":["/references/6"]},{"style":0,"text":"Schwenk et al., 2012","origin":{"pointer":"/sections/4/paragraphs/0","offset":263,"length":20},"authors":[{"last":"Schwenk"},{"last":"al."}],"year":"2012","references":["/references/20"]},{"style":0,"text":"Mikolov et al., 2013a","origin":{"pointer":"/sections/4/paragraphs/0","offset":285,"length":21},"authors":[{"last":"Mikolov"},{"last":"al."}],"year":"2013a","references":["/references/15"]},{"style":0,"text":"Mansur et al. (2013)","origin":{"pointer":"/sections/4/paragraphs/11","offset":535,"length":20},"authors":[{"last":"Mansur"},{"last":"al."}],"year":"2013","references":["/references/14"]},{"style":0,"text":"Collobert et al., 2011","origin":{"pointer":"/sections/4/paragraphs/14","offset":61,"length":22},"authors":[{"last":"Collobert"},{"last":"al."}],"year":"2011","references":["/references/6"]},{"style":0,"text":"Zheng et al., 2013","origin":{"pointer":"/sections/4/paragraphs/14","offset":85,"length":18},"authors":[{"last":"Zheng"},{"last":"al."}],"year":"2013","references":["/references/34"]},{"style":0,"text":"Zheng et al. (2013)","origin":{"pointer":"/sections/4/paragraphs/15","offset":179,"length":19},"authors":[{"last":"Zheng"},{"last":"al."}],"year":"2013","references":["/references/34"]},{"style":0,"text":"Collins (2002)","origin":{"pointer":"/sections/4/paragraphs/15","offset":271,"length":14},"authors":[{"last":"Collins"}],"year":"2002","references":["/references/4"]},{"style":0,"text":"Mansur et al. (2013)","origin":{"pointer":"/sections/4/paragraphs/15","offset":301,"length":20},"authors":[{"last":"Mansur"},{"last":"al."}],"year":"2013","references":["/references/14"]},{"style":0,"text":"Collobert et al. (2011)","origin":{"pointer":"/sections/5/paragraphs/5","offset":126,"length":23},"authors":[{"last":"Collobert"},{"last":"al."}],"year":"2011","references":["/references/6"]},{"style":0,"text":"Zheng et al. (2013)","origin":{"pointer":"/sections/5/paragraphs/5","offset":154,"length":19},"authors":[{"last":"Zheng"},{"last":"al."}],"year":"2013","references":["/references/34"]},{"style":0,"text":"Salakhutdinov et al., 2007","origin":{"pointer":"/sections/5/paragraphs/5","offset":621,"length":26},"authors":[{"last":"Salakhutdinov"},{"last":"al."}],"year":"2007","references":["/references/19"]},{"style":0,"text":"Krizhevsky et al., 2010","origin":{"pointer":"/sections/5/paragraphs/5","offset":649,"length":23},"authors":[{"last":"Krizhevsky"},{"last":"al."}],"year":"2010","references":["/references/11"]},{"style":0,"text":"Socher et al., 2013b","origin":{"pointer":"/sections/5/paragraphs/5","offset":674,"length":20},"authors":[{"last":"Socher"},{"last":"al."}],"year":"2013b","references":["/references/22"]},{"style":0,"text":"Taskar et al., 2005","origin":{"pointer":"/sections/5/paragraphs/49","offset":783,"length":19},"authors":[{"last":"Taskar"},{"last":"al."}],"year":"2005","references":["/references/26"]},{"style":0,"text":"Ratliff et al., 2007","origin":{"pointer":"/sections/5/paragraphs/56","offset":139,"length":20},"authors":[{"last":"Ratliff"},{"last":"al."}],"year":"2007","references":["/references/18"]},{"style":0,"text":"Socher et al. (2013a)","origin":{"pointer":"/sections/5/paragraphs/58","offset":104,"length":21},"authors":[{"last":"Socher"},{"last":"al."}],"year":"2013a","references":["/references/21"]},{"style":0,"text":"Duchi et al., 2011","origin":{"pointer":"/sections/5/paragraphs/66","offset":285,"length":18},"authors":[{"last":"Duchi"},{"last":"al."}],"year":"2011","references":["/references/7"]},{"style":0,"text":"Emerson, 2005","origin":{"pointer":"/sections/6/paragraphs/0","offset":101,"length":13},"authors":[{"last":"Emerson"}],"year":"2005","references":["/references/8"]},{"style":0,"text":"Lafferty et al., 2001","origin":{"pointer":"/sections/6/paragraphs/3","offset":119,"length":21},"authors":[{"last":"Lafferty"},{"last":"al."}],"year":"2001","references":["/references/12"]},{"style":0,"text":"Wang and Manning, 2013","origin":{"pointer":"/sections/6/paragraphs/9","offset":422,"length":22},"authors":[{"last":"Wang"},{"last":"Manning"}],"year":"2013","references":["/references/29"]},{"style":0,"text":"Wang and Manning (2013)","origin":{"pointer":"/sections/6/paragraphs/9","offset":447,"length":23},"authors":[{"last":"Wang"},{"last":"Manning"}],"year":"2013","references":["/references/29"]},{"style":0,"text":"Zheng et al. (2013)","origin":{"pointer":"/sections/6/paragraphs/10","offset":102,"length":19},"authors":[{"last":"Zheng"},{"last":"al."}],"year":"2013","references":["/references/34"]},{"style":0,"text":"Mansur et al., 2013","origin":{"pointer":"/sections/6/paragraphs/12","offset":21,"length":19},"authors":[{"last":"Mansur"},{"last":"al."}],"year":"2013","references":["/references/14"]},{"style":0,"text":"Zheng et al., 2013","origin":{"pointer":"/sections/6/paragraphs/12","offset":83,"length":18},"authors":[{"last":"Zheng"},{"last":"al."}],"year":"2013","references":["/references/34"]},{"style":0,"text":"Mansur et al., 2013","origin":{"pointer":"/sections/6/paragraphs/12","offset":190,"length":19},"authors":[{"last":"Mansur"},{"last":"al."}],"year":"2013","references":["/references/14"]},{"style":0,"text":"Zheng et al., 2013","origin":{"pointer":"/sections/6/paragraphs/12","offset":267,"length":18},"authors":[{"last":"Zheng"},{"last":"al."}],"year":"2013","references":["/references/34"]},{"style":0,"text":"Mansur et al., 2013","origin":{"pointer":"/sections/6/paragraphs/12","offset":460,"length":19},"authors":[{"last":"Mansur"},{"last":"al."}],"year":"2013","references":["/references/14"]},{"style":0,"text":"Zheng et al., 2013","origin":{"pointer":"/sections/6/paragraphs/12","offset":481,"length":18},"authors":[{"last":"Zheng"},{"last":"al."}],"year":"2013","references":["/references/34"]},{"style":0,"text":"Mikolov et al. (2013b)","origin":{"pointer":"/sections/6/paragraphs/12","offset":502,"length":22},"authors":[{"last":"Mikolov"},{"last":"al."}],"year":"2013b","references":["/references/16"]},{"style":0,"text":"Mansur et al. (2013)","origin":{"pointer":"/sections/6/paragraphs/12","offset":731,"length":20},"authors":[{"last":"Mansur"},{"last":"al."}],"year":"2013","references":["/references/14"]},{"style":0,"text":"Bengio et al. (2003)","origin":{"pointer":"/sections/6/paragraphs/12","offset":779,"length":20},"authors":[{"last":"Bengio"},{"last":"al."}],"year":"2003","references":["/references/0"]},{"style":0,"text":"Zheng et al. (2013)","origin":{"pointer":"/sections/6/paragraphs/12","offset":860,"length":19},"authors":[{"last":"Zheng"},{"last":"al."}],"year":"2013","references":["/references/34"]},{"style":0,"text":"Collobert et al. (2008)","origin":{"pointer":"/sections/6/paragraphs/12","offset":911,"length":23},"authors":[{"last":"Collobert"},{"last":"al."}],"year":"2008","references":[]},{"style":0,"text":"Mikolov et al. (2013a)","origin":{"pointer":"/sections/6/paragraphs/12","offset":1108,"length":22},"authors":[{"last":"Mikolov"},{"last":"al."}],"year":"2013a","references":["/references/15"]},{"style":0,"text":"Graff and Chen, 2005","origin":{"pointer":"/sections/6/paragraphs/13","offset":317,"length":20},"authors":[{"last":"Graff"},{"last":"Chen"}],"year":"2005","references":["/references/9"]},{"style":0,"text":"Mansur et al. (2013)","origin":{"pointer":"/sections/6/paragraphs/13","offset":860,"length":20},"authors":[{"last":"Mansur"},{"last":"al."}],"year":"2013","references":["/references/14"]},{"style":0,"text":"Chen et al., 2005","origin":{"pointer":"/sections/6/paragraphs/14","offset":22,"length":17},"authors":[{"last":"Chen"},{"last":"al."}],"year":"2005","references":["/references/2"]},{"style":0,"text":"Tseng et al., 2005","origin":{"pointer":"/sections/6/paragraphs/14","offset":58,"length":18},"authors":[{"last":"Tseng"},{"last":"al."}],"year":"2005","references":["/references/27"]},{"style":0,"text":"Zhang et al., 2006","origin":{"pointer":"/sections/6/paragraphs/14","offset":89,"length":18},"authors":[{"last":"Zhang"},{"last":"al."}],"year":"2006","references":["/references/32"]},{"style":0,"text":"Zhang and Clark, 2007","origin":{"pointer":"/sections/6/paragraphs/14","offset":120,"length":21},"authors":[{"last":"Zhang"},{"last":"Clark"}],"year":"2007","references":["/references/31"]},{"style":0,"text":"Sun et al., 2009","origin":{"pointer":"/sections/6/paragraphs/14","offset":154,"length":16},"authors":[{"last":"Sun"},{"last":"al."}],"year":"2009","references":["/references/24"]},{"style":0,"text":"Sun et al., 2012","origin":{"pointer":"/sections/6/paragraphs/14","offset":183,"length":16},"authors":[{"last":"Sun"},{"last":"al."}],"year":"2012","references":["/references/25"]},{"style":0,"text":"Zhang et al., 2013","origin":{"pointer":"/sections/6/paragraphs/14","offset":212,"length":18},"authors":[{"last":"Zhang"},{"last":"al."}],"year":"2013","references":["/references/33"]},{"style":0,"text":"Mansur et al. (2013)","origin":{"pointer":"/sections/6/paragraphs/15","offset":384,"length":20},"authors":[{"last":"Mansur"},{"last":"al."}],"year":"2013","references":["/references/14"]},{"style":0,"text":"Sun et al. (2012)","origin":{"pointer":"/sections/6/paragraphs/15","offset":1210,"length":17},"authors":[{"last":"Sun"},{"last":"al."}],"year":"2012","references":["/references/25"]},{"style":0,"text":"Zhang et al. (2013)","origin":{"pointer":"/sections/6/paragraphs/15","offset":1265,"length":19},"authors":[{"last":"Zhang"},{"last":"al."}],"year":"2013","references":["/references/33"]},{"style":0,"text":"Xue (2003)","origin":{"pointer":"/sections/7/paragraphs/0","offset":201,"length":10},"authors":[{"last":"Xue"}],"year":"2003","references":["/references/30"]},{"style":0,"text":"Li and Sun, 2009","origin":{"pointer":"/sections/7/paragraphs/0","offset":371,"length":16},"authors":[{"last":"Li"},{"last":"Sun"}],"year":"2009","references":["/references/13"]},{"style":0,"text":"Sun and Xu, 2011","origin":{"pointer":"/sections/7/paragraphs/0","offset":418,"length":16},"authors":[{"last":"Sun"},{"last":"Xu"}],"year":"2011","references":["/references/23"]},{"style":0,"text":"Mansur et al., 2013","origin":{"pointer":"/sections/7/paragraphs/0","offset":623,"length":19},"authors":[{"last":"Mansur"},{"last":"al."}],"year":"2013","references":["/references/14"]},{"style":0,"text":"Zheng et al., 2013","origin":{"pointer":"/sections/7/paragraphs/0","offset":644,"length":18},"authors":[{"last":"Zheng"},{"last":"al."}],"year":"2013","references":["/references/34"]},{"style":0,"text":"Socher et al. (2013b)","origin":{"pointer":"/sections/7/paragraphs/1","offset":144,"length":21},"authors":[{"last":"Socher"},{"last":"al."}],"year":"2013b","references":["/references/22"]},{"style":0,"text":"Cohen et al., 2013","origin":{"pointer":"/sections/7/paragraphs/2","offset":119,"length":18},"authors":[{"last":"Cohen"},{"last":"al."}],"year":"2013","references":["/references/3"]},{"style":0,"text":"Cruys et al., 2013","origin":{"pointer":"/sections/7/paragraphs/2","offset":146,"length":18},"authors":[{"last":"Cruys"},{"last":"al."}],"year":"2013","references":["/references/28"]},{"style":0,"text":"Chang et al., 2013","origin":{"pointer":"/sections/7/paragraphs/2","offset":166,"length":18},"authors":[{"last":"Chang"},{"last":"al."}],"year":"2013","references":["/references/1"]},{"style":0,"text":"Chang et al. (2013)","origin":{"pointer":"/sections/7/paragraphs/2","offset":200,"length":19},"authors":[{"last":"Chang"},{"last":"al."}],"year":"2013","references":["/references/1"]},{"style":0,"text":"Salakhutdinov et al., 2007","origin":{"pointer":"/sections/7/paragraphs/2","offset":438,"length":26},"authors":[{"last":"Salakhutdinov"},{"last":"al."}],"year":"2007","references":["/references/19"]},{"style":0,"text":"Ranzato et al., 2010","origin":{"pointer":"/sections/7/paragraphs/2","offset":490,"length":20},"authors":[{"last":"Ranzato"},{"last":"al."}],"year":"2010","references":["/references/17"]}]}
