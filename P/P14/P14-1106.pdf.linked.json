{"sections":[{"title":"","paragraphs":["Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1123–1133, Baltimore, Maryland, USA, June 23-25 2014. c⃝2014 Association for Computational Linguistics"]},{"title":"A Unified Model for Soft Linguistic Reordering Constraints in Statistical Machine Translation Junhui Li","paragraphs":["†"]},{"title":"Yuval Marton","paragraphs":["‡"]},{"title":"Philip Resnik","paragraphs":["†"]},{"title":"Hal Daumé III","paragraphs":["† †"]},{"title":"UMIACS, University of Maryland, College Park, MD {lijunhui, resnik, hal}@umiacs.umd.edu","paragraphs":["‡"]},{"title":"Microsoft Corp., City Center Plaza, Bellevue, WA yumarton@microsoft.com Abstract","paragraphs":["This paper explores a simple and effective unified framework for incorporating soft linguistic reordering constraints into a hierarchical phrase-based translation system: 1) a syntactic reordering model that explores reorderings for context free grammar rules; and 2) a semantic reordering model that focuses on the reordering of predicate-argument structures. We develop novel features based on both models and use them as soft constraints to guide the translation process. Experiments on Chinese-English translation show that the reordering approach can significantly improve a state-of-the-art hierarchical phrase-based translation system. However, the gain achieved by the semantic reordering model is limited in the presence of the syntactic reordering model, and we therefore provide a detailed analysis of the behavior differences between the two."]},{"title":"1 Introduction","paragraphs":["Reordering models in statistical machine translation (SMT) model the word order difference when translating from one language to another. The popular distortion or lexicalized reordering models in phrase-based SMT make good local predictions by focusing on reordering on word level, while the synchronous context free grammars in hierarchical phrase-based (HPB) translation models are capable of handling non-local reordering on the translation phrase level. However, reordering, especially without any help of external knowledge, remains a great challenge because an accurate reordering is usually beyond these word level or translation phrase level reordering models’ ability. In addition, often these translation models fail to respect linguistically-motivated syntax and semantics. As a result, they tend to produce translations containing both syntactic and semantic reordering confusions. In this paper our goal is to take advantage of syntactic and semantic parsing to improve translation quality. Rather than introducing reordering models on either the word level or the translation phrase level, we propose a unified approach to modeling reordering on the linguistic unit level, e.g., syntactic constituents and semantic roles. The reordering unit falls into multiple granularities, from single words to more complex constituents and semantic roles, and often crosses translation phrases. To show the effectiveness of our reordering models, we integrate both syntactic constituent reordering models and semantic role reordering models into a state-of-the-art HPB system (Chiang, 2007; Dyer et al., 2010). We further contrast it with a stronger baseline, already including fine-grained soft syntactic constraint features (Marton and Resnik, 2008; Chiang et al., 2008). The general ideas, however, are applicable to other translation models, e.g., phrase-based model, as well.","Our syntactic constituent reordering model considers context free grammar (CFG) rules in the source language and predicts the reordering of their elements on the target side, using word alignment information. Due to the fact that a constituent, especially a long one, usually maps into multiple discontinuous blocks in the target language, there is more than one way to describe the monotonicity or swapping patterns; we therefore design two reordering models: one is based on the leftmost aligned target word and the other based on the rightmost target word.","While recently there has also been some encouraging work on incorporating semantic structure (or, more specifically, predicate-argument structure: PAS) reordering in SMT, it is still an open question whether semantic structure reordering 1123 strongly overlaps with syntactic structure reordering, since the semantic structure is closely tied to syntax. To this end, we employ the same reordering framework as syntactic constituent reordering and focus on semantic roles in a PAS. We then an-alyze the differences between the syntactic and semantic features.","The contributions of this paper include the following:","• We introduce novel soft reordering constraints, using syntactic constituents or semantic roles, composed over word alignment information in translation rules used during decoding time;","• We introduce a unified framework to incorporate syntactic and semantic reordering constraints;","• We provide a detailed analysis providing in-sight into why the semantic reordering model is significantly less effective when syntactic reordering features are also present.","The rest of the paper is organized as follows. Section 2 provides an overview of HPB translation model. Section 3 describes the details of our unified reordering models. Section 4 gives our experimental results and Section 5 discusses the behavior difference between syntactic constituent reordering and semantic role reordering. Section 6 reviews related work and, finally Section 7 concludes the paper."]},{"title":"2 HPB Translation Model: an Overview","paragraphs":["In HPB models (Chiang, 2007), synchronous rules take the form X → ⟨γ, α, ∼⟩, where X is the nonterminal symbol, γ and α are strings of lexical items and non-terminals in the source and target side, respectively, and ∼ indicates the one-to-one correspondence between non-terminals in γ and α. Each such rule is associated with a set of translation model features {φi}, such as phrase translation probability p (α | γ) and its inverse p (γ | α), the lexical translation probability plex (α | γ) and its inverse plex (γ | α), and a rule penalty that affects preference for longer or shorter derivations. Two other widely used features are a target language model feature and a target word penalty.","Given a derivation d, its translation log-probability is estimated as: log P (d) ∝ ∑ i λiφi (d) (1)  PAS A0 (NP) TMP (NP) Pre (VBD) A1 (NP)","Applicants yesterday filled the forms Figure 1: Example of predicate-argument structure. where λi is the corresponding weight of feature φi. See (Chiang, 2007) for more details."]},{"title":"3 Unified Linguistic Reordering Models","paragraphs":["As mentioned earlier, the linguistic reordering unit is the syntactic constituent for syntactic reordering, and the semantic role for semantic reordering. The syntactic reordering model takes a CFG rule (e.g., VP → VP PP PP) and models the reordering of the constituents on the left hand side by examining their translation or visit order according to the target language. For the semantic reordering model, it takes a PAS and models its reordering on the target side. Figure 1 shows an example of a PAS where the predicate (Pre) has two core arguments (A0 and A1) and one adjunct (TMP). Note that we refer all core arguments, adjuncts, and predicates as semantic roles; thus we say the PAS in Figure 1 has 4 roles. According to the annotation principles in (Chinese) PropBank (Palmer et al., 2005; Xue and Palmer, 2009), all the roles in a PAS map to a corresponding constituent in the parse tree, and these constituents (e.g., NPs and VBD in Figure 1) do not overlap with each other.","Next, we use a CFG rule to describe our syntactic reordering model. Treating the two forms of reorderings in a unified way, the semantic reordering model is obtainable by regarding a PAS as a CFG rule and considering a semantic role as a constituent.","Because the translation of a source constituent might result in multiple discontinuous blocks, there can be several ways to describe or group the reordering patterns. Therefore, we design two general constituent reordering sub-models. One is based on the leftmost aligned word (leftmost reordering model) and the other is based on the rightmost aligned word (rightmost reordering model), as follows. Figure 2 shows the modeling steps for the leftmost reordering model. Figure 2(a) is an example of a CFG rule in the source 1124  XP XP1 XP2 XP3 XP4 f3 f4 f5 f6 f7 f8 ... ... ... ... ... e2 e3 e4 e5 e6 e7 e8 e9 ... XP1 XP2 XP3 XP4 e2 e3 e5 (a) a CFG rule and its alignment (b) leftmost aligned target words XP1 XP2 XP3 XP4 1 4 2 3 XP1 XP2 XP3 XP4 DM DS M (c) visit order (d) reordering types Figure 2: Modeling process illustration for leftmost reordering model. parse tree and its word alignment links to the target language. Note that constituent XP4, which covers word f8, has no alignment. Then for each XPi, we find the leftmost target word which is aligned to a source word covered by XPi. Figure 2(b) shows that the leftmost target words for XP1, XP2, and XP3 are e2, e5, and e3, respectively, while XP4 has no aligned target word. Then we get visit order V = {vi} for {XPi} in the transformation from Figure 2(b) to Figure 2(c), with the following strategies for special cases:","• if the first constituent XP1 is unaligned, we add a NULL word at the beginning of the target side and link XP1 to the NULL word;","• if a constituent XPi (i > 1) is unaligned, we add a link to the target word which is aligned to XPi−1, e.g., XP4 will be linked to e3; and","• if k constituents XPm1 . . . XPmk (m1 < . . . < mk) are linked to the same target word, then vmi = vmi+1 − 1, e.g., since XP3 and XP4 are both linked to e3, then v3 = v4 − 1.","Finally Figure 2(d) converts the visit order V = {v1, . . . vn} into a sequence of leftmost reordering types LRT = {lrt1, . . . , lrtn−1}. For every two adjacent constituents XPi and XPi+1 with corresponding visit order vi and vi+1, their reordering could be one of the following: • Monotone (M) if vi+1 = vi + 1; • Discontinuous Monotone (DM) if vi+1 > vi + 1; • Swap (S) if vi+1 = vi − 1; • Discontinuous Swap (DS) if vi+1 < vi − 1.","Up to this point, we have generated a sequence of leftmost reordering types LRT = {lrt1, . . . , lrtn−1} for a given CFG rule cfg: XP → XP1 . . . XPn. The leftmost reordering model takes the following form: scorelrt (cfg) = Pl (lrt1, . . . , lrtn−1 | ψ (cfg))","(2) where ψ (cfg) indicates the surrounding context of the CFG. By assuming that any two reordering types in LRT = {lrt1, . . . , lrtn−1} are independent of each other, we reformulate Eq. 2 into: scorelrt (cfg) = n−1 ∏ i=1 Pl (lrti | ψ (cfg)) (3) Similarly, the sequence of rightmost reordering types RRT can be decided for a CFG rule XP → XP1 . . . XPn.","Accordingly, for a PAS pas: PAS → R1 . . . Rn, we can obtain its sequences of leftmost and rightmost reordering types by using the same way described above. 3.1 Probability Estimation In order to predict either the leftmost or rightmost reordering type for two adjacent constituents, we use a maximum entropy classifier to estimate the probability of the reordering type rt ∈ {M, DM, S, DS} as follows: P (rt | ψ (cfg)) =","exp (∑ k θkfk (rt, ψ (cfg)))","∑","rt′ exp (∑","k θkfi (rt′",", ψ (cfg))) (4) where fk are binary features, θk are the weights of these features. Most of our features fk are syntax-based. For XPi and XPi+1 in cfg, the features 1125 #Index Feature cf1 L(XPi) & L(XPi+1) & L(XP) cf2","for each XPj (j < i) L(XPi) & L(XPi+1) & L(XP) & L(XPj) cf3","for each XPj (j > i + 1)","L(XPi) & L(XPi+1) & L(XP) & L(XPj) cf4 L(XPi) & L(XPi+1) & P(XPi) cf5 L(XPi) & L(XPi+1) & H(XPi) cf6 L(XPi) & L(XPi+1) & P(XPi+1) cf7 L(XPi) & L(XPi+1) & H(XPi+1) cf8 L(XPi) & L(XPi+1) & S(XPi) cf9 L(XPi) & L(XPi+1) & S(XPi+1) cf10 L(XPi) & L(XP) cf11 L(XPi+1) & L(XP) Table 1: Features adopted in the syntactic leftmost and rightmost reordering models. L (XP) returns the syntactic category of XP, e.g., NP, VP, PP etc.; H (XP) returns the head word of XP; P (XP) returns the POS tagger of the head word; S (XP) returns the translation status of XP on the target language: un. if it is untranslated; cont. if it is a continuous block; and discont. if it maps into multiple discontinuous blocks. are aimed to examine which of them should be translated first. Therefore, most features share two common components: the syntactic categories of XPi and XPi+1. Table 1 shows the features used in syntactic leftmost and rightmost reordering models. Note that we use the same features for both.","Although the semantic reordering model is structured in precisely the same way, we use different feature sets to predict the reordering between two semantic roles. Given the two adjacent roles Ri and Ri+1 in a PAS pas, Table 2 shows the features that are used in the semantic leftmost and rightmost reordering models. 3.2 Integrating into the HPB Model For models with syntactic reordering, we add two new features (i.e., one for the leftmost reordering model and the other for the rightmost reordering model) into the log-linear translation model in Eq. 1. Unlike the conventional phrase and lexical translation features, whose values are phrase pair-determined and thus can be calculated offline, the value of the reordering features can only be obtained during decoding time, and requires word alignment information as well. Before we present the algorithm integrating the reordering models, we define the following functions by assuming XPi and XPi+1 are the constituent pair of interest in CFG rule cfg, H is the translation hypothesis and a is its word alignment: #Index Feature rf1 R(Ri) & R(Ri+1) & P(pas) R(Ri) & R(Ri+1) rf2","for each Rj (j < i) R(Ri) & R(Ri+1) & R(Rj) & P(pas) R(Ri) & R(Ri+1) & R(Rj) rf3","for each Rj (j > i + 1)","R(Ri) & R(Ri+1) & R(Rj) & P(pas)","R(Ri) & R(Ri+1) & R(Rj) rf4 R(Ri) & R(Ri+1) & P(Ri) rf5 R(Ri) & R(Ri+1) & H(Ri) rf6 R(Ri) & R(Ri+1) & L(Ri) rf7 R(Ri) & R(Ri+1) & P(Ri+1) rf8 R(Ri) & R(Ri+1) & H(Ri+1) rf9 R(Ri) & R(Ri+1) & L(Ri+1) rf10 R(Ri) & R(Ri+1) & S(Ri) rf11 R(Ri) & R(Ri+1) & S(Ri+1) rf12 R(Ri) & P(pas) R(Ri) rf13 R(Ri+1) & P(pas) R(Ri+1) Table 2: Features adopted in the semantic leftmost and rightmost reordering models. P (pas) returns the predicate content of pas; R (R) returns the role type of R, e.g., Pred, A0, TMP, etc. For features rf1, rf2, rf3, rf12 and rf13, we include another version which excludes the predicate content P(pas) for reasons of sparsity.","• F1 (w1, w2, XP): returns true if constituent XP is within the span from word w1 to w2; otherwise returns false.","• F2 (H, cfg, XPi, XPi+1) returns true if the reordering of the pair ⟨XPi, XPi+1⟩ in rule cfg has not been calculated yet; otherwise returns false.","• F3 (H, a, XPi, XPi+1) returns the leftmost and rightmost reordering types for the constituent pair ⟨XPi, XPi+1⟩, given alignment a, according to Section 3.","• F4 (rt, cfg, XPi, XPi+1) returns the probability of leftmost reordering type rt for the constituent pair ⟨XPi, XPi+1⟩ in rule cfg.","• F5 (rt, cfg, XPi, XPi+1) returns the probability of rightmost reordering type rt for the constituent pair ⟨XPi, XPi+1⟩ in rule cfg.","Algorithm 1 integrates the syntactic leftmost and rightmost reordering models into a CKY-style decoder whenever a new hypothesis is generated. Given a hypothesis H with its alignment a, it traverses all CFG rules in the parse tree and sees if two adjacent constituents are conditioned to trigger the reordering models (lines 2-4). For each pair of constituents, it first extracts its leftmost and rightmost reordering types (line 6) and then gets their respective probabilities returned by the maximum entropy classifiers defined in Section 3.1 1126","Algorithm 1: Integrating the syntactic reordering models into a CKY-style decoder Input: Sentence f in the source language","Parse tree t of f","All CFG rules {cfg} in t","Hypothesis H spanning from word w1 to w2","Alignment a of H Output: Log-Probabilities of the syntactic leftmost","and rightmost reordering models 1. set l prob = rprob = 0.0 2. foreach cfg in {cfg} 3. foreach pair XPi and XPi+1 in cfg 4. if F1 (w1, w2, XPi) = false or","F1 (w1, w2, XPi+1) = false or","F2 (H, cfg, XPi, XPi+1) = false 5. continue 6. (l type, r type) = F3 (H, a, XPi, XPi+1) 7. l prob += log F4 (l type, cfg, XPi, XPi+1) 8. r prob += log F5 (r type, cfg, XPi, XPi+1) 9. return (l prob, r prob) (lines 7-8). Then the algorithm returns two log-probabilities of the syntactic reordering models. Note that Function F1 returns true if hypothesis H fully covers, or fully contains, constituent XPi, regardless of the reordering type of XPi. Do not confuse any parsing tag XPi with the nameless variables Xi in Hiero or cdec rules.","For the semantic reordering models, we also add two new features into the log-linear translation model. To get the two semantic reordering model feature values, we simply use Algorithm 1 and its associated functions from F1 to F5 replac-ing a CFG rule cfg with a PAS pas, and a constituent XPi with a semantic role Ri. Algorithm 1 therefore permits a unified treatment of syntactic and PAS-based reordering, even though it is expressed in terms of syntactic reordering here for ease of presentation."]},{"title":"4 Experiments","paragraphs":["We have presented our unified approach to incorporating syntactic and semantic soft reordering constraints in an HPB system. In this section, we test its effectiveness in Chinese-English translation. 4.1 Experimental Settings For training we use 1.6M sentence pairs of the non-UN and non-HK Hansards portions of NIST MT training corpora, segmented with the Stanford segmenter (Tseng et al., 2005). The English data is lowercased, tokenized and aligned with GIZA++ (Och and Ney, 2000) to obtain bidirectional alignments, which are symmetrized using the grow-diag-final-and method (Koehn et al., 2003). We train a 4-gram LM on the English side of the corpus with 600M additional words from non-NYT and non-LAT, randomly selected portions of the Gigaword v4 corpus, using modified Kneser-Ney smoothing (Chen and Goodman, 1996). We use the HPB decoder cdec (Dyer et al., 2010), with Mr. Mira (Eidelman et al., 2013), which is a k-best variant of MIRA (Chiang et al., 2008), to tune the parameters of the system.","We use NIST MT 06 dataset (1664 sentence pairs) for tuning, and NIST MT 03, 05, and 08 datasets (919, 1082, and 1357 sentence pairs, respectively) for evaluation.1","We use BLEU (Papineni et al., 2002) for both tuning and evaluation.","To obtain syntactic parse trees and semantic roles on the tuning and test datasets, we first parse the source sentences with the Berkeley Parser (Petrov and Klein, 2007), trained on the Chinese Treebank 7.0 (Xue et al., 2005). We then pass the parses to a Chinese semantic role labeler (Li et al., 2010), trained on the Chinese PropBank 3.0 (Xue and Palmer, 2009), to annotate semantic roles for all verbal predicates (part-of-speech tag VV, VE, or VC).","Our basic baseline system employs 19 basic features: a language model feature, 7 translation model features, word penalty, unknown word penalty, the glue rule, date, number and 6 passthrough features. Our stronger baseline employs, in addition, the fine-grained syntactic soft constraint features of Marton and Resnik (2008), hereafter MR08. The syntactic soft constraint features include both MR08 exact-matching and cross-boundary constraints (denoted XP= and XP+). Since the syntactic parses of the tuning and test data contain 29 types of constituent labels and 35 types of POS tags, we have 29 types of XP+ features and 64 types of XP= features. 4.2 Model Training To train the syntactic and semantic reordering models, we use a gold alignment dataset.2","It contains 7,870 sentences with 191,364 Chinese words and 261,399 English words. We first run syn-","1","http://www.itl.nist.gov/iad/mig//tests/mt","2","This dataset includes LDC2006E86, and newswire parts of LDC2012T16, LDC2012T20, LDC2012T24, and LDC2013T05. Indeed, the reordering models can also be trained on the MT training data with its automatic alignment. However, our preliminary experiments showed that the reordering models trained on gold alignment yielded higher improvement. 1127 Reordering Type","Syntactic Semantic","l-m r-m l-m r-m M 73.5 80.6 63.8 67.9 DM 3.9 3.3 14.0 12.0 S 19.5 13.2 13.1 10.7 DS 3.2 3.0 9.1 9.5 #instance 199,234 66,757 Table 3: Reordering type distribution over the reordering model’s training data. Hereafter, l-m and r-m are for leftmost and rightmost, respectively. tactic parsing and semantic role labeling on the Chinese sentences, then train the models by using MaxEnt toolkit with L1 regularizer (Tsuruoka et al., 2009).3","Table 3 shows the reordering type distribution over the training data. Interestingly, about 17% of the syntactic instances and 16% of the semantic instances differ in their leftmost and rightmost reordering types, indicating that the leftmost/rightmost distinction is informative. We also see that the number of semantic instances is about 1/3 of that of syntactic instances, but the entropy of the semantic reordering classes is higher, indicating the reordering of semantic roles is harder than that of syntactic constituents.","A deeper examination of the reordering model’s training data reveals that some constituent pairs and semantic role pairs have a preference for a specific reordering type (monotone or swap). In order to understand how well the MR08 system respects their reordering preference, we use the gold alignment dataset LDC2006E86, in which the source sentences are from the Chinese Treebank, and thus both the gold parse trees and gold predicate-argument structures are available. Table 4 presents examples comparing the reordering distribution between gold alignment and the output of the MR08 system. For example, the first row shows that based on the gold alignment, for ⟨PP,VP⟩, 16% are in monotone and 76% are in swap reordering. However, our MR08 system outputs 46% of them in monotone and and 50% in swap reordering. Hence, the reordering accuracy for ⟨PP,VP⟩ is 54%. Table 4 also shows that the semantic reordering between core arguments and predicates (e.g., ⟨Pred, A1⟩, ⟨A0, Pred⟩) has a less ambiguous pattern than that between adjuncts and other roles (e.g., ⟨LOC,Pred⟩, ⟨A0,TMP⟩), indicating the higher reordering flexibility of adjuncts. 3 http://www.logos.ic.i.u-tokyo.ac.jp/∼tsuruoka/maxent/ Const. Pair Gold MR08 output","M S M S acc. PP VP 16 76 46 50 54 NP LC 26 74 58 42 50 DNP NP 24 72 78 19 39 CP NP 26 67 84 10 33 NP DEG 39 61 31 69 66","... ... ...","all 81 13 79 14 80 Role Pair Gold MR08 output","M S M S acc. Pred A1 84 6 82 9 72 A0 Pred 82 11 79 8 75 LOC Pred 17 30 36 25 49 A0 TMP 35 25 61 6 45 TMP Pred 30 22 49 19 43","... ... ...","all 63 13 73 9 64 Table 4: Examples of the reordering distribution (%) of gold alignment and the MR08 system output. For simplicity, we only focus on (M)onotone and (S)wap based on leftmost reordering. 4.3 Translation Experiment Results Our first group of experiments investigates whether the syntactic reordering models are able to improve translation quality in terms of BLEU. To this end, we respectively add our syntactic reordering models into both the baseline and MR08 systems. The effect is shown in the rows of “+ syn-reorder” in Table 5. From the table, we have the following two observations.","• Although the HPB model is capable of handling non-local phrase reordering using synchronous context free grammars, both our syntactic leftmost reordering model and rightmost model are still able to achieve improvement over both the baseline and MR08. This suggests that our syntactic reordering features interact well with the MR08 syntactic soft constraints: the XP+ and XP= features focus on a single constituent each, while our reordering features focus on a pair of constituents each.","• There is no clear indication of whether the leftmost reordering model works better than the other. In addition, integrating both the leftmost and rightmost reordering models has limited improvement over a single reordering model.","Our second group of experiments is to validate the semantic reordering models. Results are 1128 System Tuning Test","MT06 MT03 MT05 MT08 Avg. Baseline 34.1 36.1 32.3 27.4 31.9 + syn-reorder l-m 35.2 36.9‡ 33.6‡ 28.4‡ 33.0 r-m 35.2 37.2‡ 33.7‡ 28.6‡ 33.2 both 35.6 37.1‡ 33.6‡ 28.8‡ 33.1 + sem-reorder l-m 34.4 36.7‡ 33.0‡ 27.8† 32.5 r-m 34.5 36.7‡ 33.1‡ 27.8‡ 32.5 both 34.5 37.0‡ 33.6‡ 27.7† 32.8","+syn+sem 35.5 37.3‡ 33.7‡ 29.0‡ 33.3","MR08 35.6 37.4 34.2 28.7 33.4 + syn-reorder l-m 36.0 38.2‡ 35.0‡ 29.2‡ 34.1 r-m 36.0 38.1‡ 34.8‡ 29.2‡ 34.0 both 35.9 38.2‡ 35.3‡ 29.5‡ 34.3 + sem-reorder l-m 35.8 37.6† 34.7‡ 28.7 33.7 r-m 35.8 37.4 34.5† 28.8 33.6 both 35.8 37.6† 34.7‡ 28.8 33.7","+syn+sem 36.1 38.4‡ 35.2‡ 29.5‡ 34.4 Table 5: System performance in BLEU scores. ‡/†: significant over baseline or MR08 at 0.01 / 0.05, respectively, as tested by bootstrap resampling (Koehn, 2004) shown in the rows of “+ sem-reorder” in Table 5. Here we observe:","• The semantic reordering models also achieve significant gain of 0.8 BLEU on average over the baseline system, demonstrating the effectiveness of PAS-based reordering. However, the gain diminishes to 0.3 BLEU on the MR08 system.","• The syntactic reordering models outperform the semantic reordering models on both the baseline and MR08 systems.","Finally, we integrate both the syntactic and semantic reordering models into the final system. The two models collectively achieve a gain of up to 1.4 BLEU over the baseline and 1.0 BLEU over MR08 on average, which is shown in the rows of “+syn+sem” in Table 5."]},{"title":"5 Discussion","paragraphs":["The trend of the results, summarized as performance gain over the baseline and MR08 systems averaged over all test sets, is presented in Table 6. The syntactic reordering models outperform the semantic reordering models, and the gain achieved by the semantic reordering models is limited in the presence of the MR08 syntactic features. In this section, we look at MR08 system and the systems improving it to explore the behavior differences between the two reordering models.","Coverage analysis: Our statistics show that syntactic reordering features (either leftmost or","System Baseline MR08","+syn-reorder 1.2 0.9","+sem-reorder 0.8 0.3","+ both 1.4 1.0 Table 6: Performance gain in BLEU over baseline and MR08 systems averaged over all test sets. rightmost) are called 24 times per sentence on average. This is compared to only 9 times per sentence for semantic reordering features. This is not surprising since the semantic reordering features are exclusively attached to predicates, and the span set of the semantic roles is a strict subset of the span set of the syntactic constituents; only 22% of syntactic constituents are semantic roles. On average, a sentences has 4 PASs and each PAS contains 3 semantic roles. Of all the semantic role pairs, 44% are in the same CFG rules, indicating that this part of semantic reordering has overlap with syntactic reordering. Therefore, the PAS model has fewer opportunities to influence reordering.","Reordering accuracy analysis: The reordering type distribution on the reordering model training data in Table 3 suggests that semantic reordering is more difficult than syntactic reordering. To validate this conjecture on our translation test data, we compare the reordering performance among the MR08 system, the improved systems and the maximum entropy classifiers. For the test set, we have four reference translations. We run GIZA++ on the data combination of our translation training data and test data to get the alignment for the test data and each reference translation. Once we have the (semi-)gold alignment, we compute the gold reordering types between two adjacent syntactic constituents or semantic roles. Then we evaluate the automatic reordering outputs generated from both our translation systems and maximum entropy classifiers. Table 7 shows the accuracy averaged over the four gold reordering sets (the four reference translations). It shows that 1) as expected, our classifiers do worse on the harder semantic reordering prediction than syntactic reordering prediction; 2) thanks to the high accuracy obtained by the maxent classifiers, integrating either the syntactic or the semantic reordering constraints results in better reordering performance from both syntactic and semantic perspectives; 3) in terms of the mutual impact, the syntactic reordering models help improving semantic reordering more than the semantic reordering 1129 System Syntactic Semantic","l-m r-m l-m r-m MR08 75.0 78.0 66.3 68.5","+syn-reorder 78.4 80.9 69.0 70.2","+sem-reorder 76.0 78.8 70.7 72.7 +both 78.6 81.7 70.6 72.1","Maxent Classifier 80.7 85.6 70.9 73.5 Table 7: Reordering accuracy on four gold sets. System Syntactic Semantic","l-m r-m l-m r-m","+syn-reorder 1.2 1.2 - -","+sem-reorder - - 0.7 0.9 +both 1.2 1.0 0.5 0.4 Table 8: Reordering feature weights. models help improving syntactic reordering; and 4) the rightmost models have a learnability advantage over the leftmost models, achieving higher accuracy across the board.","Feature weight analysis: Table 8 shows the syntactic and semantic reordering feature weights. It shows that the semantic feature weights decrease in the presence of the syntactic features, indicating that the decoder learns to trust semantic features less in the presence of the more accurate syntactic features. This is consistent with our observation that semantic reordering is harder than syntactic reordering, as seen in Tables 3 and 7.","Potential improvement analysis: Table 7 also shows that our current maximum entropy classifiers have room for improvement, especially for semantic reordering. In order to explore the error propagation from the classifiers themselves and explore the upper bound for improvement from the reordering models, we perform an “oracle” study, letting the classifiers be aware of the “gold” reordering type between two syntactic constituents or two semantic roles, and returning a higher probability for the gold reordering type and a smaller one for the others (i.e., we set 0.9 for the gold System MT 03 MT 05 MT 08 Avg. Non-Oracle MR08 37.4 34.2 28.7 33.4 +syn-reorder 38.2 35.3 29.5 34.3 +sem-reorder 37.6 34.7 28.8 33.7 + both 38.4 35.2 29.5 34.4 Oracle +syn-reorder 39.2 35.9 29.6 34.9 +sem-reorder 37.9 34.8 28.9 33.9 + both 39.1 36.0 29.8 35.0 Table 9: Performance (BLEU score) comparison between non-oracle and oracle experiments. reordering type, and let the other non-gold three types share 0.1). Again, to get the gold reordering type, we run GIZA++ to get the alignment for tuning/test source sentences and each of four reference translations. We report the averaged performance by using the gold reordering type extracted from the four reference translations. Table 9 compares the performance between the non-oracle and oracle settings. We clearly see that using gold syntactic reordering types significantly improves the performance (e.g., 34.9 vs. 33.4 on average) and there is still some room for improvement by building a better maximum entropy classifiers (e.g., 34.9 vs. 34.3). To our surprise, however, the improvement achieved by gold semantic reordering types is still small (e.g., 33.9 vs. 33.4), suggesting that the potential improvement of semantic reordering models is much more limited. And we again see that the improvement achieved by semantic reordering models is limited in the presence of the syntactic reordering models."]},{"title":"6 Related Work Syntax-based reordering:","paragraphs":["Some previous work pre-ordered words in the source sentences, so that the word order of source and target sentences is similar. The reordering rules were either manually designed (Collins et al., 2005; Wang et al., 2007; Xu et al., 2009; Lee et al., 2010) or automatically learned (Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2010; Khalilov and Sima’an, 2011; Lerner and Petrov, 2013), using syntactic parses. Li et al. (2007) focused on finding then-best pre-ordered source sentences by predicting the reordering of sibling constituents, while Yang et al. (2012) obtained word order by using a reranking approach to reposition nodes in syntactic parse trees. Both are close to our work; however, our model generates reordering features that are integrated into the log-linear translation model during decoding.","Another approach in previous work added soft constraints as weighted features in the SMT decoder to reward good reorderings and penalize bad ones. Marton and Resnik (2008) employed soft syntactic constraints with weighted binary features and no MaxEnt model. They did not explicitly target reordering (beyond applying constraints on HPB rules). Although employing linguistically motivated labels in SCFG is capable of captur-ing constituent reorderings (Chiang, 2010; Mylon-1130 akis and Sima’an, 2011), the rules are sparser than SCFG with nameless non-terminals (i.e., Xs) and soft constraints. Ge (2010) presented a syntax-driven maximum entropy reordering model that predicted the source word translation order. Gao et al. (2011) employed dependency trees to predict the translation order of a word and its head word. Huang et al. (2013) predicted the translation order of two source words.4","Our work, which shares this approach, differs from their work primarily in that our syntactic reordering models are based on the constituent level, rather than the word level.","Semantics-based reordering: Semantics-based reordering has also seen an increase in activity recently. In the pre-ordering approach, Wu et al. (2011) automatically learned pre-ordering rules from PAS. In the soft constraint or reordering model approach, Liu and Gildea (2010) modeled the reordering/deletion of source-side semantic roles in a tree-to-string translation model. Xiong et al. (2012) and Li et al. (2013) predicted the translation order between either two arguments or an argument and its predicate. Instead of decomposing a PAS into individual units, Zhai et al. (2013) constructed a classifier for each source side PAS. Finally in the post-processing approach category, Wu and Fung (2009) performed semantic role labeling on translation output and reordered arguments to maximize the cross-lingual match of the semantic frames between the source sentence and the target translation. To our knowledge, their semantic reordering models were PAS-specific. In contrast, our model is universal and can be easily adopted to model the reordering of other linguistic units (e.g., syntactic constituents). Moreover, we have studied the effectiveness of the semantic reordering model in different scenarios.","Non-syntax-based reorderings in HPB: Recently we have also seen work on lexicalized reordering models without syntactic information in HPB (Setiawan et al., 2009; Huck et al., 2013; Nguyen and Vogel, 2013). The non-syntax-based reordering approach models the reordering of translation words/phrases while the syntax-based approach models the reordering of syntactic constituents. Although there are overlaps between translation phrases and syntactic constituents, it is reasonable to think that the two re-","4","Note that they obtained the translation order of source word pairs by predicting the reordering of adjacent constituents, which was quite close to our work. ordering approaches can work together well and even complement each other, as the linguistic patterns they capture differ substantially. Setiawan et al. (2013) modeled the orientation decisions between anchors and two neighboring multi-unit chunks which might cross phrase or rule boundaries. Last, we also note that recent work on nonsyntax-based reorderings in (flat) phrase-based models (Cherry, 2013; Feng et al., 2013) can also be potentially adopted to hpb models."]},{"title":"7 Conclusion and Future Work","paragraphs":["In this paper, we have presented a unified reordering framework to incorporate soft linguistic constraints (of syntactic or semantic nature) into the HPB translation model. The syntactic reordering models take CFG rules and model their reordering on the target side, while the semantic reordering models work with PAS. Experiments on Chinese-English translation show that the reordering approach can significantly improve a state-of-the-art hierarchical phrase-based translation system. We have also discussed the differences between the two linguistic reordering models.","There are many directions in which this work can be continued. First, the syntactic reordering model can be extended to model reordering among constituents that cross CFG rules. Second, although we do not see obvious gain from the semantic reordering model when the syntactic model is adopted, it might be beneficial to further jointly consider the two reordering models, focusing on where each one does well. Third, to better examine the overlap or synergy between our approach and the non-syntax-based reordering approach, we will conduct direct comparisons and combinations with the latter."]},{"title":"Acknowledgments","paragraphs":["This research was supported in part by the BOLT program of the Defense Advanced Research Projects Agency, Contract No. HR0012-12-C-0015. Any opinions, findings, conclusions or recommendations expressed in this paper are those of the authors and do not necessarily reflect the view of DARPA. The authors would like to thank three anonymous reviewers for providing helpful comments, and also acknowledge Ke Wu, Vladimir Eidelman, Hua He, Doug Oard, Yuening Hu, Jordan Boyd-Graber, and Jyothi Vinjumur for useful discussions. 1131"]},{"title":"References","paragraphs":["Stanley F. Chen and Joshua Goodman. 1996. An empirical study of smoothing techniques for language modeling. In Proceedings of ACL 1996, pages 310– 318.","Colin Cherry. 2013. Improved reordering for phrase-based translation using sparse features. In Proceedings of HLT-NAACL 2013, pages 22–31.","David Chiang, Yuval Marton, and Philip Resnik. 2008. Online large-margin training of syntactic and structural translation features. In Proceedings of EMNLP 2008, pages 224–233.","David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.","David Chiang. 2010. Learning to translate with source and target syntax. In Proceedings of ACL 2010, pages 1443–1452.","Michael Collins, Philipp Koehn, and Ivona Kucerova. 2005. Clause restructuring for statistical machine translation. In Proceedings of ACL 2005, pages 531–540.","Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan, Vladimir Eidelman, and Philip Resnik. 2010. cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models. In Proceedings of ACL 2010 System Demonstrations, pages 7–12.","Vladimir Eidelman, Ke Wu, Ferhan Ture, Philip Resnik, and Jimmy Lin. 2013. Mr. mira: Open-source large-margin structured learning on mapreduce. In Proceedings of ACL 2013 System Demonstrations, pages 199–204.","Minwei Feng, Jan-Thorsten Peter, and Hermann Ney. 2013. Advancements in reordering models for statistical machine translation. In Proceedings of ACL 2013, pages 322–332.","Yang Gao, Philipp Koehn, and Alexandra Birch. 2011. Soft dependency constraints for reordering in hierarchical phrase-based translation. In Proceedings of EMNLP 2011, pages 857–868.","Niyu Ge. 2010. A direct syntax-driven reordering model for phrase-based machine translation. In Proceedings of HLT-NAACL 2010, pages 849–857.","Dmitriy Genzel. 2010. Automatically learning source-side reordering rules for large scale machine translation. In Proceedings of COLING 2010, pages 376– 384.","Zhongqiang Huang, Jacob Devlin, and Rabih Zbib. 2013. Factored soft source syntactic constraints for hierarchical machine translation. In Proceedings of EMNLP 2013, pages 556–566.","Matthias Huck, Joern Wuebker, Felix Rietig, and Hermann Ney. 2013. A phrase orientation model for hierarchical machine translation. In Proceedings of WMT 2013, pages 452–463.","Maxim Khalilov and Khalil Sima’an. 2011. Contextsensitive syntactic source-reordering by statistical transduction. In Proceedings of IJCNLP 2011, pages 38–46.","Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of HLT-NAACL 2003, pages 48–54.","Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proceedings of EMNLP 2004, pages 388–395.","Young-Suk Lee, Bing Zhao, and Xiaoqian Luo. 2010. Constituent reordering and syntax models for English-to-Japanese statistical machine translation. In Proceedings of COLING 2010, pages 626–634.","Uri Lerner and Slav Petrov. 2013. Source-side classifier preordering for machine translation. In Proceedings of EMNLP 2013, pages 513–523.","Chi-Ho Li, Minghui Li, Dongdong Zhang, Mu Li, Ming Zhou, and Yi Guan. 2007. A probabilistic approach to syntax-based reordering for statistical machine translation. In Proceedings of ACL 2007, pages 720–727.","Junhui Li, Guodong Zhou, and Hwee Tou Ng. 2010. Joint syntactic and semantic parsing of Chinese. In Proceedings of ACL 2010, pages 1108–1117.","Junhui Li, Philip Resnik, and Hal Daumé III. 2013. Modeling syntactic and semantic structures in hierarchical phrase-based translation. In Proceedings of HLT-NAACL 2013, pages 540–549.","Ding Liu and Daniel Gildea. 2010. Semantic role features for machine translation. In Proceedings of COLING 2010, pages 716–724.","Yuval Marton and Philip Resnik. 2008. Soft syntactic constraints for hierarchical phrased-based translation. In Proceedings of ACL-HLT 2008, pages 1003–1011.","Markos Mylonakis and Khalil Sima’an. 2011. Learning hierarchical translation structure with linguistic annotations. In Proceedings of ACL 2011, pages 642–652.","ThuyLinh Nguyen and Stephan Vogel. 2013. Integrating phrase-based reordering features into a chart-based decoder for machine translation. In Proceedings of ACL 2013, pages 1587–1596.","Franz Josef Och and Hermann Ney. 2000. Improved statistical alignment models. In Proceedings of ACL 2000, pages 440–447. 1132","Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The Proposition Bank: An annotated corpus of semantic roles. Computational Linguistics, 31(1):71–106.","Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: A method for automatic evaluation of machine translation. In Proceedings of ACL 2002, pages 311–318.","Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In Proceedings of HLT-NAACL 2007, pages 404–411.","Hendra Setiawan, Min Yen Kan, Haizhou Li, and Philip Resnik. 2009. Topological ordering of function words in hierarchical phrase-based translation. In Proceedings of ACL-IJCNLP 2009, pages 324–332.","Hendra Setiawan, Bowen Zhou, Bing Xiang, and Libin Shen. 2013. Two-neighbor orientation model with cross-boundary global contexts. In Proceedings of ACL 2013, pages 1264–1274.","Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel Jurafsky, and Christopher Manning. 2005. A conditional random field word segmenter for sighan bakeoff 2005. In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing, pages 168–171.","Yoshimasa Tsuruoka, Jun’ichi Tsujii, and Sophia Ananiadou. 2009. Stochastic gradient descent training for l1-regularized log-linear models with cumulative penalty. In Proceedings of ACL-IJCNLP 2009, pages 477–485.","Karthik Visweswariah, Jiri Navratil, Jeffrey Sorensen, Vijil Chenthamarakshan, and Nandakishore Kambhatla. 2010. Syntax based reordering with automatically derived rules for improved statistical machine translation. In Proceedings of COLING 2010, pages 1119–1127.","Chao Wang, Michael Collins, and Philipp Koehn. 2007. Chinese syntactic reordering for statistical machine translation. In Proceedings of EMNLP 2007, pages 737–745.","Dekai Wu and Pascale Fung. 2009. Semantic roles for smt: A hybrid two-pass model. In Proceedings of HLT-NAACL 2009: short papers, pages 13–16.","Xianchao Wu, Katsuhito Sudoh, Kevin Duh, Hajime Tsukada, and Masaaki Nagata. 2011. Extracting pre-ordering rules from predicate-argument structures. In Proceedings of IJCNLP 2011, pages 29– 37.","Fei Xia and Michael McCord. 2004. Improving a statistical mt system with automatically learned rewrite patterns. In Proceedings of COLING 2004, pages 508–514.","Deyi Xiong, Min Zhang, and Haizhou Li. 2012. Modeling the translation of predicate-argument structure for smt. In Proceedings of ACL 2012, pages 902– 911.","Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz Och. 2009. Using a dependency parser to improve smt for subject-object-verb languages. In Proceedings of HLT-NAACL 2009, pages 245–253.","Nianwen Xue and Martha Palmer. 2009. Adding semantic roles to the Chinese Treebank. Natural Language Engineering, 15(1):143–172.","Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha Palmer. 2005. The Penn Chinese Treebank: Phrase structure annotation of a large corpus. Natural Language Engineering, 11(2):207–238.","Nan Yang, Mu Li, Dongdong Zhang, and Nenghai Yu. 2012. A ranking-based approach to word reordering for statistical machine translation. In Proceedings of ACL 2012, pages 912–920.","Feifei Zhai, Jiajun Zhang, Yu Zhou, and Chengqing Zong. 2013. Handling ambiguities of bilingual predicate-argument structures for statistical machine translation. In Proceedings of ACL 2013, pages 1127–1136. 1133"]}],"references":[{"authors":[{"first":"Stanley","middle":"F.","last":"Chen"},{"first":"Joshua","last":"Goodman"}],"year":"1996","title":"An empirical study of smoothing techniques for language modeling","source":"Stanley F. Chen and Joshua Goodman. 1996. An empirical study of smoothing techniques for language modeling. In Proceedings of ACL 1996, pages 310– 318."},{"authors":[{"first":"Colin","last":"Cherry"}],"year":"2013","title":"Improved reordering for phrase-based translation using sparse features","source":"Colin Cherry. 2013. Improved reordering for phrase-based translation using sparse features. In Proceedings of HLT-NAACL 2013, pages 22–31."},{"authors":[{"first":"David","last":"Chiang"},{"first":"Yuval","last":"Marton"},{"first":"Philip","last":"Resnik"}],"year":"2008","title":"Online large-margin training of syntactic and structural translation features","source":"David Chiang, Yuval Marton, and Philip Resnik. 2008. Online large-margin training of syntactic and structural translation features. In Proceedings of EMNLP 2008, pages 224–233."},{"authors":[{"first":"David","last":"Chiang"}],"year":"2007","title":"Hierarchical phrase-based translation","source":"David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228."},{"authors":[{"first":"David","last":"Chiang"}],"year":"2010","title":"Learning to translate with source and target syntax","source":"David Chiang. 2010. Learning to translate with source and target syntax. In Proceedings of ACL 2010, pages 1443–1452."},{"authors":[{"first":"Michael","last":"Collins"},{"first":"Philipp","last":"Koehn"},{"first":"Ivona","last":"Kucerova"}],"year":"2005","title":"Clause restructuring for statistical machine translation","source":"Michael Collins, Philipp Koehn, and Ivona Kucerova. 2005. Clause restructuring for statistical machine translation. In Proceedings of ACL 2005, pages 531–540."},{"authors":[{"first":"Chris","last":"Dyer"},{"first":"Adam","last":"Lopez"},{"first":"Juri","last":"Ganitkevitch"},{"first":"Jonathan","last":"Weese"},{"first":"Ferhan","last":"Ture"},{"first":"Phil","last":"Blunsom"},{"first":"Hendra","last":"Setiawan"},{"first":"Vladimir","last":"Eidelman"},{"first":"Philip","last":"Resnik"}],"year":"2010","title":"cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models","source":"Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan, Vladimir Eidelman, and Philip Resnik. 2010. cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models. In Proceedings of ACL 2010 System Demonstrations, pages 7–12."},{"authors":[{"first":"Vladimir","last":"Eidelman"},{"first":"Ke","last":"Wu"},{"first":"Ferhan","last":"Ture"},{"first":"Philip","last":"Resnik"},{"first":"Jimmy","last":"Lin"}],"year":"2013","title":"Mr","source":"Vladimir Eidelman, Ke Wu, Ferhan Ture, Philip Resnik, and Jimmy Lin. 2013. Mr. mira: Open-source large-margin structured learning on mapreduce. In Proceedings of ACL 2013 System Demonstrations, pages 199–204."},{"authors":[{"first":"Minwei","last":"Feng"},{"first":"Jan-Thorsten","last":"Peter"},{"first":"Hermann","last":"Ney"}],"year":"2013","title":"Advancements in reordering models for statistical machine translation","source":"Minwei Feng, Jan-Thorsten Peter, and Hermann Ney. 2013. Advancements in reordering models for statistical machine translation. In Proceedings of ACL 2013, pages 322–332."},{"authors":[{"first":"Yang","last":"Gao"},{"first":"Philipp","last":"Koehn"},{"first":"Alexandra","last":"Birch"}],"year":"2011","title":"Soft dependency constraints for reordering in hierarchical phrase-based translation","source":"Yang Gao, Philipp Koehn, and Alexandra Birch. 2011. Soft dependency constraints for reordering in hierarchical phrase-based translation. In Proceedings of EMNLP 2011, pages 857–868."},{"authors":[{"first":"Niyu","last":"Ge"}],"year":"2010","title":"A direct syntax-driven reordering model for phrase-based machine translation","source":"Niyu Ge. 2010. A direct syntax-driven reordering model for phrase-based machine translation. In Proceedings of HLT-NAACL 2010, pages 849–857."},{"authors":[{"first":"Dmitriy","last":"Genzel"}],"year":"2010","title":"Automatically learning source-side reordering rules for large scale machine translation","source":"Dmitriy Genzel. 2010. Automatically learning source-side reordering rules for large scale machine translation. In Proceedings of COLING 2010, pages 376– 384."},{"authors":[{"first":"Zhongqiang","last":"Huang"},{"first":"Jacob","last":"Devlin"},{"first":"Rabih","last":"Zbib"}],"year":"2013","title":"Factored soft source syntactic constraints for hierarchical machine translation","source":"Zhongqiang Huang, Jacob Devlin, and Rabih Zbib. 2013. Factored soft source syntactic constraints for hierarchical machine translation. In Proceedings of EMNLP 2013, pages 556–566."},{"authors":[{"first":"Matthias","last":"Huck"},{"first":"Joern","last":"Wuebker"},{"first":"Felix","last":"Rietig"},{"first":"Hermann","last":"Ney"}],"year":"2013","title":"A phrase orientation model for hierarchical machine translation","source":"Matthias Huck, Joern Wuebker, Felix Rietig, and Hermann Ney. 2013. A phrase orientation model for hierarchical machine translation. In Proceedings of WMT 2013, pages 452–463."},{"authors":[{"first":"Maxim","last":"Khalilov"},{"first":"Khalil","last":"Sima’an"}],"year":"2011","title":"Contextsensitive syntactic source-reordering by statistical transduction","source":"Maxim Khalilov and Khalil Sima’an. 2011. Contextsensitive syntactic source-reordering by statistical transduction. In Proceedings of IJCNLP 2011, pages 38–46."},{"authors":[{"first":"Philipp","last":"Koehn"},{"first":"Franz","middle":"J.","last":"Och"},{"first":"Daniel","last":"Marcu"}],"year":"2003","title":"Statistical phrase-based translation","source":"Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of HLT-NAACL 2003, pages 48–54."},{"authors":[{"first":"Philipp","last":"Koehn"}],"year":"2004","title":"Statistical significance tests for machine translation evaluation","source":"Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proceedings of EMNLP 2004, pages 388–395."},{"authors":[{"first":"Young-Suk","last":"Lee"},{"first":"Bing","last":"Zhao"},{"first":"Xiaoqian","last":"Luo"}],"year":"2010","title":"Constituent reordering and syntax models for English-to-Japanese statistical machine translation","source":"Young-Suk Lee, Bing Zhao, and Xiaoqian Luo. 2010. Constituent reordering and syntax models for English-to-Japanese statistical machine translation. In Proceedings of COLING 2010, pages 626–634."},{"authors":[{"first":"Uri","last":"Lerner"},{"first":"Slav","last":"Petrov"}],"year":"2013","title":"Source-side classifier preordering for machine translation","source":"Uri Lerner and Slav Petrov. 2013. Source-side classifier preordering for machine translation. In Proceedings of EMNLP 2013, pages 513–523."},{"authors":[{"first":"Chi-Ho","last":"Li"},{"first":"Minghui","last":"Li"},{"first":"Dongdong","last":"Zhang"},{"first":"Mu","last":"Li"},{"first":"Ming","last":"Zhou"},{"first":"Yi","last":"Guan"}],"year":"2007","title":"A probabilistic approach to syntax-based reordering for statistical machine translation","source":"Chi-Ho Li, Minghui Li, Dongdong Zhang, Mu Li, Ming Zhou, and Yi Guan. 2007. A probabilistic approach to syntax-based reordering for statistical machine translation. In Proceedings of ACL 2007, pages 720–727."},{"authors":[{"first":"Junhui","last":"Li"},{"first":"Guodong","last":"Zhou"},{"first":"Hwee","middle":"Tou","last":"Ng"}],"year":"2010","title":"Joint syntactic and semantic parsing of Chinese","source":"Junhui Li, Guodong Zhou, and Hwee Tou Ng. 2010. Joint syntactic and semantic parsing of Chinese. In Proceedings of ACL 2010, pages 1108–1117."},{"authors":[{"first":"Junhui","last":"Li"},{"first":"Philip","last":"Resnik"},{"first":"Hal","last":"Daumé III"}],"year":"2013","title":"Modeling syntactic and semantic structures in hierarchical phrase-based translation","source":"Junhui Li, Philip Resnik, and Hal Daumé III. 2013. Modeling syntactic and semantic structures in hierarchical phrase-based translation. In Proceedings of HLT-NAACL 2013, pages 540–549."},{"authors":[{"first":"Ding","last":"Liu"},{"first":"Daniel","last":"Gildea"}],"year":"2010","title":"Semantic role features for machine translation","source":"Ding Liu and Daniel Gildea. 2010. Semantic role features for machine translation. In Proceedings of COLING 2010, pages 716–724."},{"authors":[{"first":"Yuval","last":"Marton"},{"first":"Philip","last":"Resnik"}],"year":"2008","title":"Soft syntactic constraints for hierarchical phrased-based translation","source":"Yuval Marton and Philip Resnik. 2008. Soft syntactic constraints for hierarchical phrased-based translation. In Proceedings of ACL-HLT 2008, pages 1003–1011."},{"authors":[{"first":"Markos","last":"Mylonakis"},{"first":"Khalil","last":"Sima’an"}],"year":"2011","title":"Learning hierarchical translation structure with linguistic annotations","source":"Markos Mylonakis and Khalil Sima’an. 2011. Learning hierarchical translation structure with linguistic annotations. In Proceedings of ACL 2011, pages 642–652."},{"authors":[{"first":"ThuyLinh","last":"Nguyen"},{"first":"Stephan","last":"Vogel"}],"year":"2013","title":"Integrating phrase-based reordering features into a chart-based decoder for machine translation","source":"ThuyLinh Nguyen and Stephan Vogel. 2013. Integrating phrase-based reordering features into a chart-based decoder for machine translation. In Proceedings of ACL 2013, pages 1587–1596."},{"authors":[{"first":"Franz","middle":"Josef","last":"Och"},{"first":"Hermann","last":"Ney"}],"year":"2000","title":"Improved statistical alignment models","source":"Franz Josef Och and Hermann Ney. 2000. Improved statistical alignment models. In Proceedings of ACL 2000, pages 440–447. 1132"},{"authors":[{"first":"Martha","last":"Palmer"},{"first":"Daniel","last":"Gildea"},{"first":"Paul","last":"Kingsbury"}],"year":"2005","title":"The Proposition Bank: An annotated corpus of semantic roles","source":"Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The Proposition Bank: An annotated corpus of semantic roles. Computational Linguistics, 31(1):71–106."},{"authors":[{"first":"Kishore","last":"Papineni"},{"first":"Salim","last":"Roukos"},{"first":"Todd","last":"Ward"},{"first":"Wei-Jing","last":"Zhu"}],"year":"2002","title":"Bleu: A method for automatic evaluation of machine translation","source":"Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: A method for automatic evaluation of machine translation. In Proceedings of ACL 2002, pages 311–318."},{"authors":[{"first":"Slav","last":"Petrov"},{"first":"Dan","last":"Klein"}],"year":"2007","title":"Improved inference for unlexicalized parsing","source":"Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In Proceedings of HLT-NAACL 2007, pages 404–411."},{"authors":[{"first":"Hendra","last":"Setiawan"},{"first":"Min","middle":"Yen","last":"Kan"},{"first":"Haizhou","last":"Li"},{"first":"Philip","last":"Resnik"}],"year":"2009","title":"Topological ordering of function words in hierarchical phrase-based translation","source":"Hendra Setiawan, Min Yen Kan, Haizhou Li, and Philip Resnik. 2009. Topological ordering of function words in hierarchical phrase-based translation. In Proceedings of ACL-IJCNLP 2009, pages 324–332."},{"authors":[{"first":"Hendra","last":"Setiawan"},{"first":"Bowen","last":"Zhou"},{"first":"Bing","last":"Xiang"},{"first":"Libin","last":"Shen"}],"year":"2013","title":"Two-neighbor orientation model with cross-boundary global contexts","source":"Hendra Setiawan, Bowen Zhou, Bing Xiang, and Libin Shen. 2013. Two-neighbor orientation model with cross-boundary global contexts. In Proceedings of ACL 2013, pages 1264–1274."},{"authors":[{"first":"Huihsin","last":"Tseng"},{"first":"Pichuan","last":"Chang"},{"first":"Galen","last":"Andrew"},{"first":"Daniel","last":"Jurafsky"},{"first":"Christopher","last":"Manning"}],"year":"2005","title":"A conditional random field word segmenter for sighan bakeoff 2005","source":"Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel Jurafsky, and Christopher Manning. 2005. A conditional random field word segmenter for sighan bakeoff 2005. In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing, pages 168–171."},{"authors":[{"first":"Yoshimasa","last":"Tsuruoka"},{"first":"Jun’ichi","last":"Tsujii"},{"first":"Sophia","last":"Ananiadou"}],"year":"2009","title":"Stochastic gradient descent training for l1-regularized log-linear models with cumulative penalty","source":"Yoshimasa Tsuruoka, Jun’ichi Tsujii, and Sophia Ananiadou. 2009. Stochastic gradient descent training for l1-regularized log-linear models with cumulative penalty. In Proceedings of ACL-IJCNLP 2009, pages 477–485."},{"authors":[{"first":"Karthik","last":"Visweswariah"},{"first":"Jiri","last":"Navratil"},{"first":"Jeffrey","last":"Sorensen"},{"first":"Vijil","last":"Chenthamarakshan"},{"first":"Nandakishore","last":"Kambhatla"}],"year":"2010","title":"Syntax based reordering with automatically derived rules for improved statistical machine translation","source":"Karthik Visweswariah, Jiri Navratil, Jeffrey Sorensen, Vijil Chenthamarakshan, and Nandakishore Kambhatla. 2010. Syntax based reordering with automatically derived rules for improved statistical machine translation. In Proceedings of COLING 2010, pages 1119–1127."},{"authors":[{"first":"Chao","last":"Wang"},{"first":"Michael","last":"Collins"},{"first":"Philipp","last":"Koehn"}],"year":"2007","title":"Chinese syntactic reordering for statistical machine translation","source":"Chao Wang, Michael Collins, and Philipp Koehn. 2007. Chinese syntactic reordering for statistical machine translation. In Proceedings of EMNLP 2007, pages 737–745."},{"authors":[{"first":"Dekai","last":"Wu"},{"first":"Pascale","last":"Fung"}],"year":"2009","title":"Semantic roles for smt: A hybrid two-pass model","source":"Dekai Wu and Pascale Fung. 2009. Semantic roles for smt: A hybrid two-pass model. In Proceedings of HLT-NAACL 2009: short papers, pages 13–16."},{"authors":[{"first":"Xianchao","last":"Wu"},{"first":"Katsuhito","last":"Sudoh"},{"first":"Kevin","last":"Duh"},{"first":"Hajime","last":"Tsukada"},{"first":"Masaaki","last":"Nagata"}],"year":"2011","title":"Extracting pre-ordering rules from predicate-argument structures","source":"Xianchao Wu, Katsuhito Sudoh, Kevin Duh, Hajime Tsukada, and Masaaki Nagata. 2011. Extracting pre-ordering rules from predicate-argument structures. In Proceedings of IJCNLP 2011, pages 29– 37."},{"authors":[{"first":"Fei","last":"Xia"},{"first":"Michael","last":"McCord"}],"year":"2004","title":"Improving a statistical mt system with automatically learned rewrite patterns","source":"Fei Xia and Michael McCord. 2004. Improving a statistical mt system with automatically learned rewrite patterns. In Proceedings of COLING 2004, pages 508–514."},{"authors":[{"first":"Deyi","last":"Xiong"},{"first":"Min","last":"Zhang"},{"first":"Haizhou","last":"Li"}],"year":"2012","title":"Modeling the translation of predicate-argument structure for smt","source":"Deyi Xiong, Min Zhang, and Haizhou Li. 2012. Modeling the translation of predicate-argument structure for smt. In Proceedings of ACL 2012, pages 902– 911."},{"authors":[{"first":"Peng","last":"Xu"},{"first":"Jaeho","last":"Kang"},{"first":"Michael","last":"Ringgaard"},{"first":"Franz","last":"Och"}],"year":"2009","title":"Using a dependency parser to improve smt for subject-object-verb languages","source":"Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz Och. 2009. Using a dependency parser to improve smt for subject-object-verb languages. In Proceedings of HLT-NAACL 2009, pages 245–253."},{"authors":[{"first":"Nianwen","last":"Xue"},{"first":"Martha","last":"Palmer"}],"year":"2009","title":"Adding semantic roles to the Chinese Treebank","source":"Nianwen Xue and Martha Palmer. 2009. Adding semantic roles to the Chinese Treebank. Natural Language Engineering, 15(1):143–172."},{"authors":[{"first":"Nianwen","last":"Xue"},{"first":"Fei","last":"Xia"},{"first":"Fu-Dong","last":"Chiou"},{"first":"Martha","last":"Palmer"}],"year":"2005","title":"The Penn Chinese Treebank: Phrase structure annotation of a large corpus","source":"Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha Palmer. 2005. The Penn Chinese Treebank: Phrase structure annotation of a large corpus. Natural Language Engineering, 11(2):207–238."},{"authors":[{"first":"Nan","last":"Yang"},{"first":"Mu","last":"Li"},{"first":"Dongdong","last":"Zhang"},{"first":"Nenghai","last":"Yu"}],"year":"2012","title":"A ranking-based approach to word reordering for statistical machine translation","source":"Nan Yang, Mu Li, Dongdong Zhang, and Nenghai Yu. 2012. A ranking-based approach to word reordering for statistical machine translation. In Proceedings of ACL 2012, pages 912–920."},{"authors":[{"first":"Feifei","last":"Zhai"},{"first":"Jiajun","last":"Zhang"},{"first":"Yu","last":"Zhou"},{"first":"Chengqing","last":"Zong"}],"year":"2013","title":"Handling ambiguities of bilingual predicate-argument structures for statistical machine translation","source":"Feifei Zhai, Jiajun Zhang, Yu Zhou, and Chengqing Zong. 2013. Handling ambiguities of bilingual predicate-argument structures for statistical machine translation. In Proceedings of ACL 2013, pages 1127–1136. 1133"}],"cites":[{"style":0,"text":"Chiang, 2007","origin":{"pointer":"/sections/7/paragraphs/0","offset":1580,"length":12},"authors":[{"last":"Chiang"}],"year":"2007","references":["/references/3"]},{"style":0,"text":"Dyer et al., 2010","origin":{"pointer":"/sections/7/paragraphs/0","offset":1594,"length":17},"authors":[{"last":"Dyer"},{"last":"al."}],"year":"2010","references":["/references/6"]},{"style":0,"text":"Marton and Resnik, 2008","origin":{"pointer":"/sections/7/paragraphs/0","offset":1730,"length":23},"authors":[{"last":"Marton"},{"last":"Resnik"}],"year":"2008","references":["/references/23"]},{"style":0,"text":"Chiang et al., 2008","origin":{"pointer":"/sections/7/paragraphs/0","offset":1755,"length":19},"authors":[{"last":"Chiang"},{"last":"al."}],"year":"2008","references":["/references/2"]},{"style":0,"text":"Chiang, 2007","origin":{"pointer":"/sections/8/paragraphs/0","offset":15,"length":12},"authors":[{"last":"Chiang"}],"year":"2007","references":["/references/3"]},{"style":0,"text":"Chiang, 2007","origin":{"pointer":"/sections/8/paragraphs/2","offset":146,"length":12},"authors":[{"last":"Chiang"}],"year":"2007","references":["/references/3"]},{"style":0,"text":"Palmer et al., 2005","origin":{"pointer":"/sections/9/paragraphs/0","offset":778,"length":19},"authors":[{"last":"Palmer"},{"last":"al."}],"year":"2005","references":["/references/27"]},{"style":0,"text":"Xue and Palmer, 2009","origin":{"pointer":"/sections/9/paragraphs/0","offset":799,"length":20},"authors":[{"last":"Xue"},{"last":"Palmer"}],"year":"2009","references":["/references/41"]},{"style":0,"text":"Tseng et al., 2005","origin":{"pointer":"/sections/10/paragraphs/0","offset":377,"length":18},"authors":[{"last":"Tseng"},{"last":"al."}],"year":"2005","references":["/references/32"]},{"style":0,"text":"Och and Ney, 2000","origin":{"pointer":"/sections/10/paragraphs/0","offset":465,"length":17},"authors":[{"last":"Och"},{"last":"Ney"}],"year":"2000","references":["/references/26"]},{"style":0,"text":"Koehn et al., 2003","origin":{"pointer":"/sections/10/paragraphs/0","offset":580,"length":18},"authors":[{"last":"Koehn"},{"last":"al."}],"year":"2003","references":["/references/15"]},{"style":0,"text":"Chen and Goodman, 1996","origin":{"pointer":"/sections/10/paragraphs/0","offset":800,"length":22},"authors":[{"last":"Chen"},{"last":"Goodman"}],"year":"1996","references":["/references/0"]},{"style":0,"text":"Dyer et al., 2010","origin":{"pointer":"/sections/10/paragraphs/0","offset":854,"length":17},"authors":[{"last":"Dyer"},{"last":"al."}],"year":"2010","references":["/references/6"]},{"style":0,"text":"Eidelman et al., 2013","origin":{"pointer":"/sections/10/paragraphs/0","offset":889,"length":21},"authors":[{"last":"Eidelman"},{"last":"al."}],"year":"2013","references":["/references/7"]},{"style":0,"text":"Chiang et al., 2008","origin":{"pointer":"/sections/10/paragraphs/0","offset":948,"length":19},"authors":[{"last":"Chiang"},{"last":"al."}],"year":"2008","references":["/references/2"]},{"style":0,"text":"Papineni et al., 2002","origin":{"pointer":"/sections/10/paragraphs/2","offset":13,"length":21},"authors":[{"last":"Papineni"},{"last":"al."}],"year":"2002","references":["/references/28"]},{"style":0,"text":"Petrov and Klein, 2007","origin":{"pointer":"/sections/10/paragraphs/3","offset":146,"length":22},"authors":[{"last":"Petrov"},{"last":"Klein"}],"year":"2007","references":["/references/29"]},{"style":0,"text":"Xue et al., 2005","origin":{"pointer":"/sections/10/paragraphs/3","offset":208,"length":16},"authors":[{"last":"Xue"},{"last":"al."}],"year":"2005","references":["/references/42"]},{"style":0,"text":"Li et al., 2010","origin":{"pointer":"/sections/10/paragraphs/3","offset":287,"length":15},"authors":[{"last":"Li"},{"last":"al."}],"year":"2010","references":["/references/20"]},{"style":0,"text":"Xue and Palmer, 2009","origin":{"pointer":"/sections/10/paragraphs/3","offset":342,"length":20},"authors":[{"last":"Xue"},{"last":"Palmer"}],"year":"2009","references":["/references/41"]},{"style":0,"text":"Marton and Resnik (2008)","origin":{"pointer":"/sections/10/paragraphs/4","offset":300,"length":24},"authors":[{"last":"Marton"},{"last":"Resnik"}],"year":"2008","references":["/references/23"]},{"style":0,"text":"Tsuruoka et al., 2009","origin":{"pointer":"/sections/10/paragraphs/11","offset":410,"length":21},"authors":[{"last":"Tsuruoka"},{"last":"al."}],"year":"2009","references":["/references/33"]},{"style":0,"text":"Koehn, 2004","origin":{"pointer":"/sections/10/paragraphs/26","offset":185,"length":11},"authors":[{"last":"Koehn"}],"year":"2004","references":["/references/16"]},{"style":0,"text":"Collins et al., 2005","origin":{"pointer":"/sections/12/paragraphs/0","offset":180,"length":20},"authors":[{"last":"Collins"},{"last":"al."}],"year":"2005","references":["/references/5"]},{"style":0,"text":"Wang et al., 2007","origin":{"pointer":"/sections/12/paragraphs/0","offset":202,"length":17},"authors":[{"last":"Wang"},{"last":"al."}],"year":"2007","references":["/references/35"]},{"style":0,"text":"Xu et al., 2009","origin":{"pointer":"/sections/12/paragraphs/0","offset":221,"length":15},"authors":[{"last":"Xu"},{"last":"al."}],"year":"2009","references":["/references/40"]},{"style":0,"text":"Lee et al., 2010","origin":{"pointer":"/sections/12/paragraphs/0","offset":238,"length":16},"authors":[{"last":"Lee"},{"last":"al."}],"year":"2010","references":["/references/17"]},{"style":0,"text":"Xia and McCord, 2004","origin":{"pointer":"/sections/12/paragraphs/0","offset":282,"length":20},"authors":[{"last":"Xia"},{"last":"McCord"}],"year":"2004","references":["/references/38"]},{"style":0,"text":"Genzel, 2010","origin":{"pointer":"/sections/12/paragraphs/0","offset":304,"length":12},"authors":[{"last":"Genzel"}],"year":"2010","references":["/references/11"]},{"style":0,"text":"Visweswariah et al., 2010","origin":{"pointer":"/sections/12/paragraphs/0","offset":318,"length":25},"authors":[{"last":"Visweswariah"},{"last":"al."}],"year":"2010","references":["/references/34"]},{"style":0,"text":"Khalilov and Sima’an, 2011","origin":{"pointer":"/sections/12/paragraphs/0","offset":345,"length":26},"authors":[{"last":"Khalilov"},{"last":"Sima’an"}],"year":"2011","references":["/references/14"]},{"style":0,"text":"Lerner and Petrov, 2013","origin":{"pointer":"/sections/12/paragraphs/0","offset":373,"length":23},"authors":[{"last":"Lerner"},{"last":"Petrov"}],"year":"2013","references":["/references/18"]},{"style":0,"text":"Li et al. (2007)","origin":{"pointer":"/sections/12/paragraphs/0","offset":423,"length":16},"authors":[{"last":"Li"},{"last":"al."}],"year":"2007","references":["/references/19"]},{"style":0,"text":"Yang et al. (2012)","origin":{"pointer":"/sections/12/paragraphs/0","offset":558,"length":18},"authors":[{"last":"Yang"},{"last":"al."}],"year":"2012","references":["/references/43"]},{"style":0,"text":"Marton and Resnik (2008)","origin":{"pointer":"/sections/12/paragraphs/1","offset":147,"length":24},"authors":[{"last":"Marton"},{"last":"Resnik"}],"year":"2008","references":["/references/23"]},{"style":0,"text":"Chiang, 2010","origin":{"pointer":"/sections/12/paragraphs/1","offset":454,"length":12},"authors":[{"last":"Chiang"}],"year":"2010","references":["/references/4"]},{"style":0,"text":"Sima’an, 2011","origin":{"pointer":"/sections/12/paragraphs/1","offset":488,"length":13},"authors":[{"last":"Sima’an"}],"year":"2011","references":[]},{"style":0,"text":"Ge (2010)","origin":{"pointer":"/sections/12/paragraphs/1","offset":597,"length":9},"authors":[{"last":"Ge"}],"year":"2010","references":["/references/10"]},{"style":0,"text":"Gao et al. (2011)","origin":{"pointer":"/sections/12/paragraphs/1","offset":716,"length":17},"authors":[{"last":"Gao"},{"last":"al."}],"year":"2011","references":["/references/9"]},{"style":0,"text":"Huang et al. (2013)","origin":{"pointer":"/sections/12/paragraphs/1","offset":822,"length":19},"authors":[{"last":"Huang"},{"last":"al."}],"year":"2013","references":["/references/12"]},{"style":0,"text":"Wu et al. (2011)","origin":{"pointer":"/sections/12/paragraphs/3","offset":133,"length":16},"authors":[{"last":"Wu"},{"last":"al."}],"year":"2011","references":["/references/37"]},{"style":0,"text":"Liu and Gildea (2010)","origin":{"pointer":"/sections/12/paragraphs/3","offset":254,"length":21},"authors":[{"last":"Liu"},{"last":"Gildea"}],"year":"2010","references":["/references/22"]},{"style":0,"text":"Xiong et al. (2012)","origin":{"pointer":"/sections/12/paragraphs/3","offset":377,"length":19},"authors":[{"last":"Xiong"},{"last":"al."}],"year":"2012","references":["/references/39"]},{"style":0,"text":"Li et al. (2013)","origin":{"pointer":"/sections/12/paragraphs/3","offset":401,"length":16},"authors":[{"last":"Li"},{"last":"al."}],"year":"2013","references":["/references/21"]},{"style":0,"text":"Zhai et al. (2013)","origin":{"pointer":"/sections/12/paragraphs/3","offset":565,"length":18},"authors":[{"last":"Zhai"},{"last":"al."}],"year":"2013","references":["/references/44"]},{"style":0,"text":"Wu and Fung (2009)","origin":{"pointer":"/sections/12/paragraphs/3","offset":685,"length":18},"authors":[{"last":"Wu"},{"last":"Fung"}],"year":"2009","references":["/references/36"]},{"style":0,"text":"Setiawan et al., 2009","origin":{"pointer":"/sections/12/paragraphs/4","offset":140,"length":21},"authors":[{"last":"Setiawan"},{"last":"al."}],"year":"2009","references":["/references/30"]},{"style":0,"text":"Huck et al., 2013","origin":{"pointer":"/sections/12/paragraphs/4","offset":163,"length":17},"authors":[{"last":"Huck"},{"last":"al."}],"year":"2013","references":["/references/13"]},{"style":0,"text":"Nguyen and Vogel, 2013","origin":{"pointer":"/sections/12/paragraphs/4","offset":182,"length":22},"authors":[{"last":"Nguyen"},{"last":"Vogel"}],"year":"2013","references":["/references/25"]},{"style":0,"text":"Setiawan et al. (2013)","origin":{"pointer":"/sections/12/paragraphs/6","offset":294,"length":22},"authors":[{"last":"Setiawan"},{"last":"al."}],"year":"2013","references":["/references/31"]},{"style":0,"text":"Cherry, 2013","origin":{"pointer":"/sections/12/paragraphs/6","offset":548,"length":12},"authors":[{"last":"Cherry"}],"year":"2013","references":["/references/1"]},{"style":0,"text":"Feng et al., 2013","origin":{"pointer":"/sections/12/paragraphs/6","offset":562,"length":17},"authors":[{"last":"Feng"},{"last":"al."}],"year":"2013","references":["/references/8"]}]}
