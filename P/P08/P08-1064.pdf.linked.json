{"sections":[{"title":"","paragraphs":["Proceedings of ACL-08: HLT, pages 559–567, Columbus, Ohio, USA, June 2008. c⃝2008 Association for Computational Linguistics"]},{"title":"A Tree Sequence Alignment-based Tree-to-Tree Translation Model   Min Zhang 1 Hongfei Jiang2 Aiti Aw1 Haizhou Li 1 Chew Lim Tan3 and Sheng Li 2","paragraphs":["1","Institute for Infocomm Research 2","Harbin Institute of Technology 3","National University of Singapore mzhang@i2r.a-star.edu.sg hfjiang@mtlab.hit.edu.cn tancl@comp.nus.edu.sg aaiti@i2r.a-star.edu.sg lisheng@hit.edu.cn hli@i2r.a-star.edu.sg  "]},{"title":"Abstract","paragraphs":["This paper presents a translation model that is based on tree sequence alignment, where a tree sequence refers to a single sequence of sub-trees that covers a phrase. The model leverages on the strengths of both phrase-based and linguistically syntax-based method. It automatically learns aligned tree sequence pairs with mapping probabilities from word-aligned bi-parsed parallel texts. Compared with previous models, it not only captures non-syntactic phrases and discontinuous phrases with linguistically structured features, but also supports multi-level structure reordering of tree typology with larger span. This gives our model stronger expressive power than other reported models. Experimental results on the NIST MT-2005 Chinese-English translation task show that our method statistically significantly outperforms the baseline systems."]},{"title":"1 Introduction","paragraphs":["Phrase-based modeling method (Koehn et al., 2003; Och and Ney, 2004a) is a simple, but powerful mechanism to machine translation since it can model local reorderings and translations of multi-word expressions well. However, it cannot handle long-distance reorderings properly and does not exploit discontinuous phrases and linguistically syntactic structure features (Quirk and Menezes, 2006). Recently, many syntax-based models have been proposed to address the above deficiencies (Wu, 1997; Chiang, 2005; Eisner, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Cowan et al., 2006; Zhang et al., 2007; Bod, 2007; Yamada and Knight, 2001; Liu et al., 2006; Liu et al., 2007; Gildea, 2003; Poutsma, 2000; Hearne and Way, 2003). Although good progress has been reported, the fundamental issues in applying linguistic syntax to SMT, such as non-isomorphic tree alignment, structure reordering and non-syntactic phrase modeling, are still worth well studying.","In this paper, we propose a tree-to-tree translation model that is based on tree sequence alignment. It is designed to combine the strengths of phrase-based and syntax-based methods. The proposed model adopts tree sequence 1","as the basic translation unit and utilizes tree sequence alignments to model the translation process. Therefore, it not only describes non-syntactic phrases with syntactic structure information, but also supports multi-level tree structure reordering in larger span. These give our model much more expressive power and flexibility than those previous models. Experiment results on the NIST MT-2005 Chinese-English translation task show that our method significantly outperforms Moses (Koehn et al., 2007), a state-of-the-art phrase-based SMT system, and other linguistically syntax-based methods, such as SCFG-based and STSG-based methods (Zhang et al., 2007). In addition, our study further demonstrates that 1) structure reordering rules in our model are very useful for performance improvement while discontinuous phrase rules have less contribution and 2) tree sequence rules are able to model non-syntactic phrases with syntactic structure information, and thus contribute much to the performance improvement, but those rules consisting of more than three sub-trees have almost no contribution.","The rest of this paper is organized as follows: Section 2 reviews previous work. Section 3 elabo-  1 A tree sequence refers to an ordered sub-tree sequence that covers a phrase or a consecutive tree fragment in a parse tree. It is the same as the concept “forest” used in Liu et al (2007). 559 rates the modelling process while Sections 4 and 5 discuss the training and decoding algorithms. The experimental results are reported in Section 6. Finally, we conclude our work in Section 7."]},{"title":"2 Related Work","paragraphs":["Many techniques on linguistically syntax-based SMT have been proposed in literature. Yamada and Knight (2001) use noisy-channel model to transfer a target parse tree into a source sentence. Eisner (2003) studies how to learn non-isomorphic tree-to-tree/string mappings using a STSG. Ding and Palmer (2005) propose a syntax-based translation model based on a probabilistic synchronous dependency insertion grammar. Quirk et al. (2005) propose a dependency treelet-based translation model. Cowan et al. (2006) propose a feature-based discriminative model for target language syntactic structures prediction, given a source parse tree. Huang et al. (2006) study a TSG-based tree-to-string alignment model. Liu et al. (2006) propose a tree-to-string model. Zhang et al. (2007b) present a STSG-based tree-to-tree translation model. Bod (2007) reports that the unsupervised STSG-based translation model performs much better than the supervised one. The motiva-tion behind all these work is to exploit linguistically syntactic structure features to model the translation process. However, most of them fail to utilize non-syntactic phrases well that are proven useful in the phrase-based methods (Koehn et al., 2003).","The formally syntax-based model for SMT was first advocated by Wu (1997). Xiong et al. (2006) propose a MaxEnt-based reordering model for BTG (Wu, 1997) while Setiawan et al. (2007) propose a function word-based reordering model for BTG. Chiang (2005)’s hierarchal phrase-based model achieves significant performance improvement. However, no further significant improvement is achieved when the model is made sensitive to syntactic structures by adding a constituent feature (Chiang, 2005).","In the last two years, many research efforts were devoted to integrating the strengths of phrase-based and syntax-based methods. In the following, we review four representatives of them.","1) Hassan et al. (2007) integrate supertags (a kind of lexicalized syntactic description) into the target side of translation model and language model under the phrase-based translation framework, resulting in good performance improvement. However, neither source side syntactic knowledge nor reordering model is further explored.","2) Galley et al. (2006) handle non-syntactic phrasal translations by traversing the tree upwards until a node that subsumes the phrase is reached. This solution requires larger applicability contexts (Marcu et al., 2006). However, phrases are utilized independently in the phrase-based method without depending on any contexts.","3) Addressing the issues in Galley et al. (2006), Marcu et al. (2006) create an xRS rule headed by a pseudo, non-syntactic non-terminal symbol that subsumes the phrase and its corresponding multiheaded syntactic structure; and one sibling xRS rule that explains how the pseudo symbol can be combined with other genuine non-terminals for acquiring the genuine parse trees. The name of the pseudo non-terminal is designed to reflect the full realization of the corresponding rule. The problem in this method is that it neglects alignment consistency in creating sibling rules and the naming mechanism faces challenges in describing more complicated phenomena (Liu et al., 2007).","4) Liu et al. (2006) treat all bilingual phrases as lexicalized tree-to-string rules, including those non-syntactic phrases in training corpus. Although the solution shows effective empirically, it only utilizes the source side syntactic phrases of the input parse tree during decoding. Furthermore, the translation probabilities of the bilingual phrases and other tree-to-string rules are not compatible since they are estimated independently, thus having different parameter spaces. To address the above problems, Liu et al. (2007) propose to use forest-to-string rules to enhance the expressive power of their tree-to-string model. As is inherent in a tree-to-string framework, Liu et al.’s method defines a kind of auxiliary rules to integrate forest-to-string rules into tree-to-string models. One problem of this method is that the auxiliary rules are not described by probabilities since they are constructed during decoding, rather than learned from the training corpus. So, to balance the usage of different kinds of rules, they use a very simple feature counting the number of auxiliary rules used in a derivation for penalizing the use of forest-to-string and auxiliary rules.","In this paper, an alternative solution is presented to combine the strengths of phrase-based and syn-560","1()I Te","1()J Tf A","","Figure 1: A word-aligned parse tree pairs of a Chinese sentence and its English translation "," Figure 2: Two Examples of tree sequences ","  Figure 3: Two examples of translation rules tax-based methods. Unlike previous work, our solution neither requires larger applicability contexts (Galley et al., 2006), nor depends on pseudo nodes (Marcu et al., 2006) or auxiliary rules (Liu et al., 2007). We go beyond the single sub-tree mapping model to propose a tree sequence alignment-based translation model. To the best of our knowledge, this is the first attempt to empirically explore the tree sequence alignment based model in SMT."]},{"title":"3 Tree Sequence Alignment Model 3.1 Tree Sequence Translation Rule","paragraphs":["The leaf nodes of a sub-tree in a tree sequence can be either non-terminal symbols (grammar tags) or terminal symbols (lexical words). Given a pair of source and target parse trees 1"]},{"title":"()","paragraphs":["J"]},{"title":"Tf","paragraphs":["and 1"]},{"title":"()","paragraphs":["I"]},{"title":"Te","paragraphs":["in Fig. 1, Fig. 2 illustrates two examples of tree sequences derived from the two parse trees. A tree sequence translation rule"]},{"title":"r","paragraphs":["is a pair of aligned tree sequences"]},{"title":"r","paragraphs":["=< 2 1"]},{"title":"()","paragraphs":["j j"]},{"title":"TS f","paragraphs":[", 2 1"]},{"title":"()","paragraphs":["i i"]},{"title":"TS e","paragraphs":[", A% >, where:","z 2 1"]},{"title":"()","paragraphs":["j j"]},{"title":"TS f","paragraphs":["is a source tree sequence, covering the span [ 12"]},{"title":",j j","paragraphs":["] in 1"]},{"title":"()","paragraphs":["J"]},{"title":"Tf","paragraphs":[", and","z 2 1"]},{"title":"()","paragraphs":["i i"]},{"title":"TS e","paragraphs":["is a target one, covering the span [ 12"]},{"title":",ii","paragraphs":["] in 1"]},{"title":"()","paragraphs":["I"]},{"title":"Te","paragraphs":[", and","z A% are the alignments between leaf nodes of","two tree sequences, satisfying the following","condition: 121 2(, ) :ij Ai i i j j j∀∈ ≤≤↔≤≤% . Fig. 3 shows two rules extracted from the tree pair shown in Fig. 1, where r1 is a tree-to-tree rule and r2 is a tree sequence-to-tree sequence rule. Obviously, tree sequence rules are more powerful than phrases or tree rules as they can capture all phrases (including both syntactic and non-syntactic phrases) with syntactic structure information and allow any tree node operations in a longer span. We expect that these properties can well address the issues of non-isomorphic structure alignments, structure reordering, non-syntactic phrases and discontinuous phrases translations. 3.2"]},{"title":"Tree Sequence Translation Model","paragraphs":["Given the source and target sentences 1 J"]},{"title":"f","paragraphs":["and 1I"]},{"title":"e","paragraphs":["and their parse trees 1"]},{"title":"()","paragraphs":["J"]},{"title":"Tf","paragraphs":["and 1"]},{"title":"()","paragraphs":["I"]},{"title":"Te","paragraphs":[", the tree sequence-to-tree sequence translation model is formulated as: 11 11","11 1 1 1 1 (),() 11 (),() 111 11 11 (| ) (,(),( )| ) ((( )| ) (( )| ( ), ) ( | ( ), ( ), ))    JI JI I JIIJJ Tf Te JJ Tf Te IJJ II JJ rr r r r Pe f PeTe Tf f PTf f PTe Tf f Pe Te Tf f = ="]},{"title":"⋅ ⋅ ∑ ∑","paragraphs":["(1) In our implementation, we have: 561 1) 11"]},{"title":"(( )| ) 1","paragraphs":["JJ r"]},{"title":"PTf f ≡","paragraphs":["since we only use the best source and target parse tree pairs in training.","2) 11 11"]},{"title":"(|(),( ), )1","paragraphs":["II JJ r"]},{"title":"Pe Te Tf f ≡","paragraphs":["since we just output the leaf nodes of 1"]},{"title":"()","paragraphs":["I"]},{"title":"Te","paragraphs":["to generate 1I"]},{"title":"e","paragraphs":["regardless of source side information. Since 1"]},{"title":"()","paragraphs":["J"]},{"title":"Tf","paragraphs":["contains the information of 1 J"]},{"title":"f","paragraphs":[", now we have: 11 1 1 1 11"]},{"title":"(| ) (()|( ), ) ( ( ) | ( ))","paragraphs":["IJ I J J IJrr r"]},{"title":"Pe f PTe Tf f PTe Tf = = ","paragraphs":["(2)","By Eq. (2), translation becomes a tree structure mapping issue. We model it using our tree sequence-based translation rules. Given the source parse tree 1"]},{"title":"()","paragraphs":["J"]},{"title":"Tf","paragraphs":[", there are multiple derivations that could lead to the same target tree 1"]},{"title":"()","paragraphs":["I"]},{"title":"Te","paragraphs":[", the mapping probability 11"]},{"title":"(( )| ( ))","paragraphs":["IJ r"]},{"title":"PTe Tf","paragraphs":["is obtained by summing over the probabilities of all derivations. The probability of each derivation"]},{"title":"θ","paragraphs":["is given as the product of the probabilities of all the rules"]},{"title":"()","paragraphs":["i"]},{"title":"p r","paragraphs":["used in the derivation (here we assume that a rule is applied independently in a derivation). 22 11 11 1 1"]},{"title":"(| ) (()|( )) = ( : ( ), ( ), )","paragraphs":["i IJ I J","ij ii j r rr"]},{"title":"Pe f PTe Tf p rTSeTSf A","paragraphs":["θ θ∈"]},{"title":"= <>∑ ∏ %","paragraphs":["(3)","Eq. (3) formulates the tree sequence alignment-based translation model. Figs. 1 and 3 show how the proposed model works. First, the source sentence is parsed into a source parse tree. Next, the source parse tree is detached into two source tree sequences (the left hand side of rules in Fig. 3). Then the two rules in Fig. 3 are used to map the two source tree sequences to two target tree sequences, which are then combined to generate a target parse tree. Finally, a target translation is yielded from the target tree.","Our model is implemented under log-linear framework (Och and Ney, 2002). We use seven basic features that are analogous to the commonly used features in phrase-based systems (Koehn, 2004): 1) bidirectional rule mapping probabilities; 2) bidirectional lexical rule translation probabilities; 3) the target language model; 4) the number of rules used and 5) the number of target words. In addition, we define two new features: 1) the number of lexical words in a rule to control the model’s preference for lexicalized rules over un-lexicalized rules and 2) the average tree depth in a rule to balance the usage of hierarchical rules and flat rules. Note that we do not distinguish between larger (taller) and shorter source side tree sequences, i.e. we let these rules compete directly with each other."]},{"title":"4 Rule Extraction","paragraphs":["Rules are extracted from word-aligned, bi-parsed sentence pairs 11"]},{"title":"(),(),","paragraphs":["JI"]},{"title":"Tf Te A<>","paragraphs":[", which are","classified into two categories:","z initial rule, if all leaf nodes of the rule are terminals (i.e. lexical word), and","z abstract rule, otherwise, i.e. at least one leaf node is a non-terminal (POS or phrase tag).","Given an initial rule 22","11(),(),ji","jiTS f TS e A<>% ,","its sub initial rule is defined as a triple","44","33"]},{"title":"(̂),(),","paragraphs":["ji ji"]},{"title":"TS f TS e A<>","paragraphs":["if and only if:","z 44 33"]},{"title":"(̂),(),","paragraphs":["ji ji"]},{"title":"TS f TS e A<>","paragraphs":["is an initial rule.","z 343 4"]},{"title":"(, ) :ij Ai i i j j j∀∈ ≤≤↔≤≤%","paragraphs":[", i.e."]},{"title":"ÂA⊆ %","paragraphs":["z 4 3"]},{"title":"()","paragraphs":["j j"]},{"title":"TS f","paragraphs":["is a sub-graph of 2 1"]},{"title":"()","paragraphs":["j j"]},{"title":"TS f","paragraphs":["while 4 3"]},{"title":"()","paragraphs":["i i"]},{"title":"TS e","paragraphs":["is a sub-graph of 2","1()i","iTS e . Rules are extracted in two steps: 1) Extracting initial rules first. 2) Extracting abstract rules from extracted ini-","tial rules with the help of sub initial rules. It is straightforward to extract initial rules. We","first generate all fully lexicalized source and target","tree sequences using a dynamic programming algo-","rithm and then iterate over all generated source and","target tree sequence pairs 22","11"]},{"title":"(),()","paragraphs":["ji ji"]},{"title":"TS f TS e<>","paragraphs":[". If the condition “"]},{"title":"(, )ij∀","paragraphs":["121 2"]},{"title":":A iii jjj∈≤≤↔≤≤","paragraphs":["”","is satisfied, the triple 22 11"]},{"title":"(),(),","paragraphs":["ji ji"]},{"title":"TS f TS e A<>%","paragraphs":["is an initial rule, where"]},{"title":"A%","paragraphs":["are alignments between","leaf nodes of 2 1"]},{"title":"()","paragraphs":["j j"]},{"title":"TS f","paragraphs":["and 2 1()i iTS e . We then derive abstract rules from initial rules by removing one or more of its sub initial rules. The abstract rule extraction algorithm presented next is implemented using dynamic programming. Due to space limitation, we skip the details here. In order to control the number of rules, we set three constraints for both finally extracted initial and abstract rules:","1) The depth of a tree in a rule is not greater 562 than h .","2) The number of non-terminals as leaf nodes is not greater than"]},{"title":"c","paragraphs":[".","3) The tree number in a rule is not greater than d.","In addition, we limit initial rules to have at most seven lexical words as leaf nodes on either side. However, in order to extract long-distance reordering rules, we also generate those initial rules with more than seven lexical words for abstract rules extraction only (not used in decoding). This makes our abstract rules more powerful in handling global structure reordering. Moreover, by configur-ing these parameters we can implement other translation models easily: 1) STSG-based model when"]},{"title":"1d =","paragraphs":["; 2) SCFG-based model when"]},{"title":"1d =","paragraphs":["and 2h = ; 3) phrase-based translation model only (no reordering model) when"]},{"title":"0c =","paragraphs":["and 1h = .  Algorithm 1: abstract rules extraction Input: initial rule set ini"]},{"title":"r Output:","paragraphs":["abstract rule set abs"]},{"title":"r","paragraphs":["1: for each iini"]},{"title":"rr∈","paragraphs":[", do 2: put all sub initial rules of i"]},{"title":"r","paragraphs":["into a set subini i"]},{"title":"r","paragraphs":["3: for each subset subini i"]},{"title":"rΘ⊆ do","paragraphs":["4: if there are spans overlapping between any two rules in the subset"]},{"title":"Θ then","paragraphs":["5: continue //go to line 3 6: end if 7: generate an abstract rule by removing","the portions covered by"]},{"title":"Θ","paragraphs":["from i"]},{"title":"r","paragraphs":["and co-indexing the pairs of non-terminals that rooting the removed source and target parts","8: add them into the abstract rule set abs"]},{"title":"r","paragraphs":["9: end do 10: end do "]},{"title":"5 Decoding","paragraphs":["Given 1"]},{"title":"()","paragraphs":["J"]},{"title":"Tf","paragraphs":[", the decoder is to find the best derivation"]},{"title":"θ","paragraphs":["that generates < 1"]},{"title":"()","paragraphs":["J"]},{"title":"Tf","paragraphs":[", 1"]},{"title":"()","paragraphs":["I"]},{"title":"Te","paragraphs":[">. 1 1 11 ,"]},{"title":"̂arg max ( ( ) | ( )) arg max ( )","paragraphs":["I I i IJ e i e r r"]},{"title":"ePTeTf pr","paragraphs":["θ θ∈"]},{"title":"= ≈ ∏ ","paragraphs":["(4) Algorithm 2: Tree Sequence-based Decoder Input: 1"]},{"title":"()","paragraphs":["J"]},{"title":"Tf Output:","paragraphs":["1"]},{"title":"()","paragraphs":["I"]},{"title":"Te Data structures:","paragraphs":["12"]},{"title":"[, ]hj j","paragraphs":["To store translations to a span 12"]},{"title":"[, ]j j","paragraphs":["1: for"]},{"title":"s","paragraphs":["= 0 to"]},{"title":"J","paragraphs":["-1 do // s: span length 2: for 1"]},{"title":"j","paragraphs":["= 1 to"]},{"title":"J","paragraphs":["-"]},{"title":"s","paragraphs":[", 2"]},{"title":"j","paragraphs":["= 1"]},{"title":"j","paragraphs":["+"]},{"title":"s do","paragraphs":["3: for each rule"]},{"title":"r","paragraphs":["spanning 12"]},{"title":"[, ]j j do","paragraphs":["4: if"]},{"title":"r","paragraphs":["is an initial rule then 5: insert"]},{"title":"r","paragraphs":["into 12"]},{"title":"[, ]hj j","paragraphs":["6: else 7: generate new translations from"]},{"title":"r","paragraphs":["by replacing non-terminal leaf nodes of"]},{"title":"r","paragraphs":["with their corresponding spans’ translations that are already translated in previous steps","8: insert them into 12"]},{"title":"[, ]hj j","paragraphs":["9: end if 10: end for 11: end for 12: end for 13: output the hypothesis with the highest score","in"]},{"title":"[1, ]hJ","paragraphs":["as the final best translation ","The decoder is a span-based beam search to-gether with a function for mapping the source derivations to the target ones. Algorithm 2 illustrates the decoding algorithm. It translates each span iteratively from small one to large one (lines 1-2). This strategy can guarantee that when translating the current span, all spans smaller than the current one have already been translated before if they are translatable (line 7). When translating a span, if the usable rule is an initial rule, then the tree sequence on the target side of the rule is a candidate translation (lines 4-5). Otherwise, we replace the non-terminal leaf nodes of the current abstract rule with their corresponding spans’ translations that are already translated in previous steps (line 7). To speed up the decoder, we use several thresholds to limit search beams for each span:","1)"]},{"title":"α","paragraphs":[", the maximal number of rules used 2)"]},{"title":"β","paragraphs":[", the minimal log probability of rules 3)"]},{"title":"γ","paragraphs":[", the maximal number of translations yield","It is worth noting that the decoder does not force a complete target parse tree to be generated. If no rules can be used to generate a complete target parse tree, the decoder just outputs whatever have 563 been translated so far monotonically as one hypothesis."]},{"title":"6 Experiments 6.1 Experimental Settings","paragraphs":["We conducted Chinese-to-English translation experiments. We trained the translation model on the FBIS corpus (7.2M+9.2M words) and trained a 4-gram language model on the Xinhua portion of the English Gigaword corpus (181M words) using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing. We used sentences with less than 50 characters from the NIST MT-2002 test set as our development set and the NIST MT-2005 test set as our test set. We used the Stanford parser (Klein and Manning, 2003) to parse bilingual sentences on the training set and Chinese sentences on the development and test sets. The evaluation metric is case-sensitive BLEU-4 (Papineni et al., 2002). We used GIZA++ (Och and Ney, 2004) and the heuristics “grow-diag-final” to generate m-to-n word alignments. For the MER training (Och, 2003), we modified Koehn’s MER trainer (Koehn, 2004) for our tree sequence-based system. For significance test, we used Zhang et al’s implementation (Zhang et al, 2004).","We set three baseline systems: Moses (Koehn et al., 2007), and SCFG-based and STSG-based tree-to-tree translation models (Zhang et al., 2007). For Moses, we used its default settings. For the SCFG/STSG and our proposed model, we used the same settings except for the parameters"]},{"title":"d","paragraphs":["and"]},{"title":"h","paragraphs":["("]},{"title":"1d =","paragraphs":["and 2h = for the SCFG;"]},{"title":"1d =","paragraphs":["and 6h = for the STSG;"]},{"title":"4d =","paragraphs":["and 6h = for our model). We optimized these parameters on the training and development sets:"]},{"title":"c","paragraphs":["=3,"]},{"title":"α","paragraphs":["=20,"]},{"title":"β","paragraphs":["=-100 and"]},{"title":"γ","paragraphs":["=100. 6.2"]},{"title":"Experimental Results","paragraphs":["We carried out a number of experiments to examine the proposed tree sequence alignment-based translation model. In this subsection, we first report the rule distributions and compare our model with the three baseline systems. Then we study the model’s expressive ability by comparing the contributions made by different kinds of rules, including strict tree sequence rules, non-syntactic phrase rules, structure reordering rules and discontinuous phrase rules2",". Finally, we investigate the impact of maximal sub-tree number and sub-tree depth in our model. All of the following discussions are held on the training and test data.   Rule","Initial Rules Abstract Rules","L P U Total BP 322,965 0 0 322,965 TR 443,010 144,459 24,871 612,340 TSR 225,570 103,932 714 330,216  Table 1: # of rules used in the testing ("]},{"title":"4d =","paragraphs":[", h = 6) (BP: bilingual phrase (used in Moses), TR: tree rule (only 1 tree), TSR: tree sequence rule (> 1 tree), L: fully lexicalized, P: partially lexicalized, U: unlexicalized) ","Table 1 reports the statistics of rules used in the experiments. It shows that:","1) We verify that the BPs are fully covered by the initial rules (i.e. lexicalized rules), in which the lexicalized TSRs model all non-syntactic phrase pairs with rich syntactic information. In addition, we find that the number of initial rules is greater than that of bilingual phrases. This is because one bilingual phrase can be covered by more than one initial rule which having different sub-tree structures.","2) Abstract rules generalize initial rules to unseen data and with structure reordering ability. The number of the abstract rule is far less than that of the initial rules. This is because leaf nodes of an abstract rule can be non-terminals that can represent any sub-trees using the non-terminals as roots.","Fig. 4 compares the performance of different models. It illustrates that:","1) Our tree sequence-based model significantly outperforms (p < 0.01) previous phrase-based and linguistically syntax-based methods. This empirically verifies the effect of the proposed method.","2) Both our method and STSG outperform Moses significantly. Our method also clearly outperforms STSG. These results suggest that:","z The linguistically motivated structure features","are very useful for SMT, which can be cap-  2 To be precise, we examine the contributions of strict tree sequence rules and single tree rules separately in this section. Therefore, unless specified, the term “tree sequence rules” used in this section only refers to the strict tree sequence rules, which must contain at least two sub-trees on the source side. 564","tured by the two syntax-based models through","tree node operations.","z Our model is much more effective in utilizing","linguistic structures than STSG since it uses","tree sequence as basic translation unit. This","allows our model not only to handle structure","reordering by tree node operations in a larger","span, but also to capture non-syntactic phras-","es, which circumvents previous syntactic","constraints, thus giving our model more ex-","pressive power.","3) The linguistically motivated SCFG shows much lower performance. This is largely because SCFG only allows sibling nodes reordering and fails to utilize both non-syntactic phrases and those syntactic phrases that cannot be covered by a single CFG rule. It thereby suggests that SCFG is less effective in modelling parse tree structure transfer between Chinese and English when using Penn Treebank style linguistic grammar and under word-alignment constraints. However, formal SCFG show much better performance in the formally syntax-based translation framework (Chiang, 2005). This is because the formal syntax is learned from phrases directly without relying on any linguistic theory (Chiang, 2005). As a result, it is more robust to the issue of non-syntactic phrase usage and non-isomorphic structure alignment. 24.71 26.07 23.86 22.72 21.522.523.524.525.526.5 SCFG Moses STSG Ours BL E U (% )  Figure 4: Performance comparison of different methods  Rule Type","TR (STSG)","TR +TSR_L","TR+TSR_L +TSR_P TR +TSR BLEU(%) 24.71 25.72 25.93 26.07  Table 2: Contributions of TSRs (see Table 1 for the definitions of the abbreviations used in this table)  Table 2 measures the contributions of different","kinds of tree sequence rules. It suggests that: 1) All the three kinds of TSRs contribute to the","performance improvement and their combination further improves the performance. It suggests that they are complementary to each other since the lexicalized TSRs are used to model non-syntactic phrases while the other two kinds of TSRs can generalize the lexicalized rules to unseen phrases.","2) The lexicalized TSRs make the major contribution since they can capture non-syntactic phrases with syntactic structure features."," Rule Type BLEU (%) TR+TSR 26.07 (TR+TSR) w/o SRR 24.62 (TR+TSR) w/o DPR 25.78  Table 3: Effect of Structure Reordering Rules (SRR: refers to the structure reordering rules that have at least two non-terminal leaf nodes with inverted order in the source and target sides, which are usually not captured by phrase-based models. Note that the reordering between lexical words and non-terminal leaf nodes is not considered here) and Discontinuous Phrase Rules (DPR: refers to these rules having at least one non-terminal leaf node between two lexicalized leaf nodes) in our tree sequence-based model ("]},{"title":"4d =","paragraphs":["and 6h = ) ","Rule Type # of rules # of rules overlapped","(Intersection) SRR 68,217 18,379 (26.9%) DPR 57,244 18,379 (32.1%)  Table 4: numbers of SRR and DPR rules ","Table 3 shows the contributions of SRR and DPR. It clearly indicates that SRRs are very effective in reordering structures, which improve performance by 1.45 (26.07-24.62) BLEU score. However, DPRs have less impact on performance in our tree sequence-based model. This seems in contradiction to the previous observations3","in literature. However, it is not surprising simply because we use tree sequences as the basic translation units. Thereby, our model can capture all phrases. In this sense, our model behaves like a phrase-based model, less sensitive to discontinuous phras-  3 Wellington et al. (2006) reports that discontinuities are very useful for translational equivalence analysis using binarybranching structures under word alignment and parse tree constraints while they are almost of no use if under word alignment constraints only. Bod (2007) finds that discontinues phrase rules make significant performance improvement in linguistically STSG-based SMT models. 565 es (Wellington et al., 2006). Our additional experiments also verify that discontinuous phrase rules are complementary to syntactic phrase rules (Bod, 2007) while non-syntactic phrase rules may compromise the contribution of discontinuous phrase rules. Table 4 reports the numbers of these two kinds of rules. It shows that around 30% rules are shared by the two kinds of rule sets. These overlapped rules contain at least two non-terminal leaf nodes plus two terminal leaf nodes, which implies that longer rules do not affect performance too much."," 22.07 25.28 26.1425.94 26.02 26.07 21.5 22.5 23.5 24.5 25.5 26.5 123456 BL E U (% )  Figure 5: Accuracy changing with different maximal tree depths ( h = 1 to 6 when"]},{"title":"4d =","paragraphs":[")  22.72 24.71 26.0526.03 26.07 25.74 25.2925.2825.26 24.78 21.5 22.5 23.5 24.5 25.5 26.5 12345 BLEU ( % )"," Figure 6: Accuracy changing with the different maximal number of trees in a tree sequence ("]},{"title":"d","paragraphs":["=1 to 5), the upper","line is for 6h = while the lower line is for 2h = . ","Fig. 5 studies the impact when setting different maximal tree depth ( h ) in a rule on the performance. It demonstrates that:","1) Significant performance improvement is achieved when the value of h is increased from 1 to 2. This can be easily explained by the fact that when h = 1, only monotonic search is conducted, while h = 2 allows non-terminals to be leaf nodes, thus introducing preliminary structure features to the search and allowing non-monotonic search.","2) Internal structures and large span (due to h increasing) are also useful as attested by the gain of 0.86 (26.14-25.28) Blue score when the value of h increases from 2 to 4.","Fig. 6 studies the impact on performance by setting different maximal tree number (d) in a rule. It further indicates that:","1) Tree sequence rules (d >1) are useful and even more helpful if we limit the tree depth to no more than two (lower line, h=2). However, tree sequence rules consisting of more than three sub-trees have almost no contribution to the performance improvement. This is mainly due to data sparseness issue when d >3.","2) Even if only two-layer sub-trees (lower line) are allowed, our method still outperforms STSG and Moses when d>1. This further validates the effectiveness of our design philosophy of using multi-sub-trees as basic translation unit in SMT."]},{"title":"7 Conclusions and Future Work","paragraphs":["In this paper, we present a tree sequence alignment-based translation model to combine the strengths of phrase-based and syntax-based methods. The experimental results on the NIST MT-2005 Chinese-English translation task demonstrate the effectiveness of the proposed model. Our study also finds that in our model the tree sequence rules are very useful since they can model non-syntactic phrases and reorderings with rich linguistic structure features while discontinuous phrases and tree sequence rules with more than three sub-trees have less impact on performance.","There are many interesting research topics on the tree sequence-based translation model worth exploring in the future. The current method extracts large amount of rules. Many of them are redundant, which make decoding very slow. Thus, effective rule optimization and pruning algorithms are highly desirable. Ideally, a linguistically and empirically motivated theory can be worked out, suggesting what kinds of rules should be extracted given an input phrase pair. For example, most function words and headwords can be kept in abstract rules as features. In addition, word alignment is a hard constraint in our rule extraction. We will study direct structure alignments to reduce the impact of word alignment errors. We are also in-terested in comparing our method with the forest-to-string model (Liu et al., 2007). Finally, we would also like to study unsupervised learning-based bilingual parsing for SMT. 566 "]},{"title":"References","paragraphs":["Rens Bod. 2007. Unsupervised Syntax-Based Machine Translation: The Contribution of Discontinuous Phrases. MT-Summmit-07. 51-56.","David Chiang. 2005. A hierarchical phrase-based model for SMT. ACL-05. 263-270.","Brooke Cowan, Ivona Kucerova and Michael Collins. 2006. A discriminative model for tree-to-tree translation. EMNLP-06. 232-241.","Yuan Ding and Martha Palmer. 2005. Machine translation using probabilistic synchronous dependency insertion grammars. ACL-05. 541-548.","Jason Eisner. 2003. Learning non-isomorphic tree mappings for MT. ACL-03 (companion volume).","Michel Galley, Mark Hopkins, Kevin Knight and Daniel Marcu. 2004. What’s in a translation rule? HLT-NAACL-04.","Michel Galley, J. Graehl, K. Knight, D. Marcu, S. De-Neefe, W. Wang and I. Thayer. 2006. Scalable Inference and Training of Context-Rich Syntactic Translation Models. COLING-ACL-06. 961-968","Daniel Gildea. 2003. Loosely Tree-Based Alignment for Machine Translation. ACL-03. 80-87.","Jonathan Graehl and Kevin Knight. 2004. Training tree transducers. HLT-NAACL-2004. 105-112.","Mary Hearne and Andy Way. 2003. Seeing the wood for the trees: data-oriented translation. MT Summit IX, 165-172.","Liang Huang, Kevin Knight and Aravind Joshi. 2006. Statistical Syntax-Directed Translation with Extended Domain of Locality. AMTA-06 (poster).","Dan Klein and Christopher D. Manning. 2003. Accurate Unlexicalized Parsing. ACL-03. 423-430.","Philipp Koehn, F. J. Och and D. Marcu. 2003. Statistical phrase-based translation. HLT-NAACL-03. 127-133.","Philipp Koehn. 2004. Pharaoh: a beam search decoder for phrase-based statistical machine translation models. AMTA-04, 115-124","Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin and Evan Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. ACL-07 (poster) 77-180.","Yang Liu, Qun Liu and Shouxun Lin. 2006. Tree-to-String Alignment Template for Statistical Machine Translation. COLING-ACL-06. 609-616.","Yang Liu, Yun Huang, Qun Liu and Shouxun Lin. 2007. Forest-to-String Statistical Translation Rules. ACL-07. 704-711.","Daniel Marcu, W. Wang, A. Echihabi and K. Knight. 2006. SPMT: Statistical Machine Translation with Syntactified Target Language Phrases. EMNLP-06. 44-52.","I. Dan Melamed. 2004. Statistical machine translation by parsing. ACL-04. 653-660.","Franz J. Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. ACL-02. 295-302.","Franz J. Och. 2003. Minimum error rate training in statistical machine translation. ACL-03. 160-167.","Franz J. Och and Hermann Ney. 2004a. The alignment template approach to statistical machine translation. Computational Linguistics, 30(4):417-449.","Kishore Papineni, Salim Roukos, ToddWard and Wei-Jing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. ACL-02. 311-318.","Arjen Poutsma. 2000. Data-oriented translation. COLING-2000. 635-641","Chris Quirk and Arul Menezes. 2006. Do we need phrases? Challenging the conventional wisdom in SMT. COLING-ACL-06. 9-16.","Chris Quirk, Arul Menezes and Colin Cherry. 2005. Dependency treelet translation: Syntactically in-formed phrasal SMT. ACL-05. 271-279.","Stefan Riezler and John T. Maxwell III. 2006. Grammatical Machine Translation. HLT-NAACL-06. 248-255.","Hendra Setiawan, Min-Yen Kan and Haizhou Li. 2007. Ordering Phrases with Function Words. ACL-7. 712-719.","Andreas Stolcke. 2002. SRILM - an extensible language modeling toolkit. ICSLP-02. 901-904.","Benjamin Wellington, Sonjia Waxmonsky and I. Dan Melamed. 2006. Empirical Lower Bounds on the Complexity of Translational Equivalence. COLING-ACL-06. 977-984.","Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377-403.","Deyi Xiong, Qun Liu and Shouxun Lin. 2006. Maximum Entropy Based Phrase Reordering Model for SMT. COLING-ACL-06. 521– 528.","Kenji Yamada and Kevin Knight. 2001. A syntax-based statistical translation model. ACL-01. 523-530.","Min Zhang, Hongfei Jiang, Ai Ti Aw, Jun Sun, Sheng Li and Chew Lim Tan. 2007. A Tree-to-Tree Alignment-based Model for Statistical Machine Translation. MT-Summit-07. 535-542.","Ying Zhang. Stephan Vogel. Alex Waibel. 2004. Interpreting BLEU/NIST scores: How much improvement do we need to have a better system? LREC-04. 2051-2054. 567"]}],"references":[{"authors":[{"first":"Rens","last":"Bod"}],"year":"2007","title":"Unsupervised Syntax-Based Machine Translation: The Contribution of Discontinuous Phrases","source":"Rens Bod. 2007. Unsupervised Syntax-Based Machine Translation: The Contribution of Discontinuous Phrases. MT-Summmit-07. 51-56."},{"authors":[{"first":"David","last":"Chiang"}],"year":"2005","title":"A hierarchical phrase-based model for SMT","source":"David Chiang. 2005. A hierarchical phrase-based model for SMT. ACL-05. 263-270."},{"authors":[{"first":"Brooke","last":"Cowan"},{"first":"Ivona","last":"Kucerova"},{"first":"Michael","last":"Collins"}],"year":"2006","title":"A discriminative model for tree-to-tree translation","source":"Brooke Cowan, Ivona Kucerova and Michael Collins. 2006. A discriminative model for tree-to-tree translation. EMNLP-06. 232-241."},{"authors":[{"first":"Yuan","last":"Ding"},{"first":"Martha","last":"Palmer"}],"year":"2005","title":"Machine translation using probabilistic synchronous dependency insertion grammars","source":"Yuan Ding and Martha Palmer. 2005. Machine translation using probabilistic synchronous dependency insertion grammars. ACL-05. 541-548."},{"authors":[{"first":"Jason","last":"Eisner"}],"year":"2003","title":"Learning non-isomorphic tree mappings for MT","source":"Jason Eisner. 2003. Learning non-isomorphic tree mappings for MT. ACL-03 (companion volume)."},{"authors":[{"first":"Michel","last":"Galley"},{"first":"Mark","last":"Hopkins"},{"first":"Kevin","last":"Knight"},{"first":"Daniel","last":"Marcu"}],"year":"2004","title":"What’s in a translation rule? HLT-NAACL-04","source":"Michel Galley, Mark Hopkins, Kevin Knight and Daniel Marcu. 2004. What’s in a translation rule? HLT-NAACL-04."},{"authors":[{"first":"Michel","last":"Galley"},{"first":"J.","last":"Graehl"},{"first":"K.","last":"Knight"},{"first":"D.","last":"Marcu"},{"first":"S.","last":"De-Neefe"},{"first":"W.","last":"Wang"},{"first":"I.","last":"Thayer"}],"year":"2006","title":"Scalable Inference and Training of Context-Rich Syntactic Translation Models","source":"Michel Galley, J. Graehl, K. Knight, D. Marcu, S. De-Neefe, W. Wang and I. Thayer. 2006. Scalable Inference and Training of Context-Rich Syntactic Translation Models. COLING-ACL-06. 961-968"},{"authors":[{"first":"Daniel","last":"Gildea"}],"year":"2003","title":"Loosely Tree-Based Alignment for Machine Translation","source":"Daniel Gildea. 2003. Loosely Tree-Based Alignment for Machine Translation. ACL-03. 80-87."},{"authors":[{"first":"Jonathan","last":"Graehl"},{"first":"Kevin","last":"Knight"}],"year":"2004","title":"Training tree transducers","source":"Jonathan Graehl and Kevin Knight. 2004. Training tree transducers. HLT-NAACL-2004. 105-112."},{"authors":[{"first":"Mary","last":"Hearne"},{"first":"Andy","last":"Way"}],"year":"2003","title":"Seeing the wood for the trees: data-oriented translation","source":"Mary Hearne and Andy Way. 2003. Seeing the wood for the trees: data-oriented translation. MT Summit IX, 165-172."},{"authors":[{"first":"Liang","last":"Huang"},{"first":"Kevin","last":"Knight"},{"first":"Aravind","last":"Joshi"}],"year":"2006","title":"Statistical Syntax-Directed Translation with Extended Domain of Locality","source":"Liang Huang, Kevin Knight and Aravind Joshi. 2006. Statistical Syntax-Directed Translation with Extended Domain of Locality. AMTA-06 (poster)."},{"authors":[{"first":"Dan","last":"Klein"},{"first":"Christopher","middle":"D.","last":"Manning"}],"year":"2003","title":"Accurate Unlexicalized Parsing","source":"Dan Klein and Christopher D. Manning. 2003. Accurate Unlexicalized Parsing. ACL-03. 423-430."},{"authors":[{"first":"Philipp","last":"Koehn"},{"first":"F.","middle":"J.","last":"Och"},{"first":"D.","last":"Marcu"}],"year":"2003","title":"Statistical phrase-based translation","source":"Philipp Koehn, F. J. Och and D. Marcu. 2003. Statistical phrase-based translation. HLT-NAACL-03. 127-133."},{"authors":[{"first":"Philipp","last":"Koehn"}],"year":"2004","title":"Pharaoh: a beam search decoder for phrase-based statistical machine translation models","source":"Philipp Koehn. 2004. Pharaoh: a beam search decoder for phrase-based statistical machine translation models. AMTA-04, 115-124"},{"authors":[{"first":"Philipp","last":"Koehn"},{"first":"Hieu","last":"Hoang"},{"first":"Alexandra","last":"Birch"},{"first":"Chris","last":"Callison-Burch"},{"first":"Marcello","last":"Federico"},{"first":"Nicola","last":"Bertoldi"},{"first":"Brooke","last":"Cowan"},{"first":"Wade","last":"Shen"},{"first":"Christine","last":"Moran"},{"first":"Richard","last":"Zens"},{"first":"Chris","last":"Dyer"},{"first":"Ondrej","last":"Bojar"},{"first":"Alexandra","last":"Constantin"},{"first":"Evan","last":"Herbst"}],"year":"2007","title":"Moses: Open Source Toolkit for Statistical Machine Translation","source":"Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin and Evan Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. ACL-07 (poster) 77-180."},{"authors":[{"first":"Yang","last":"Liu"},{"first":"Qun","last":"Liu"},{"first":"Shouxun","last":"Lin"}],"year":"2006","title":"Tree-to-String Alignment Template for Statistical Machine Translation","source":"Yang Liu, Qun Liu and Shouxun Lin. 2006. Tree-to-String Alignment Template for Statistical Machine Translation. COLING-ACL-06. 609-616."},{"authors":[{"first":"Yang","last":"Liu"},{"first":"Yun","last":"Huang"},{"first":"Qun","last":"Liu"},{"first":"Shouxun","last":"Lin"}],"year":"2007","title":"Forest-to-String Statistical Translation Rules","source":"Yang Liu, Yun Huang, Qun Liu and Shouxun Lin. 2007. Forest-to-String Statistical Translation Rules. ACL-07. 704-711."},{"authors":[{"first":"Daniel","last":"Marcu"},{"first":"W.","last":"Wang"},{"first":"A.","last":"Echihabi"},{"first":"K.","last":"Knight"}],"year":"2006","title":"SPMT: Statistical Machine Translation with Syntactified Target Language Phrases","source":"Daniel Marcu, W. Wang, A. Echihabi and K. Knight. 2006. SPMT: Statistical Machine Translation with Syntactified Target Language Phrases. EMNLP-06. 44-52."},{"authors":[{"first":"I.","middle":"Dan","last":"Melamed"}],"year":"2004","title":"Statistical machine translation by parsing","source":"I. Dan Melamed. 2004. Statistical machine translation by parsing. ACL-04. 653-660."},{"authors":[{"first":"Franz","middle":"J.","last":"Och"},{"first":"Hermann","last":"Ney"}],"year":"2002","title":"Discriminative training and maximum entropy models for statistical machine translation","source":"Franz J. Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. ACL-02. 295-302."},{"authors":[{"first":"Franz","middle":"J.","last":"Och"}],"year":"2003","title":"Minimum error rate training in statistical machine translation","source":"Franz J. Och. 2003. Minimum error rate training in statistical machine translation. ACL-03. 160-167."},{"authors":[{"first":"Franz","middle":"J.","last":"Och"},{"first":"Hermann","last":"Ney"}],"year":"2004a","title":"The alignment template approach to statistical machine translation","source":"Franz J. Och and Hermann Ney. 2004a. The alignment template approach to statistical machine translation. Computational Linguistics, 30(4):417-449."},{"authors":[{"first":"Kishore","last":"Papineni"},{"first":"Salim","last":"Roukos"},{"last":"ToddWard"},{"first":"Wei-Jing","last":"Zhu"}],"year":"2002","title":"BLEU: a method for automatic evaluation of machine translation","source":"Kishore Papineni, Salim Roukos, ToddWard and Wei-Jing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. ACL-02. 311-318."},{"authors":[{"first":"Arjen","last":"Poutsma"}],"year":"2000","title":"Data-oriented translation","source":"Arjen Poutsma. 2000. Data-oriented translation. COLING-2000. 635-641"},{"authors":[{"first":"Chris","last":"Quirk"},{"first":"Arul","last":"Menezes"}],"year":"2006","title":"Do we need phrases? Challenging the conventional wisdom in SMT","source":"Chris Quirk and Arul Menezes. 2006. Do we need phrases? Challenging the conventional wisdom in SMT. COLING-ACL-06. 9-16."},{"authors":[{"first":"Chris","last":"Quirk"},{"first":"Arul","last":"Menezes"},{"first":"Colin","last":"Cherry"}],"year":"2005","title":"Dependency treelet translation: Syntactically in-formed phrasal SMT","source":"Chris Quirk, Arul Menezes and Colin Cherry. 2005. Dependency treelet translation: Syntactically in-formed phrasal SMT. ACL-05. 271-279."},{"authors":[{"first":"Stefan","last":"Riezler"},{"first":"John","middle":"T.","last":"Maxwell III"}],"year":"2006","title":"Grammatical Machine Translation","source":"Stefan Riezler and John T. Maxwell III. 2006. Grammatical Machine Translation. HLT-NAACL-06. 248-255."},{"authors":[{"first":"Hendra","last":"Setiawan"},{"first":"Min-Yen","last":"Kan"},{"first":"Haizhou","last":"Li"}],"year":"2007","title":"Ordering Phrases with Function Words","source":"Hendra Setiawan, Min-Yen Kan and Haizhou Li. 2007. Ordering Phrases with Function Words. ACL-7. 712-719."},{"authors":[{"first":"Andreas","last":"Stolcke"}],"year":"2002","title":"SRILM - an extensible language modeling toolkit","source":"Andreas Stolcke. 2002. SRILM - an extensible language modeling toolkit. ICSLP-02. 901-904."},{"authors":[{"first":"Benjamin","last":"Wellington"},{"first":"Sonjia","last":"Waxmonsky"},{"first":"I.","middle":"Dan","last":"Melamed"}],"year":"2006","title":"Empirical Lower Bounds on the Complexity of Translational Equivalence","source":"Benjamin Wellington, Sonjia Waxmonsky and I. Dan Melamed. 2006. Empirical Lower Bounds on the Complexity of Translational Equivalence. COLING-ACL-06. 977-984."},{"authors":[{"first":"Dekai","last":"Wu"}],"year":"1997","title":"Stochastic inversion transduction grammars and bilingual parsing of parallel corpora","source":"Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377-403."},{"authors":[{"first":"Deyi","last":"Xiong"},{"first":"Qun","last":"Liu"},{"first":"Shouxun","last":"Lin"}],"year":"2006","title":"Maximum Entropy Based Phrase Reordering Model for SMT","source":"Deyi Xiong, Qun Liu and Shouxun Lin. 2006. Maximum Entropy Based Phrase Reordering Model for SMT. COLING-ACL-06. 521– 528."},{"authors":[{"first":"Kenji","last":"Yamada"},{"first":"Kevin","last":"Knight"}],"year":"2001","title":"A syntax-based statistical translation model","source":"Kenji Yamada and Kevin Knight. 2001. A syntax-based statistical translation model. ACL-01. 523-530."},{"authors":[{"first":"Min","last":"Zhang"},{"first":"Hongfei","last":"Jiang"},{"first":"Ai","middle":"Ti","last":"Aw"},{"first":"Jun","last":"Sun"},{"first":"Sheng","last":"Li"},{"first":"Chew","middle":"Lim","last":"Tan"}],"year":"2007","title":"A Tree-to-Tree Alignment-based Model for Statistical Machine Translation","source":"Min Zhang, Hongfei Jiang, Ai Ti Aw, Jun Sun, Sheng Li and Chew Lim Tan. 2007. A Tree-to-Tree Alignment-based Model for Statistical Machine Translation. MT-Summit-07. 535-542."},{"authors":[{"first":"Ying","middle":"Zhang. Stephan Vogel. Alex","last":"Waibel"}],"year":"2004","title":"Interpreting BLEU/NIST scores: How much improvement do we need to have a better system? LREC-04","source":"Ying Zhang. Stephan Vogel. Alex Waibel. 2004. Interpreting BLEU/NIST scores: How much improvement do we need to have a better system? LREC-04. 2051-2054. 567"}],"cites":[{"style":0,"text":"Koehn et al., 2003","origin":{"pointer":"/sections/3/paragraphs/0","offset":30,"length":18},"authors":[{"last":"Koehn"},{"last":"al."}],"year":"2003","references":["/references/12"]},{"style":0,"text":"Och and Ney, 2004a","origin":{"pointer":"/sections/3/paragraphs/0","offset":50,"length":18},"authors":[{"last":"Och"},{"last":"Ney"}],"year":"2004a","references":["/references/21"]},{"style":0,"text":"Quirk and Menezes, 2006","origin":{"pointer":"/sections/3/paragraphs/0","offset":368,"length":23},"authors":[{"last":"Quirk"},{"last":"Menezes"}],"year":"2006","references":["/references/24"]},{"style":0,"text":"Wu, 1997","origin":{"pointer":"/sections/3/paragraphs/0","offset":483,"length":8},"authors":[{"last":"Wu"}],"year":"1997","references":["/references/30"]},{"style":0,"text":"Chiang, 2005","origin":{"pointer":"/sections/3/paragraphs/0","offset":493,"length":12},"authors":[{"last":"Chiang"}],"year":"2005","references":["/references/1"]},{"style":0,"text":"Eisner, 2003","origin":{"pointer":"/sections/3/paragraphs/0","offset":507,"length":12},"authors":[{"last":"Eisner"}],"year":"2003","references":["/references/4"]},{"style":0,"text":"Ding and Palmer, 2005","origin":{"pointer":"/sections/3/paragraphs/0","offset":521,"length":21},"authors":[{"last":"Ding"},{"last":"Palmer"}],"year":"2005","references":["/references/3"]},{"style":0,"text":"Cowan et al., 2006","origin":{"pointer":"/sections/3/paragraphs/0","offset":563,"length":18},"authors":[{"last":"Cowan"},{"last":"al."}],"year":"2006","references":["/references/2"]},{"style":0,"text":"Zhang et al., 2007","origin":{"pointer":"/sections/3/paragraphs/0","offset":583,"length":18},"authors":[{"last":"Zhang"},{"last":"al."}],"year":"2007","references":["/references/33"]},{"style":0,"text":"Bod, 2007","origin":{"pointer":"/sections/3/paragraphs/0","offset":603,"length":9},"authors":[{"last":"Bod"}],"year":"2007","references":["/references/0"]},{"style":0,"text":"Yamada and Knight, 2001","origin":{"pointer":"/sections/3/paragraphs/0","offset":614,"length":23},"authors":[{"last":"Yamada"},{"last":"Knight"}],"year":"2001","references":["/references/32"]},{"style":0,"text":"Liu et al., 2006","origin":{"pointer":"/sections/3/paragraphs/0","offset":639,"length":16},"authors":[{"last":"Liu"},{"last":"al."}],"year":"2006","references":["/references/15"]},{"style":0,"text":"Liu et al., 2007","origin":{"pointer":"/sections/3/paragraphs/0","offset":657,"length":16},"authors":[{"last":"Liu"},{"last":"al."}],"year":"2007","references":["/references/16"]},{"style":0,"text":"Gildea, 2003","origin":{"pointer":"/sections/3/paragraphs/0","offset":675,"length":12},"authors":[{"last":"Gildea"}],"year":"2003","references":["/references/7"]},{"style":0,"text":"Poutsma, 2000","origin":{"pointer":"/sections/3/paragraphs/0","offset":689,"length":13},"authors":[{"last":"Poutsma"}],"year":"2000","references":["/references/23"]},{"style":0,"text":"Hearne and Way, 2003","origin":{"pointer":"/sections/3/paragraphs/0","offset":704,"length":20},"authors":[{"last":"Hearne"},{"last":"Way"}],"year":"2003","references":["/references/9"]},{"style":0,"text":"Koehn et al., 2007","origin":{"pointer":"/sections/3/paragraphs/2","offset":485,"length":18},"authors":[{"last":"Koehn"},{"last":"al."}],"year":"2007","references":["/references/14"]},{"style":0,"text":"Zhang et al., 2007","origin":{"pointer":"/sections/3/paragraphs/2","offset":640,"length":18},"authors":[{"last":"Zhang"},{"last":"al."}],"year":"2007","references":["/references/33"]},{"style":0,"text":"Yamada and Knight (2001)","origin":{"pointer":"/sections/4/paragraphs/0","offset":85,"length":24},"authors":[{"last":"Yamada"},{"last":"Knight"}],"year":"2001","references":["/references/32"]},{"style":0,"text":"Eisner (2003)","origin":{"pointer":"/sections/4/paragraphs/0","offset":190,"length":13},"authors":[{"last":"Eisner"}],"year":"2003","references":["/references/4"]},{"style":0,"text":"Ding and Palmer (2005)","origin":{"pointer":"/sections/4/paragraphs/0","offset":283,"length":22},"authors":[{"last":"Ding"},{"last":"Palmer"}],"year":"2005","references":["/references/3"]},{"style":0,"text":"Quirk et al. (2005)","origin":{"pointer":"/sections/4/paragraphs/0","offset":414,"length":19},"authors":[{"last":"Quirk"},{"last":"al."}],"year":"2005","references":["/references/25"]},{"style":0,"text":"Cowan et al. (2006)","origin":{"pointer":"/sections/4/paragraphs/0","offset":488,"length":19},"authors":[{"last":"Cowan"},{"last":"al."}],"year":"2006","references":["/references/2"]},{"style":0,"text":"Huang et al. (2006)","origin":{"pointer":"/sections/4/paragraphs/0","offset":633,"length":19},"authors":[{"last":"Huang"},{"last":"al."}],"year":"2006","references":["/references/10"]},{"style":0,"text":"Liu et al. (2006)","origin":{"pointer":"/sections/4/paragraphs/0","offset":703,"length":17},"authors":[{"last":"Liu"},{"last":"al."}],"year":"2006","references":["/references/15"]},{"style":0,"text":"Zhang et al. (2007b)","origin":{"pointer":"/sections/4/paragraphs/0","offset":753,"length":20},"authors":[{"last":"Zhang"},{"last":"al."}],"year":"2007b","references":[]},{"style":0,"text":"Bod (2007)","origin":{"pointer":"/sections/4/paragraphs/0","offset":827,"length":10},"authors":[{"last":"Bod"}],"year":"2007","references":["/references/0"]},{"style":0,"text":"Koehn et al., 2003","origin":{"pointer":"/sections/4/paragraphs/0","offset":1190,"length":18},"authors":[{"last":"Koehn"},{"last":"al."}],"year":"2003","references":["/references/12"]},{"style":0,"text":"Wu (1997)","origin":{"pointer":"/sections/4/paragraphs/1","offset":63,"length":9},"authors":[{"last":"Wu"}],"year":"1997","references":["/references/30"]},{"style":0,"text":"Xiong et al. (2006)","origin":{"pointer":"/sections/4/paragraphs/1","offset":74,"length":19},"authors":[{"last":"Xiong"},{"last":"al."}],"year":"2006","references":["/references/31"]},{"style":0,"text":"Wu, 1997","origin":{"pointer":"/sections/4/paragraphs/1","offset":143,"length":8},"authors":[{"last":"Wu"}],"year":"1997","references":["/references/30"]},{"style":0,"text":"Setiawan et al. (2007)","origin":{"pointer":"/sections/4/paragraphs/1","offset":159,"length":22},"authors":[{"last":"Setiawan"},{"last":"al."}],"year":"2007","references":["/references/27"]},{"style":0,"text":"Chiang (2005)","origin":{"pointer":"/sections/4/paragraphs/1","offset":238,"length":13},"authors":[{"last":"Chiang"}],"year":"2005","references":["/references/1"]},{"style":0,"text":"Chiang, 2005","origin":{"pointer":"/sections/4/paragraphs/1","offset":476,"length":12},"authors":[{"last":"Chiang"}],"year":"2005","references":["/references/1"]},{"style":0,"text":"Hassan et al. (2007)","origin":{"pointer":"/sections/4/paragraphs/3","offset":3,"length":20},"authors":[{"last":"Hassan"},{"last":"al."}],"year":"2007","references":[]},{"style":0,"text":"Galley et al. (2006)","origin":{"pointer":"/sections/4/paragraphs/4","offset":3,"length":20},"authors":[{"last":"Galley"},{"last":"al."}],"year":"2006","references":["/references/6"]},{"style":0,"text":"Marcu et al., 2006","origin":{"pointer":"/sections/4/paragraphs/4","offset":201,"length":18},"authors":[{"last":"Marcu"},{"last":"al."}],"year":"2006","references":["/references/17"]},{"style":0,"text":"Galley et al. (2006)","origin":{"pointer":"/sections/4/paragraphs/5","offset":28,"length":20},"authors":[{"last":"Galley"},{"last":"al."}],"year":"2006","references":["/references/6"]},{"style":0,"text":"Marcu et al. (2006)","origin":{"pointer":"/sections/4/paragraphs/5","offset":50,"length":19},"authors":[{"last":"Marcu"},{"last":"al."}],"year":"2006","references":["/references/17"]},{"style":0,"text":"Liu et al., 2007","origin":{"pointer":"/sections/4/paragraphs/5","offset":658,"length":16},"authors":[{"last":"Liu"},{"last":"al."}],"year":"2007","references":["/references/16"]},{"style":0,"text":"Liu et al. (2006)","origin":{"pointer":"/sections/4/paragraphs/6","offset":3,"length":17},"authors":[{"last":"Liu"},{"last":"al."}],"year":"2006","references":["/references/15"]},{"style":0,"text":"Liu et al. (2007)","origin":{"pointer":"/sections/4/paragraphs/6","offset":516,"length":17},"authors":[{"last":"Liu"},{"last":"al."}],"year":"2007","references":["/references/16"]},{"style":0,"text":"Galley et al., 2006","origin":{"pointer":"/sections/4/paragraphs/13","offset":148,"length":19},"authors":[{"last":"Galley"},{"last":"al."}],"year":"2006","references":["/references/6"]},{"style":0,"text":"Marcu et al., 2006","origin":{"pointer":"/sections/4/paragraphs/13","offset":199,"length":18},"authors":[{"last":"Marcu"},{"last":"al."}],"year":"2006","references":["/references/17"]},{"style":0,"text":"Liu et al., 2007","origin":{"pointer":"/sections/4/paragraphs/13","offset":239,"length":16},"authors":[{"last":"Liu"},{"last":"al."}],"year":"2007","references":["/references/16"]},{"style":0,"text":"Och and Ney, 2002","origin":{"pointer":"/sections/57/paragraphs/2","offset":53,"length":17},"authors":[{"last":"Och"},{"last":"Ney"}],"year":"2002","references":["/references/19"]},{"style":0,"text":"Koehn, 2004","origin":{"pointer":"/sections/57/paragraphs/2","offset":175,"length":11},"authors":[{"last":"Koehn"}],"year":"2004","references":["/references/13"]},{"style":0,"text":"Stolcke, 2002","origin":{"pointer":"/sections/133/paragraphs/0","offset":255,"length":13},"authors":[{"last":"Stolcke"}],"year":"2002","references":["/references/28"]},{"style":0,"text":"Klein and Manning, 2003","origin":{"pointer":"/sections/133/paragraphs/0","offset":483,"length":23},"authors":[{"last":"Klein"},{"last":"Manning"}],"year":"2003","references":["/references/11"]},{"style":0,"text":"Papineni et al., 2002","origin":{"pointer":"/sections/133/paragraphs/0","offset":661,"length":21},"authors":[{"last":"Papineni"},{"last":"al."}],"year":"2002","references":["/references/22"]},{"style":0,"text":"Och and Ney, 2004","origin":{"pointer":"/sections/133/paragraphs/0","offset":701,"length":17},"authors":[{"last":"Och"},{"last":"Ney"}],"year":"2004","references":[]},{"style":0,"text":"Och, 2003","origin":{"pointer":"/sections/133/paragraphs/0","offset":815,"length":9},"authors":[{"last":"Och"}],"year":"2003","references":["/references/20"]},{"style":0,"text":"Koehn, 2004","origin":{"pointer":"/sections/133/paragraphs/0","offset":860,"length":11},"authors":[{"last":"Koehn"}],"year":"2004","references":["/references/13"]},{"style":0,"text":"Koehn et al., 2007","origin":{"pointer":"/sections/133/paragraphs/1","offset":38,"length":18},"authors":[{"last":"Koehn"},{"last":"al."}],"year":"2007","references":["/references/14"]},{"style":0,"text":"Zhang et al., 2007","origin":{"pointer":"/sections/133/paragraphs/1","offset":122,"length":18},"authors":[{"last":"Zhang"},{"last":"al."}],"year":"2007","references":["/references/33"]},{"style":0,"text":"Chiang, 2005","origin":{"pointer":"/sections/144/paragraphs/20","offset":563,"length":12},"authors":[{"last":"Chiang"}],"year":"2005","references":["/references/1"]},{"style":0,"text":"Chiang, 2005","origin":{"pointer":"/sections/144/paragraphs/20","offset":687,"length":12},"authors":[{"last":"Chiang"}],"year":"2005","references":["/references/1"]},{"style":0,"text":"Wellington et al. (2006)","origin":{"pointer":"/sections/145/paragraphs/4","offset":260,"length":24},"authors":[{"last":"Wellington"},{"last":"al."}],"year":"2006","references":["/references/29"]},{"style":0,"text":"Bod (2007)","origin":{"pointer":"/sections/145/paragraphs/4","offset":524,"length":10},"authors":[{"last":"Bod"}],"year":"2007","references":["/references/0"]},{"style":0,"text":"Wellington et al., 2006","origin":{"pointer":"/sections/145/paragraphs/4","offset":662,"length":23},"authors":[{"last":"Wellington"},{"last":"al."}],"year":"2006","references":["/references/29"]},{"style":0,"text":"Bod, 2007","origin":{"pointer":"/sections/145/paragraphs/4","offset":804,"length":9},"authors":[{"last":"Bod"}],"year":"2007","references":["/references/0"]},{"style":0,"text":"Liu et al., 2007","origin":{"pointer":"/sections/148/paragraphs/1","offset":798,"length":16},"authors":[{"last":"Liu"},{"last":"al."}],"year":"2007","references":["/references/16"]}]}
