{"sections":[{"title":"","paragraphs":["Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 832–839, Prague, Czech Republic, June 2007. c⃝2007 Association for Computational Linguistics"]},{"title":"Grammar Approximation by Representative Sublanguage: A New Model for Language Learning Smaranda Muresan","paragraphs":["Institute for Advanced Computer Studies University of Maryland College Park, MD 20742, USA smara@umiacs.umd.edu"]},{"title":"Owen Rambow","paragraphs":["Center for Computational Learning Systems Columbia University","New York, NY 10027, USA rambow@cs.columbia.edu"]},{"title":"Abstract","paragraphs":["We propose a new language learning model that learns a syntactic-semantic grammar from a small number of natural language strings annotated with their semantics, along with basic assumptions about natural language syntax. We show that the search space for grammar induction is a complete grammar lattice, which guarantees the uniqueness of the learned grammar."]},{"title":"1 Introduction","paragraphs":["There is considerable interest in learning computational grammars.1","While much attention has focused on learning syntactic grammars either in a supervised or unsupervised manner, recently there is a growing interest toward learning grammars/parsers that capture semantics as well (Bos et al., 2004; Zettlemoyer and Collins, 2005; Ge and Mooney, 2005).","Learning both syntax and semantics is arguably more difficult than learning syntax alone. In formal grammar learning theory it has been shown that learning from “good examples,” or representative examples, is more powerful than learning from all the examples (Freivalds et al., 1993). Haghighi and Klein (2006) show that using a handful of “proto-","1","This research was supported by the National Science Foundation under Digital Library Initiative Phase II Grant Number IIS-98-17434 (Judith Klavans and Kathleen McKeown, PIs). We would like to thank Judith Klavans for her contributions over the course of this research, Kathy McKeown for her in-put, and several anonymous reviewers for very useful feedback on earlier drafts of this paper. types” significantly improves over a fully unsupervised PCFG induction model (their prototypes were formed by sequences of POS tags; for example, prototypical NPs were DT NN, JJ NN).","In this paper, we present a new grammar formalism and a new learning method which together address the problem of learning a syntactic-semantic grammar in the presence of a representative sample of strings annotated with their semantics, along with minimal assumptions about syntax (such as syntactic categories). The semantic representation is an ontology-based semantic representation. The annotation of the representative examples does not in-clude the entire derivation, unlike most of the existing syntactic treebanks. The aim of the paper is to present the formal aspects of our grammar induction model.","In Section 2, we present a new grammar formalism, called Lexicalized Well-Founded Grammars, a type of constraint-based grammars that combine syntax and semantics. We then turn to the two main results of this paper. In Section 3 we show that our grammars can always be learned from a set of positive representative examples (with no negative examples), and the search space for grammar induction is a complete grammar lattice, which guarantees the uniqueness of the learned grammar. In Section 4, we propose a new computationally efficient model for grammar induction from pairs of utterances and their semantic representations, called Grammar Approximation by Representative Sublanguage (GARS). Section 5 discusses the practical use of our model and Section 6 states our conclusions and future work. 832"]},{"title":"2 Lexicalized Well-Founded Grammars","paragraphs":["Lexicalized Well-Founded Grammars (LWFGs) are a type of Definite Clause Grammars (Pereira and Warren, 1980) where: (1) the Context-Free Grammar backbone is extended by introducing a partial ordering relation among nonterminals (well-founded) 2) each string is associated with a syntactic-semantic representation called semantic molecule; 3) grammar rules have two types of constraints: one for semantic composition and one for ontology-based semantic interpretation.","The partial ordering among nonterminals allows the ordering of the grammar rules, and thus facilitates the bottom-up induction of these grammars.","The semantic molecule is a syntactic-semantic representation of natural language strings"," where","(head) encodes the information required for semantic composition, and","(body) is the actual semantic representation of the string. Figure 1 shows examples of semantic molecules for an adjective, a noun and a noun phrase. The representations associated with the lexical items are called elementary semantic molecules (I), while the representations built by the combination of others are called derived semantic molecules (II). The head of the semantic molecule is a flat feature structure, having at least two attributes encoding the syntactic category of the associated string, cat, and the head of the string, head. The set of attributes is finite and known a priori for each syntactic category. The body of the semantic molecule is a flat, ontology-based semantic representation. It is a logical form, built as a conjunction of atomic predicates","","",", where variables are either concept or slot identifiers in an ontology. For example, the adjective major is represented as","","","","",", which says that the meaning of an adjective is a concept (","","","), which is a value of a property of another concept (","","",") in the ontology.","The grammar nonterminals are augmented with pairs of strings and their semantic molecules. These pairs are called syntagmas, and are denoted by","  ","","  ","","    ","",". There are two types of con-","straints at the grammar rule level — one for semantic","composition (defines how the meaning of a natural","language expression is composed from the meaning","I. Elementary Semantic Molecules","(major/adj)","=           cat adj head  mod       ",".isa = major, .Y=        ","(damage/noun)","=            cat noun nr sg head          .isa = damage         ","II. Derived Semantic Molecule","(major damage)","=            cat n nr sg head X        .isa = major, X.Y=  , X.isa=damage        ","III. Constraint Grammar Rule","","","","     ","","","","",""," ","   ","","","","","","","  ","","","","","  "," ","","  ",""," ","","","   ","","","","   ","","  ","returns","=MAJOR, =DAMAGE,","=DEGREE from ontology Figure 1: Examples of two elementary semantic molecules (I), a derived semantic molecule (II) obtained by combining them, and a constraint grammar rule together with the constraints",",","(III) . of its parts) and one for ontology-based semantic in-terpretation. An example of a LWFG rule is given in Figure 1(III). The composition constraints"," applied to the heads of the semantic molecules, form a system of equations that is a simplified version of “path equations” (Shieber et al., 1983), because the heads are flat feature structures. These constraints are learned together with the grammar rules. The ontology-based constraints represent the validation on the ontology, and are applied to the body of the semantic molecule associated with the left-hand side nonterminal. They are not learned. Currently, is a predicate which can succeed or fail. When it succeeds, it instantiates the variables of the semantic representation with concepts/slots in the ontology. For example, given the phrase major damage, succeeds and returns (","=MAJOR,","=DAMAGE, ","=DEGREE), while given the phrase major birth it fails. We leave the discussion of the ontology constraints for a future paper, since it is not needed for the main result of this paper. We give below the formal definition of Lexical-833","ized Well-Founded Grammars, except that we do not","define formally the constraints due to lack of space","(see (Muresan, 2006) for details).","Definition 1. A Lexicalized Well-Founded Gram-","mar (LWFG) is a 6-tuple,","","","","","","","","","","","","","  ","",", where:","1.","is a finiteset of terminal symbols.","2.","","is a finite set of elementary semantic molecules corresponding to the set of terminal symbols.","3.","is a finiteset of nonterminal symbols.","4.","is a partial ordering relation among the nonterminals.","5.","is a set of constraint rules. A constraint rule is written   ","   ","     ","   ","","   , where   ","","","  such that","","","","","   ","  ","","","","","","","","","","","","","","","","","","","",""," ","","","","   , and","is the semantic compo-","sition operator. For brevity, we denote a rule","by",""," ",", where","","","","","","",".","For the rules whose left-hand side are","preterminals, ","","","",", we use the notation","","  . There are three types of rules:","ordered non-recursive, ordered recursive,","and non-ordered rules. A grammar rule","    ","  ","     ","  ","","   , is an ordered rule, if for all   , we have ","",". In LWFGs, each nonterminal symbol is a left-hand side in at least one ordered non-recursive rule and the empty string cannot be derived from any nonterminal symbol.","6.","is the start nonterminal symbol, and   ","(we use the same notation for the refle xive, transitive closure of",").","The relation","is a partial ordering only among nonterminals, and it should not be confused with information ordering derived from the flat feature structures. This relation makes the set of nonterminals well-founded, which allows the ordering of the grammar rules, as well as the ordering of the syntagmas generated by LWFGs. Definition 2. Given a LWFG,",", the ground syntagma derivation relation,  ",",2 is de-","fined as:    ","(if","","","","","   ",""," 2 The ground derivation (“reduction” in (Wintner, 1999)) can","be viewed as the bottom-up counterpart of the usual derivation.","","","  ","","",", i.e.,","is a preterminal), and","    ","",""," ","","","     ","    ","       .","In LWFGs all syntagmas  ","","","","",", derived","from a nonterminal","have the same category of","their semantic molecules  .3","The language of a grammar","is the set of all","syntagmas generated from the start symbol",", i.e.,","","","","","   ","","","","","","   ","   ","  ","","",".","The set of all syntagmas generated by a grammar","","is   ","","","","  ","","","","","","  ","","   ","        . Given a LWFG","we call a set","","  ","  a sublanguage of",". Extending the notation,","given a LWFG",", the set of syntagmas generated by","a rule ","","","  ","","is  ","","  ","","","",""," ","","","","","   ","","   ","","","  ","","  ",", where ","","  ","",""," denotes the ground derivation  ","","obtained using the rule","","","","in","the last derivation step (we have bottom-up deriva-","tion). We will use the short notation      , where","is a grammar rule. Given a LWFG","and a sublanguage  (not nec-","essarily of ) we denote by ","  ","  ","  "," ",",","the set of syntagmas generated by","reduced to the","sublanguage ",". Given a grammar rule ",", we call ","  ","     "," ","the set of syntagmas","generated by reduced to the sublanguage ",".","As we have previously mentioned, the partial ordering among grammar nonterminals allows the ordering of the syntagmas generated by the grammar, which allows us to define the representative examples of a LWFG. Representative Examples. Informally, the representative examples","of a LWFG,",", are the simplest syntagmas ground-derived by the grammar",", i.e., for each grammar rule there exist a syntagma which is ground-derived from it in the minimum number of steps. Thus, the size of the representative example set is equal with the size of the set of grammar rules,","","","","",".","This set of representative examples is used by the grammar learning model to generate the candidate hypotheses. For generalization, a larger sublanguage ","","is used, which we call representa-","tive sublanguage. 3 This property is used for determining the lhs nonterminal","of the learned rule. 834 PSfrag replacements","         ","  "," ","               =","the, noise, loud, clear"," =","noise, loud noise, the noise"," =  ","","clear loud noise, the loud noise","","=   ="," ","","clear loud noise","","=  ","","the loud noise","","=  Rule specialization steps  ","    ","   Rule generalization steps     ","     ","  ","Figure 2: Example of a simple grammar lattice. All grammars generate , and only","generates ","( is a common lexicon for all the grammars)"]},{"title":"3 A Grammar Lattice as a Search Space for Grammar Induction","paragraphs":["In this section we present a class of Lexicalized Well-Founded Grammars that form a complete lattice. This grammar lattice is the search space for our grammar induction model, which we present in Section 4. An example of a grammar lattice is given in Figure 2, where for simplicity, we only show the context-free backbone of the grammar rules, and only strings, not syntagmas. Intuitively, the grammars found lower in the lattice are more specialized than the ones higher in the lattice. For learning, ","","is used to generate the most specifichypotheses (grammar rules), and thus all the grammars should be able to generate those examples. The sublanguage ","is used during generalization, thus only the most general grammar,",", is able to generate the entire sublanguage. In other words, the generalization process is bounded by ",", that is why our model is called Grammar Approximation by Representative Sublanguage.","There are two properties that LWFGs should have in order to form a complete lattice: 1) they should be unambiguous, and 2) they should preserve the parsing of the representative example set,",". We define these two properties in turn. Definition 3. A LWFG,",", is unambiguous w.r.t. a sublanguage","    ","  if"," ","  there is one","and only one rule that derives .","Since the unambiguity is relative to a set of syntagmas (pairs of strings and their semantic molecules) and not to a set of natural language strings, the requirement is compatible with modeling natural language. For example, an ambiguous string such as John saw the man with the telescope corresponds to two unambiguous syntagmas.","In order to define the second property, we need to define the rule specialization step and the rule generalization step of unambiguous LWFGs, such that they are","-parsing-preserving and are the inverse of each other. The property of","-parsing-preserving means that both the initial and the specialized/generalized rules ground-derive the same syntagma, ","  ",". Definition4. The rule specialization step:","","","","                  ","","","      ","is  -parsing-preserving, if there exists "," ","","and  ","    ","and    ","     ",", where  ","=       ","",","," =","",""," , and     ","=   "," ","",". We write","","        ",". The rule generalization step :","","","","","","          ","    ","","         ","is  -parsing-preserving, if there exists "," ","","and    ","     ","and  ","    ",". We write          ",". Since ","is a representative example, it is derived","in the minimum number of derivation steps, and thus","the rule ","is always an ordered, non-recursive rule. 835 The goal of the rule specialization step is to obtain a new target grammar","","from","by modifying a rule of",". Similarly, the goal of the rule generalization step is to obtain a new target grammar ","from","","by modifying a rule of","",". They are not to be taken as the derivation/reduction concepts in parsing. The specialization/generalization steps are the inverse of each other. From both the specialization and the generalization step we have that:      ","         ","",". In Figure 2, the specialization step      ","is","  -parsing-preserving, because the rule","","","ground-","derives the syntagma loud noise. If instead we would have a specialization step   ","    (","",""," ","","","","","","    ","","), it would not be","","-parsing-","preserving since the syntagma loud noise could no","longer be ground-derived from the rule","","","","(which","requires two adjectives).","Definition 5. A grammar","","is one-step special-","ized from a grammar ,   ","","",", if","","","","and","","","","","","",", s.t.      , and","","     ","iff ","","   . A grammar","","is specialized from","a grammar ,","","","  , if it is obtained from","in  -specialization steps:       ","  , where","is fi-","nite. We extend the notation so that we have","","","",".","Similarly, we define the concept of a grammar","generalized from a grammar","",", "," "," using the rule generalization step.","In Figure 2, the grammar is one-step special-","ized from the grammar  , i.e.,","    ",", since","","preserve the parsing of the representative exam-","ples",". A grammar which contains the rule","",""," ","","","      ","","","instead of","","","is not specialized","from the grammar","","since it does not preserve the","parsing of the representative example set,","",". Such","grammars will not be in the lattice.","In order to define the grammar lattice we need to","introduce one more concept: a normalized grammar","w.r.t. a sublanguage. Definition6. A LWFG","is called normalized w.r.t. a sublanguage  (not necessarily of G), if none of","the grammar rules    ","of","can be further gener-","alized to a rule",""," by the rule generalization step","such that      ","","","   ","",".","In Figure 2, grammar is normalized w.r.t.  ,","while ,","","and","","are not.","We now definea grammar lattice","which will be","the search space for our grammar learning model.","We firstdefinethe set of lattice elements",".","Let be a LWFG, normalized and unambiguous","w.r.t. a sublanguage","    ","  which includes","the representative example set of the grammar"," ("," ","","). Let",""," ","","","","","","","be the set of","grammars specialized from . We call","the top","element of",", and","the bottom element of",", if","","    ","","",""," ","","","",". The bottom element,"," , is the grammar specialized from",", such that the","right-hand side of all grammar rules contains only","preterminals. We have ","  ","  and ","  ","","",".","The grammars in","have the following two prop-","erties (Muresan, 2006):","","For two grammars","and","",", we have that","","is specialized from","if and only if","is gener-","alized from","",", with  ","     ","   ."," All grammars in","preserve the parsing of the representative example set",".","Note that we have that for","","   ",", if","","","  then ","    ","   .","The system ","","","","","","","is a complete gram-","mar lattice (see (Muresan, 2006) for the full formal","proof). In Figure 2 the grammars  ,","",",",",","pre-","serve the parsing of the representative examples","",". We have that   ","  ,   ","  ,","   "," ,","    ","and  "," . Due to space limitation we do not define","here the least upper bound (","","),","and the greatest","lower bound ( ","),","operators, but in this example","","=","","","  ,","=","","","","",".","In oder to give a learnability theorem we need to","show that and","elements of the lattice can be","built. First, an assumption in our learning model is","that the rules corresponding to the grammar preter-","minals are given. Thus, for a given set of representa-","tive examples,","",", we can build the grammar","us-","ing a bottom-up robust parser, which returns partial","analyses (chunks) if it cannot return a full parse. In","order to soundly build the","element of the grammar","lattice from the","grammar through generalization,","we must give the definitionof a grammar","confor-","mal w.r.t.  . 836 Definition7. A LWFG","is conformal w.r.t. a sublanguage","    ","  iff","is normalized and unambiguous w.r.t.  and the rule specialization step guarantees that ","","","","","","     ","","for all grammars","specialized from .","The only rule generalization steps allowed in the grammar induction process are those which guarantee the same relation      ","","","","","","","",", which en-","sures that all the generalized grammars belong to the","grammar lattice.","In Figure 2,","is conformal to the given sub-","language ",". If the sublanguage were   ","","","  clear loud noise","then","would not be con-","formal to",""," since ","  ","","  ","","","","and thus","the specialization step would not satisfy the relation"," ","","","","  ","","","","","","  ","","","","","","",". Dur-","ing learning, the generalization step cannot general-","ize from grammar","","to",". Theorem 1 (Learnability Theorem). If","is the set of representative examples associated with a LWFG","conformal w.r.t. a sublanguage"," "," ,","then can always be learned from","and  as","the grammar lattice top element (  ",").","The proof is given in (Muresan, 2006).","If the hypothesis of Theorem 1 holds, then any grammar induction algorithm that uses the complete lattice search space can converge to the lattice top element, using different search strategies. In the next section we present our new model of grammar learning which relies on the property of the search space as grammar lattice."]},{"title":"4 Grammar Induction Model","paragraphs":["Based on the theoretical foundation of the hypothesis search space for LWFG learning given in the previous section, we define our grammar induction model. First, we present the LWFG induction as an Inductive Logic Programming problem. Second, we present our new relational learning model for LWFG induction, called Grammar Approximation by Representative Sublanguage (GARS).","4.1 Grammar Induction Problem in ILP-setting Inductive Logic Programming (ILP) is a class of relational learning methods concerned with inducing first-order Horn clauses from examples and background knowledge. Kietz and Džeroski (1994) have formally definedthe ILP-learning problem as the tuple           , where ","is the provability relation (also called the generalization model),  is the language of the background knowledge, ","","is","the language of the (positive and negative) exam-","ples, and  ","is the hypothesis language. The general ILP-learning problem is undecidable. Possible choices to restrict the ILP-problem are: the provability relation, ",", the background knowledge and the hypothesis language. Research in ILP has presented positive results only for very limited subclasses of first-order logic (Kietz and Džeroski, 1994; Cohen, 1995), which are not appropriate to model natural language grammars.","Our grammar induction problem can be formulated as an ILP-learning problem     ","    ","as follows: ","The provability relation,",", is given by robust parsing, and we denote it by ","","",". We use the “parsing as deduction” technique (Shieber et al., 1995). For all syntagmas we can say in polynomial time whether they belong or not to the grammar language. Thus, using the   ","as","generalization model, our grammar induction","problem is decidable."," The language of background knowledge,",", is the set of LWFG rules that are already learned together with elementary syntagmas (i.e., corresponding to the lexicon), which are ground atoms (the variables are made constants)."," The language of examples,","","are syntagmas","of the representative sublanguage, which are","ground atoms. We only have positive examples."," The hypothesis language,","",", is a LWFG lattice whose top element is a conformal grammar, and which preserve the parsing of representative examples.","4.2 Grammar Approximation by Representative Sublanguage Model We have formulated the grammar induction problem in the ILP-setting. The theoretical learning model, 837","called Grammar Approximation by Representative","Sublanguage (GARS), can be formulated as follows:","Given:"," a representative example set",", lexically consistent (i.e., it allows the construction of the grammar lattice","element)"," a finite sublanguage ",", conformal and thus unambiguous, which includes the representative example set,"," ","",". We called this","sublanguage, the representative sublanguage","Learn a grammar , using the above ILP-learning","setting, such that is unique and","    ","","",".","The hypothesis space is a complete grammar lattice, and thus the uniqueness property of the learned grammar is guaranteed by the learnability theorem (i.e., the learned grammar is the lattice top element). This learnability result extends significantly the class of problems learnable by ILP methods.","The GARS model uses two polynomial algorithms for LWFG learning. In the first algorithm, the learner is presented with an ordered set of representative examples (syntagmas), i.e., the examples are ordered from the simplest to the most complex. The reader should remember that for a LWFG",", there exists a partial ordering among the grammar nonterminals, which allows a total ordering of the representative examples of the grammar",". Thus, in this algorithm, the learner has access to the ordered representative syntagmas when learning the grammar. However, in practice it might be difficult to provide the learner with the “true” order of examples, especially when modeling complex language phenomena. The second algorithm is an iterative algorithm that learns starting from a random order of the representative example set. Due to the property of the search space, both algorithms converge to the same target grammar.","Using ILP and theory revision terminology (Greiner, 1999), we can establish the following analogy: syntagmas (examples) are “labeled queries”, the LWFG lattice is the “space of theories”, and a LWFG in the lattice is “a theory.” The firstalgorithm learns from an “empty theory”, while the second algorithm is an instance of “theory revision”, since the grammar (“theory”) learned during the firstiteration, is then revised, by deleting and adding rules.","Both of these algorithms are cover set algorithms. In the first step the most specific grammar rule is generated from the current representative example. The category name annotated in the representative example gives the name of the lhs nonterminal (predicate invention in ILP terminology), while the robust parser returns the minimum number of chunks that cover the representative example. In the second step this most specificrule is generalized using as performance criterion the number of the examples in ","that can be parsed using the candidate grammar rule (hypothesis) together with the previous learned rules. For the full details for these two algorithms, and the proof of their polynomial efficiency, we refer the reader to (Muresan, 2006)."]},{"title":"5 Discussion","paragraphs":["A practical advantage of our GARS model is that instead of writing syntactic-semantic grammars by hand (both rules and constraints), we construct just a small annotated treebank - utterances and their semantic molecules. If the grammar needs to be refined, or enhanced, we only refine, or enhance the representative examples/sublanguage, and not the grammar rules and constraints, which would be a more difficulttask.","We have built a framework to test whether our GARS model can learn diverse and complex linguistic phenomena. We have primarily analyzed a set of definitional-type sentences in the medical do-main. The phenomena covered by our learned grammar includes complex noun phrases (including noun compounds, nominalization), prepositional phrases, relative clauses and reduced relative clauses, finite and non-finiteverbal constructions (including, tense, aspect, negation, and subject-verb agreement), copula to be, and raising and control constructions. We also learned rules for wh-questions (including longdistance dependencies). In Figure 3 we show the ontology-level representation of a definition-type sentence obtained using our learned grammar. It includes the treatment of reduced relative clauses, raising construction (tends to persist, where virus is not the argument of tends but the argument of persist), and noun compounds. The learned grammar together with a semantic interpreter targeted to terminological knowledge has been used in an acquisition-query experiment, where the answers are at the concept level (the querying is a graph 838 Hepatitis B is an acute viral hepatitis caused by a virus that tends to persist in the blood serum. #hepatitis #acute #viral #cause #blood #virus sub kind_of th of duration ag prop locationth #tend #persist #serum #’HepatitisB’ Figure 3: A definition-type sentence and its ontology-based representation obtained using our learned LWFG matching problem where the “wh-w ord” matches the answer concept). A detailed discussion of the linguistic phenomena covered by our learned grammar using the GARS model, as well as the use of this grammar for terminological knowledge acquisition, is given in (Muresan, 2006).","To learn the grammar used in these experiments we annotated 151 representative examples and 448 examples used as a representative sublanguage for generalization. Annotating these examples requires knowledge about categories and their attributes. We used 31 categories (nonterminals) and 37 attributes (e.g., category, head, number, person). In this experiment, we chose the representative examples guided by the type of phenomena we wanted to modeled and which occurred in our corpus. We also used 13 lexical categories (i.e., parts of speech). The learned grammar contains 151 rules and 151 constraints."]},{"title":"6 Conclusion","paragraphs":["We have presented Lexicalized Well-Founded Grammars, a type of constraint-based grammars for natural language specifically designed to en-able learning from representative examples annotated with semantics. We have presented a new grammar learning model and showed that the search space is a complete grammar lattice that guarantees the uniqueness of the learned grammar. Starting from these fundamental theoretical results, there are several directions into which to take this research. A first obvious extension is to have probabilistic-LWFGs. For example, the ontology constraints might not be “hard” constraints, but “soft” ones (because language expressions are more or less likely to be used in a certain context). Investigating where to add probabilities (ontology, grammar rules, or both) is part of our planned future work. Another future extension of this work is to investigate how to automatically select the representative examples from an existing treebank."]},{"title":"References","paragraphs":["Johan Bos, Stephen Clark, Mark Steedman, James R. Curran, and Julia Hockenmaier. 2004. Wide-coverage semantic representations from a CCG parser. In Proceedings of COLING-04.","William Cohen. 1995. Pac-learning recursive logic programs: Negative results. Journal of ArtificialIntelligence Research, 2:541–573.","Rusins Freivalds, Efim B. Kinber, and Rolf Wiehagen. 1993. On the power of inductive inference from good examples. Theoretical Computer Science, 110(1):131–144.","R. Ge and R.J. Mooney. 2005. A statistical semantic parser that integrates syntax and semantics. In Proceedings of CoNLL-2005.","Russell Greiner. 1999. The complexity of theory revision. ArtificialIntelligence Journal, 107(2):175–217.","Aria Haghighi and Dan Klein. 2006. Prototype-driven grammar induction. In Proceedings of ACL’06.","Jörg-Uwe Kietz and Sašo Džeroski. 1994. Inductive logic programming and learnability. ACM SIGART Bulletin., 5(1):22–32.","Smaranda Muresan. 2006. Learning Constraint-based Grammars from Representative Examples: Theory and Applications. Ph.D. thesis, Columbia University. http://www1.cs.columbia.edu/","smara/muresan thesis.pdf.","Fernando C. Pereira and David H.D Warren. 1980. Definite Clause Grammars for language analysis. Artificial Intelligence, 13:231–278.","Stuart Shieber, Hans Uszkoreit, Fernando Pereira, Jane Robinson, and Mabry Tyson. 1983. The formalism and implementation of PATR-II. In Barbara J. Grosz and Mark Stickel, editors, Research on Interactive Acquisition and Use of Knowledge, pages 39–79. SRI In-ternational, Menlo Park, CA, November.","Stuart Shieber, Yves Schabes, and Fernando Pereira. 1995. Principles and implementation of deductive parsing. Journal of Logic Programming, 24(1-2):3– 36.","Shuly Wintner. 1999. Compositional semantics for linguistic formalisms. In Proceedings of the ACL’99.","Luke S. Zettlemoyer and Michael Collins. 2005. Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars. In Proceedings of UAI-05. 839"]}],"references":[{"authors":[{"first":"Johan","last":"Bos"},{"first":"Stephen","last":"Clark"},{"first":"Mark","last":"Steedman"},{"first":"James","middle":"R.","last":"Curran"},{"first":"Julia","last":"Hockenmaier"}],"year":"2004","title":"Wide-coverage semantic representations from a CCG parser","source":"Johan Bos, Stephen Clark, Mark Steedman, James R. Curran, and Julia Hockenmaier. 2004. Wide-coverage semantic representations from a CCG parser. In Proceedings of COLING-04."},{"authors":[{"first":"William","last":"Cohen"}],"year":"1995","title":"Pac-learning recursive logic programs: Negative results","source":"William Cohen. 1995. Pac-learning recursive logic programs: Negative results. Journal of ArtificialIntelligence Research, 2:541–573."},{"authors":[{"first":"Rusins","last":"Freivalds"},{"first":"Efim","middle":"B.","last":"Kinber"},{"first":"Rolf","last":"Wiehagen"}],"year":"1993","title":"On the power of inductive inference from good examples","source":"Rusins Freivalds, Efim B. Kinber, and Rolf Wiehagen. 1993. On the power of inductive inference from good examples. Theoretical Computer Science, 110(1):131–144."},{"authors":[{"first":"R.","last":"Ge"},{"first":"R.","middle":"J.","last":"Mooney"}],"year":"2005","title":"A statistical semantic parser that integrates syntax and semantics","source":"R. Ge and R.J. Mooney. 2005. A statistical semantic parser that integrates syntax and semantics. In Proceedings of CoNLL-2005."},{"authors":[{"first":"Russell","last":"Greiner"}],"year":"1999","title":"The complexity of theory revision","source":"Russell Greiner. 1999. The complexity of theory revision. ArtificialIntelligence Journal, 107(2):175–217."},{"authors":[{"first":"Aria","last":"Haghighi"},{"first":"Dan","last":"Klein"}],"year":"2006","title":"Prototype-driven grammar induction","source":"Aria Haghighi and Dan Klein. 2006. Prototype-driven grammar induction. In Proceedings of ACL’06."},{"authors":[{"first":"Jörg-Uwe","last":"Kietz"},{"first":"Sašo","last":"Džeroski"}],"year":"1994","title":"Inductive logic programming and learnability","source":"Jörg-Uwe Kietz and Sašo Džeroski. 1994. Inductive logic programming and learnability. ACM SIGART Bulletin., 5(1):22–32."},{"authors":[{"first":"Smaranda","last":"Muresan"}],"year":"2006","title":"Learning Constraint-based Grammars from Representative Examples: Theory and Applications","source":"Smaranda Muresan. 2006. Learning Constraint-based Grammars from Representative Examples: Theory and Applications. Ph.D. thesis, Columbia University. http://www1.cs.columbia.edu/"},{"authors":[],"source":"smara/muresan thesis.pdf."},{"authors":[{"first":"Fernando","middle":"C.","last":"Pereira"},{"first":"David","middle":"H. D","last":"Warren"}],"year":"1980","title":"Definite Clause Grammars for language analysis","source":"Fernando C. Pereira and David H.D Warren. 1980. Definite Clause Grammars for language analysis. Artificial Intelligence, 13:231–278."},{"authors":[{"first":"Stuart","last":"Shieber"},{"first":"Hans","last":"Uszkoreit"},{"first":"Fernando","last":"Pereira"},{"first":"Jane","last":"Robinson"},{"first":"Mabry","last":"Tyson"}],"year":"1983","title":"The formalism and implementation of PATR-II","source":"Stuart Shieber, Hans Uszkoreit, Fernando Pereira, Jane Robinson, and Mabry Tyson. 1983. The formalism and implementation of PATR-II. In Barbara J. Grosz and Mark Stickel, editors, Research on Interactive Acquisition and Use of Knowledge, pages 39–79. SRI In-ternational, Menlo Park, CA, November."},{"authors":[{"first":"Stuart","last":"Shieber"},{"first":"Yves","last":"Schabes"},{"first":"Fernando","last":"Pereira"}],"year":"1995","title":"Principles and implementation of deductive parsing","source":"Stuart Shieber, Yves Schabes, and Fernando Pereira. 1995. Principles and implementation of deductive parsing. Journal of Logic Programming, 24(1-2):3– 36."},{"authors":[{"first":"Shuly","last":"Wintner"}],"year":"1999","title":"Compositional semantics for linguistic formalisms","source":"Shuly Wintner. 1999. Compositional semantics for linguistic formalisms. In Proceedings of the ACL’99."},{"authors":[{"first":"Luke","middle":"S.","last":"Zettlemoyer"},{"first":"Michael","last":"Collins"}],"year":"2005","title":"Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars","source":"Luke S. Zettlemoyer and Michael Collins. 2005. Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars. In Proceedings of UAI-05. 839"}],"cites":[{"style":0,"text":"Bos et al., 2004","origin":{"pointer":"/sections/4/paragraphs/1","offset":213,"length":16},"authors":[{"last":"Bos"},{"last":"al."}],"year":"2004","references":["/references/0"]},{"style":0,"text":"Zettlemoyer and Collins, 2005","origin":{"pointer":"/sections/4/paragraphs/1","offset":231,"length":29},"authors":[{"last":"Zettlemoyer"},{"last":"Collins"}],"year":"2005","references":["/references/13"]},{"style":0,"text":"Ge and Mooney, 2005","origin":{"pointer":"/sections/4/paragraphs/1","offset":262,"length":19},"authors":[{"last":"Ge"},{"last":"Mooney"}],"year":"2005","references":["/references/3"]},{"style":0,"text":"Freivalds et al., 1993","origin":{"pointer":"/sections/4/paragraphs/2","offset":260,"length":22},"authors":[{"last":"Freivalds"},{"last":"al."}],"year":"1993","references":["/references/2"]},{"style":0,"text":"Haghighi and Klein (2006)","origin":{"pointer":"/sections/4/paragraphs/2","offset":285,"length":25},"authors":[{"last":"Haghighi"},{"last":"Klein"}],"year":"2006","references":["/references/5"]},{"style":0,"text":"Pereira and Warren, 1980","origin":{"pointer":"/sections/5/paragraphs/0","offset":82,"length":24},"authors":[{"last":"Pereira"},{"last":"Warren"}],"year":"1980","references":["/references/9"]},{"style":0,"text":"Shieber et al., 1983","origin":{"pointer":"/sections/5/paragraphs/83","offset":126,"length":20},"authors":[{"last":"Shieber"},{"last":"al."}],"year":"1983","references":["/references/10"]},{"style":0,"text":"Muresan, 2006","origin":{"pointer":"/sections/5/paragraphs/89","offset":6,"length":13},"authors":[{"last":"Muresan"}],"year":"2006","references":["/references/7"]},{"style":0,"text":"Wintner, 1999","origin":{"pointer":"/sections/5/paragraphs/204","offset":42,"length":13},"authors":[{"last":"Wintner"}],"year":"1999","references":["/references/12"]},{"style":0,"text":"Muresan, 2006","origin":{"pointer":"/sections/6/paragraphs/253","offset":8,"length":13},"authors":[{"last":"Muresan"}],"year":"2006","references":["/references/7"]},{"style":0,"text":"Muresan, 2006","origin":{"pointer":"/sections/6/paragraphs/288","offset":18,"length":13},"authors":[{"last":"Muresan"}],"year":"2006","references":["/references/7"]},{"style":0,"text":"Muresan, 2006","origin":{"pointer":"/sections/6/paragraphs/416","offset":23,"length":13},"authors":[{"last":"Muresan"}],"year":"2006","references":["/references/7"]},{"style":0,"text":"Kietz and Džeroski (1994)","origin":{"pointer":"/sections/7/paragraphs/1","offset":210,"length":25},"authors":[{"last":"Kietz"},{"last":"Džeroski"}],"year":"1994","references":["/references/6"]},{"style":0,"text":"Kietz and Džeroski, 1994","origin":{"pointer":"/sections/7/paragraphs/8","offset":158,"length":24},"authors":[{"last":"Kietz"},{"last":"Džeroski"}],"year":"1994","references":["/references/6"]},{"style":0,"text":"Cohen, 1995","origin":{"pointer":"/sections/7/paragraphs/8","offset":184,"length":11},"authors":[{"last":"Cohen"}],"year":"1995","references":["/references/1"]},{"style":0,"text":"Shieber et al., 1995","origin":{"pointer":"/sections/7/paragraphs/16","offset":47,"length":20},"authors":[{"last":"Shieber"},{"last":"al."}],"year":"1995","references":["/references/11"]},{"style":0,"text":"Greiner, 1999","origin":{"pointer":"/sections/7/paragraphs/53","offset":43,"length":13},"authors":[{"last":"Greiner"}],"year":"1999","references":["/references/4"]},{"style":0,"text":"Muresan, 2006","origin":{"pointer":"/sections/7/paragraphs/55","offset":224,"length":13},"authors":[{"last":"Muresan"}],"year":"2006","references":["/references/7"]},{"style":0,"text":"Muresan, 2006","origin":{"pointer":"/sections/8/paragraphs/1","offset":1742,"length":13},"authors":[{"last":"Muresan"}],"year":"2006","references":["/references/7"]}]}
