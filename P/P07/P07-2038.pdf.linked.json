{"sections":[{"title":"","paragraphs":["Proceedings of the ACL 2007 Demo and Poster Sessions, pages 149–152, Prague, June 2007. c⃝2007 Association for Computational Linguistics"]},{"title":"Empirical Measurements of Lexical Similarity in Noun Phrase Conjuncts Deirdre Hogan","paragraphs":["∗"]},{"title":"Department of Computer Science Trinity College Dublin Dublin 2, Ireland dhogan@computing.dcu.ie Abstract","paragraphs":["The ability to detect similarity in conjunct heads is potentially a useful tool in help-ing to disambiguate coordination structures - a difficult task for parsers. We propose a distributional measure of similarity designed for such a task. We then compare several different measures of word similarity by testing whether they can empirically detect similarity in the head nouns of noun phrase conjuncts in the Wall Street Journal (WSJ) treebank. We demonstrate that several measures of word similarity can successfully detect conjunct head similarity and suggest that the measure proposed in this paper is the most appropriate for this task."]},{"title":"1 Introduction","paragraphs":["Some noun pairs are more likely to be conjoined than others. Take the follow two alternate bracketings: 1. busloads of ((executives) and (their spouses)) and 2. ((busloads of executives) and (their spouses)). The two head nouns coordinated in 1 are executives and spouses, and (incorrectly) in 2: busloads and spouses. Clearly, the former pair of head nouns is more likely and, for the purpose of discrimination, a parsing model would benefit if it could learn that executives and spouses is a more likely combination than busloads and spouses. If nouns co-occurring in coordination patterns are often semantically similar, and if a simi-","∗","Now at the National Centre for Language Technology, Dublin City University, Ireland. larity measure could be defined so that, for example: sim(executives, spouses) > sim(busloads, spouses) then it is potentially useful for coordination disambiguation.","The idea that nouns co-occurring in conjunctions tend to be semantically related has been noted in (Riloff and Shepherd, 1997) and used effectively to automatically cluster semantically similar words (Roark and Charniak, 1998; Caraballo, 1999; Widdows and Dorow, 2002). The tendency for conjoined nouns to be semantically similar has also been exploited for coordinate noun phrase disambiguation by Resnik (1999) who employed a measure of similarity based on WordNet to measure which were the head nouns being conjoined in certain types of coordinate noun phrase.","In this paper we look at different measures of word similarity in order to discover whether they can detect empirically a tendency for conjoined nouns to be more similar than nouns which co-occur but are not conjoined. In Section 2 we introduce a measure of word similarity based on word vectors and in Section 3 we briefly describe some WordNet similarity measures which, in addition to our word vector measure, will be tested in the experiments of Section 4."]},{"title":"2 Similarity based on Coordination Co-occurrences","paragraphs":["The potential usefulness of a similarity measure depends on the particular application. An obvious place to start, when looking at similarity functions for measuring the type of semantic similarity common for coordinate nouns, is a similarity function based on distributional similarity with context de-149 fined in terms of coordination patterns. Our measure of similarity is based on noun co-occurrence information, extracted from conjunctions and lists. We collected co-occurrence data on 82, 579 distinct word types from the BNC and the WSJ treebank.","We extracted all noun pairs from the BNC which occurred in a pattern of the form: noun cc noun1",", as well as lists of any number of nouns separated by commas and ending in cc noun. Each noun in the list is linked with every other noun in the list. Thus for a list: n1, n2, and n3, there will be co-occurrences between words n1 and n2, between n1 and n3 and between n2 and n3. To the BNC data we added all head noun pairs from the WSJ (sections 02 to 21) that occurred together in a coordinate noun phrase.2","From the co-occurrence data we constructed word vectors. Every dimension of a word vector represents another word type and the values of the components of the vector, the term weights, are derived from the coordinate word co-occurrence counts. We used dampened co-occurrence counts, of the form: 1 + log(count), as the term weights for the word vectors. To measure the similarity of two words, w1 and w2, we calculate the cosine of the angle between the two word vectors, ⃗w1 and ⃗w2."]},{"title":"3 WordNet-Based Similarity Measures","paragraphs":["We also examine the following measures of semantic similarity which are WordNet-based.3","Wu and Palmer (1994) propose a measure of similarity of two concepts c1 and c2 based on the depth of concepts in the WordNet hierarchy. Similarity is measured from the depth of the most specificnode dominating both c1 and c2, (their lowest common subsumer), and normalised by the depths of c1 and c2. In (Resnik, 1995) concepts in WordNet are augmented by corpus statistics and an information-theoretic measure of semantic similarity is calculated. Similarity of two concepts is measured","1","It would be preferable to ensure that the pairs extracted are unambiguously conjoined heads. We leave this to future work.","2","We did not include coordinate head nouns from base noun phrases (NPB) (i.e. noun phrases that do not dominate other noun phrases) because the underspecified annotation of NPBs in the WSJ means that the conjoined head nouns can not always be easily identified.","3","All of the WordNet-based similarity measure experiments, as well as a random similarity measure, were carried out with the WordNet::Similarity package, http://search.cpan.org/dist/WordNet-Similarity. by the information content of their lowest common subsumer in the is-a hierarchy of WordNet. Both Jiang and Conrath (1997) and Lin (1998) propose extentions of Resnik’s measure. Leacock and Chodorow (1998)’s measure takes into account the path length between two concepts, which is scaled by the depth of the hierarchy in which they reside. In (Hirst and St-Onge, 1998) similarity is based on path length as well as the number of changes in the direction in the path. In (Banerjee and Pedersen, 2003) semantic relatedness between two concepts is based on the number of shared words in their WordNet definitions (glosses). The gloss of a particular concept is extended to include the glosses of other concepts to which it is related in the WordNet hierarchy. Finally, Patwardhan and Pederson (2006) build on previous work on second-order co-occurrence vectors (Schütze, 1998) by constructing second-order co-occurrence vectors from WordNet glosses, where, as in (Banerjee and Pedersen, 2003), the gloss of a concept is extended so that it includes the gloss of concepts to which it is directly related in WordNet."]},{"title":"4 Experiments","paragraphs":["We selected two sets of data from sections 00, 01, 22 and 24 of the WSJ treebank. The first consists of all nouns pairs which make up the head words of two conjuncts in coordinate noun phrases (again not including coordinate NPBs). We found 601 such coordinate noun pairs. The second data set consists of 601 word pairs which were selected at random from all head-modifier pairs where both head and modifier words are nouns and are not coordinated. We tested the 9 different measures of word similarity just described on each data set in order to see if a significant difference could be detected between the similarity scores for the coordinate words sample and non-coordinate words sample.","Initially both the coordinate and non-coordinate pair samples each contained 601 word pairs. However, before running the experiments we removed all pairs where the words in the pair were identical. This is because identical words occur more often in coordinate head words than in other lexical dependencies (there were 43 pairs with identical words in the coordination set, compared to 3 such pairs in the 150 SimTest ncoord xcoord SDcoord nnonCoord xnonCoord SDnonCoord 95% CI p-value coordDistrib 503 0.11 0.13 485 0.06 0.09 [0.04 0.07] 0.000 (Resnik, 1995) 444 3.19 2.33 396 2.43 2.10 [0.46 1.06] 0.000 (Lin, 1998) 444 0.27 0.26 396 0.19 0.22 [0.04 0.11] 0.000 (Jiang and Conrath, 1997) 444 0.13 0.65 395 0.07 0.08 [-0.01 0.11] 0.083 (Wu and Palmer, 1994) 444 0.63 0.19 396 0.55 0.19 [0.06 0.11] 0.000 (Leacock and Chodorow, 1998) 444 1.72 0.51 396 1.52 0.47 [0.13 0.27] 0.000 (Hirst and St-Onge, 1998) 459 1.599 2.03 447 1.09 1.87 [0.25 0.76] 0.000 (Banerjee and Pedersen, 2003) 451 114.12 317.18 436 82.20 168.21 [-1.08 64.92] 0.058 (Patwardhan and Pedersen, 2006) 459 0.67 0.18 447 0.66 0.2 [-0.02 0.03] 0.545 random 483 0.89 0.17 447 0.88 0.18 [-0.02 0.02] 0.859 Table 1: Summary statistics for 9 different word similarity measures (plus one random measure):ncoord and nnonCoord are the sample sizes for the coordinate and non-coordinate noun pairs samples, respectively; xcoord, SDcoord and xnonCoord, SDnonCoord are the sample means and standard deviations for the two sets. The 95% CI column shows the 95% confidence interval for the difference between the two sample means. The p-value is for a Welch two sample two-sided t-test. coordDistrib is the measure introduced in Section 2. non-coordination set). If we had not removed them, a statistically significantdifference between the similarity scores of the pairs in the two sets could be found simply by using a measure which, say, gave one score for identical words and another (lower) score for all non-identical word pairs.","Results for all similarity measure tests on the data sets described above are displayed in Table 1. In one finalexperiment we used a random measure of similarity. For each experiment we produced two samples, one consisting of the similarity scores given by the similarity measure for the coordinate noun pairs, and another set of similarity scores generated for the non-coordinate pairs. The sample sizes, means, and standard deviations for each experiment are shown in the table. Note that the variation in the sample size is due to coverage: the different measures did not produce a score for all word pairs. Also displayed in Table 1 are the results of statistical significance tests based on the Welsh two sample t-test. A 95% confidence interval for the difference of the sample means is shown along with the p-value."]},{"title":"5 Discussion","paragraphs":["For all but three of the experiments (excluding the random measure), the difference between the mean similarity measures is statistically significant. Interestingly, the three tests where no significant difference was measured between the scores on the coordination set and the non-coordination set (Jiang and Conrath, 1997; Banerjee and Pedersen, 2003; Patwardhan and Pedersen, 2006) were the three top scoring measures in (Patwardhan and Pedersen, 2006), where a subset of six of the above WordNet-based experiments were compared and the measures evaluated against human relatedness judgements and in a word sense disambiguation task. In another comparative study (Budanitsky and Hirst, 2002) of five of the above WordNet-based measures, evaluated as part of a real-word spelling correction system, Jiang and Conrath (1997)’s similarity score performed best. Although performing relatively well under other evaluation criteria, these three measures seem less suited to measuring the kind of similarity occurring in coordinate noun pairs. One possible explanation for the unsuitability of the measures of (Patwardhan and Pedersen, 2006) for the coordinate similarity task could be based on how context is defined when building context vectors. Context for an instance of the the word w is taken to be the words that surround w in the corpus within a given number of positions, where the corpus is taken as all the glosses in WordNet. Words that form part of collocations such as disk drives or task force would then tend to have very similar contexts, and thus such word pairs, from non-coordinate modifier-head relations, could be given too high a similarity score.","Although the difference between the mean similarity scores seems rather slight in all experiments, it is worth noting that not all coordinate head words are semantically related. To take a couple of examples from the coordinate word pair set: work/harmony extracted from hard work and harmony, and power/clause extracted from executive power and the appropriations clause. We would not expect these word pairs to get a high similarity score. On the other hand, it is also possible that 151 some of the examples of non-coordinate dependencies involve semantically similar words. For example, nouns in lists are often semantically similar, and we did not exclude nouns extracted from lists from the non-coordinate test set.","Although not all coordinate noun pairs are semantically similar, it seems clear, on inspection of the two sets of data, that they are more likely to be semantically similar than modifier-head word pairs, and the tests carried out for most of the measures of semantic similarity detect a significantdifference between the similarity scores assigned to coordinate pairs and those assigned to non-coordinate pairs.","It is not possible to judge, based on the significance tests alone, which might be the most useful measure for the purpose of disambiguation. However, in terms of coverage, the distributional measure introduced in Section 2 clearly performs best4",". This measure of distributional similarity is perhaps more suited to the task of coordination disambiguation because it directly measures the type of similarity that occurs between coordinate nouns. That is, the distributional similarity measure presented in Section 2 definestwo words as similar if they occur in coordination patterns with a similar set of words and with similar distributions. Whether the words are semantically similar becomes irrelevant. A measure of semantic similarity, on the other hand, might find words similar which are quite unlikely to appear in coordination patterns. For example, Cederberg and Widdows (2003) note that words appearing in coordination patterns tend to be on the same ontological level: ‘fruit and vegetables’ is quite likely to occur, whereas ‘fruit and apples’ is an unlikely co-occurrence. A WordNet-based measure of semantic similarity, however, might give a high score to both of the noun pairs.","In the future we intend to use the similarity measure outlined in Section 2 in a lexicalised parser to help resolve coordinate noun phrase ambiguities.","Acknowledgements Thanks to the TCD Broad Curriculum Fellowship and to the SFI Research Grant 04/BR/CS370 for funding this research. Thanks also to Pádraig Cunningham, Saturnino Luz and Jennifer Foster for helpful discussions. 4 Somewhat unsurprisingly given it is part trained on data","from the same domain."]},{"title":"References","paragraphs":["Satanjeev Banerjee and Ted Pedersen. 2003 Extended Gloss Overlaps as a Measure of Semantic Relatedness. In Proceeding of the 18th IJCAI.","Alexander Budanitsky and Graeme Hirst. 2002 Semantic Distance in WordNet: An experimental, application-oriented Evaluation of Five Measures In Proceedings of the 3rd CI-CLING.","Sharon Caraballo. 1999 Automatic construction of a hypernym-labeled noun hierarchy from text In Proceedings of the 37th ACL.","Scott Cederberg and Dominic Widdows. 2003. Using LSA and Noun Coordination Information to Improve the Precision and Recall of Automatic Hyponymy Extraction. In Proceedings of the 7th CoNLL.","G. Hirst and D. St-Onge 1998. Lexical Chains as representations of context for the detection and correction of malapropisms. WordNet: An electronic lexical database. MIT Press.","J. Jiang and D. Conrath. 1997. Semantic similarity based on corpus statistics and lexical taxonomy. In Proceedings of the ROCLING.","C. Leacock and M. Chodorow. 1998. Combining local context and WordNet similarity for word sense identification. WordNet: An electronic lexical database. MIT Press.","D. Lin. 1998. An information-theoretic definition of similarity. In Proceedings of the 15th ICML.","Siddharth Patwardhan and Ted Pedersen. 2006. Using WordNet-based Context Vectors to Estimate the Semantic Relatedness of Concepts. In Proceedings of Making Sense of Sense - Bringing Computational Linguistics and Psycholinguistics Together, EACL.","Philip Resnik. 1995. Using Information Content to Evaluate Semantic Similarity. In Proceedings of IJCAI.","Philip Resnik. 1999. Semantic Similarity in a Taxonomy: An Information-Based Measure and its Application to Problems of Ambiguity in Natural Language. In Journal of Artificial Intelligence Research, 11:95-130.","Ellen Riloff and Jessica Shepherd 1997. A Corpus-based Approach for Building Semantic Lexicon. In Proceedings of the 2nd EMNLP.","Brian Roark and Eugene Charniak 1998. Noun-phrase Cooccurrence Statistics for Semi-automatic semantic lexicon construction. In Proceedings of the COLING-ACL.","Hinrich Schütze. 1998. Automatic Word Sense Discrimination. Computational Linguistics, 24(1):97-123.","Dominic Widdows and Beate Dorow. 2002. A Graph Model for Unsupervised Lexical Acquisition. In Proceedings of the 19th COLING.","Zhibiao Wu and Martha Palmer. 1994. Verb Semantics and Lexical Selection. In Proceedings of the ACL. 152"]}],"references":[{"authors":[],"source":"Satanjeev Banerjee and Ted Pedersen. 2003 Extended Gloss Overlaps as a Measure of Semantic Relatedness. In Proceeding of the 18th IJCAI."},{"authors":[],"source":"Alexander Budanitsky and Graeme Hirst. 2002 Semantic Distance in WordNet: An experimental, application-oriented Evaluation of Five Measures In Proceedings of the 3rd CI-CLING."},{"authors":[],"source":"Sharon Caraballo. 1999 Automatic construction of a hypernym-labeled noun hierarchy from text In Proceedings of the 37th ACL."},{"authors":[{"first":"Scott","last":"Cederberg"},{"first":"Dominic","last":"Widdows"}],"year":"2003","title":"Using LSA and Noun Coordination Information to Improve the Precision and Recall of Automatic Hyponymy Extraction","source":"Scott Cederberg and Dominic Widdows. 2003. Using LSA and Noun Coordination Information to Improve the Precision and Recall of Automatic Hyponymy Extraction. In Proceedings of the 7th CoNLL."},{"authors":[{"first":"G.","last":"Hirst"},{"first":"D.","last":"St-Onge"}],"year":"1998","title":"Lexical Chains as representations of context for the detection and correction of malapropisms","source":"G. Hirst and D. St-Onge 1998. Lexical Chains as representations of context for the detection and correction of malapropisms. WordNet: An electronic lexical database. MIT Press."},{"authors":[{"first":"J.","last":"Jiang"},{"first":"D.","last":"Conrath"}],"year":"1997","title":"Semantic similarity based on corpus statistics and lexical taxonomy","source":"J. Jiang and D. Conrath. 1997. Semantic similarity based on corpus statistics and lexical taxonomy. In Proceedings of the ROCLING."},{"authors":[{"first":"C.","last":"Leacock"},{"first":"M.","last":"Chodorow"}],"year":"1998","title":"Combining local context and WordNet similarity for word sense identification","source":"C. Leacock and M. Chodorow. 1998. Combining local context and WordNet similarity for word sense identification. WordNet: An electronic lexical database. MIT Press."},{"authors":[{"first":"D.","last":"Lin"}],"year":"1998","title":"An information-theoretic definition of similarity","source":"D. Lin. 1998. An information-theoretic definition of similarity. In Proceedings of the 15th ICML."},{"authors":[{"first":"Siddharth","last":"Patwardhan"},{"first":"Ted","last":"Pedersen"}],"year":"2006","title":"Using WordNet-based Context Vectors to Estimate the Semantic Relatedness of Concepts","source":"Siddharth Patwardhan and Ted Pedersen. 2006. Using WordNet-based Context Vectors to Estimate the Semantic Relatedness of Concepts. In Proceedings of Making Sense of Sense - Bringing Computational Linguistics and Psycholinguistics Together, EACL."},{"authors":[{"first":"Philip","last":"Resnik"}],"year":"1995","title":"Using Information Content to Evaluate Semantic Similarity","source":"Philip Resnik. 1995. Using Information Content to Evaluate Semantic Similarity. In Proceedings of IJCAI."},{"authors":[{"first":"Philip","last":"Resnik"}],"year":"1999","title":"Semantic Similarity in a Taxonomy: An Information-Based Measure and its Application to Problems of Ambiguity in Natural Language","source":"Philip Resnik. 1999. Semantic Similarity in a Taxonomy: An Information-Based Measure and its Application to Problems of Ambiguity in Natural Language. In Journal of Artificial Intelligence Research, 11:95-130."},{"authors":[{"first":"Ellen","last":"Riloff"},{"first":"Jessica","last":"Shepherd"}],"year":"1997","title":"A Corpus-based Approach for Building Semantic Lexicon","source":"Ellen Riloff and Jessica Shepherd 1997. A Corpus-based Approach for Building Semantic Lexicon. In Proceedings of the 2nd EMNLP."},{"authors":[{"first":"Brian","last":"Roark"},{"first":"Eugene","last":"Charniak"}],"year":"1998","title":"Noun-phrase Cooccurrence Statistics for Semi-automatic semantic lexicon construction","source":"Brian Roark and Eugene Charniak 1998. Noun-phrase Cooccurrence Statistics for Semi-automatic semantic lexicon construction. In Proceedings of the COLING-ACL."},{"authors":[{"first":"Hinrich","last":"Schütze"}],"year":"1998","title":"Automatic Word Sense Discrimination","source":"Hinrich Schütze. 1998. Automatic Word Sense Discrimination. Computational Linguistics, 24(1):97-123."},{"authors":[{"first":"Dominic","last":"Widdows"},{"first":"Beate","last":"Dorow"}],"year":"2002","title":"A Graph Model for Unsupervised Lexical Acquisition","source":"Dominic Widdows and Beate Dorow. 2002. A Graph Model for Unsupervised Lexical Acquisition. In Proceedings of the 19th COLING."},{"authors":[{"first":"Zhibiao","last":"Wu"},{"first":"Martha","last":"Palmer"}],"year":"1994","title":"Verb Semantics and Lexical Selection","source":"Zhibiao Wu and Martha Palmer. 1994. Verb Semantics and Lexical Selection. In Proceedings of the ACL. 152"}],"cites":[{"style":0,"text":"Riloff and Shepherd, 1997","origin":{"pointer":"/sections/3/paragraphs/3","offset":100,"length":25},"authors":[{"last":"Riloff"},{"last":"Shepherd"}],"year":"1997","references":["/references/11"]},{"style":0,"text":"Roark and Charniak, 1998","origin":{"pointer":"/sections/3/paragraphs/3","offset":201,"length":24},"authors":[{"last":"Roark"},{"last":"Charniak"}],"year":"1998","references":["/references/12"]},{"style":0,"text":"Caraballo, 1999","origin":{"pointer":"/sections/3/paragraphs/3","offset":227,"length":15},"authors":[{"last":"Caraballo"}],"year":"1999","references":[]},{"style":0,"text":"Widdows and Dorow, 2002","origin":{"pointer":"/sections/3/paragraphs/3","offset":244,"length":23},"authors":[{"last":"Widdows"},{"last":"Dorow"}],"year":"2002","references":["/references/14"]},{"style":0,"text":"Resnik (1999)","origin":{"pointer":"/sections/3/paragraphs/3","offset":399,"length":13},"authors":[{"last":"Resnik"}],"year":"1999","references":["/references/10"]},{"style":0,"text":"Wu and Palmer (1994)","origin":{"pointer":"/sections/5/paragraphs/1","offset":0,"length":20},"authors":[{"last":"Wu"},{"last":"Palmer"}],"year":"1994","references":["/references/15"]},{"style":0,"text":"Resnik, 1995","origin":{"pointer":"/sections/5/paragraphs/1","offset":305,"length":12},"authors":[{"last":"Resnik"}],"year":"1995","references":["/references/9"]},{"style":0,"text":"Jiang and Conrath (1997)","origin":{"pointer":"/sections/5/paragraphs/7","offset":298,"length":24},"authors":[{"last":"Jiang"},{"last":"Conrath"}],"year":"1997","references":["/references/5"]},{"style":0,"text":"Lin (1998)","origin":{"pointer":"/sections/5/paragraphs/7","offset":327,"length":10},"authors":[{"last":"Lin"}],"year":"1998","references":["/references/7"]},{"style":0,"text":"Leacock and Chodorow (1998)","origin":{"pointer":"/sections/5/paragraphs/7","offset":378,"length":27},"authors":[{"last":"Leacock"},{"last":"Chodorow"}],"year":"1998","references":["/references/6"]},{"style":0,"text":"Hirst and St-Onge, 1998","origin":{"pointer":"/sections/5/paragraphs/7","offset":545,"length":23},"authors":[{"last":"Hirst"},{"last":"St-Onge"}],"year":"1998","references":["/references/4"]},{"style":0,"text":"Banerjee and Pedersen, 2003","origin":{"pointer":"/sections/5/paragraphs/7","offset":672,"length":27},"authors":[{"last":"Banerjee"},{"last":"Pedersen"}],"year":"2003","references":[]},{"style":0,"text":"Patwardhan and Pederson (2006)","origin":{"pointer":"/sections/5/paragraphs/7","offset":967,"length":30},"authors":[{"last":"Patwardhan"},{"last":"Pederson"}],"year":"2006","references":[]},{"style":0,"text":"Schütze, 1998","origin":{"pointer":"/sections/5/paragraphs/7","offset":1060,"length":13},"authors":[{"last":"Schütze"}],"year":"1998","references":["/references/13"]},{"style":0,"text":"Banerjee and Pedersen, 2003","origin":{"pointer":"/sections/5/paragraphs/7","offset":1162,"length":27},"authors":[{"last":"Banerjee"},{"last":"Pedersen"}],"year":"2003","references":[]},{"style":0,"text":"Resnik, 1995","origin":{"pointer":"/sections/6/paragraphs/1","offset":546,"length":12},"authors":[{"last":"Resnik"}],"year":"1995","references":["/references/9"]},{"style":0,"text":"Lin, 1998","origin":{"pointer":"/sections/6/paragraphs/1","offset":607,"length":9},"authors":[{"last":"Lin"}],"year":"1998","references":["/references/7"]},{"style":0,"text":"Jiang and Conrath, 1997","origin":{"pointer":"/sections/6/paragraphs/1","offset":665,"length":23},"authors":[{"last":"Jiang"},{"last":"Conrath"}],"year":"1997","references":["/references/5"]},{"style":0,"text":"Wu and Palmer, 1994","origin":{"pointer":"/sections/6/paragraphs/1","offset":738,"length":19},"authors":[{"last":"Wu"},{"last":"Palmer"}],"year":"1994","references":["/references/15"]},{"style":0,"text":"Leacock and Chodorow, 1998","origin":{"pointer":"/sections/6/paragraphs/1","offset":806,"length":26},"authors":[{"last":"Leacock"},{"last":"Chodorow"}],"year":"1998","references":["/references/6"]},{"style":0,"text":"Hirst and St-Onge, 1998","origin":{"pointer":"/sections/6/paragraphs/1","offset":881,"length":23},"authors":[{"last":"Hirst"},{"last":"St-Onge"}],"year":"1998","references":["/references/4"]},{"style":0,"text":"Banerjee and Pedersen, 2003","origin":{"pointer":"/sections/6/paragraphs/1","offset":954,"length":27},"authors":[{"last":"Banerjee"},{"last":"Pedersen"}],"year":"2003","references":[]},{"style":0,"text":"Patwardhan and Pedersen, 2006","origin":{"pointer":"/sections/6/paragraphs/1","offset":1039,"length":29},"authors":[{"last":"Patwardhan"},{"last":"Pedersen"}],"year":"2006","references":["/references/8"]},{"style":0,"text":"Jiang and Conrath, 1997","origin":{"pointer":"/sections/7/paragraphs/0","offset":300,"length":23},"authors":[{"last":"Jiang"},{"last":"Conrath"}],"year":"1997","references":["/references/5"]},{"style":0,"text":"Banerjee and Pedersen, 2003","origin":{"pointer":"/sections/7/paragraphs/0","offset":325,"length":27},"authors":[{"last":"Banerjee"},{"last":"Pedersen"}],"year":"2003","references":[]},{"style":0,"text":"Patwardhan and Pedersen, 2006","origin":{"pointer":"/sections/7/paragraphs/0","offset":354,"length":29},"authors":[{"last":"Patwardhan"},{"last":"Pedersen"}],"year":"2006","references":["/references/8"]},{"style":0,"text":"Patwardhan and Pedersen, 2006","origin":{"pointer":"/sections/7/paragraphs/0","offset":425,"length":29},"authors":[{"last":"Patwardhan"},{"last":"Pedersen"}],"year":"2006","references":["/references/8"]},{"style":0,"text":"Budanitsky and Hirst, 2002","origin":{"pointer":"/sections/7/paragraphs/0","offset":667,"length":26},"authors":[{"last":"Budanitsky"},{"last":"Hirst"}],"year":"2002","references":[]},{"style":0,"text":"Jiang and Conrath (1997)","origin":{"pointer":"/sections/7/paragraphs/0","offset":801,"length":24},"authors":[{"last":"Jiang"},{"last":"Conrath"}],"year":"1997","references":["/references/5"]},{"style":0,"text":"Patwardhan and Pedersen, 2006","origin":{"pointer":"/sections/7/paragraphs/0","offset":1107,"length":29},"authors":[{"last":"Patwardhan"},{"last":"Pedersen"}],"year":"2006","references":["/references/8"]},{"style":0,"text":"Cederberg and Widdows (2003)","origin":{"pointer":"/sections/7/paragraphs/4","offset":612,"length":28},"authors":[{"last":"Cederberg"},{"last":"Widdows"}],"year":"2003","references":["/references/3"]}]}
