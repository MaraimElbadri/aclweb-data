{"sections":[{"title":"","paragraphs":["Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 25–28, Sydney, July 2006. c⃝2006 Association for Computational Linguistics"]},{"title":"FERRET: Interactive Question-Answering for Real-World Environments Andrew Hickl, Patrick Wang, John Lehmann, and Sanda Harabagiu Language Computer Corporation 1701 North Collins Boulevard Richardson, Texas 75080 USA ferret@languagecomputer.com Abstract","paragraphs":["This paper describes FERRET, an interactive question-answering (Q/A) system designed to address the challenges of integrating automatic Q/A applications into real-world environments. FERRET utilizes a novel approach to Q/A known as predictive questioning which attempts to identify the questions (and answers) that users need by analyzing how a user interacts with a system while gathering information related to a particular scenario."]},{"title":"1 Introduction","paragraphs":["As the accuracy of today’s best factoid question-answering (Q/A) systems (Harabagiu et al., 2005; Sun et al., 2005) approaches 70%, research has be-gun to address the challenges of integrating automatic Q/A systems into real-world environments. A new class of applications known as interactive Q/A systems are now being developed which allow users to ask questions in the context of extended dialogues in order to gather information related to any number of complex scenarios. In this paper, we describe our interactive Q/A system known as FERRET which uses an approach based on predictive questioning in order to meet the changing information needs of users over the course of a Q/A dialogue.","Answering questions in an interactive setting poses three new types of challenges for traditional Q/A systems. First, since current Q/A systems are designed to answer single questions in isolation, interactive Q/A systems must look for ways to foster interaction with a user throughout all phases of the research process. Unlike traditional Q/A applications, interactive Q/A systems must do more than cooperatively answer a user’s single question. Instead, in order to keep a user collaborating with the system, interactive Q/A systems need to provide access to new types of information that are somehow relevant to the user’s stated and unstated information needs.","Second, we have found that users of Q/A systems in real-world settings often ask questions that are much more complex than the types of factoid questions that have been evaluated in the an-nual Text Retrieval Conference (TREC) evaluations. When faced with a limited period of time to gather information, even experienced users of Q/A may nd it difcult to translate their information needs into the simpler types of questions that Q/A systems can answer. In order to provide effective answers to these questions, interactive question-answering systems need to include question decomposition techniques that can break down complex questions into the types of simpler factoid-like questions that traditional Q/A systems were designed to answer.","Finally, interactive Q/A systems must be sensitive not only to the content of a user’s question but also to the context that it is asked in. Like other types of task-oriented dialogue systems, interactive Q/A systems need to model both what a user knows and what a user wants to know over the course of a Q/A dialogue: systems that fail to represent a user’s knowledge base run the risk of returning redundant information, while systems that do not model a user’s intentions can end up returning irrelevant information.","In the rest of this paper, we discuss how the FERRET interactive Q/A system currently addresses the rst two of these three challenges. 25 Figure 1: The FERRET Interactive Q/A System"]},{"title":"2 The F","paragraphs":["ERRET"]},{"title":"Interactive Question-Answering System","paragraphs":["This section provides a basic overview of the functionality provided by the FERRET interactive Q/A system. 1","FERRET returns three types of information in response to a user’s query. First, FERRET utilizes an automatic Q/A system to nd answers to users’ questions in a document collection. In order to provide users with the timely results that they expect from information gathering applications (such as Internet search engines), every effort was made to reduce the time FERRET takes to extract answers from text. (In the current version of the system, answers are returned on average in 12.78 seconds. 2",")","In addition to answers, FERRET also provides information in the form of two different types of predictive question-answer pairs (or QUABs). With FERRET, users can select from QUABs that","1","For more details on FERRET’s question-answering capabilities, the reader is invited to consult (Harabagiu et al., 2005a); for more information on FERRET’s predictive question generation component, please see (Harabagiu et al., 2005b).","2","This test was run on a machine with a Pentium 4 3.0 GHz processor with 2 GB of RAM. were either generated automatically from the set of documents returned by the Q/A system or that were selected from a large database of more than 10,000 question-answer pairs created ofine by human annotators. In the current version of FERRET, the top 10 automatically-generated and handcrafted QUABs that are most judged relevant to the user’s original question are returned to the user as potential continuations of the dialogue. Each set of QUABs is presented in a separate pane found to the right of the answers returned by the Q/A system; QUABs are ranked in order of relevance to the user’s original query.","Figure 1 provides a screen shot of FERRET’s interface. Q/A answers are presented in the center pane of the FERRET browser, while QUAB question-answer pairs are presented in two separate tabs found in the rightmost pane of the browser. FERRET’s leftmost pane includes a drag-and-drop clipboard which facilitates notetaking and annotation over the course of an interactive Q/A dialogue."]},{"title":"3 Predictive Question-Answering","paragraphs":["First introduced in (Harabagiu et al., 2005b), a predictive questioning approach to automatic 26 question-answering assumes that Q/A systems can use the set of documents relevant to a user’s query in order to generate sets of questions known as predictive questions that anticipate a user’s information needs. Under this approach, topic representations like those introduced in (Lin and Hovy, 2000) and (Harabagiu, 2004) are used to identify a set of text passages that are relevant to a user’s do-main of interest. Topic-relevant passages are then semantically parsed (using a PropBank-style semantic parser) and submitted to a question generation module, which uses a set of syntactic rewrite rules in order to create natural language questions from the original passage.","Generated questions are then assembled into question-answer pairs known as QUABs with the original passage serving as the question’s answer, and are then returned to the user. For example, two of the predictive question-answer pairs generated from the documents returned for question Q0, What has been the impact of job outsourcing programs on India’s relationship with the U.S.?, are presented in Table 1.","Q0 What has been the impact of job outsourcing programs on India’s","relationship with the U.S.? PQ1 How could India respond to U.S. efforts to limit job outsourcing?","A1 U.S. ofcials have countered that the best way for India to","counter U.S. efforts to limit job outsourcing is to further liber-","alize its markets. PQ2 What benets does outsourcing provide to India?","A2 India’s prowess in outsourcing is no longer the only reason why","outsourcing to India is an attractive option. The difference lies","in the scalability of major Indian vendors, their strong focus on","quality and their experience delivering a wide range of services,","says John Blanco, senior vice president at Cablevision Systems","Corp. in Bethpage, N.Y. PQ2 Besides India, what other countries are popular destinations for","outsourcing?","A2 A number of countries are now beginning to position themselves","as outsourcing centers including China, Russia, Malaysia, the","Philippines, South Africa and several countries in Eastern Eu-","rope. Table 1: Predictive Question-Answer Pairs","While neither PQ1 nor PQ2 provide users with an exact answer to the original question Q0, both QUABs can be seen as providing users information which is complementary to acquiring information on the topic of job outsourcing: PQ1 provides details on how India could respond to anti-outsourcing legislation, while PQ2 talks about other countries that are likely targets for outsourcing.","We believe that QUABs can play an important role in fostering extended dialogue-like interactions with users. We have observed that the incorporation of predictive-question answer pairs into an interactive question-answering system like FERRET can promote dialogue-like interactions between users and the system. When presented with a set of QUAB questions, users typically selected a coherent set of follow-on questions which served to elaborate or clarify their initial question. The dialogue fragment in Table 2 provides an example of the kinds of dialogues that users can generate by interacting with the predictive questions that FERRET generates. UserQ1: What has been the impact of job outsourcing programs","on India’s relationship with the U.S.? QUAB1: How could India respond to U.S. efforts to limit job out-","sourcing? QUAB2: Besides India, what other countries are popular destinations","for outsourcing? UserQ2: What industries are outsourcing jobs to India? QUAB3: Which U.S. technology companies have opened customer","service departments in India? QUAB4: Will Dell follow through on outsourcing technical support","jobs to India? QUAB5: Why do U.S. companies nd India an attractive destination","for outsourcing? UserQ3: What anti-outsourcing legislation has been considered in","the U.S.? QUAB6: Which Indiana legislator introduced a bill that would make","it illegal to outsource Indiana jobs? QUAB7: What U.S. Senators have come out against anti-outsourcing","legislation? Table 2: Dialogue Fragment","In experiments with human users of FERRET, we have found that QUAB pairs enhanced the quality of information retrieved that users were able to retrieve during a dialogue with the system. 3","In 100 user dialogues with FERRET, users clicked hyperlinks associated with QUAB pairs 56.7% of the time, despite the fact the system returned (on average) approximately 20 times more answers than QUAB pairs. Users also derived value from information contained in QUAB pairs: reports written by users who had access to QUABs while gathering information were judged to be signicantly (p < 0.05) better than those reports written by users who only had access to FERRET’s Q/A system alone."]},{"title":"4 Answering Complex Questions","paragraphs":["As was mentioned in Section 2, FERRET uses a special dialogue-optimized version of an automatic question-answering system in order to nd high-precision answers to users’ questions in a document collection.","During a Q/A dialogue, users of interactive Q/A systems frequently ask complex questions that must be decomposed syntactically and semantically before they can be answered using traditional Q/A techniques. Complex questions submitted to 3 For details of user experiments with FERRET, please","see (Harabagiu et al., 2005b). 27 FERRET are rst subject to a set of syntactic decomposition heuristics which seek to extract each overtly-mentioned subquestion from the original question. Under this approach, questions featuring coordinated question stems, entities, verb phrases, or clauses are split into their separate conjuncts; answers to each syntactically decomposed question are presented separately to the user. Table 3 provides an example of syntactic decomposition performed in FERRET.","CQ1 What industries have been outsourcing or offshoring jobs","to India or Malaysia?","QD1 What industries have been outsourcing jobs to India?","QD2 What industries have been offshoring jobs to India?","QD3 What industries have been outsourcing jobs to Malaysia?","QD4 What industries have been offshoring jobs to Malaysia? Table 3: Syntactic Decomposition","FERRET also performs semantic decomposition of complex questions using techniques rst outlined in (Harabagiu et al., 2006). Under this approach, three types of semantic and pragmatic information are identied in complex questions: (1) information associated with a complex question’s expected answer type, (2) semantic dependencies derived from predicate-argument structures discovered in the question, and (3) and topic information derived from documents retrieved using the keywords contained the question. Examples of the types of automatic semantic decomposition that is performed in FERRET is presented in Table 4.","CQ2 What has been the impact of job outsourcing programs","on India’s relationship with the U.S.?","QD5 What is meant by India’s relationship with the U.S.?","QD6 What outsourcing programs involve India and the U.S.?","QD7 Who has started outsourcing programs for India and the","U.S.?","QD8 What statements were made regarding outsourcing on In-","dia’s relationship with the U.S.? Table 4: Semantic Question Decomposition","Complex questions are decomposed by a procedure that operates on a Markov chain, by following a random walk on a bipartite graph of question decompositions and relations relevant to the topic of the question. Unlike with syntactic decomposition, FERRET combines answers from semantically decomposed question automatically and presents users with a single set of answers that represents the contributions of each question. Users are notied that semantic decomposition has occurred, however; decomposed questions are displayed to the user upon request.","In addition to techniques for answering complex questions, FERRET’s Q/A system improves performance for a variety of question types by employing separate question processing strategies in order to provide answers to four different types of questions, including factoid questions, list questions, relationship questions, and denition questions."]},{"title":"5 Conclusions","paragraphs":["We created FERRET as part of a larger effort designed to address the challenges of integrating automatic question-answering systems into real-world research environments. We have focused on two components that have been implemented into the latest version of FERRET: (1) predictive questioning, which enables systems to provide users with question-answer pairs that may anticipate their information needs, and (2) question decomposition, which serves to break down complex questions into sets of conceptually-simpler questions that Q/A systems can answer successfully."]},{"title":"6 Acknowledgments","paragraphs":["This material is based upon work funded in whole or in part by the U.S. Government and any opinions, ndings, conclusions, or recommendations expressed in this material are those of the authors and do not necessarily reect the views of the U.S. Government."]},{"title":"References","paragraphs":["S. Harabagiu, D. Moldovan, C. Clark, M. Bowden, A. Hickl, and P. Wang. 2005a. Employing Two Question Answering Systems in TREC 2005. In Proceedings of the Fourteenth Text REtrieval Conference.","Sanda Harabagiu, Andrew Hickl, John Lehmann, and Dan Moldovan. 2005b. Experiments with Interactive Question-Answering. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05).","Sanda Harabagiu, Finley Lacatusu, and Andrew Hickl. 2006. Answering complex questions with random walk models. In Proceedings of the 29th Annual International ACM SI-GIR Conference on Research and Development in Infor-mation Retrieval, Seattle, WA.","Sanda Harabagiu. 2004. Incremental Topic Representations. In Proceedings of the 20th COLING Conference, Geneva, Switzerland.","Chin-Yew Lin and Eduard Hovy. 2000. The automated acquisition of topic signatures for text summariza-tion. In Proceedings of the 18th COLING Conference, Saarbr¤ucken, Germany.","R. Sun, J. Jiang, Y. F. Tan, H. Cui, T.-S. Chua, and M.-Y. Kan. 2005. Using Syntactic and Semantic Relation Analysis in Question Answering. In Proceedings of The Fourteenth Text REtrieval Conference (TREC 2005). 28"]}],"references":[{"authors":[{"first":"S.","last":"Harabagiu"},{"first":"D.","last":"Moldovan"},{"first":"C.","last":"Clark"},{"first":"M.","last":"Bowden"},{"first":"A.","last":"Hickl"},{"first":"P.","last":"Wang"}],"year":"2005a","title":"Employing Two Question Answering Systems in TREC 2005","source":"S. Harabagiu, D. Moldovan, C. Clark, M. Bowden, A. Hickl, and P. Wang. 2005a. Employing Two Question Answering Systems in TREC 2005. In Proceedings of the Fourteenth Text REtrieval Conference."},{"authors":[{"first":"Sanda","last":"Harabagiu"},{"first":"Andrew","last":"Hickl"},{"first":"John","last":"Lehmann"},{"first":"Dan","last":"Moldovan"}],"year":"2005b","title":"Experiments with Interactive Question-Answering","source":"Sanda Harabagiu, Andrew Hickl, John Lehmann, and Dan Moldovan. 2005b. Experiments with Interactive Question-Answering. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05)."},{"authors":[{"first":"Sanda","last":"Harabagiu"},{"first":"Finley","last":"Lacatusu"},{"first":"Andrew","last":"Hickl"}],"year":"2006","title":"Answering complex questions with random walk models","source":"Sanda Harabagiu, Finley Lacatusu, and Andrew Hickl. 2006. Answering complex questions with random walk models. In Proceedings of the 29th Annual International ACM SI-GIR Conference on Research and Development in Infor-mation Retrieval, Seattle, WA."},{"authors":[{"first":"Sanda","last":"Harabagiu"}],"year":"2004","title":"Incremental Topic Representations","source":"Sanda Harabagiu. 2004. Incremental Topic Representations. In Proceedings of the 20th COLING Conference, Geneva, Switzerland."},{"authors":[{"first":"Chin-Yew","last":"Lin"},{"first":"Eduard","last":"Hovy"}],"year":"2000","title":"The automated acquisition of topic signatures for text summariza-tion","source":"Chin-Yew Lin and Eduard Hovy. 2000. The automated acquisition of topic signatures for text summariza-tion. In Proceedings of the 18th COLING Conference, Saarbr¤ucken, Germany."},{"authors":[{"first":"R.","last":"Sun"},{"first":"J.","last":"Jiang"},{"first":"Y.","middle":"F.","last":"Tan"},{"first":"H.","last":"Cui"},{"first":"T.","middle":"-S.","last":"Chua"},{"first":"M.","middle":"-Y.","last":"Kan"}],"year":"2005","title":"Using Syntactic and Semantic Relation Analysis in Question Answering","source":"R. Sun, J. Jiang, Y. F. Tan, H. Cui, T.-S. Chua, and M.-Y. Kan. 2005. Using Syntactic and Semantic Relation Analysis in Question Answering. In Proceedings of The Fourteenth Text REtrieval Conference (TREC 2005). 28"}],"cites":[{"style":0,"text":"Harabagiu et al., 2005","origin":{"pointer":"/sections/2/paragraphs/0","offset":74,"length":22},"authors":[{"last":"Harabagiu"},{"last":"al."}],"year":"2005","references":[]},{"style":0,"text":"Sun et al., 2005","origin":{"pointer":"/sections/2/paragraphs/0","offset":98,"length":16},"authors":[{"last":"Sun"},{"last":"al."}],"year":"2005","references":["/references/5"]},{"style":0,"text":"Harabagiu et al., 2005a","origin":{"pointer":"/sections/4/paragraphs/5","offset":96,"length":23},"authors":[{"last":"Harabagiu"},{"last":"al."}],"year":"2005a","references":["/references/0"]},{"style":0,"text":"Harabagiu et al., 2005b","origin":{"pointer":"/sections/4/paragraphs/5","offset":209,"length":23},"authors":[{"last":"Harabagiu"},{"last":"al."}],"year":"2005b","references":["/references/1"]},{"style":0,"text":"Harabagiu et al., 2005b","origin":{"pointer":"/sections/5/paragraphs/0","offset":21,"length":23},"authors":[{"last":"Harabagiu"},{"last":"al."}],"year":"2005b","references":["/references/1"]},{"style":0,"text":"Lin and Hovy, 2000","origin":{"pointer":"/sections/5/paragraphs/0","offset":379,"length":18},"authors":[{"last":"Lin"},{"last":"Hovy"}],"year":"2000","references":["/references/4"]},{"style":0,"text":"Harabagiu, 2004","origin":{"pointer":"/sections/5/paragraphs/0","offset":404,"length":15},"authors":[{"last":"Harabagiu"}],"year":"2004","references":["/references/3"]},{"style":0,"text":"Harabagiu et al., 2005b","origin":{"pointer":"/sections/6/paragraphs/2","offset":5,"length":23},"authors":[{"last":"Harabagiu"},{"last":"al."}],"year":"2005b","references":["/references/1"]},{"style":0,"text":"Harabagiu et al., 2006","origin":{"pointer":"/sections/6/paragraphs/9","offset":99,"length":22},"authors":[{"last":"Harabagiu"},{"last":"al."}],"year":"2006","references":["/references/2"]}]}
