{"sections":[{"title":"","paragraphs":["Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 729–736, Sydney, July 2006. c⃝2006 Association for Computational Linguistics"]},{"title":"Phoneme-to-Text Transcription System with an Infinite Vocabulary Shinsuke Mori Daisuke Takuma Gakuto Kurata IBM Research, Tokyo Research Laboratory, IBM Japan, Ltd. 1623-14 Shimotsuruma Yamato-shi, 242-8502, Japan mori@fw.ipsj.or.jp Abstract","paragraphs":["The noisy channel model approach is successfully applied to various natural language processing tasks. Currently the main research focus of this approach is adaptation methods, how to capture characteristics of words and expressions in a target domain given example sentences in that domain. As a solution we describe a method enlarging the vocabulary of a language model to an almost infinite size and capturing their context information. Especially the new method is suitable for languages in which words are not delimited by whitespace. We applied our method to a phoneme-to-text transcription task in Japanese and reduced about 10% of the errors in the results of an existing method."]},{"title":"1 Introduction","paragraphs":["The noisy channel model approach is being successfully applied to various natural language processing (NLP) tasks, such as speech recognition (Jelinek, 1985), spelling correction (Kernighan et al., 1990), machine translation (Brown et al., 1990), etc. In this approach an NLP system is composed of two modules: one is a task-dependent part (an acoustic model for speech recognition) which describes a relationship between an input signal sequence and a word, the other is a language model (LM) which measures the likelihood of a sequence of words as a sentence in the language. Since the LM is a common part, its improvement augments the accuracies of all NLP systems based on a noisy channel model.","Recently the main research focus of LM is shift-ing to the adaptation method, how to capture the characteristics of words and expressions in a target domain. The standard adaptation method is to prepare a corpus in the application domain, count the frequencies of words and word sequences, and manually annotate new words with their input signal sequences to be added to the vocabulary. It is now easy to gather machine-readable sentences in various domains because of the ease of publication and access via the Web (Kilgarriff and Grefenstette, 2003). In addition, traditional machine-readable forms of medical reports or business reports are also available. When we need to develop an NLP system in various domains, there is a huge but unannotated corpus.","For languages, such as Japanese and Chinese, in which the words are not delimited by whitespace, one encounters a word identification problem before counting the frequencies of words and word sequences. To solve this problem one must have a good word segmenter in the domain of the corpus. The only robust and reliable word segmenter in the domain is, however, a word segmenter based on the statistics of the lexicons in the domain! Thus we are obliged to pay a high cost for the manual annotation of a corpus for each new subject domain.","In this paper, we propose a novel framework for building an NLP system based on a noisy channel model with an almost infinite vocabulary. In our method, first we estimate the probability of a word boundary existing between two characters at each point of a raw corpus in the target domain. Using these probabilities we regard the corpus as a stochastically segmented corpus (SSC). We then estimate word","-gram probabilities from the SSC. Then we build an NLP system, the phoneme-to-text transcription system in this paper. To describe the stochastic relationship between a character sequence and its phoneme sequence, we also propose a character-based unknown word model. With this unknown word model and a word","- gram model estimated from the SSC, the vocabulary of our LM, a set of known words with their context information, is expanded from words in a 729 small annotated corpus to an almost infinite size, including all substrings appearing in the large corpus in the target domain. In experiments, we estimated LMs from a relatively small annotated corpus in the general domain and a large raw corpus in the target domain. A phoneme-to-text transcription system based on our LM and unknown word model eliminated about 10% of the errors in the results of an existing method."]},{"title":"2 Task Complexity","paragraphs":["In this section we explain the phoneme-to-text transcription task which our new framework is applied to. 2.1 Phoneme-to-text Transcription To input a sentence in a language using a device with fewer keys than the alphabet we need some kind of transcription system. In French stenotypy, for example, a special keyboard with 21 keys is used to input French letters with accents (Derouault and Merialdo, 1986). A similar problem arises when we write an e-mail in any language with a mobile phone or a PDA. For languages with a much larger character set, such as Chinese, Japanese, and Korean, a transcription system called an input method is indispensable for writing on a computer (Lunde, 1998).","The task we chose for the evaluation of our method is phoneme-to-text transcription in Japanese, which can also be regarded as a pseudo-speech recognition in which the acoustic model is perfect. In order to input Japanese to a computer, the user types phoneme sequences and the computer offers possible transcription candidates in the descending order of their estimated similarities to the characters the user wants to input. Then the user chooses the proper one. 2.2 Ambiguities A phoneme sequence in Japanese (written in sansserif font in this paper) is highly ambiguous for a computer. There are many possible word sequences with similar pronunciations. These ambiguities are mainly due to three factors:"," Homonyms: There are many words sharing the same phoneme sequences. In the spoken language, they are less ambiguous since they are  Generally one of Japanese phonogram sets is used as","phoneme. A phonogram is input by a combination of un-","ambiguous ASCII characters. pronounced with different intonations. Intonational signals are, however, omitted in the input of phoneme-to-text transcription.","","Lack of word boundaries: A word of a long","sequence of phonemes can be split into sev-","eral shorter words, such as frequent content","words, particles, etc. (ex.","-","-","-","-","/thanks","vs.","-","/ant","/is","-","/ten).","","Variations in writing: Some words have more","than one acceptable spellings. For example,","/","- - ","-","/bank-transfer is often writ-","ten as / -","-","","-","omitting two verbal end-","ings, especially in business writing. Most of these ambiguities are not difficult to re-solve for a native speaker who is familiar with the domain. So the transcription system should offer the candidate word sequences for each context and domain. 2.3 Available Resources Generally speaking, three resources are available for a phoneme-to-text transcription based on the noisy channel model:"," annotated corpus: a small corpus in the general domain annotated with word boundary information and phoneme sequences for each word"," single character dictionary: a dictionary containing all possible phoneme sequences for each single character"," raw corpus in the target domain: a collection of text samples in the target domain extracted from the Web or documents in machine-readable form"]},{"title":"3 Language Model and its Application","paragraphs":["A stochastic LM","is a function from a sequence of characters","","","to the probability. The summation over all possible sequences of characters must be equal to or less than 1. This probability is used as the likelihood in the NLP system.","3.1 Word -gram Model","The most famous LM is an","-gram model based","on words. In this model, a sentence is regarded as","a word sequence","","","(  ","","","","","",") and words","are predicted from beginning to end:","   ","","     ","","","","","","  ","  ","","",""," 730","where","","",""," ","and","","","is a special symbol","called a","(boundary token). Since it is impossi-","ble to define the complete vocabulary, we prepare","a special token","for unknown words and an un-","known word spelling","","","","is predicted by the fol-","lowing character-based","-gram model after","is","predicted by  ",":","","   "," ","      ","","","","","","  ","  ","","","","","(1)","where","","",""," ","and","","","","is a special symbol",".","Thus, when  is outside of the vocabulary",",","","","","","","","","","","","","","","","","  ","","","","","","","","","","","","","","",""," 3.2 Automatic Word Segmentation Nagata (1994) proposed a stochastic word segmenter based on a word","-gram model to solve the word segmentation problem. According to this method, the word segmenter divides a sentence into a word sequence with the highest probability","",""," argmax ","","","","","  ",""," Nagata (1994) reported an accuracy of about 97% on a test corpus in the same domain using a learning corpus of 10,945 sentences in Japanese. 3.3 Phoneme-to-text Transcription","A phoneme-to-text transcription system based on","an LM","(Mori et al., 1999) receives a phoneme","sequence and returns a list of candidate sen-","tences","","","  ","","","","in descending order of the","probability  ","","","",":","","","","","   ","","","","","","where","","","","","","","   ","","","","","","","",""," Similar to speech recognition, the probability is decomposed into two independent parts: a pronunciation model (PM) and an LM.","","","","","","  ","","","","","",""," ","","","","","   ","","","","","  "," ","","","","","   ","","","","","  ","","","","","","","","","","","   ","","","","","","","","","","","","","","(2)","","","","","","",""," is independent of","","and","","","","In this formula  ","","is an LM representing the","likelihood of a sentence",". For the LM, we can","use a word","-gram model we explained above.","The other part in the above formula","","","","","","is a","PM representing the probability that a given sen-","tence","is pronounced as",". Since it is impossible","to collect the phoneme sequences","for all pos-","sible sentences",", the model is decomposed into","a word-based model ","in which the words are","pronounced independently","","","  ","","","    ","","","   ","","","","(3)","where","","is a phoneme sequence corresponding to","the word","","and the condition","","","","","is met.","The probabilities    ","","","","are estimated from","a corpus in which each word is annotated with a","phoneme sequence as follows:","","","   ","","","","","  ","","","","   ",""," (4)","where","   stands for the frequency of an event","in the corpus. For unknown words no transcription","model has been proposed and the phoneme-to-text","transcription system (Mori et al., 1999) simply re-","turns the phoneme sequence itself.","This is done","by replacing the unknown word model based on","the Japanese character set","","","","","by a model","based on the phonemic alphabet","","","","","",".","Thus the candidate evaluation metric of a","phoneme-to-text transcription (Mori et al., 1999)","composed of the word","-gram model and the","word-based pronunciation model is as follows:","","","    ","","","    ","","","","","    ","","","","","","","","","","   ","","","","","(5)  ","","","","","","","","","","","","","  ","","","","","","","","if","","","","","","","","","","","","","","","  ","","","","","","","if","","","",""]},{"title":"4 LM Estimation from a Stochastically Segmented Corpus (SSC)","paragraphs":["To cope with segmentation errors, the concept of stochastic segmentation is proposed (Mori and Takuma, 2004). In this section, we briefly explain a method of calculating word","-gram probabilities on a stochastically segmented corpus in the target domain. For a detailed explanation and proofs of the mathematical soundness, please refer to the paper (Mori and Takuma, 2004).","","One of the Japanese syllabaries Katakana is used to spell out imported words by imitating their Japanese-constrained pronunciation and the phoneme sequence itself is the correct transcription result for them. Mori et. al. (1999) reported that approximately 33.0% of the unknown words in a test corpus were imported words. 731"]},{"title":"x","paragraphs":["k+1"]},{"title":"x","paragraphs":["bnne"]},{"title":"x","paragraphs":["bn+1"]},{"title":"x w","paragraphs":["n"]},{"title":"x","paragraphs":["i"]},{"title":"x","paragraphs":["b1"]},{"title":"x","paragraphs":["e1"]},{"title":"x","paragraphs":["b2 e2"]},{"title":"x","paragraphs":["1"]},{"title":"ww","paragraphs":["2"]},{"title":"1-P","paragraphs":["bn"]},{"title":"()1-P","paragraphs":["bn+1"]},{"title":"()P","paragraphs":["ne"]},{"title":"PP","paragraphs":["ie1"]},{"title":"P","paragraphs":["e2b2"]},{"title":"1-P()1-P","paragraphs":["b1"]},{"title":"()","paragraphs":["r 1n"]},{"title":"f(w ) =","paragraphs":["Figure 1: Word -gram frequency in a stochastically segmented corpus (SSC). 4.1 Stochastically Segmented Corpus (SSC)","A stochastically segmented corpus (SSC) is de-","fined as a combination of a raw corpus","","(here-","after referred to as the character sequence","","","",")","and word boundary probabilities","","that a word","boundary exists between two characters","","and","","","",". Since there are word boundaries before the","first character and after the last character of the","corpus, ","","","","","",".","In (Mori and Takuma, 2004), the word bound-","ary probabilities are defined as follows. First the","word boundary estimation accuracy","of an auto-","matic word segmenter is calculated on a test cor-","pus with word boundary information. Then the","raw corpus is segmented by the word segmenter.","Finally  is set to be","for each","where the word","segmenter put a word boundary and","","is set to","be","  for each","where it did not put a word","boundary. We adopted the same method in the ex-","periments.","4.2 Word -gram Frequency Word","-gram frequencies on an SSC is calculated as follows:","Word 0-gram frequency: This is defined as an expected number of words in the SSC:","       ","      ","Word","-gram frequency (","","","): Let us think","of a situation (see Figure 1) in which a word se-","quence","","","occurs in the SSC as a subsequence","beginning at the   ","-th character and end-","ing at the","-th character and each word","","in the word sequence is equal to the character","sequence beginning at the","","-th character and","ending at the","","-th character (","","","","","","","","","","","","","","",";","","","  ","","","","","","","","","",";","","","","","",";","","  ). The word","-gram fre-","quency of a word sequence","","","","","","","in the SSC is","defined by the summation of the stochastic fre-","quency at each occurrence of the character se-","quence of the word sequence","","","over all of the occurrences in the SSC:","","  ","",""," ","","    ","","","              ","     ","","  ","       "," ","where","","","","","","","  ","","","","","and","","","","","","","","","","","    ","","","","","","","","","","",".","4.3 Word -gram probability Similar to the word","-gram probability estimation from a decisively segmented corpus, word","-gram probabilities in an SSC are estimated by the maximum likelihood estimation method as relative values of word","-gram frequencies:","   ","","","   ","","     ","","","","","  ","","","","","","","","  ","","","","","","","  "," ","",""," ","",""]},{"title":"5 Phoneme-to-Text Transcription with an Infinite Vocabulary","paragraphs":["The vocabulary of an LM estimated from an SSC consists of all subsequences occurring in it. Adding a module describing a stochastic relationship between these subsequences and input signal sequences, we can build a phoneme-to-text transcription system equipped with an almost infinite vocabulary. 5.1 Word Candidate Enumeration Given a phoneme sequence as an input, the dictionary of a phoneme-to-text transcription system described in Subsection 3.3 returns pairs of a word and a probability per Equation (4). Similarly, the dictionary of a phoneme-to-text system with an infinite vocabulary must be able to take a phoneme sequence","and return all possible pairs of a character sequence","and the probability","","","","","","as word candidates. This is done as follows:","1. First we prepare a single character dictionary","containing all characters","in the language an-","notated with their all possible phoneme se-","quences","","","","","  ","","","","","","","",".For 732","example, the Japanese single character dictio-","nary contains a character","","“” annotated","with its all possible phoneme sequences","","","","","","","","   ","","","","","","",".","2. Then we build a phoneme-to-text transcrip-","tion system for single characters equipped with","the vocabulary consisting of the union set of","phoneme sequences for all characters. Given","a phoneme sequence",", this module returns all","possible character sequences","with its gener-","ation probability","","","","","",". For example, given","a subsequence of the input phoneme sequence","","","",", this module returns","","","","","","","","","","","","","as a word candidate set along with their","generation probabilities.","3. There are various methods to calculate the","probability  ","","","",". The only condition is that","given","  ","","","","","",",","","","","","","must be a","stochastic language model (cf. Section 3) on the","alphabet",". In the experiments, we assumed the","uniform distribution of phoneme sequences for","each character as follows:","","","","","","","    ","","","","","","","     ","   "," (6)","The module we described above receives a phoneme sequence and enumerates its decompositions to subsequences contained in the single character dictionary. This module is implemented using a dynamic programming method. In the experiments we limited the maximum length of the input to 16 phonemes. 5.2 Modeling Contexts of Word Candidates Word","-gram probability estimated from an SSC may not be as accurate as an LM estimated from a corpus segmented appropriately by hand. Thus we use the following interpolation technique:","","","","","","","","","","","","","","   ","","","","","","","","","","","","","","","where"," is history before","",",","","is the probabil-","ity estimated from a segmented corpus","",", and","","is the probability estimated by our method from a","raw corpus ",". The","","and","","are interpolation","coefficients which are estimated by the deleted in-","terpolation method (Jelinek et al., 1991).","","More precisely, it may happen that the same phoneme","sequence is generated from a character sequence in multiple","ways. In this case the generation probability is calculated as","the summation over all possible generations.","In the experiments, the word bi-gram model in our phoneme-to-text transcription system is combined with word bi-gram probabilities estimated from an SSC. Thus the phoneme-to-text transcription system of our new framework refers to the following LM to measure the likelihood of word sequences:","","   ","(7)                  ","","","","","","","","","","","","","","","","","","","","","","","","","","","if","","","","","","","","","","","","","","",""," ","","","","","","","","","","","","","","","","","","if","","","","   ","","","","","","","","","","","","","","  ","","","","","","","","","","","","","","","","if","","","","  ","","","","","where","","is the set of all subsequences appearing","in the SSC. Our LM based on Equation (7) and an existing","LM (cf. Equation (5)) behave differently when","they predict an out-of-vocabulary word appearing","in the SSC, that is","","","","","","","","","","",".In","this case our LM has reliable context informa-","tion on the OOV word to help the system choose","the proper word. Our system also clearly func-","tions better than the LM interpolated with a word"," -gram model estimated from the automatic seg-","mentation result of the corpus when the result is a","wrong segmentation. For example, when the au-","tomatic segmentation result of the sequence “","” (the abbreviation of Japan TV broadcasting","corporation) has a word boundary between “”","and “,” the uni-gram probability","","","is","equal to 0 and an OOV word “ ”isnever","enumerated as a candidate.","To the contrary, us-","ing our method","","","","","when the sequence","“” appears in the SSC at least once. Thus","the sequence is enumerated as a candidate word.","In addition, when the sequence appears frequently","in the SSC,","","","","","and the word may ap-","pear at a high position in the candidate list even if","the automatic segmenter always wrongly segments","the sequence into “” and “.” 5.3 Default Character for Phoneme In very rare cases, it happens that the input phoneme sequence cannot be decomposed into phoneme sequences in the vocabulary and those","","Two word fragments “” and “” may be enumerated as word candidates. The notion of word may be necessary for the user’s facility. However, we do not discuss the necessity of the notion of word in the phoneme-to-text transcription system. 733 corresponding to subsequences of the SSC and, as a result, the transcription system does not output any candidate sentence. To avoid this situation, we prepare a default character for every phoneme and the transcription system also enumerates the default character for each phoneme. In Japanese from the viewpoint of transcription accuracy, it is better to set the default characters to katakana, which are used mainly for translitera-tion of imported words. Since a katakana is pronunced uniquely (","","","","","","),","","","","","","","","","   ","","","","","","","","","(8)","From Equations (4), (6), and (8), the PM of our transcription system is as follows:","","","   ","","","(9)                ","","","  ","","","","   ","","","if  ","     ","   ","","if    ","","","","","","","","if    ","","","","","","","where","","","   ","","","",".","5.4 Phoneme-to-Text Transcription with an Infinite Vocabulary","Finally, the transcription system with an infinite","vocabulary enumerates candidate sentence","","","","  ","","","in the descending order of the follow-","ing evaluation function value composed of an LM","","","   defined by Equation (7) and a PM","","","","","","","","defined by Equation (9):","","","    ","","","    ","","","","","    ","","",""," Note that there are only three cases since the case decompositions in Equation (7) and Equation (9) are identical."]},{"title":"6 Evaluation","paragraphs":["As an evaluation of our phoneme-to-text transcription system, we measured transcription accuracies of several systems on test corpora in two domains: one is a general domain in which we have a small annotated corpus with word boundary information and phoneme sequence for each word, and the other is a target domain in which only a large raw corpus is available. As the transcription result, we took the word sequence of the highest probability. In this section we show the results and evaluate our new framework. Table 1: Annotated corpus in general domain","#sentences #words #chars","learning 20,808 406,021 598,264 test 2,311 45,180 66,874 Table 2: Raw corpus in the target domain","#sentences #words #chars","learning 797,345 — 17,645,920 test 1,000 — 20,935 6.1 Conditions on the Experiments The segmented corpus used in our experiments is composed of articles extracted from newspapers and example sentences in a dictionary of daily conversation. Each sentence in the corpus is segmented into words and each word is annotated with a phoneme sequence. The corpus was divided into ten parts. The parameters of the model were estimated from nine of them (learning) and the model was tested on the remaining one (test). Table 1 shows the corpus size. Another corpus we used in the experiments is composed of daily business reports. This corpus is not annotated with word boundary information nor phoneme sequence for each word. For evaluation, we selected 1,000 sentences randomly and annotated them with the phoneme sequences to be used as a test set. The rest was used for LM estimation (see Table 2). 6.2 Evaluation Criterion","The criterion we used for transcription systems is","precision and recall based on the number of char-","acters in the longest common subsequence (LCS)","(Aho, 1990). Let",""," be the number of char-","acters in the correct sentence,","","","be that in the","output of a system, and  ","be that of the LCS","of the correct sentence and the output of the sys-","tem, so the recall is defined as","","","","","","","and","the precision as","","","","  ",". 6.3 Models for Comparison In order to clarify the difference in the usages of the target domain corpus, we built four transcription systems and compared their accuracies. Below we explain the models in detail.","Model",": Baseline A word bi-gram model built from the segmented general domain corpus. 734 Table 3: Phoneme-to-text transcription accuracy. word bi-gram from raw corpus unknown General domain Target domain the annotated corpus usage word model Precision Recall Precision Recall","","Ye s No No 89.80% 92.30% 68.62% 78.40%","","Ye s Auto. Seg. No 92.67% 93.42% 80.59% 86.19%","","","Ye s Auto. Seg. Ye s 92.52% 93.17% 90.35% 93.48%","","Ye s Stoch. Seg. Ye s 92.78% 93.40% 91.10% 94.09%","The vocabulary contains 10,728 words appearing","in more than one corpora of the nine learning cor-","pora. The automatic word segmenter used to build","the other three models is based on the method ex-","plained in Section 3 with this LM.","Model",": Decisive segmentation A word bi-gram model estimated from the automatic segmentation result of the target corpus interpolated with model",".","Model","",": Decisive segmentation Model","extended with our PM for unknown words","Model",": Stochastic segmentation A word bi-gram model estimated from the SSC in the target domain interpolated with model and equipped with our PM for unknown words 6.4 Evaluation Table 3 shows the transcription accuracy of the models. A comparison of the accuracies in the target domain of the Model","and Model","confirms the well known fact that even an automatic segmentation result containing errors helps an LM improve its performance. The accuracy of Model ","in the general domain is also higher than that of Model",". From this result we can say that over-adaptation has not occurred.","Model","",", equipped with our PM for unknown words, is a natural extension of Model",", a model based on an existing method. The accuracy of Model","","is higher than that of Model","in the target domain, but worse in the general domain. This is because the vocabulary of Model","","is enlarged with the words and the word fragments contained in the automatic segmentation result. Though no study has been reported on the method of Model ","",", below we take Model","","as an existing method for a more severe evaluation.","Comparing the accuracies of Model","","and Model","in both domain, it can be said that using our method we can build a more accurate model than the existing methods. The main reason is that Table 4: Relationship between the raw corpus size and the accuracies. Raw corpus size Precision Recall","",""," ","","","chars (1/100) 89.18% 92.32%","",""," ","","","chars (1/10) 90.33% 93.40%","",""," ","","","chars (1/1) 91.10% 94.09% our phoneme model PM is able to enumerate transcription candidates for out-of-vocabulary words and word","-gram probabilities estimated from the SSC helps the model choose the appropriate ones.","A detailed study of Table 3 tells us that the reduction rate of character error rate (","","recall) of Model","in the target domain (9.36%) is much larger than that in the general domain (3.37%). The reason for this is that the automatic word segmenter tends to make mistakes around character-istic words and expressions in the target domain and our method is much less influenced by those segmentation errors than the existing method is.","In order to clarify the relationship between the size of the SSC and the transcription accuracy, we calculated the accuracies while changing the size of the SSC (1/1, 1/10, 1/100). The result, shown in Table 4, shows that we can still achieve a further improvement just by gathering more example sentences in the target domain.","The main difference between the models is the LM part. Thus the accuracy increase is yielded by the LM improvements. This fact indicates that we can expect a similar improvement in other generative NLP systems using the noisy channel model by expanding the LM vocabulary with context information to an infinite size."]},{"title":"7 Related Work","paragraphs":["The well-known methods for the unknown word problem are classified into two groups: one is to use an unknown word model and the other is to extract word candidates from a corpus before the application. Below we describe the relationship 735 between these methods and the proposed method.","In the method using an unknown word model, first the generation probability of an unknown word is modeled by a character","-gram, and then an NLP system, such as a morphological analyzer, searches for the best solution considering the possibility that all subsequences might be unknown words (Nagata, 1994; Bazzi and Glass, 2000). In the same way, we can build a phoneme-to-text transcription system which can enumerate unknown word candidates, but the LM is not able to refer to lexical context information to choose the appropriate word, since the unknown words are modeled to be generated from a single state. We solved this problem by allowing the LM to refer to information from an SSC.","When a machine-readable corpus in the target domain is available, we can extract word candidates from the corpus with a certain criterion and use them in application. An advantage of this method is that all of the occurrences of each candidate in the corpus are considered. Nagata (1996) proposed a method calculating word candidates with their uni-gram frequencies using a forward-backward algorithm. and reported that the accuracy of a morphological analyzer can be improved by adding the extracted words to its vocabulary. Comparing our method with this research, it can be said that our method executes the word candidate enumeration and their context calculation dynamically at the time of the solution search for an NLP task, phoneme-to-text transcription here. One of the advantages of our framework is that the system considers all substrings in the corpus as word candidates (that is the recall of the word extraction is 100%) and a higher accuracy is expected using a consistent criterion, namely the generation probability, for the word candidate enumeration process and solution search process.","The framework we propose in this paper, enlarging the vocabulary to an almost infinite size, is general and applicable to many other NLP systems based on the noisy channel model, such as speech recognition, statistical machine translation, etc. Our framework is potentially capable of improving the accuracies in these tasks as well."]},{"title":"8 Conclusion","paragraphs":["In this paper we proposed a generative NLP system with an almost infinite vocabulary for languages without obvious word boundary information in written texts. In the experiments we compared four phoneme-to-text transcription systems in Japanese. The transcription system equipped with an infinite vocabulary showed a higher accuracy than the baseline model and the model based on the existing method. These results show the efficacy of our method and tell us that our approach is promising for the phoneme-to-text transcription task or other NLP systems based on the noisy channel model."]},{"title":"References","paragraphs":["Alfred V. Aho. 1990. Algorithms for finding patterns in strings. In Handbook of Theoretical Computer Science, volume A: Algorithms and Complexity, pages 273–278. Elseveir Science Publishers.","Issam Bazzi and James R. Glass. 2000. Modeling out-of-vocabulary words for robust speech recognition. In Proc. of the ICSLP2000.","Peter F. Brown, John Cocke, Stephen A. Della Pietra, Vincent J. Della Pietra, Frederick Jelinek, John D. Lafferty, Robert L. Mercer, and Paul S. Roossin. 1990. A statistical approach to machine translation. Computational Linguistics, 16(2):79–85.","Anne-Marie Derouault and Bernard Merialdo. 1986. Natural language modeling for phoneme-to-text transcription. IEEE PAMI, 8(6):742–749.","Frederick Jelinek, Robert L. Mercer, and Salim Roukos. 1991. Principles of lexical language modeling for speech recognition. In Advances in Speech Signal Processing, chapter 21, pages 651– 699. Dekker.","Frederick Jelinek. 1985. Self-organized language modeling for speech recognition. Technical report, IBM T. J. Watson Research Center.","Mark D. Kernighan, Kenneth W. Church, and William A. Gale. 1990. A spelling correction program based on a noisy channel model. In Proc. of the COLING90, pages 205–210.","Adam Kilgarriff and Gregory Grefenstette. 2003. In-troduction to the special issue on the web as corpus. Computational Linguistics, 29(3):333–347.","Ken Lunde. 1998. CJKV Information Processing. O’Reilly & Associates.","Shinsuke Mori and Daisuke Takuma. 2004. Word n-gram probability estimation from a Japanese raw corpus. In Proc. of the ICSLP2004.","Shinsuke Mori, Tsuchiya Masatoshi, Osamu Yamaji, and Makoto Nagao. 1999. Kana-kanji conversion by a stochastic model. Transactions of IPSJ, 40(7):2946–2953. (in Japanese).","Masaaki Nagata. 1994. A stochastic Japanese morphological analyzer using a forward-DP backward-A n-best search algorithm. In Proc. of the COLING94, pages 201–207.","Masaaki Nagata. 1996. Automatic extraction of new words from Japanese texts using generalized forward-backward search. In EMNLP. 736"]}],"references":[{"authors":[{"first":"Alfred V","middle":".","last":"Aho"}],"year":"1990","title":"Algorithms for finding patterns in strings","source":"Alfred V. Aho. 1990. Algorithms for finding patterns in strings. In Handbook of Theoretical Computer Science, volume A: Algorithms and Complexity, pages 273–278. Elseveir Science Publishers."},{"authors":[{"first":"Issam","last":"Bazzi"},{"first":"James","middle":"R.","last":"Glass"}],"year":"2000","title":"Modeling out-of-vocabulary words for robust speech recognition","source":"Issam Bazzi and James R. Glass. 2000. Modeling out-of-vocabulary words for robust speech recognition. In Proc. of the ICSLP2000."},{"authors":[{"first":"Peter","middle":"F.","last":"Brown"},{"first":"John","last":"Cocke"},{"first":"Stephen","middle":"A. Della","last":"Pietra"},{"first":"Vincent","middle":"J. Della","last":"Pietra"},{"first":"Frederick","last":"Jelinek"},{"first":"John","middle":"D.","last":"Lafferty"},{"first":"Robert","middle":"L.","last":"Mercer"},{"first":"Paul","middle":"S.","last":"Roossin"}],"year":"1990","title":"A statistical approach to machine translation","source":"Peter F. Brown, John Cocke, Stephen A. Della Pietra, Vincent J. Della Pietra, Frederick Jelinek, John D. Lafferty, Robert L. Mercer, and Paul S. Roossin. 1990. A statistical approach to machine translation. Computational Linguistics, 16(2):79–85."},{"authors":[{"first":"Anne-Marie","last":"Derouault"},{"first":"Bernard","last":"Merialdo"}],"year":"1986","title":"Natural language modeling for phoneme-to-text transcription","source":"Anne-Marie Derouault and Bernard Merialdo. 1986. Natural language modeling for phoneme-to-text transcription. IEEE PAMI, 8(6):742–749."},{"authors":[{"first":"Frederick","last":"Jelinek"},{"first":"Robert","middle":"L.","last":"Mercer"},{"first":"Salim","last":"Roukos"}],"year":"1991","title":"Principles of lexical language modeling for speech recognition","source":"Frederick Jelinek, Robert L. Mercer, and Salim Roukos. 1991. Principles of lexical language modeling for speech recognition. In Advances in Speech Signal Processing, chapter 21, pages 651– 699. Dekker."},{"authors":[{"first":"Frederick","last":"Jelinek"}],"year":"1985","title":"Self-organized language modeling for speech recognition","source":"Frederick Jelinek. 1985. Self-organized language modeling for speech recognition. Technical report, IBM T. J. Watson Research Center."},{"authors":[{"first":"Mark","middle":"D.","last":"Kernighan"},{"first":"Kenneth","middle":"W.","last":"Church"},{"first":"William","middle":"A.","last":"Gale"}],"year":"1990","title":"A spelling correction program based on a noisy channel model","source":"Mark D. Kernighan, Kenneth W. Church, and William A. Gale. 1990. A spelling correction program based on a noisy channel model. In Proc. of the COLING90, pages 205–210."},{"authors":[{"first":"Adam","last":"Kilgarriff"},{"first":"Gregory","last":"Grefenstette"}],"year":"2003","title":"In-troduction to the special issue on the web as corpus","source":"Adam Kilgarriff and Gregory Grefenstette. 2003. In-troduction to the special issue on the web as corpus. Computational Linguistics, 29(3):333–347."},{"authors":[{"first":"Ken","last":"Lunde"}],"year":"1998","title":"CJKV Information Processing","source":"Ken Lunde. 1998. CJKV Information Processing. O’Reilly & Associates."},{"authors":[{"first":"Shinsuke","last":"Mori"},{"first":"Daisuke","last":"Takuma"}],"year":"2004","title":"Word n-gram probability estimation from a Japanese raw corpus","source":"Shinsuke Mori and Daisuke Takuma. 2004. Word n-gram probability estimation from a Japanese raw corpus. In Proc. of the ICSLP2004."},{"authors":[{"first":"Shinsuke","last":"Mori"},{"first":"Tsuchiya","last":"Masatoshi"},{"first":"Osamu","last":"Yamaji"},{"first":"Makoto","last":"Nagao"}],"year":"1999","title":"Kana-kanji conversion by a stochastic model","source":"Shinsuke Mori, Tsuchiya Masatoshi, Osamu Yamaji, and Makoto Nagao. 1999. Kana-kanji conversion by a stochastic model. Transactions of IPSJ, 40(7):2946–2953. (in Japanese)."},{"authors":[{"first":"Masaaki","last":"Nagata"}],"year":"1994","title":"A stochastic Japanese morphological analyzer using a forward-DP backward-A n-best search algorithm","source":"Masaaki Nagata. 1994. A stochastic Japanese morphological analyzer using a forward-DP backward-A n-best search algorithm. In Proc. of the COLING94, pages 201–207."},{"authors":[{"first":"Masaaki","last":"Nagata"}],"year":"1996","title":"Automatic extraction of new words from Japanese texts using generalized forward-backward search","source":"Masaaki Nagata. 1996. Automatic extraction of new words from Japanese texts using generalized forward-backward search. In EMNLP. 736"}],"cites":[{"style":0,"text":"Jelinek, 1985","origin":{"pointer":"/sections/2/paragraphs/0","offset":143,"length":13},"authors":[{"last":"Jelinek"}],"year":"1985","references":["/references/5"]},{"style":0,"text":"Kernighan et al., 1990","origin":{"pointer":"/sections/2/paragraphs/0","offset":180,"length":22},"authors":[{"last":"Kernighan"},{"last":"al."}],"year":"1990","references":["/references/6"]},{"style":0,"text":"Brown et al., 1990","origin":{"pointer":"/sections/2/paragraphs/0","offset":226,"length":18},"authors":[{"last":"Brown"},{"last":"al."}],"year":"1990","references":["/references/2"]},{"style":0,"text":"Kilgarriff and Grefenstette, 2003","origin":{"pointer":"/sections/2/paragraphs/1","offset":517,"length":33},"authors":[{"last":"Kilgarriff"},{"last":"Grefenstette"}],"year":"2003","references":["/references/7"]},{"style":0,"text":"Derouault and Merialdo, 1986","origin":{"pointer":"/sections/3/paragraphs/0","offset":377,"length":28},"authors":[{"last":"Derouault"},{"last":"Merialdo"}],"year":"1986","references":["/references/3"]},{"style":0,"text":"Lunde, 1998","origin":{"pointer":"/sections/3/paragraphs/0","offset":680,"length":11},"authors":[{"last":"Lunde"}],"year":"1998","references":["/references/8"]},{"style":0,"text":"Nagata (1994)","origin":{"pointer":"/sections/4/paragraphs/118","offset":33,"length":13},"authors":[{"last":"Nagata"}],"year":"1994","references":["/references/11"]},{"style":0,"text":"Nagata (1994)","origin":{"pointer":"/sections/4/paragraphs/129","offset":1,"length":13},"authors":[{"last":"Nagata"}],"year":"1994","references":["/references/11"]},{"style":0,"text":"Mori et al., 1999","origin":{"pointer":"/sections/4/paragraphs/132","offset":1,"length":17},"authors":[{"last":"Mori"},{"last":"al."}],"year":"1999","references":["/references/10"]},{"style":0,"text":"Mori et al., 1999","origin":{"pointer":"/sections/4/paragraphs/320","offset":22,"length":17},"authors":[{"last":"Mori"},{"last":"al."}],"year":"1999","references":["/references/10"]},{"style":0,"text":"Mori et al., 1999","origin":{"pointer":"/sections/4/paragraphs/338","offset":31,"length":17},"authors":[{"last":"Mori"},{"last":"al."}],"year":"1999","references":["/references/10"]},{"style":0,"text":"Mori and Takuma, 2004","origin":{"pointer":"/sections/5/paragraphs/0","offset":86,"length":21},"authors":[{"last":"Mori"},{"last":"Takuma"}],"year":"2004","references":["/references/9"]},{"style":0,"text":"Mori and Takuma, 2004","origin":{"pointer":"/sections/5/paragraphs/1","offset":175,"length":21},"authors":[{"last":"Mori"},{"last":"Takuma"}],"year":"2004","references":["/references/9"]},{"style":0,"text":"Mori and Takuma, 2004","origin":{"pointer":"/sections/23/paragraphs/28","offset":4,"length":21},"authors":[{"last":"Mori"},{"last":"Takuma"}],"year":"2004","references":["/references/9"]},{"style":0,"text":"Jelinek et al., 1991","origin":{"pointer":"/sections/24/paragraphs/170","offset":20,"length":20},"authors":[{"last":"Jelinek"},{"last":"al."}],"year":"1991","references":["/references/4"]},{"style":0,"text":"Aho, 1990","origin":{"pointer":"/sections/25/paragraphs/8","offset":1,"length":9},"authors":[{"last":"Aho"}],"year":"1990","references":["/references/0"]},{"style":0,"text":"Nagata, 1994","origin":{"pointer":"/sections/26/paragraphs/2","offset":170,"length":12},"authors":[{"last":"Nagata"}],"year":"1994","references":["/references/11"]},{"style":0,"text":"Bazzi and Glass, 2000","origin":{"pointer":"/sections/26/paragraphs/2","offset":184,"length":21},"authors":[{"last":"Bazzi"},{"last":"Glass"}],"year":"2000","references":["/references/1"]},{"style":0,"text":"Nagata (1996)","origin":{"pointer":"/sections/26/paragraphs/3","offset":274,"length":13},"authors":[{"last":"Nagata"}],"year":"1996","references":["/references/12"]}]}
