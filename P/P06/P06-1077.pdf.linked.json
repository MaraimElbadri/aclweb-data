{"sections":[{"title":"","paragraphs":["Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 609–616, Sydney, July 2006. c⃝2006 Association for Computational Linguistics"]},{"title":"Tree-to-String Alignment Template for Statistical Machine Translation Yang Liu , Qun Liu , and Shouxun Lin Institute of Computing Technology Chinese Academy of Sciences No.6 Kexueyuan South Road, Haidian District P. O. Box 2704, Beijing, 100080, China {yliu,liuqun,sxlin}@ict.ac.cn Abstract","paragraphs":["We present a novel translation model based on tree-to-string alignment template (TAT) which describes the alignment between a source parse tree and a target string. A TAT is capable of generating both terminals and non-terminals and performing reordering at both low and high levels. The model is linguistically syntax-based because TATs are extracted automatically from word-aligned, source side parsed parallel texts. To translate a source sentence, we first employ a parser to produce a source parse tree and then apply TATs to transform the tree into a target string. Our experiments show that the TAT-based model significantly outperforms Pharaoh, a state-of-the-art decoder for phrase-based models."]},{"title":"1 Introduction","paragraphs":["Phrase-based translation models (Marcu and Wong, 2002; Koehn et al., 2003; Och and Ney, 2004), which go beyond the original IBM translation models (Brown et al., 1993) 1","by modeling translations of phrases rather than individual words, have been suggested to be the state-of-the-art in statistical machine translation by empirical evaluations.","In phrase-based models, phrases are usually strings of adjacent words instead of syntactic constituents, excelling at capturing local reordering and performing translations that are localized to","1","The mathematical notation we use in this paper is taken from that paper: a source string fJ","1 = f1, . . . , fj, . . . , fJ is to be translated into a target string eI","1 = e1, . . . , ei, . . . , eI. Here, I is the length of the target string, and J is the length of the source string. substrings that are common enough to be observed on training data. However, a key limitation of phrase-based models is that they fail to model reordering at the phrase level robustly. Typically, phrase reordering is modeled in terms of offset positions at the word level (Koehn, 2004; Och and Ney, 2004), making little or no direct use of syntactic information.","Recent research on statistical machine translation has lead to the development of syntax-based models. Wu (1997) proposes Inversion Transduction Grammars, treating translation as a process of parallel parsing of the source and target language via a synchronized grammar. Alshawi et al. (2000) represent each production in parallel dependency tree as a finite transducer. Melamed (2004) formalizes machine translation problem as synchronous parsing based on multitext grammars. Graehl and Knight (2004) describe training and decoding algorithms for both generalized tree-to-tree and tree-to-string transducers. Chiang (2005) presents a hierarchical phrase-based model that uses hierarchical phrase pairs, which are formally productions of a synchronous context-free grammar. Ding and Palmer (2005) propose a syntax-based translation model based on a probabilistic synchronous dependency insert grammar, a version of synchronous grammars defined on dependency trees. All these approaches, though different in formalism, make use of synchronous grammars or tree-based transduction rules to model both source and target languages.","Another class of approaches make use of syntactic information in the target language alone, treating the translation problem as a parsing problem. Yamada and Knight (2001) use a parser in the target language to train probabilities on a set of 609 operations that transform a target parse tree into a source string.","Paying more attention to source language analysis, Quirk et al. (2005) employ a source language dependency parser, a target language word segmentation component, and an unsupervised word alignment component to learn treelet translations from parallel corpus.","In this paper, we propose a statistical translation model based on tree-to-string alignment template which describes the alignment between a source parse tree and a target string. A TAT is capable of generating both terminals and non-terminals and performing reordering at both low and high levels. The model is linguistically syntax-based because TATs are extracted automatically from word-aligned, source side parsed parallel texts. To translate a source sentence, we first employ a parser to produce a source parse tree and then apply TATs to transform the tree into a target string.","One advantage of our model is that TATs can be automatically acquired to capture linguistically motivated reordering at both low (word) and high (phrase, clause) levels. In addition, the training of TAT-based model is less computationally expensive than tree-to-tree models. Similarly to (Galley et al., 2004), the tree-to-string alignment templates discussed in this paper are actually transformation rules. The major difference is that we model the syntax of the source language instead of the target side. As a result, the task of our decoder is to find the best target string while Galley’s is to seek the most likely target tree."]},{"title":"2 Tree-to-String Alignment Template","paragraphs":["A tree-to-string alignment template z is a triple ⟨ T̃ , S̃, Ã⟩, which describes the alignment Ã between a source parse tree T̃ = T (F J′","1 ) 2","and a target string S̃ = EI′","1 . A source string F J′","1 , which is the sequence of leaf nodes of T (F J′","1 ), consists of both terminals (source words) and non-terminals (phrasal categories). A target string EI′","1 is also composed of both terminals (target words) and non-terminals (placeholders). An alignment Ã is defined as a subset of the Cartesian product of source and target symbol positions:","Ã ⊆ {(j, i) : j = 1, . . . , J ′ ; i = 1, . . . , I′","} (1) 2","We use T (·) to denote a parse tree. To reduce notational overhead, we use T (z) to represent the parse tree in z. Similarly, S(z) denotes the string in z.","Figure 1 shows three TATs automatically learned from training data. Note that when demonstrating a TAT graphically, we represent non-terminals in the target strings by blanks. NP NR  NN  LCP NP NR  CC  NR LC  NP DNP NP DEG NP President Bush between United States and Figure 1: Examples of tree-to-string alignment templates obtained in training In the following, we formally describe how to","introduce tree-to-string alignment templates into","probabilistic dependencies to model P r(eI","1|f J","1 ) 3",". In a first step, we introduce the hidden variable","T (f J 1 ) that denotes a parse tree of the source sen-","tence f J","1 :","P r(eI 1|f J","1 ) = ∑","T (fJ 1 )","P r(eI","1, T (f J","1 )|f J","1 ) (2) = ∑","T (fJ 1 )","P r(T (f J","1 )|f J","1 )P r(eI","1|T (f J","1 ), f J","1 ) (3) Next, another hidden variable D is introduced","to detach the source parse tree T (f J","1 ) into a sequence of K subtrees T̃ K","1 with a preorder transversal. We assume that each subtree T̃k produces a target string S̃k. As a result, the sequence of subtrees T̃ K","1 produces a sequence of target","strings S̃K","1 , which can be combined serially to","generate the target sentence eI","1. We assume that","P r(eI","1|D, T (f J 1 ), f J","1 ) ≡ P r( S̃K","1 |T̃ K","1 ) because eI","1","is actually generated by the derivation of S̃K","1 .","Note that we omit an explicit dependence on the","detachment D to avoid notational overhead.","P r(eI","1|T (f J","1 ), f J","1 ) = ∑ D","P r(eI","1, D|T (f J","1 ), f J","1 ) (4) = ∑ D","P r(D|T (f J","1 ), f J","1 )P r(eI","1|D, T (f J","1 ), f J","1 ) (5) = ∑ D","P r(D|T (f J","1 ), f J 1 )P r( S̃K","1 |T̃ K","1 ) (6) = ∑ D","P r(D|T (f J 1 ), f J","1 ) K∏ k=1 P r( S̃k|T̃k) (7)","3","The notational convention will be as follows. We use the symbol P r(·) to denote general probability distribution with no specific assumptions. In contrast, for model-based probability distributions, we use generic symbol p(·). 610 NP DNP NP NR  DEG  NP NN  NN  NP DNP NP DEG  NP NP NR  NP NN NN NN  NN "," "," parsing detachment production of China economic development combination","economic development of China Figure 2: Graphic illustration for translation process","To further decompose P r( S̃|T̃ ), the tree-to-string alignment template, denoted by the variable z, is introduced as a hidden variable. P r( S̃|T̃ ) = ∑ z P r( S̃, z|T̃ ) (8) = ∑ z P r(z|T̃ )P r( S̃|z, T̃ ) (9)","Therefore, the TAT-based translation model can be decomposed into four sub-models:","1. parse model: P r(T (f J 1 )|f J","1 )","2. detachment model: P r(D|T (f J 1 ), f J","1 ) 3. TAT selection model: P r(z|T̃ ) 4. TAT application model: P r( S̃|z, T̃ )","Figure 2 shows how TATs work to perform translation. First, the input source sentence is parsed. Next, the parse tree is detached into five subtrees with a preorder transversal. For each subtree, a TAT is selected and applied to produce a string. Finally, these strings are combined serially to generate the translation (we use X to denote the non-terminal): X1 ⇒ X2 of X3 ⇒ X2 of China ⇒ X3 X4 of China ⇒ economic X4 of China ⇒ economic development of China","Following Och and Ney (2002), we base our model on log-linear framework. Hence, all knowledge sources are described as feature functions that include the given source string f J","1 , the target string eI","1, and hidden variables. The hidden variable T (f J","1 ) is omitted because we usually make use of only single best output of a parser. As we assume that all detachment have the same probability, the hidden variable D is also omitted. As a result, the model we actually adopt for experiments is limited because the parse, detachment, and TAT application sub-models are simplified.","P r(eI 1, zK 1 |f J","1 ) =","exp[∑M","m=1 λmhm(eI 1, f J","1 , zK","1 )]","∑ e′I 1,z′K","1","exp[∑M","m=1 λmhm(e′I 1, f J","1 , z′K","1 )]","êI","1 = argmax eI 1,zK","1 { M∑ m=1","λmhm(eI 1, f J 1 , zK","1 )}","For our experiments we use the following seven feature functions 4","that are analogous to default feature set of Pharaoh (Koehn, 2004). To simplify the notation, we omit the dependence on the hidden variables of the model.","h1(eI 1, f J","1 ) = log K∏ k=1","N (z) · δ(T (z), T̃k) N (T (z))","h2(eI 1, f J","1 ) = log K∏ k=1","N (z) · δ(T (z), T̃k) N (S(z))","h3(eI 1, f J","1 ) = log K∏ k=1 lex(T (z)|S(z)) · δ(T (z), T̃k)","h4(eI 1, f J","1 ) = log K∏ k=1 lex(S(z)|T (z)) · δ(T (z), T̃k)","h5(eI 1, f J","1 ) = K","h6(eI 1, f J","1 ) = log I∏ i=1 p(ei|ei−2, ei−1) h7(eI","1, f J","1 ) = I","4","When computing lexical weighting features (Koehn et al., 2003), we take only terminals into account. If there are no terminals, we set the feature value to 1. We use lex(·) to denote lexical weighting. We denote the number of TATs used for decoding by K and the length of target string by I. 611","Tree String Alignment","( NR ) Bush 1:1","( NN ) President 1:1","( VV ) made 1:1","( NN ) speech 1:1 ( NP ( NR ) ( NN ) ) X1 |X2 1:2 2:1","( NP ( NR ) ( NN ) ) X |Bush 1:2 2:1","( NP ( NR ) ( NN ) ) President |X 1:2 2:1","( NP ( NR ) ( NN ) ) President |Bush 1:2 2:1 ( VP ( VV ) ( NN ) ) X1 |a |X2 1:1 2:3","( VP ( VV ) ( NN ) ) made |a |X 1:1 2:3","( VP ( VV ) ( NN ) ) X |a |speech 1:1 2:3","( VP ( VV ) ( NN ) ) made |a |speech 1:1 2:3 ( IP ( NP ) ( VP ) ) X1 |X2 1:1 2:2 Table 1: Examples of TATs extracted from the TSA in Figure 3 with h = 2 and c = 2"]},{"title":"3 Training","paragraphs":["To extract tree-to-string alignment templates from a word-aligned, source side parsed sentence pair ⟨T (f J","1 ), eI","1, A⟩, we need first identify TSAs (Tree-String-Alignment) using similar criterion as suggested in (Och and Ney, 2004). A TSA is a triple ⟨T (f j2 j1 ), ei2","i1, Ā)⟩ that is in accordance with the","following constraints: 1. ∀(i, j) ∈ A : i1 ≤ i ≤ i2 ↔ j1 ≤ j ≤ j2 2. T (f j2 j1 ) is a subtree of T (f J","1 ) Given a TSA ⟨T (f j2 j1 ), ei2","i1, Ā⟩, a triple ⟨T (f j4 j3 ), ei4","i3, Â⟩ is its sub TSA if and only if: 1. T (f j4 j3 ), ei4","i3, Â⟩ is a TSA 2. T (f j4 j3 ) is rooted at the direct descendant of the root node of T (f j1 j2 ) 3. i1 ≤ i3 ≤ i4 ≤ i2 4. ∀(i, j) ∈ Ā : i3 ≤ i ≤ i4 ↔ j3 ≤ j ≤ j4","Basically, we extract TATs from a TSA ⟨T (f j2 j1 ), ei2","i1, Ā⟩ using the following two rules: 1. If T (f j2 j1 ) contains only one node, then ⟨T (f j2 j1 ), ei2","i1, Ā⟩ is a TAT 2. If the height of T (f","j2","j1 ) is greater than one,","then build TATs using those extracted from","sub TSAs of ⟨T (f j2 j1 ), ei2","i1, Ā⟩. IP NP NR  NN  VP VV  NN  President Bush made a speech Figure 3: An example of TSA","Usually, we can extract a very large amount of TATs from training data using the above rules, making both training and decoding very slow. Therefore, we impose three restrictions to reduce the magnitude of extracted TATs:","1. A third constraint is added to the definition of","TSA:","∃j′ , j′′",": j1 ≤ j′","≤ j2 and j1 ≤ j′′","≤ j2 and (i1, j′",") ∈ Ā and (i2, j′′",") ∈ Ā This constraint requires that both the first and last symbols in the target string must be aligned to some source symbols.","2. The height of T (z) is limited to no greater than h.","3. The number of direct descendants of a node of T (z) is limited to no greater than c. Table 1 shows the TATs extracted from the TSA","in Figure 3 with h = 2 and c = 2. As we restrict that T (f j2 j1 ) must be a subtree of","T (f J 1 ), TATs may be treated as syntactic hierar-612 chical phrase pairs (Chiang, 2005) with tree structure on the source side. At the same time, we face the risk of losing some useful non-syntactic phrase pairs. For example, the phrase pair ←→ President Bush made can never be obtained in form of TAT from the TSA in Figure 3 because there is no subtree for that source string."]},{"title":"4 Decoding","paragraphs":["We approach the decoding problem as a bottom-up beam search.","To translate a source sentence, we employ a parser to produce a parse tree. Moving bottom-up through the source parse tree, we compute a list of candidate translations for the input subtree rooted at each node with a postorder transversal. Candidate translations of subtrees are placed in stacks. Figure 4 shows the organization of candidate translation stacks. NP DNP NP NR  DEG  NP NN  NN  8","4 7 2 3 5 6 1 . . . 1 . . . 2 . . . 3 . . . 4 . . . 5 . . . 6 . . . 7 . . . 8 Figure 4: Candidate translations of subtrees are placed in stacks according to the root index set by postorder transversal","A candidate translation contains the following information: 1. the partial translation 2. the accumulated feature values 3. the accumulated probability","A TAT z is usable to a parse tree T if and only if T (z) is rooted at the root of T and covers part of nodes of T . Given a parse tree T , we find all usable TATs. Given a usable TAT z, if T (z) is equal to T , then S(z) is a candidate translation of T . If T (z) covers only a portion of T , we have to compute a list of candidate translations for T by replacing the non-terminals of S(z) with candidate translations of the corresponding uncovered subtrees. NP DNP NP DEG  NP 8 4 7 2 3 of . . . 1 . . . 2 . . . 3 . . . 4 . . . 5 . . . 6 . . . 7 . . . 8 Figure 5: Candidate translation construction","For example, when computing the candidate translations for the tree rooted at node 8, the TAT used in Figure 5 covers only a portion of the parse tree in Figure 4. There are two uncovered subtrees that are rooted at node 2 and node 7 respectively. Hence, we replace the third symbol with the candidate translations in stack 2 and the first symbol with the candidate translations in stack 7. At the same time, the feature values and probabilities are also accumulated for the new candidate translations.","To speed up the decoder, we limit the search space by reducing the number of TATs used for each input node. There are two ways to limit the TAT table size: by a fixed limit (tatTable-limit) of how many TATs are retrieved for each input node, and by a probability threshold (tatTable-threshold) that specify that the TAT probability has to be above some value. On the other hand, instead of keeping the full list of candidates for a given node, we keep a top-scoring subset of the candidates. This can also be done by a fixed limit (stack-limit) or a threshold (stack-threshold). To perform recombination, we combine candidate translations that share the same leading and trailing bigrams in each stack."]},{"title":"5 Experiments","paragraphs":["Our experiments were on Chinese-to-English translation. The training corpus consists of 31, 149 sentence pairs with 843, 256 Chinese words and 613","System Features BLEU4","d + φ(e|f ) 0.0573 ± 0.0033","Pharaoh d + lm + φ(e|f ) + wp 0.2019 ± 0.0083 d + lm + φ(f |e) + lex(f |e) + φ(e|f ) + lex(e|f ) + pp + wp 0.2089 ± 0.0089","h1 0.1639 ± 0.0077","Lynx h1 + h6 + h7 0.2100 ± 0.0089","h1 + h2 + h3 + h4 + h5 + h6 + h7 0.2178 ± 0.0080 Table 2: Comparison of Pharaoh and Lynx with different feature settings on the test corpus 949, 583 English words. For the language model, we used SRI Language Modeling Toolkit (Stolcke, 2002) to train a trigram model with modified Kneser-Ney smoothing (Chen and Goodman, 1998) on the 31, 149 English sentences. We selected 571 short sentences from the 2002 NIST MT Evaluation test set as our development corpus, and used the 2005 NIST MT Evaluation test set as our test corpus. We evaluated the translation quality using the BLEU metric (Papineni et al., 2002), as calculated by mteval-v11b.pl with its default setting except that we used case-sensitive matching of n-grams. 5.1 Pharaoh The baseline system we used for comparison was Pharaoh (Koehn et al., 2003; Koehn, 2004), a freely available decoder for phrase-based translation models:","p(e|f ) = pφ(f |e)λφ × p","LM(e)λ LM ×","pD(e, f )λ D × ω","length(e)λ W(e) (10)","We ran GIZA++ (Och and Ney, 2000) on the training corpus in both directions using its default setting, and then applied the refinement rule “diag-and” described in (Koehn et al., 2003) to obtain a single many-to-many word alignment for each sentence pair. After that, we used some heuristics, which including rule-based translation of numbers, dates, and person names, to further improve the alignment accuracy.","Given the word-aligned bilingual corpus, we obtained 1, 231, 959 bilingual phrases (221, 453 used on test corpus) using the training toolkits publicly released by Philipp Koehn with its default setting.","To perform minimum error rate training (Och, 2003) to tune the feature weights to maximize the system’s BLEU score on development set, we used optimizeV5IBMBLEU.m (Venugopal and Vogel, 2005). We used default pruning settings for Pharaoh except that we set the distortion limit to 4. 5.2 Lynx On the same word-aligned training data, it took us about one month to parse all the 31, 149 Chinese sentences using a Chinese parser written by Deyi Xiong (Xiong et al., 2005). The parser was trained on articles 1 − 270 of Penn Chinese Treebank version 1.0 and achieved 79.4% (F1 measure) as well as a 4.4% relative decrease in error rate. Then, we performed TAT extraction described in section 3 with h = 3 and c = 5 and obtained 350, 575 TATs (88, 066 used on test corpus). To run our decoder Lynx on development and test corpus, we set tatTable-limit = 20, tatTable-threshold = 0, stack-limit = 100, and stack-threshold = 0.00001. 5.3 Results Table 2 shows the results on test set using Pharaoh and Lynx with different feature settings. The 95% confidence intervals were computed using Zhang’s significance tester (Zhang et al., 2004). We modified it to conform to NIST’s current definition of the BLEU brevity penalty. For Pharaoh, eight features were used: distortion model d, a trigram language model lm, phrase translation probabilities φ(f |e) and φ(e|f ), lexical weightings lex(f |e) and lex(e|f ), phrase penalty pp, and word penalty wp. For Lynx, seven features described in section 2 were used. We find that Lynx outperforms Pharaoh with all feature settings. With full features, Lynx achieves an absolute improvement of 0.006 over Pharaoh (3.1% relative). This difference is statistically significant (p < 0.01). Note that Lynx made use of only 88, 066 TATs on test corpus while 221, 453 bilingual phrases were used for Pharaoh.","The feature weights obtained by minimum er-614 Features System","d lm φ(f |e) lex(f |e) φ(e|f ) lex(e|f ) pp wp Pharaoh 0.0476 0.1386 0.0611 0.0459 0.1723 0.0223 0.3122 -0.2000 Lynx - 0.3735 0.0061 0.1081 0.1656 0.0022 0.0824 0.2620 Table 3: Feature weights obtained by minimum error rate training on the development corpus","BLEU4 tat 0.2178 ± 0.0080","tat + bp 0.2240 ± 0.0083 Table 4: Effect of using bilingual phrases for Lynx ror rate training for both Pharaoh and Lynx are shown in Table 3. We find that φ(f |e) (i.e. h2) is not a helpful feature for Lynx. The reason is that we use only a single non-terminal symbol instead of assigning phrasal categories to the target string. In addition, we allow the target string consists of only non-terminals, making translation decisions not always based on lexical evidence. 5.4 Using bilingual phrases It is interesting to use bilingual phrases to strengthen the TAT-based model. As we mentioned before, some useful non-syntactic phrase pairs can never be obtained in form of TAT because we restrict that there must be a corresponding parse tree for the source phrase. Moreover, it takes more time to obtain TATs than bilingual phrases on the same training data because parsing is usually very time-consuming.","Given an input subtree T (F j2 j1 ), if F","j2","j1 is a string","of terminals, we find all bilingual phrases that the","source phrase is equal to F j2 j1 . Then we build a","TAT for each bilingual phrase ⟨f J′","1 , eI′","1 , Â⟩: the tree of the TAT is T (F","j2","j1 ), the string is eI′","1 , and the alignment is Â. If a TAT built from a bilingual phrase is the same with a TAT in the TAT table, we prefer to the greater translation probabilities.","Table 4 shows the effect of using bilingual phrases for Lynx. Note that these bilingual phrases are the same with those used for Pharaoh. 5.5 Results on large data We also conducted an experiment on large data to further examine our design philosophy. The training corpus contains 2.6 million sentence pairs. We used all the data to extract bilingual phrases and a portion of 800K pairs to obtain TATs. Two trigram language models were used for Lynx. One was trained on the 2.6 million English sentences and another was trained on the first 1/3 of the Xinhua portion of Gigaword corpus. We also included rule-based translations of named entities, dates, and numbers. By making use of these data, Lynx achieves a BLEU score of 0.2830 on the 2005 NIST Chinese-to-English MT evaluation test set, which is a very promising result for linguistically syntax-based models."]},{"title":"6 Conclusion","paragraphs":["In this paper, we introduce tree-to-string alignment templates, which can be automatically learned from syntactically-annotated training data. The TAT-based translation model improves translation quality significantly compared with a state-of-the-art phrase-based decoder. Treated as special TATs without tree on the source side, bilingual phrases can be utilized for the TAT-based model to get further improvement.","It should be emphasized that the restrictions we impose on TAT extraction limit the expressive power of TAT. Preliminary experiments reveal that removing these restrictions does improve translation quality, but leads to large memory requirements. We feel that both parsing and word alignment qualities have important effects on the TAT-based model. We will retrain the Chinese parser on Penn Chinese Treebank version 5.0 and try to improve word alignment quality using log-linear models as suggested in (Liu et al., 2005)."]},{"title":"Acknowledgement","paragraphs":["This work is supported by National High Technology Research and Development Program contract “Generally Technical Research and Basic Database Establishment of Chinese Platform”(Subject No. 2004AA114010). We are grateful to Deyi Xiong for providing the parser and Haitao Mi for making the parser more efficient and robust. Thanks to Dr. Yajuan Lv for many helpful comments on an earlier draft of this paper. 615"]},{"title":"References","paragraphs":["Hiyan Alshawi, Srinivas Bangalore, and Shona Douglas. 2000. Learning dependency translation models as collections of finite-state head transducers. Computational Linguistics, 26(1):45-60.","Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263-311.","Stanley F. Chen and Joshua Goodman. 1998. Am empirical study of smoothing techniques for language modeling. Technical Report TR-10-98, Harvard University Center for Research in Computing Technology.","David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of 43rd Annual Meeting of the ACL, pages 263-270.","Yuan Ding and Martha Palmer. 2005. Machine translation using probabilistic synchronous dependency insert grammars. In Proceedings of 43rd Annual Meeting of the ACL, pages 541-548.","Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In Proceedings of NAACL-HLT 2004, pages 273-280.","Jonathan Graehl and Kevin Knight. 2004. Training tree transducers. In Proceedings of NAACL-HLT 2004, pages 105-112.","Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of HLT-NAACL 2003, pages 127-133.","Philipp Koehn. 2004. Pharaoh: a beam search decoder for phrase-based statistical machine trnasla-tion models. In Proceedings of the Sixth Conference of the Association for Machine Translation in the Americas, pages 115-124.","Yang Liu, Qun Liu, and Shouxun Lin. 2005. Loglinear models for word alignment. In Proceedings of 43rd Annual Meeting of the ACL, pages 459-466.","Daniel Marcu and William Wong. 2002. A phrase-based, joint probability model for statistical machine translation. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 133-139.","Dan Melamed. 2004. Statistical machine translation by parsing. In Proceedings of 42nd Annual Meeting of the ACL, pages 653-660.","Franz J. Och and Hermann Ney. 2000. Improved statistical alignment models. In Proceedings of 38th Annual Meeting of the ACL, pages 440-447.","Franz J. Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In Proceedings of 40th Annual Meeting of the ACL, pages 295-302.","Franz J. Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30(4):417-449.","Franz J. Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of 41st Annual Meeting of the ACL, pages 160-167.","Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of 40th Annual Meeting of the ACL, pages 311-318.","Chris Quirk, Arul Menezes, and Colin Cherry. 2005. Dependency treelet translation: Syntactically in-formed phrasal SMT. In Proceedings of 43rd Annual Meeting of the ACL, pages 271-279.","Andreas Stolcke. 2002. SRILM - an extensible language modeling toolkit. In Proceedings of International Conference on Spoken Language Processing, volume 2, pages 901-904.","Ashish Venugopal and Stephan Vogel. 2005. Considerations in maximum mutual information and minimum classification error training for statistical machine translation. In Proceedings of the Tenth Conference of the European Association for Machine Translation (EAMT-05).","Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377-403.","Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin, and Yueliang Qian. 2005. Parsing the Penn Chinese treebank with semantic knowledge. In Proceedings of IJCNLP 2005, pages 70-81.","Kenji Yamada and Kevin Knight. 2001. A syntax-based statistical translation model. In Proceedings of 39th Annual Meeting of the ACL, pages 523-530.","Ying Zhang, Stephan Vogel, and Alex Waibel. 2004. Interpreting BLEU/NIST scores: How much improvement do we need to have a better system? In Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC), pages 2051-2054. 616"]}],"references":[{"authors":[{"first":"Hiyan","last":"Alshawi"},{"first":"Srinivas","last":"Bangalore"},{"first":"Shona","last":"Douglas"}],"year":"2000","title":"Learning dependency translation models as collections of finite-state head transducers","source":"Hiyan Alshawi, Srinivas Bangalore, and Shona Douglas. 2000. Learning dependency translation models as collections of finite-state head transducers. Computational Linguistics, 26(1):45-60."},{"authors":[{"first":"Peter","middle":"F.","last":"Brown"},{"first":"Stephen","middle":"A. Della","last":"Pietra"},{"first":"Vincent","middle":"J. Della","last":"Pietra"},{"first":"Robert","middle":"L.","last":"Mercer"}],"year":"1993","title":"The mathematics of statistical machine translation: Parameter estimation","source":"Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263-311."},{"authors":[{"first":"Stanley","middle":"F.","last":"Chen"},{"first":"Joshua","last":"Goodman"}],"year":"1998","title":"Am empirical study of smoothing techniques for language modeling","source":"Stanley F. Chen and Joshua Goodman. 1998. Am empirical study of smoothing techniques for language modeling. Technical Report TR-10-98, Harvard University Center for Research in Computing Technology."},{"authors":[{"first":"David","last":"Chiang"}],"year":"2005","title":"A hierarchical phrase-based model for statistical machine translation","source":"David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of 43rd Annual Meeting of the ACL, pages 263-270."},{"authors":[{"first":"Yuan","last":"Ding"},{"first":"Martha","last":"Palmer"}],"year":"2005","title":"Machine translation using probabilistic synchronous dependency insert grammars","source":"Yuan Ding and Martha Palmer. 2005. Machine translation using probabilistic synchronous dependency insert grammars. In Proceedings of 43rd Annual Meeting of the ACL, pages 541-548."},{"authors":[{"first":"Michel","last":"Galley"},{"first":"Mark","last":"Hopkins"},{"first":"Kevin","last":"Knight"},{"first":"Daniel","last":"Marcu"}],"year":"2004","title":"What’s in a translation rule? In Proceedings of NAACL-HLT 2004, pages 273-280","source":"Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In Proceedings of NAACL-HLT 2004, pages 273-280."},{"authors":[{"first":"Jonathan","last":"Graehl"},{"first":"Kevin","last":"Knight"}],"year":"2004","title":"Training tree transducers","source":"Jonathan Graehl and Kevin Knight. 2004. Training tree transducers. In Proceedings of NAACL-HLT 2004, pages 105-112."},{"authors":[{"first":"Philipp","last":"Koehn"},{"first":"Franz","middle":"J.","last":"Och"},{"first":"Daniel","last":"Marcu"}],"year":"2003","title":"Statistical phrase-based translation","source":"Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of HLT-NAACL 2003, pages 127-133."},{"authors":[{"first":"Philipp","last":"Koehn"}],"year":"2004","title":"Pharaoh: a beam search decoder for phrase-based statistical machine trnasla-tion models","source":"Philipp Koehn. 2004. Pharaoh: a beam search decoder for phrase-based statistical machine trnasla-tion models. In Proceedings of the Sixth Conference of the Association for Machine Translation in the Americas, pages 115-124."},{"authors":[{"first":"Yang","last":"Liu"},{"first":"Qun","last":"Liu"},{"first":"Shouxun","last":"Lin"}],"year":"2005","title":"Loglinear models for word alignment","source":"Yang Liu, Qun Liu, and Shouxun Lin. 2005. Loglinear models for word alignment. In Proceedings of 43rd Annual Meeting of the ACL, pages 459-466."},{"authors":[{"first":"Daniel","last":"Marcu"},{"first":"William","last":"Wong"}],"year":"2002","title":"A phrase-based, joint probability model for statistical machine translation","source":"Daniel Marcu and William Wong. 2002. A phrase-based, joint probability model for statistical machine translation. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 133-139."},{"authors":[{"first":"Dan","last":"Melamed"}],"year":"2004","title":"Statistical machine translation by parsing","source":"Dan Melamed. 2004. Statistical machine translation by parsing. In Proceedings of 42nd Annual Meeting of the ACL, pages 653-660."},{"authors":[{"first":"Franz","middle":"J.","last":"Och"},{"first":"Hermann","last":"Ney"}],"year":"2000","title":"Improved statistical alignment models","source":"Franz J. Och and Hermann Ney. 2000. Improved statistical alignment models. In Proceedings of 38th Annual Meeting of the ACL, pages 440-447."},{"authors":[{"first":"Franz","middle":"J.","last":"Och"},{"first":"Hermann","last":"Ney"}],"year":"2002","title":"Discriminative training and maximum entropy models for statistical machine translation","source":"Franz J. Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In Proceedings of 40th Annual Meeting of the ACL, pages 295-302."},{"authors":[{"first":"Franz","middle":"J.","last":"Och"},{"first":"Hermann","last":"Ney"}],"year":"2004","title":"The alignment template approach to statistical machine translation","source":"Franz J. Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30(4):417-449."},{"authors":[{"first":"Franz","middle":"J.","last":"Och"}],"year":"2003","title":"Minimum error rate training in statistical machine translation","source":"Franz J. Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of 41st Annual Meeting of the ACL, pages 160-167."},{"authors":[{"first":"Kishore","last":"Papineni"},{"first":"Salim","last":"Roukos"},{"first":"Todd","last":"Ward"},{"first":"Wei-Jing","last":"Zhu"}],"year":"2002","title":"BLEU: a method for automatic evaluation of machine translation","source":"Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of 40th Annual Meeting of the ACL, pages 311-318."},{"authors":[{"first":"Chris","last":"Quirk"},{"first":"Arul","last":"Menezes"},{"first":"Colin","last":"Cherry"}],"year":"2005","title":"Dependency treelet translation: Syntactically in-formed phrasal SMT","source":"Chris Quirk, Arul Menezes, and Colin Cherry. 2005. Dependency treelet translation: Syntactically in-formed phrasal SMT. In Proceedings of 43rd Annual Meeting of the ACL, pages 271-279."},{"authors":[{"first":"Andreas","last":"Stolcke"}],"year":"2002","title":"SRILM - an extensible language modeling toolkit","source":"Andreas Stolcke. 2002. SRILM - an extensible language modeling toolkit. In Proceedings of International Conference on Spoken Language Processing, volume 2, pages 901-904."},{"authors":[{"first":"Ashish","last":"Venugopal"},{"first":"Stephan","last":"Vogel"}],"year":"2005","title":"Considerations in maximum mutual information and minimum classification error training for statistical machine translation","source":"Ashish Venugopal and Stephan Vogel. 2005. Considerations in maximum mutual information and minimum classification error training for statistical machine translation. In Proceedings of the Tenth Conference of the European Association for Machine Translation (EAMT-05)."},{"authors":[{"first":"Dekai","last":"Wu"}],"year":"1997","title":"Stochastic inversion transduction grammars and bilingual parsing of parallel corpora","source":"Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377-403."},{"authors":[{"first":"Deyi","last":"Xiong"},{"first":"Shuanglong","last":"Li"},{"first":"Qun","last":"Liu"},{"first":"Shouxun","last":"Lin"},{"first":"Yueliang","last":"Qian"}],"year":"2005","title":"Parsing the Penn Chinese treebank with semantic knowledge","source":"Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin, and Yueliang Qian. 2005. Parsing the Penn Chinese treebank with semantic knowledge. In Proceedings of IJCNLP 2005, pages 70-81."},{"authors":[{"first":"Kenji","last":"Yamada"},{"first":"Kevin","last":"Knight"}],"year":"2001","title":"A syntax-based statistical translation model","source":"Kenji Yamada and Kevin Knight. 2001. A syntax-based statistical translation model. In Proceedings of 39th Annual Meeting of the ACL, pages 523-530."},{"authors":[{"first":"Ying","last":"Zhang"},{"first":"Stephan","last":"Vogel"},{"first":"Alex","last":"Waibel"}],"year":"2004","title":"Interpreting BLEU/NIST scores: How much improvement do we need to have a better system? In Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC), pages 2051-2054","source":"Ying Zhang, Stephan Vogel, and Alex Waibel. 2004. Interpreting BLEU/NIST scores: How much improvement do we need to have a better system? In Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC), pages 2051-2054. 616"}],"cites":[{"style":0,"text":"Marcu and Wong, 2002","origin":{"pointer":"/sections/2/paragraphs/0","offset":33,"length":20},"authors":[{"last":"Marcu"},{"last":"Wong"}],"year":"2002","references":["/references/10"]},{"style":0,"text":"Koehn et al., 2003","origin":{"pointer":"/sections/2/paragraphs/0","offset":55,"length":18},"authors":[{"last":"Koehn"},{"last":"al."}],"year":"2003","references":["/references/7"]},{"style":0,"text":"Och and Ney, 2004","origin":{"pointer":"/sections/2/paragraphs/0","offset":75,"length":17},"authors":[{"last":"Och"},{"last":"Ney"}],"year":"2004","references":["/references/14"]},{"style":0,"text":"Brown et al., 1993","origin":{"pointer":"/sections/2/paragraphs/0","offset":148,"length":18},"authors":[{"last":"Brown"},{"last":"al."}],"year":"1993","references":["/references/1"]},{"style":0,"text":"Koehn, 2004","origin":{"pointer":"/sections/2/paragraphs/6","offset":390,"length":11},"authors":[{"last":"Koehn"}],"year":"2004","references":["/references/8"]},{"style":0,"text":"Och and Ney, 2004","origin":{"pointer":"/sections/2/paragraphs/6","offset":403,"length":17},"authors":[{"last":"Och"},{"last":"Ney"}],"year":"2004","references":["/references/14"]},{"style":0,"text":"Wu (1997)","origin":{"pointer":"/sections/2/paragraphs/7","offset":103,"length":9},"authors":[{"last":"Wu"}],"year":"1997","references":["/references/20"]},{"style":0,"text":"Alshawi et al. (2000)","origin":{"pointer":"/sections/2/paragraphs/7","offset":271,"length":21},"authors":[{"last":"Alshawi"},{"last":"al."}],"year":"2000","references":["/references/0"]},{"style":0,"text":"Melamed (2004)","origin":{"pointer":"/sections/2/paragraphs/7","offset":371,"length":14},"authors":[{"last":"Melamed"}],"year":"2004","references":["/references/11"]},{"style":0,"text":"Graehl and Knight (2004)","origin":{"pointer":"/sections/2/paragraphs/7","offset":477,"length":24},"authors":[{"last":"Graehl"},{"last":"Knight"}],"year":"2004","references":["/references/6"]},{"style":0,"text":"Chiang (2005)","origin":{"pointer":"/sections/2/paragraphs/7","offset":610,"length":13},"authors":[{"last":"Chiang"}],"year":"2005","references":["/references/3"]},{"style":0,"text":"Ding and Palmer (2005)","origin":{"pointer":"/sections/2/paragraphs/7","offset":774,"length":22},"authors":[{"last":"Ding"},{"last":"Palmer"}],"year":"2005","references":["/references/4"]},{"style":0,"text":"Yamada and Knight (2001)","origin":{"pointer":"/sections/2/paragraphs/8","offset":147,"length":24},"authors":[{"last":"Yamada"},{"last":"Knight"}],"year":"2001","references":["/references/22"]},{"style":0,"text":"Quirk et al. (2005)","origin":{"pointer":"/sections/2/paragraphs/9","offset":51,"length":19},"authors":[{"last":"Quirk"},{"last":"al."}],"year":"2005","references":["/references/17"]},{"style":0,"text":"Galley et al., 2004","origin":{"pointer":"/sections/2/paragraphs/11","offset":289,"length":19},"authors":[{"last":"Galley"},{"last":"al."}],"year":"2004","references":["/references/5"]},{"style":0,"text":"Och and Ney (2002)","origin":{"pointer":"/sections/3/paragraphs/83","offset":10,"length":18},"authors":[{"last":"Och"},{"last":"Ney"}],"year":"2002","references":["/references/13"]},{"style":0,"text":"Koehn, 2004","origin":{"pointer":"/sections/3/paragraphs/105","offset":54,"length":11},"authors":[{"last":"Koehn"}],"year":"2004","references":["/references/8"]},{"style":0,"text":"Koehn et al., 2003","origin":{"pointer":"/sections/3/paragraphs/123","offset":43,"length":18},"authors":[{"last":"Koehn"},{"last":"al."}],"year":"2003","references":["/references/7"]},{"style":0,"text":"Och and Ney, 2004","origin":{"pointer":"/sections/4/paragraphs/2","offset":100,"length":17},"authors":[{"last":"Och"},{"last":"Ney"}],"year":"2004","references":["/references/14"]},{"style":0,"text":"Chiang, 2005","origin":{"pointer":"/sections/4/paragraphs/29","offset":77,"length":12},"authors":[{"last":"Chiang"}],"year":"2005","references":["/references/3"]},{"style":0,"text":"Stolcke, 2002","origin":{"pointer":"/sections/6/paragraphs/6","offset":227,"length":13},"authors":[{"last":"Stolcke"}],"year":"2002","references":["/references/18"]},{"style":0,"text":"Chen and Goodman, 1998","origin":{"pointer":"/sections/6/paragraphs/6","offset":303,"length":22},"authors":[{"last":"Chen"},{"last":"Goodman"}],"year":"1998","references":["/references/2"]},{"style":0,"text":"Papineni et al., 2002","origin":{"pointer":"/sections/6/paragraphs/6","offset":588,"length":21},"authors":[{"last":"Papineni"},{"last":"al."}],"year":"2002","references":["/references/16"]},{"style":0,"text":"Koehn et al., 2003","origin":{"pointer":"/sections/6/paragraphs/6","offset":793,"length":18},"authors":[{"last":"Koehn"},{"last":"al."}],"year":"2003","references":["/references/7"]},{"style":0,"text":"Koehn, 2004","origin":{"pointer":"/sections/6/paragraphs/6","offset":813,"length":11},"authors":[{"last":"Koehn"}],"year":"2004","references":["/references/8"]},{"style":0,"text":"Och and Ney, 2000","origin":{"pointer":"/sections/6/paragraphs/11","offset":15,"length":17},"authors":[{"last":"Och"},{"last":"Ney"}],"year":"2000","references":["/references/12"]},{"style":0,"text":"Koehn et al., 2003","origin":{"pointer":"/sections/6/paragraphs/11","offset":165,"length":18},"authors":[{"last":"Koehn"},{"last":"al."}],"year":"2003","references":["/references/7"]},{"style":0,"text":"Och, 2003","origin":{"pointer":"/sections/6/paragraphs/13","offset":40,"length":9},"authors":[{"last":"Och"}],"year":"2003","references":["/references/15"]},{"style":0,"text":"Venugopal and Vogel, 2005","origin":{"pointer":"/sections/6/paragraphs/13","offset":164,"length":25},"authors":[{"last":"Venugopal"},{"last":"Vogel"}],"year":"2005","references":["/references/19"]},{"style":0,"text":"Xiong et al., 2005","origin":{"pointer":"/sections/6/paragraphs/13","offset":448,"length":18},"authors":[{"last":"Xiong"},{"last":"al."}],"year":"2005","references":["/references/21"]},{"style":0,"text":"Zhang et al., 2004","origin":{"pointer":"/sections/6/paragraphs/13","offset":1110,"length":18},"authors":[{"last":"Zhang"},{"last":"al."}],"year":"2004","references":["/references/23"]},{"style":0,"text":"Liu et al., 2005","origin":{"pointer":"/sections/7/paragraphs/1","offset":504,"length":16},"authors":[{"last":"Liu"},{"last":"al."}],"year":"2005","references":["/references/9"]}]}
