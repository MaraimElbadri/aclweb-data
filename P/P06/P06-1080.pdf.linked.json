{"sections":[{"title":"","paragraphs":["Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 633–640, Sydney, July 2006. c⃝2006 Association for Computational Linguistics"]},{"title":"Self-Organizing -gram Model for Automatic Word Spacing Seong-Bae Park Yoon-Shik Tae Se-Young Park Department of Computer Engineering Kyungpook National University Daegu 702-701, Korea  sbpark,ystae,sypark @sejong.knu.ac.kr Abstract","paragraphs":["An automatic word spacing is one of the important tasks in Korean language processing and information retrieval. Since there are a number of confusing cases in word spacing of Korean, there are some mistakes in many texts including news articles. This paper presents a high-accurate method for automatic word spacing based on self-organizing","-gram model. This method is basically a variant of","-gram model, but achieves high accuracy by automatically adapting context size. In order to find the optimal context size, the proposed method automatically increases the context size when the contextual distribution after increasing it dose not agree with that of the current context. It also decreases the context size when the distribution of reduced context is similar to that of the current context. This approach achieves high accuracy by considering higher dimensional data in case of necessity, and the increased computational cost are compensated by the reduced context size. The experimental results show that the self-organizing structure of","-gram model enhances the basic model."]},{"title":"1 Introduction","paragraphs":["Even though Korean widely uses Chinese characters, the ideograms, it has a word spacing model unlike Chinese and Japanese. The word spacing of Korean, however, is not a simple task, though the basic rule for it is simple. The basic rule asserts that all content words should be spaced. However,","there are a number of exceptions due to various","postpositions and endings. For instance, it is diffi-","cult to distinguish some postpositions from incom-","plete nouns. Such exceptions induce many mis-","takes of word spacing even in news articles.","The problem of the inaccurate word spacing is","that they are fatal in language processing and in-","formation retrieval. The incorrect word spacing","would result in the incorrect morphological analy-","sis. For instance, let us consider a famous Korean","sentence: “ ","","","","","","","","","","","","","","","","","","","",".” The true","word spacing for this sentence is “","","","","","#","","","","","#","","","","","   ","","",".” whose meaning is that my fa-","ther entered the room. If the sentence is written","as “","",""," # ","","","","","","#","","","","","","","","","","",".”, it means that","my father entered the bag, which is totally dif-","ferent from the original meaning. That is, since","the morphological analysis is the first-step in most","NLP applications, the sentences with incorrect","word spacing must be corrected for their further","processing. In addition, the wrong word spacing","would result in the incorrect index for terms in in-","formation retrieval. Thus, correcting the sentences","with incorrect word spacing is a critical task in Ko-","rean information processing.","One of the most simple and strong models for","automatic word spacing is","-gram model. In spite","of the advantages of the","-gram model, its prob-","lem should be also considered for achieving high","performance. The main problem of the model is","that it is usually modeled with fixed window size,","",". The small value for","represents the narrow","context in modeling, which results in poor per-","formance in general. However, it is also difficult","to increase","for better performance due to data","sparseness. Since the corpus size is physically lim-","ited, it is highly possible that many","-grams which","do not appear in the corpus exist in the real world. 633","The goal of this paper is to provide a new method for processing automatic word spacing with an","-gram model. The proposed method automatically adapts the window size",". That is, this method begins with a bigram model, and it shrinks to an unigram model when data sparseness occurs. It also grows up to a trigram, fourgram, and so on when it requires more specific information in determining word spacing. In a word, the proposed model organizes the windows size","online, and achieves high accuracy by removing both data sparseness and information lack.","The rest of the paper is organized as follows. Section 2 surveys the previous work on automatic word spacing and the smoothing methods for","- gram models. Section 3 describes the general way to automatic word spacing by an","-gram model, and Section 4 proposes a self-organizing","-gram model to overcome some drawbacks of","-gram models. Section 5 presents the experimental results. Finally, Section 6 draws conclusions."]},{"title":"2 Previous Work","paragraphs":["Many previous work has explored the possibility of automatic word spacing. While most of them reported high accuracy, they can be categorized into two parts in methodology: analytic approach and statistical approach. The analytic approach is based on the results of morphological analysis. Kang used the fundamental morphological analysis techniques (Kang, 2000), and Kim et al. distinguished each word by the morphemic information of postpositions and endings (Kim et al., 1998). The main drawbacks of this approach are that (i) the analytic step is very complex, and (ii) it is expensive to construct and maintain the analytic knowledge.","In the other hand, the statistical approach extracts from corpora the probability that a space is put between two syllables. Since this approach can obtain the necessary information automatically, it does require neither the linguistic knowledge on syllable composition nor the costs for knowledge construction and maintenance. In addition, the fact that it does not use a morphological analyzer produces solid results even for unknown words. Many previous studies using corpora are based on bigram information. According to (Kang, 2004), the number of syllables used in modern Korean is about","",", which implies that the number of bigrams reaches","",". In order to obtain stable statistics for all bigrams, a great large volume of corpora will be required. If higher order","-gram is adopted for better accuracy, the volume of corpora required will be increased exponentially.","The main drawback of","-gram model is that it suffers from data sparseness however large the corpus is. That is, there are many","-grams of which frequency is zero. To avoid this problem, many smoothing techniques have been proposed for construction of","-gram models (Chen and Goodman, 1996). Most of them belongs to one of two categories. One is to pretend each","-gram occurs once more than it actually did (Mitchell, 1996). The other is to interpolate","-grams with lower dimensional data (Jelinek and Mercer, 1980; Katz, 1987). However, these methods artificially modify the original distribution of corpus. Thus, the final probabilities used in learning with","- grams are the ones distorted by a smoothing technique.","A maximum entropy model can be considered as another way to avoid zero probability in","-gram models (Rosenfeld, 1996). Instead of construct-ing separate models and then interpolate them, it builds a single, combined model to capture all the information provided by various knowledge sources. Even though a maximum entropy approach is simple, general, and strong, it is computationally very expensive. In addition, its performance is mainly dependent on the relevance of knowledge sources, since the prior knowledge on the target problem is very important (Park and Zhang, 2002). Thus, when prior knowledge is not clear and computational cost is an important factor,","-gram models are more suitable than a maximum entropy model.","Adapting features or contexts has been an important issue in language modeling (Siu and Ostendorf, 2000). In order to incorporate long-distance features into a language model, (Rosenfeld, 1996) adopted triggers, and (Mochihashi and Mastumoto, 2006) used a particle filter. However, these methods are restricted to a specific language model. Instead of long-distance features, some other researchers tried local context extension. For this purpose, (Schütze and Singer, 1994) adopted a variable memory Markov model proposed by (Ron et al., 1996), (Kim et al., 2003) applied selective extension of features to POS tagging, and (Dickinson and Meurers, 2005) expanded context of","-gram models to find errors in syntactic anno-634 tation. In these methods, only neighbor words or features of the target","-grams became candidates to be added into the context. Since they required more information for better performance or detect-ing errors, only the context extension was considered."]},{"title":"3 Automatic Word Spacing by -gram Model","paragraphs":["The problem of automatic word spacing can be re-","garded as a binary classification task. Let a sen-","tence be given as","  ","","","","","",". If i.i.d. sam-","pling is assumed, the data from this sentence are","given as","","","","","","  ","","","","","","","","","","where","","","","","","and","","      ","",". In this rep-","resentation,","","is a contextual representation of a","syllable","",". If a space should be put after","",", then","","",", the class of","",",istrue.Itisfalse otherwise.","Therefore, the automatic word spacing is to esti-","mate a function","  ","","","","","","","","","","",". That","is, our task is to determine whether a space should","be put after a syllable","","expressed as","","with its","context.","The probabilistic method is one of the strong","and most widely used methods for estimating",".","That is, for each","",",","","   ","","","","","    ","","","","   ","","","","where","","   ","","","is rewritten as","","    ","","","","","","","    ","","","","","   "," ","Since   ","","is independent of finding the class of","","",",","","   is determined by multiplying","","","","","","","","and","","   . That is,","","   ","","","","","    ","","","","","","    ","","","","","","In","-gram model,  is expressed with","neigh-","bor syllables around  . Typically,","is taken","to be two or three, corresponding to a bigram or","trigram respectively. ","corresponds to","","","","","","when","","",". In the same way, it is","","","","","","","","","","when","","",". The simple and easy way to esti-","mate","","","","","","","","is to use maximum likelihood esti-","mate with a large corpus. For instance, consider","the case","",". Then, the probability","","","","","","","","is","represented as","","","","","   ","","","",", and is computed by","","","","","   ","","","","","","","","","","  ","","","","","","   "," (1) ","","","","","  ","","","","","","   ","  0.7 0.75 0.8 0.85 0.9 0 1e+06 2e+06 3e+06 4e+06 5e+06 6e+06 7e+06 8e+06 Accuracy (%) No. of Training Examples unigram bigram trigram 4-gram 5-gram 6-gram 7-gram 8-gram 9-gram 10-gram Figure 1: The performance of","-gram models according to the values of","in automatic word spacing. where","is a counting function. Determining the context size, the value of",",in  -gram models is closely related with the corpus size. The larger is",", the larger corpus is required to avoid data sparseness. In contrast, though low-order","-grams do not suffer from data sparseness severely, they do not reflect the language characteristics well, either. Typically researchers have used","","","or","","",", and achieved high performance in many tasks (Bengio et al., 2003). Figure 1 supports that bigram and trigram outperform low-order (","","",") and high-order (","","",")  -grams in automatic word spacing. All the experimental settings for this figure follows those in Section 5. In this figure, bigram model shows the best accuracy and trigram achieves the second best, whereas unigram model results in the worst accuracy. Since the bigram model is best, a self-organizing","-gram model explained below starts from bigram."]},{"title":"4 Self-Organizing -gram Model","paragraphs":["To tackle the problem of fixed window size in","- gram models, we propose a self-organizing structure for them.","4.1 Expanding -grams","When","-grams are compared with","","","","-grams,","their performance in many tasks is lower than that","of","","","","-grams (Charniak, 1993). Simultane-","ously the computational cost for","","","","-grams","is far higher than that for","-grams. Thus, it can","be justified to use   ","-grams instead of","- 635","Function HowLargeExpand (","",")","Input:  :","-grams","Output: an integer for expanding size","1. Retrieve","","","","-grams","","","for","",". 2. Compute","","","","","","","","","   ","","","","","","","","","","3. If ","EXP Then return 0.","4. return HowLargeExpand(  ",")+1. Figure 2: A function that determines how large a window size should be.","grams only when higher performance is expected.","In other words,   ","-grams should be different","from","-grams. Otherwise, the performance would","not be different. Since our task is attempted with","a probabilistic method, the difference can be mea-","sured with conditional distributions. If the condi-","tional distributions of","-grams and","","","","-grams","are similar each other, there is no reason to adopt","","","","","-grams.","Let","","","","","",""," be a class-conditional probabil-","ity by","-grams and","","","","","","","","","","","that by","","","","-","grams. Then, the difference","","","","","","between","them is measured by Kullback-Leibler divergence.","That is,","","","","","","","","","","","","","   ","","","","","","","","","","","","",""," which is computed by ","","    ","","","   ","","","","","   ","","","","","    ","","",""," (2)","","","","",""," that is larger than a predefined","threshold","EXP implies that","","","","","","","","","","","is dif-","ferent from","","   ","","",". In this case,","","","","-grams","is used instead of","-grams.","Figure 2 depicts an algorithm that determines","how large","-grams should be used. It recursively","finds the optimal expanding window size. For in-","stance, let bigrams (","",") be used at first. When","the difference between bigrams and trigrams (","","",") is larger than EXP, that between trigrams and","fourgrams ("," ",") is checked again. If it is less","than","EXP, then this function returns 1 and tri-","grams are used instead of bigrams. Otherwise, it","considers higher","-grams again.","Function HowSmallShrink (","",")","Input:  :","-grams","Output: an integer for shrinking size","1. If ","Then return 0.","2. Retrieve","","","","-grams  ","","for","",". 3. Compute","","","","","","","","","   ","","","","","","","","","","","4. If ","SHR Then return 0.","5. return HowSmallShrink(   ",")-1. Figure 3: A function that determines how small a window size should be used.","4.2 Shrinking -grams","Shrinking","-grams is accomplished in the direc-","tion opposite to expanding","-grams. After com-","paring","-grams with  ","","-grams,","","","","-grams","are used instead of -grams only when they are","similar enough. The difference","","","","","","be-","tween","-grams and"," ","","-grams is, once again,","measured by Kullback-Leibler divergence. That","is,","","","","","","","","","","","","","","  ","","","","","","","","","","","","","",""," If","","","","","","is smaller than another predefined threshold","SHR, then","","","","-grams are used instead of","-grams.","Figure 3 shows an algorithm which determines how deeply the shrinking is occurred. The main stream of this algorithm is equivalent to that in Figure 2. It also recursively finds the optimal shrinking window size, but can not be further reduced when the current model is an unigram.","The merit of shrinking","-grams is that it can construct a model with a lower dimensionality. Since the maximum likelihood estimate is used in calculating probabilities, this helps obtaining stable probabilities. According to the well-known curse of dimensionality, the data density required is reduced exponentially by reducing dimensions. Thus, if the lower dimensional model is not different so much from the higher dimensional one, it is highly possible that the probabilities from lower dimensional space are more stable than those from higher dimensional space. 636","Function ChangingWindowSize(","",")","Input:  :","-grams","Output: an integer for changing window size","1. Set exp := HowLargeExpand (  ).","2. If exp ","Then return exp.","3. Set shr := HowSmallShrink(  ).","4. If shr ","Then return shr. 5. return 0. Figure 4: A function that determines the changing window size of","-grams. 4.3 Overall Self-Organizing Structure For a given i.i.d. sample","",", there are three possibilities on changing","-grams. First one is not to change","-grams. It is obvious when","-grams are not changed. This occurs when both","","",""," ","","","EXP and","","","","","","","","SHR are met. This is when the expanding results in too similar distribution to that of the current","-grams and the distribution after shrinking is too different from that of the current","-grams. The remaining possibilities are then expanding and shrinking. The application order between them can affect the performance of the proposed method. In this paper, an expanding is checked prior to a shrinking as shown in Figure 4. The function ChangingWindowSize first calls HowLargeExpand. The non-zero return value of HowLargeExpand implies that the window size of the current","-grams should be enlarged. Otherwise, ChangingWindowSize checks if the window size should be shrinked by calling HowSmallShrink.IfHowSmallShrink returns a negative integer, the window size should be shrinked to (","+ shr). If both functions return zero, the window size should not be changed. The reason why HowLargeExpand is called prior to HowSmallShrink is that the expanded","- grams handle more specific data. (","",")-grams, in general, help obtaining higher accuracy than","- grams, since (","",")-gram data are more specific than","-gram ones. However, it is time-consuming to consider higher-order data, since the number of kinds of data increases. The time increased due to expanding is compensated by shrinking. After shrinking, only lower-oder data are considered, and then processing time for them decreases. 4.4 Sequence Tagging Since natural language sentences are sequential as their nature, the word spacing can be considered as a special POS tagging task (Lee et al., 2002) for which a hidden Markov model is usually adopted. The best sequence of word spacing for the sentence is defined as","","","","","","","","   ","","","","  ","","","","","      ","","","","","","   ","","","","","","","","    ","","      ","","","","","","   ","","","","","","","","by where","is a sentence length.","If we assume that the syllables are independent","of each other,","","  ","","","","","","is given by","","","","  ","","","","    ","","","   ","",""," which can be computed using Equation (1). In addition, by Markov assumption, the probability of a current tag","","conditionally depends on only the previous","tags. That is,","    ","    ","","","","","   "," ","","",""," Thus, the best sequence is determined by","","","  ","","","       ","","","","","","","","","","    ","","","","","","","","","","(3) Since this equation follows Markov assumption, the best sequence is found by applying the Viterbi algorithm."]},{"title":"5 Experiments 5.1 Data Set","paragraphs":["The data set used in this paper is the HANTEC corpora version 2.0 distributed by KISTI1",". From this corpus, we extracted only the HKIB94 part which consists of 22,000 news articles in 1994 from Hankook Ilbo. The reason why HKIB94 is chosen is that the word spacing of news articles is relatively more accurate than other texts. Even though this data set is composed of totally 12,523,688 Korean syllables, the number of unique syllables is just","1","http://www.kisti.re.kr 637 Methods Accuracy (%) baseline 72.19 bigram 88.34 trigram 87.59","self-organizing bigram 91.31","decision tree 88.68","support vector machine 89.10 Table 1: The experimental results of various methods for automatic word spacing. 2,037 after removing all special symbols, digits, and English alphabets.","The data set is divided into three parts: training (70%), held-out (20%), and test (10%). The held-out set is used only to estimate","EXP and  SHR. The number of instances in the training set is 8,766,578, that in the held-out set is 2,504,739, and that in test set is 1,252,371. Among the 1,252,371 test cases, the number of positive instances is 348,278, and that of negative instances is 904,093. Since about 72% of test cases are negative, this is the baseline of the automatic word spacing. 5.2 Experimental Results To evaluate the performance of the proposed method, two well-known machine learning algorithms are compared together. The tested machine learning algorithms are (i) decision tree and (ii) support vector machines. We use C4.5 release 8 (Quinlan, 1993) for decision tree induction and  ","","","","","(Joachims, 1998) for support vector machines. For all experiments with decision trees and support vector machines, the context size is set to two since the bigram shows the best performance in Figure 1.","Table 1 gives the experimental results of various methods including machine learning algorithms and self-organizing","-gram model. The ‘self-organizing bigram’ in this table is the one proposed in this paper. The normal","-grams achieve an accuracy of around 88%, while decision tree and support vector machine produce that of around 89%. The self-organizing","-gram model achieves 91.31%. The accuracy improvement by the self-organizing","-gram model is about 19% over the baseline, about 3% over the normal","-gram model, and 2% over decision trees and support vector machines.","In order to organize the context size for","-grams","Order No. of Errors Expanding then Shrinking 108,831 Shrinking then Expanding 114,343 Table 2: The number of errors caused by the application order of context expanding and shrinking. online, two operations of expanding and shrinking were proposed. Table 2 shows how much the number of errors is affected by their application order. The number of errors made by expanding first is 108,831 while that by shrinking first is 114,343. That is, if shrinking is applied ahead of expanding, 5,512 additional errors are made. Thus, it is clear that expanding should be considered first.","The errors by expanding can be explained with two reasons: (i) the expression power of the model and (ii) data sparseness. Since Korean is a partially-free word order language and the omission of words are very frequent,","-gram model that captures local information could not express the target task sufficiently. In addition, the class-conditional distribution after expanding could be very different from that before expanding due to data sparseness. In such cases, the expanding should not be applied since the distribution after expanding is not trustworthy. However, only the difference between two distributions is considered in the proposed method, and the errors could be made by data sparseness.","Figure 5 shows that the number of training instances does not matter in computing probabilities of","-grams. Even though the accuracy increases slightly, the accuracy difference after 900,000 instances is not significant. It implies that the errors made by the proposed method is not from the lack of training instance but from the lack of its expression power for the target task. This result also complies with Figure 1. 5.3 Effect of Right Context","All the experiments above considered left context","only. However, Kang reported that the probabilis-","tic model using both left and right context outper-","forms the one that uses left context only (Kang,","2004). In his work, the word spacing probabil-","ity   ","","","","","between two adjacent syllables","","and  ","is given as","","","","","","","","","","  ","","","","","","","","","","","","","","","","","  ","","","","","","(4)","","","","","","   ","","","","",""," 638 0.7 0.75 0.8 0.85 0.9 0.95 1","0","1e+06","2e+06","3e+06 4e+06","5e+06","6e+06","7e+06","8e+06 Accuracy (%) No. of Training Examples Figure 5: The effect of the number of training examples in the self-organizing","-gram model.","Context Accuracy (%) Left Context Only 91.31 Right Context Only 88.26 Both Contexts 92.54 Table 3: The effect of using both left and right context.","where","  ","","","are computed respectively","based on the syllable frequency.","In order to reflect the idea of bidirectional con-","text in the proposed model, the model is enhanced","by modifying","","","","","","   ","","","in Equation (1). That","is, the likelihood of","   ","","","","","","","","is expanded to","be","","","","","","","","","","","","","","   ","","","","","","","","","","","","","","","","","","","","   ","","","","","","","","","","","","  ","","","","","",""," Since the coefficients of Equation (4) were determined arbitrarily (Kang, 2004), they are replaced with parameters","","of which values are determined using a held-out data.","The change of accuracy by the context is shown","in Table 3. When only the right context is used,","the accuracy gets 88.26% which is worse than the","left context only. That is, the original","-gram","is a relatively good model. However, when both","left and right context are used, the accuracy be-","comes 92.54%. The accuracy improvement by","using additional right context is 1.23%. This re-","sults coincide with the previous report (Lee et","al., 2002). The","","’s to achieve this accuracy are","","","   ","","","","",", and","","","","",".","Method Accuracy(%) Normal HMM 92.37","Self-Organizing HMM 94.71 Table 4: The effect of considering a tag sequence. 5.4 Effect of Considering Tag Sequence The state-of-the-art performance on Korean word spacing is to use the hidden Markov model. According to the previous work (Lee et al., 2002), the hidden Markov model shows the best performance when it sees two previous tags and two previous syllables.","For the simplicity in the experiments, the value for","in Equation (3) is set to be one. The performance comparison between normal HMM and the proposed method is given in Table 4. The proposed method considers the various number of previous syllables, whereas the normal HMM has the fixed context. Thus, the proposed method in Table 4 is specified as ‘self-organizing HMM.’ The accuracy of the self-organizing HMM is 94.71%, while that of the normal HMM is just 92.37%. Even though the normal HMM considers more previous tags (","","","), the accuracy of the self-organizing model is 2.34% higher than that of the normal HMM. Therefore, the proposed method that considers the sequence of word spacing tags achieves higher accuracy than any other methods reported ever."]},{"title":"6 Conclusions","paragraphs":["In this paper we have proposed a new method to learn word spacing in Korean by adaptively organizing context size. Our method is based on the simple","-gram model, but the context size","is changed as needed. When the increased context is much different from the current one, the context size is increased. In the same way, the context is decreased, if the decreased context is not so much different from the current one. The benefits of this method are that it can consider wider context by increasing context size as required, and save the computational cost due to the reduced context.","The experiments on HANTEC corpora showed that the proposed method improves the accuracy of the trigram model by 3.72%. Even compared with some well-known machine learning algorithms, it achieved the improvement of 2.63% over decision trees and 2.21% over support vector machines. In addition, we showed two ways for improving the 639 proposed method: considering right context and word spacing sequence. By considering left and right context at the same time, the accuracy is improved by 1.23%, and the consideration of word spacing sequence gives the accuracy improvement of 2.34%.","The","-gram model is one of the most widely used methods in natural language processing and information retrieval. Especially, it is one of the successful language models, which is a key technique in language and speech processing. Therefore, the proposed method can be applied to not only word spacing but also many other tasks. Even though word spacing is one of the important tasks in Korean information processing, it is just a simple task in many other languages such as English, German, and French. However, due to its generality, the importance of the proposed method yet does hold in such languages."]},{"title":"Acknowledgements","paragraphs":["This work was supported by the Korea Research Foundation Grant funded by the Korean Government (KRF-2005-202-D00465)."]},{"title":"References","paragraphs":["Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin. 2003. A Neural Probabilistic Language Model. Journal of Machine Learning Research, Vol. 3, pp. 1137–1155.","E. Charniak. 1993. Statistical Language Learning. MIT Press.","S. Chen and J. Goodman. 1996. An Empirical Study of Smoothing Techniques for Language Modeling. In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics, pp. 310–318.","M. Dickinson and W. Meurers. 2005. Detecting Errors in Discontinuous Structural Annotation. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pp. 322–329.","F. Jelinek and R. Mercer. 1980. Interpolated Estimation of Markov Source Parameters from Sparse Data. In Proceedings of the Workshop on Pattern Recogni-tion in Practice.","T. Joachims. 1998. Making Large-Scale SVM Learn-","ing Practical. LS8, Universit","","t Dortmund.","S.-S. Kang, 2000. Eojeol-Block Bidirectional Algorithm for Automatic Word Spacing of Hangul Sentences. Journal of KISS, Vol. 27, No. 4, pp. 441– 447. (in Korean)","S.-S. Kang. 2004. Improvement of Automatic Word Segmentation of Korean by Simplifying Syllable Bigram. In Proceedings of the 15th Conference on Korean Language and Information Processing, pp. 227–231. (in Korean)","S. Katz. 1987. Estimation of Probabilities from Sparse Data for the Language Model Component of a Speech Recognizer. IEEE Transactions on Acoustics, Speech and Signal Processing. Vol. 35, No. 3, pp. 400–401.","K.-S. Kim, H.-J. Lee, and S.-J. Lee. 1998. Three-Stage Spacing System for Korean in Sentence with No Word Boundaries. Journal of KISS, Vol. 25, No. 12, pp. 1838–1844. (in Korean)","J.-D. Kim, H.-C. Rim, and J. Tsujii. 2003. Self-Organizing Markov Models and Their Application to Part-of-Speech Tagging. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics, pp. 296–302.","D.-G. Lee, S.-Z. Lee, H.-C. Rim, and H.-S. Lim, 2002. Automatic Word Spacing Using Hidden Markov Model for Refining Korean Text Corpora. In Proceedings of the 3rd Workshop on Asian Language Resources and International Standardization, pp. 51–57. T. Mitchell. 1997. Machine Learning. McGraw Hill.","D. Mochihashi and Y. Matsumoto. 2006. Context as Filtering. Advances in Neural Information Process-ing Systems 18, pp. 907–914.","S.-B. Park and B.-T. Zhang. 2002. A Boosted Maximum Entropy Model for Learning Text Chunking. In Proceedings of the 19th International Conference on Machine Learning, pp. 482–489.","R. Quinlan. 1993. C4.5: Program for Machine Learning. Morgan Kaufmann Publishers.","D. Ron, Y. Singer, and N. Tishby. 1996. The Power of Amnesia: Learning Probabilistic Automata with Variable Memory Length. Machine Learning, Vol. 25, No. 2, pp. 117–149.","R. Rosenfeld. 1996. A Maximum Entropy Approach to Adaptive Statistical Language Modeling. Computer, Speech and Language, Vol. 10, pp. 187– 228.","H. Schütze and Y. Singer. 1994. Part-of-Speech Tagging Using a Variable Memory Markov Model. In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics, pp. 181– 187.","M. Siu and M. Ostendorf. 2000. Variable N-Grams and Extensions for Conversational Speech Language Modeling. IEEE Transactions on Speech and Audio Processing, Vol. 8, No. 1, pp. 63–75. 640"]}],"references":[{"authors":[{"first":"Y.","last":"Bengio"},{"first":"R.","last":"Ducharme"},{"first":"P.","last":"Vincent"},{"first":"C.","last":"Jauvin"}],"year":"2003","title":"A Neural Probabilistic Language Model","source":"Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin. 2003. A Neural Probabilistic Language Model. Journal of Machine Learning Research, Vol. 3, pp. 1137–1155."},{"authors":[{"first":"E.","last":"Charniak"}],"year":"1993","title":"Statistical Language Learning","source":"E. Charniak. 1993. Statistical Language Learning. MIT Press."},{"authors":[{"first":"S.","last":"Chen"},{"first":"J.","last":"Goodman"}],"year":"1996","title":"An Empirical Study of Smoothing Techniques for Language Modeling","source":"S. Chen and J. Goodman. 1996. An Empirical Study of Smoothing Techniques for Language Modeling. In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics, pp. 310–318."},{"authors":[{"first":"M.","last":"Dickinson"},{"first":"W.","last":"Meurers"}],"year":"2005","title":"Detecting Errors in Discontinuous Structural Annotation","source":"M. Dickinson and W. Meurers. 2005. Detecting Errors in Discontinuous Structural Annotation. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pp. 322–329."},{"authors":[{"first":"F.","last":"Jelinek"},{"first":"R.","last":"Mercer"}],"year":"1980","title":"Interpolated Estimation of Markov Source Parameters from Sparse Data","source":"F. Jelinek and R. Mercer. 1980. Interpolated Estimation of Markov Source Parameters from Sparse Data. In Proceedings of the Workshop on Pattern Recogni-tion in Practice."},{"authors":[],"source":"T. Joachims. 1998. Making Large-Scale SVM Learn-"},{"authors":[],"source":"ing Practical. LS8, Universit"},{"authors":[],"source":""},{"authors":[],"source":"t Dortmund."},{"authors":[{"first":"S.","middle":"-S.","last":"Kang"}],"year":"2000","title":"Eojeol-Block Bidirectional Algorithm for Automatic Word Spacing of Hangul Sentences","source":"S.-S. Kang, 2000. Eojeol-Block Bidirectional Algorithm for Automatic Word Spacing of Hangul Sentences. Journal of KISS, Vol. 27, No. 4, pp. 441– 447. (in Korean)"},{"authors":[{"first":"S.","middle":"-S.","last":"Kang"}],"year":"2004","title":"Improvement of Automatic Word Segmentation of Korean by Simplifying Syllable Bigram","source":"S.-S. Kang. 2004. Improvement of Automatic Word Segmentation of Korean by Simplifying Syllable Bigram. In Proceedings of the 15th Conference on Korean Language and Information Processing, pp. 227–231. (in Korean)"},{"authors":[{"first":"S.","last":"Katz"}],"year":"1987","title":"Estimation of Probabilities from Sparse Data for the Language Model Component of a Speech Recognizer","source":"S. Katz. 1987. Estimation of Probabilities from Sparse Data for the Language Model Component of a Speech Recognizer. IEEE Transactions on Acoustics, Speech and Signal Processing. Vol. 35, No. 3, pp. 400–401."},{"authors":[{"first":"K.","middle":"-S.","last":"Kim"},{"first":"H.","middle":"-J.","last":"Lee"},{"first":"S.","middle":"-J.","last":"Lee"}],"year":"1998","title":"Three-Stage Spacing System for Korean in Sentence with No Word Boundaries","source":"K.-S. Kim, H.-J. Lee, and S.-J. Lee. 1998. Three-Stage Spacing System for Korean in Sentence with No Word Boundaries. Journal of KISS, Vol. 25, No. 12, pp. 1838–1844. (in Korean)"},{"authors":[{"first":"J.","middle":"-D.","last":"Kim"},{"first":"H.","middle":"-C.","last":"Rim"},{"first":"J.","last":"Tsujii"}],"year":"2003","title":"Self-Organizing Markov Models and Their Application to Part-of-Speech Tagging","source":"J.-D. Kim, H.-C. Rim, and J. Tsujii. 2003. Self-Organizing Markov Models and Their Application to Part-of-Speech Tagging. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics, pp. 296–302."},{"authors":[{"first":"D.","middle":"-G.","last":"Lee"},{"first":"S.","middle":"-Z.","last":"Lee"},{"first":"H.","middle":"-C.","last":"Rim"},{"first":"H.","middle":"-S.","last":"Lim"}],"year":"2002","title":"Automatic Word Spacing Using Hidden Markov Model for Refining Korean Text Corpora","source":"D.-G. Lee, S.-Z. Lee, H.-C. Rim, and H.-S. Lim, 2002. Automatic Word Spacing Using Hidden Markov Model for Refining Korean Text Corpora. In Proceedings of the 3rd Workshop on Asian Language Resources and International Standardization, pp. 51–57. T. Mitchell. 1997. Machine Learning. McGraw Hill."},{"authors":[{"first":"D.","last":"Mochihashi"},{"first":"Y.","last":"Matsumoto"}],"year":"2006","title":"Context as Filtering","source":"D. Mochihashi and Y. Matsumoto. 2006. Context as Filtering. Advances in Neural Information Process-ing Systems 18, pp. 907–914."},{"authors":[{"first":"S.","middle":"-B.","last":"Park"},{"first":"B.","middle":"-T.","last":"Zhang"}],"year":"2002","title":"A Boosted Maximum Entropy Model for Learning Text Chunking","source":"S.-B. Park and B.-T. Zhang. 2002. A Boosted Maximum Entropy Model for Learning Text Chunking. In Proceedings of the 19th International Conference on Machine Learning, pp. 482–489."},{"authors":[{"first":"R.","last":"Quinlan"}],"year":"1993","title":"C4","source":"R. Quinlan. 1993. C4.5: Program for Machine Learning. Morgan Kaufmann Publishers."},{"authors":[{"first":"D.","last":"Ron"},{"first":"Y.","last":"Singer"},{"first":"N.","last":"Tishby"}],"year":"1996","title":"The Power of Amnesia: Learning Probabilistic Automata with Variable Memory Length","source":"D. Ron, Y. Singer, and N. Tishby. 1996. The Power of Amnesia: Learning Probabilistic Automata with Variable Memory Length. Machine Learning, Vol. 25, No. 2, pp. 117–149."},{"authors":[{"first":"R.","last":"Rosenfeld"}],"year":"1996","title":"A Maximum Entropy Approach to Adaptive Statistical Language Modeling","source":"R. Rosenfeld. 1996. A Maximum Entropy Approach to Adaptive Statistical Language Modeling. Computer, Speech and Language, Vol. 10, pp. 187– 228."},{"authors":[{"first":"H.","last":"Schütze"},{"first":"Y.","last":"Singer"}],"year":"1994","title":"Part-of-Speech Tagging Using a Variable Memory Markov Model","source":"H. Schütze and Y. Singer. 1994. Part-of-Speech Tagging Using a Variable Memory Markov Model. In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics, pp. 181– 187."},{"authors":[{"first":"M.","last":"Siu"},{"first":"M.","last":"Ostendorf"}],"year":"2000","title":"Variable N-Grams and Extensions for Conversational Speech Language Modeling","source":"M. Siu and M. Ostendorf. 2000. Variable N-Grams and Extensions for Conversational Speech Language Modeling. IEEE Transactions on Speech and Audio Processing, Vol. 8, No. 1, pp. 63–75. 640"}],"cites":[{"style":0,"text":"Kang, 2000","origin":{"pointer":"/sections/3/paragraphs/0","offset":351,"length":10},"authors":[{"last":"Kang"}],"year":"2000","references":["/references/9"]},{"style":0,"text":"Kim et al., 1998","origin":{"pointer":"/sections/3/paragraphs/0","offset":462,"length":16},"authors":[{"last":"Kim"},{"last":"al."}],"year":"1998","references":["/references/12"]},{"style":0,"text":"Kang, 2004","origin":{"pointer":"/sections/3/paragraphs/1","offset":526,"length":10},"authors":[{"last":"Kang"}],"year":"2004","references":["/references/10"]},{"style":0,"text":"Chen and Goodman, 1996","origin":{"pointer":"/sections/3/paragraphs/10","offset":14,"length":22},"authors":[{"last":"Chen"},{"last":"Goodman"}],"year":"1996","references":["/references/2"]},{"style":0,"text":"Mitchell, 1996","origin":{"pointer":"/sections/3/paragraphs/11","offset":45,"length":14},"authors":[{"last":"Mitchell"}],"year":"1996","references":[]},{"style":0,"text":"Jelinek and Mercer, 1980","origin":{"pointer":"/sections/3/paragraphs/12","offset":36,"length":24},"authors":[{"last":"Jelinek"},{"last":"Mercer"}],"year":"1980","references":["/references/4"]},{"style":0,"text":"Katz, 1987","origin":{"pointer":"/sections/3/paragraphs/12","offset":62,"length":10},"authors":[{"last":"Katz"}],"year":"1987","references":["/references/11"]},{"style":0,"text":"Rosenfeld, 1996","origin":{"pointer":"/sections/3/paragraphs/15","offset":14,"length":15},"authors":[{"last":"Rosenfeld"}],"year":"1996","references":["/references/19"]},{"style":0,"text":"Park and Zhang, 2002","origin":{"pointer":"/sections/3/paragraphs/15","offset":469,"length":20},"authors":[{"last":"Park"},{"last":"Zhang"}],"year":"2002","references":["/references/16"]},{"style":0,"text":"Siu and Ostendorf, 2000","origin":{"pointer":"/sections/3/paragraphs/17","offset":80,"length":23},"authors":[{"last":"Siu"},{"last":"Ostendorf"}],"year":"2000","references":["/references/21"]},{"style":0,"text":"Rosenfeld, 1996","origin":{"pointer":"/sections/3/paragraphs/17","offset":177,"length":15},"authors":[{"last":"Rosenfeld"}],"year":"1996","references":["/references/19"]},{"style":0,"text":"Mochihashi and Mastumoto, 2006","origin":{"pointer":"/sections/3/paragraphs/17","offset":217,"length":30},"authors":[{"last":"Mochihashi"},{"last":"Mastumoto"}],"year":"2006","references":[]},{"style":0,"text":"Schütze and Singer, 1994","origin":{"pointer":"/sections/3/paragraphs/17","offset":449,"length":24},"authors":[{"last":"Schütze"},{"last":"Singer"}],"year":"1994","references":["/references/20"]},{"style":0,"text":"Ron et al., 1996","origin":{"pointer":"/sections/3/paragraphs/17","offset":527,"length":16},"authors":[{"last":"Ron"},{"last":"al."}],"year":"1996","references":["/references/18"]},{"style":0,"text":"Kim et al., 2003","origin":{"pointer":"/sections/3/paragraphs/17","offset":547,"length":16},"authors":[{"last":"Kim"},{"last":"al."}],"year":"2003","references":["/references/13"]},{"style":0,"text":"Dickinson and Meurers, 2005","origin":{"pointer":"/sections/3/paragraphs/17","offset":626,"length":27},"authors":[{"last":"Dickinson"},{"last":"Meurers"}],"year":"2005","references":["/references/3"]},{"style":0,"text":"Bengio et al., 2003","origin":{"pointer":"/sections/4/paragraphs/255","offset":47,"length":19},"authors":[{"last":"Bengio"},{"last":"al."}],"year":"2003","references":["/references/0"]},{"style":0,"text":"Charniak, 1993","origin":{"pointer":"/sections/5/paragraphs/14","offset":8,"length":14},"authors":[{"last":"Charniak"}],"year":"1993","references":["/references/1"]},{"style":0,"text":"Lee et al., 2002","origin":{"pointer":"/sections/5/paragraphs/368","offset":434,"length":16},"authors":[{"last":"Lee"},{"last":"al."}],"year":"2002","references":["/references/14"]},{"style":0,"text":"Quinlan, 1993","origin":{"pointer":"/sections/6/paragraphs/8","offset":623,"length":13},"authors":[{"last":"Quinlan"}],"year":"1993","references":["/references/17"]},{"style":0,"text":"Joachims, 1998","origin":{"pointer":"/sections/6/paragraphs/13","offset":1,"length":14},"authors":[{"last":"Joachims"}],"year":"1998","references":[]},{"style":0,"text":"Kang, 2004","origin":{"pointer":"/sections/6/paragraphs/177","offset":69,"length":10},"authors":[{"last":"Kang"}],"year":"2004","references":["/references/10"]},{"style":0,"text":"Lee et al., 2002","origin":{"pointer":"/sections/6/paragraphs/207","offset":239,"length":16},"authors":[{"last":"Lee"},{"last":"al."}],"year":"2002","references":["/references/14"]}]}
