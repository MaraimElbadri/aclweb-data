{"sections":[{"title":"Convolution Kernels with Feature Selection for Natural Language Processing Tasks Jun Suzuki, Hideki Isozaki and Eisaku Maeda NTT Communication Science Laboratories, NTT Corp. 2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto,619-0237 Japan fjun, isozaki, maedag@cslab.kecl.ntt.co.jp Abstract","paragraphs":["Convolution kernels, such as sequence and tree kernels, are advantageous for both the concept and accuracy of many natural language processing (NLP) tasks. Experiments have, however, shown that the over-fitting problem often arises when these kernels are used in NLP tasks. This paper discusses this issue of convolution kernels, and then proposes a new approach based on statistical feature selection that avoids this issue. To enable the proposed method to be executed efficiently, it is embedded into an original kernel calculation process by using sub-structure mining algorithms. Experiments are undertaken on real NLP tasks to confirm the problem with a conventional method and to compare its performance with that of the proposed method."]},{"title":"1 Introduction","paragraphs":["Over the past few years, many machine learning methods have been successfully applied to tasks in natural language processing (NLP). Especially, state-of-the-art performance can be achieved with kernel methods, such as Support Vector Machine (Cortes and Vapnik, 1995). Examples include text categorization (Joachims, 1998), chunking (Kudo and Matsumoto, 2002) and pars-ing (Collins and Duffy, 2001).","Another feature of this kernel methodology is that it not only provides high accuracy but also allows us to design a kernel function suited to modeling the task at hand. Since natural language data take the form of sequences of words, and are generally analyzed using discrete structures, such as trees (parsed trees) and graphs (relational graphs), discrete kernels, such as sequence kernels (Lodhi et al., 2002), tree kernels (Collins and Duffy, 2001), and graph kernels (Suzuki et al., 2003a), have been shown to offer excellent results.","These discrete kernels are related to convolution kernels (Haussler, 1999), which provides the concept of kernels over discrete structures. Convolution kernels allow us to treat structural features without explicitly representing the feature vectors from the input object. That is, convolution kernels are well suited to NLP tasks in terms of both accuracy and concept.","Unfortunately, experiments have shown that in some cases there is a critical issue with convolution kernels, especially in NLP tasks (Collins and Duffy, 2001; Cancedda et al., 2003; Suzuki et al., 2003b). That is, the over-fittingproblem arises if large “sub-structures” are used in the kernel calculations. As a result, the machine learning approach can never be trained efficiently.","To solve this issue, we generally eliminate large sub-structures from the set of features used. However, the main reason for using convolution kernels is that we aim to use structural features easily and efficiently. If use is limited to only very small structures, it negates the advantages of using convolution kernels.","This paper discusses this issue of convolution kernels, and proposes a new method based on statistical feature selection. The proposed method deals only with those features that are statistically significant for kernel calculation, large significant sub-structures can be used without over-fitting. Moreover, the proposed method can be executed efficiently by embedding it in an original kernel calculation process by using sub-structure mining algorithms.","In the next section, we provide a brief overview of convolution kernels. Section 3 discusses one issue of convolution kernels, the main topic of this paper, and introduces some conventional methods for solving this issue. In Section 4, we propose a new approach based on statistical feature selection to offset the issue of convolution kernels using an example consisting of sequence kernels. In Section 5, we briefly discuss the application of the proposed method to other convolution kernels. In Section 6, we compare the performance of conventional methods with that of the proposed method by using real NLP tasks: question classification and sentence modality identification. The experimental results described in Section 7 clarify the advantages of the proposed method."]},{"title":"2 Convolution Kernels","paragraphs":["Convolution kernels have been proposed as a concept of kernels for discrete structures, such as sequences, trees and graphs. This framework defines the kernel function between input objects as the convolution of “sub-k ernels”, i.e. the kernels for the decompositions (parts) of the objects.","Let X and Y be discrete objects. Conceptually, convolution kernels K(X; Y ) enumerate all sub-structures occurring in X and Y and then calculate their inner product, which is simply written as: K(X; Y ) = h(X); (Y )i = X i i(X) i(Y ): (1)","represents the feature mapping from the discrete object to the feature space; that is, (X) = (1(X); : : : ; i(X); : : :): With sequence kernels (Lodhi et al., 2002), input objects X and Y are sequences, and i(X) is a sub-sequence. With tree kernels (Collins and Duffy, 2001), X and Y are trees, and i(X) is a sub-tree.","When implemented, these kernels can be efficiently calculated in quadratic time by using dynamic programming (DP).","Finally, since the size of the input objects is not constant, the kernel value is normalized using the following equation.","K̂(X; Y ) = K(X; Y ) p K(X; X) K(Y; Y ) (2) The value of K̂(X; Y ) is from 0 to 1, K̂(X; Y ) = 1 if and only if X = Y . 2.1 Sequence Kernels To simplify the discussion, we restrict ourselves hereafter to sequence kernels. Other convolution kernels are briefly addressed in Section 5.","Many kinds of sequence kernels have been proposed for a variety of different tasks. This paper basically follows the framework of word sequence kernels (Cancedda et al., 2003), and so processes gapped word sequences to yield the kernel value.","Let be a set of finitesymbols, and n","be a set of possible (symbol) sequences whose sizes are n or less that are constructed by symbols in . The meaning of “size” in this paper is the number of symbols in the sub-structure. Namely, in the case of sequence, size n means length n. S and T can represent any sequence. si and tj represent the ith and jth symbols in S and T , respectively. Therefore, a S T 1 2 1 1 2 1 λ+λ λ1 λ λ1 1 1 1  S = T =  1010 10 0 10","2 1 1 0 1 3 λ λ+ 0 λ 0 0 λ 0   u","3 λ λ+ + λ  10 0 Figure 1: Example of sequence kernel output sequence S can be written as S = s1 : : : si : : : sjSj, where jSj represents the length of S. If sequence u is contained in sub-sequence S[i : j]","def","= si : : : sj of S (allowing the existence of gaps), the position of u in S is written as i = (i1 : ijuj). The length of S[i] is l(i) = ijuj i1 + 1. For example, if u = ab and S = cacbd, then i = (2 : 4) and l(i) = 4 2 + 1 = 3.","By using the above notations, sequence kernels can be definedas:","KSK (S; T ) = X u2 n X iju=S[i](i) X","jju=T [j](j) ; (3) where is the decay factor that handles the gap present in a common sub-sequence u, and (i) = l(i) juj. In this paper, j means “such that”. Figure 1 shows a simple example of the output of this kernel.","However, in general, the number of features j n","j, which is the dimension of the feature space, becomes very high, and it is computationally infeasible to calculate Equation (3) explicitly. The efficient recursive calculation has been introduced in (Cancedda et al., 2003). To clarify the discussion, we redefinethe sequence kernels with our notation.","The sequence kernel can be written as follows:","KSK (S; T ) = n X m=1 X 1 ij Sj X 1 jj T jJm(Si; Tj): (4) where Si and Tj represent the sub-sequences Si = s1; s2; : : : ; si and Tj = t1; t2; : : : ; tj, respectively.","Let Jm(Si; Tj) be a function that returns the value of common sub-sequences if si = tj.","Jm(Si; Tj) = J0 m 1(Si; Tj) I(si; tj) (5)","I(si; tj) is a function that returns a matching value between si and tj. This paper definesI(si; tj) as an indicator function that returns 1 if si = tj, otherwise 0. Then, J 0","m(Si; Tj) and J 00","m(Si; Tj) are introduced","to calculate the common gapped sub-sequences be-","tween Si and Tj. J0 m(Si; Tj) = 8 >< >: 1 if m = 0; 0 if j = 0 and m > 0; J0","m(Si; Tj 1) + J00","m(Si; Tj 1) otherwise (6) J00 m(Si; Tj) = 8 < : 0 if i = 0; J00 m(Si 1; Tj) + Jm(Si 1; Tj)","otherwise (7)","If we calculate Equations (5) to (7) recursively, Equation (4) provides exactly the same value as Equation (3)."]},{"title":"3 Problem of Applying Convolution Kernels to NLP tasks","paragraphs":["This section discusses an issue that arises when applying convolution kernels to NLP tasks.","According to the original definition of convolution kernels, all the sub-structures are enumerated and calculated for the kernels. The number of sub-structures in the input object usually becomes exponential against input object size. As a result, all kernel values K̂(X; Y ) are nearly 0 except the kernel value of the object itself, K̂(X; X), which is 1. In this situation, the machine learning process becomes almost the same as memory-based learning. This means that we obtain a result that is very precise but with very low recall.","To avoid this, most conventional methods use an approach that involves smoothing the kernel values or eliminating features based on the sub-structure size.","For sequence kernels, (Cancedda et al., 2003) use a feature elimination method based on the size of sub-sequence n. This means that the kernel calculation deals only with those sub-sequences whose size is n or less. For tree kernels, (Collins and Duffy, 2001) proposed a method that restricts the features based on sub-trees depth. These methods seem to work well on the surface, however, good results are achieved only when n is very small, i.e. n = 2.","The main reason for using convolution kernels is that they allow us to employ structural features simply and efficiently. When only small sized sub-structures are used (i.e. n = 2), the full benefitsof convolution kernels are missed.","Moreover, these results do not mean that larger sized sub-structures are not useful. In some cases we already know that larger sub-structures are significant features as regards solving the target problem. That is, these significantlarger sub-structures, Table 1: Contingency table and notation for the chi-squared value c c P","row u Ouc = y Ouc Ou = x u Ouc Ouc Ou P column Oc = M Oc N which the conventional methods cannot deal with efficiently, should have a possibility of improving the performance furthermore.","The aim of the work described in this paper is to be able to use any significant sub-structure efficiently, regardless of its size, to solve NLP tasks."]},{"title":"4 Proposed Feature Selection Method","paragraphs":["Our approach is based on statistical feature selection in contrast to the conventional methods, which use sub-structure size.","For a better understanding, consider the two-class (positive and negative) supervised classification problem. In our approach we test the statistical deviation of all the sub-structures in the training samples between the appearance of positive samples and negative samples. This allows us to select only the statistically significantsub-structures when calculating the kernel value.","Our approach, which uses a statistical metric to select features, is quite natural. We note, however, that kernels are calculated using the DP algorithm. Therefore, it is not clear how to calculate kernels efficientlywith a statistical feature selection method. First, we briefly explain a statistical metric, the chi-squared (2",") value, and provide an idea of how to select significant features. We then describe a method for embedding statistical feature selection into kernel calculation. 4.1 Statistical Metric: Chi-squared Value There are many kinds of statistical metrics, such as chi-squared value, correlation coefficient and mutual information. (Rogati and Yang, 2002) reported that chi-squared feature selection is the most effective method for text classification. Following this information, we use 2","values as statistical feature selection criteria. Although we selected 2","values, any other statistical metric can be used as long as it is based on the contingency table shown in Table 1. We briefly explain how to calculate the 2","value by referring to Table 1. In the table, c and c represent the names of classes, c for the positive class S T 1 2 1 1 2 1 λ+λ λ1 λ λ1 2 uχ 1 1 1   S = T =  1010 10 0 10","2 1 1 0 1 3 λ λ+ 0 λ 0 0 λ 0 τ ="," 1 1 λ   u","3 λ λ+ + 2 λ+ 0 0 0 0","2 1 1 0 1 3 λ λ+ 0 λ 0 0 λ 0    λ  10 0 0 Figure 2: Example of statistical feature selection and c for the negative class. Ouc, Ouc, Ouc and Ouc represent the number of u that appeared in the positive sample c, the number of u that appeared in the negative sample c, the number of u that did not appear in c, and the number of u that did not appear in c, respectively. Let y be the number of samples of positive class c that contain sub-sequence u, and x be the number of samples that contain u. Let N be the total number of (training) samples, and M be the number of positive samples.","Since N and M are constant for (fixed) data, 2 can be written as a function of x and y,","2 (x; y) = N (Ouc Ouc Ouc Ouc)2","Ou Ou Oc Oc : (8)","2 expresses the normalized deviation of the obser-","vation from the expectation. We simply represent 2","(x; y) as 2","(u). 4.2 Feature Selection Criterion The basic idea of feature selection is quite natural. First, we decide the threshold of the 2","value. If 2","(u) < holds, that is, u is not statistically significant, then u is eliminated from the features and the value of u is presumed to be 0 for the kernel value.","The sequence kernel with feature selection (FSSK) can be definedas follows:","KFSSK (S; T ) = X","2 (u)ju2 n X iju=S[i](i) X","jju=T [j](j) : (9) The difference between Equations (3) and (9) is simply the condition of the firstsummation. FSSK selects significantsub-sequence u by using the condition of the statistical metric 2","(u).","Figure 2 shows a simple example of what FSSK calculates for the kernel value. 4.3 Efficient2","(u) Calculation Method It is computationally infeasible to calculate 2","(u) for all possible u with a naive exhaustive method. In our approach, we use a sub-structure mining algorithm to calculate 2","(u). The basic idea comes from a sequential pattern mining technique, PrefixSpan (Pei et al., 2001), and a statistical metric pruning (SMP) method, Apriori SMP (Morishita and Sese, 2000). By using these techniques, all the significantsub-sequences u that satisfy 2","(u) can be found efficientlyby depth-firstsearch and pruning. Below, we briefly explain the concept involved in findingthe significantfeatures.","First, we denote uv, which is the concatenation of sequences u and v. Then, u is a specific sequence and uv is any sequence that is constructed by u with any suffix v. The upper bound of the 2","value of uv can be definedby the value of u (Morishita and Sese, 2000).","2 (uv) max","2 (y","u; yu); 2 (x u yu; 0) ","= b2 (u) where xu and yu represent the value of x and y of u. This inequation indicates that if b2","(u) is less than a certain threshold , all sub-sequences uv can be eliminated from the features, because no sub-sequence uv can be a feature.","The PrefixSpanalgorithm enumerates all the significantsub-sequences by using a depth-firstsearch and constructing a TRIE structure to store the significant sequences of internal results efficiently. Specifically, PrefixSpan algorithm evaluates uw, where uw represents a concatenation of a sequence u and a symbol w, using the following three conditions.","1. 2 (uw)","2. > 2 (uw), > b2","(uw)","3. > 2 (uw), b2","(uw) With 1, sub-sequence uw is selected as a significant feature. With 2, sub-sequence uw and arbitrary sub-sequences uwv, are less than the threshold . Then w is pruned from the TRIE, that is, all uwv where v represents any suffixpruned from the search space. With 3, uw is not selected as a significant feature because the 2","value of uw is less than , however, uwv can be a significantfeature because the upper-bound 2","value of uwv is greater than , thus the search is continued to uwv.","Figure 3 shows a simple example of PrefixSpan with SMP that searches for the significant features           ⊥   τ =         a w = 2 χ 2 ̂χ  x y      u =                    x y                               w = x y                            5N = 2M =         Figure 3: Efficientsearch for statistically significant sub-sequences using the PrefixSpanalgorithm with SMP by using a depth-firstsearch with a TRIE representation of the significant sequences. The values of each symbol represent 2","(u) and b2","(u) that can be calculated from the number of xu and yu. The TRIE structure in the figurerepresents the statistically significant sub-sequences that can be shown in a path from ? to the symbol.","We exploit this TRIE structure and PrefixSpan pruning method in our kernel calculation.","4.4 Embedding Feature Selection in Kernel Calculation This section shows how to integrate statistical feature selection in the kernel calculation. Our proposed method is definedin the following equations.","KFSSK (S; T ) = n X m=1 X 1 ij Sj X 1 jj T jKm(Si; Tj) (10) Let Km(Si; Tj) be a function that returns the sum value of all statistically significant common sub-sequences u if si = tj. Km(Si; Tj) = X u2 m(Si;Tj) Ju(Si; Tj); (11) where m(Si; Tj) represents a set of sub-sequences whose size juj is m and that satisfy the above condition 1. The m(Si; Tj) is definedin detail in Equation (15).","Then, let Ju(Si; Tj), J 0","u(Si; Tj) and J 00","u (Si; Tj) be functions that calculate the value of the common sub-sequences between Si and Tj recursively, as well as equations (5) to (7) for sequence kernels. We introduce a special symbol to represent an “empty sequence”, and define w = w and j wj = 1. Juw(Si; Tj) = 8 < : J 0 u(Si; Tj) I(w) if uw 2 b juwj(Si; Tj); 0 otherwise (12) where I(w) is a function that returns a matching value of w. In this paper, we defineI(w) is 1.","b m(Si; Tj) has realized conditions 2 and 3; the details are definedin Equation (16). J 0 u(Si; Tj) = 8 >< >: 1 if u = ; 0 if j = 0 and u 6= ; J 0","u(Si; Tj 1) + J 00","u (Si; Tj 1) otherwise (13) J 00 u (Si; Tj) = 8 < : 0 if i = 0; J 00 u (Si 1; Tj) + Ju(Si 1; Tj)","otherwise (14)","The following five equations are introduced to select a set of significant sub-sequences. m(Si; Tj) and b m(Si; Tj) are sets of sub-sequences (features) that satisfy condition 1 and 3, respectively, when calculating the value between Si and Tj in Equations (11) and (12).","m(Si; Tj) = fu j u 2 b m(Si; Tj); 2 (u)g (15) b m(Si; Tj) = 8 < : ( b 0","m 1(Si; Tj); si) if si = tj ; otherwise (16)","( F; w) = fuw j u 2 F; b2 (uw)g; (17) where F represents a set of sub-sequences. Notice that m(Si; Tj) and b m(Si; Tj) have only sub-sequences u that satisfy 2","(uw) or b2","(uw), respectively, if s i = tj(= w); otherwise they become empty sets.","The following two equations are introduced for recursive set operations to calculate m(Si; Tj) and b m(Si; Tj). b 0 m(Si; Tj) = 8 >>< >>: f g if m = 0; ; if j = 0 and m > 0; b 0 m(Si; Tj 1) [ b 00","m(Si; Tj 1) otherwise (18) b 00 m(Si; Tj) = 8 < : ; if i = 0 ; b 00 m(Si 1; Tj) [ b m(Si 1; Tj) otherwise (19)","In the implementation, Equations (11) to (14) can be performed in the same way as those used to calculate the original sequence kernels, if the feature selection condition of Equations (15) to (19) has been removed. Then, Equations (15) to (19), which select significantfeatures, are performed by the PrefixSpan algorithm described above and the TRIE representation of statistically significantfeatures.","The recursive calculation of Equations (12) to (14) and Equations (16) to (19) can be executed in the same way and at the same time in parallel. As a result, statistical feature selection can be embedded in oroginal sequence kernel calculation based on a dynamic programming technique. 4.5 Properties The proposed method has several important advantages over the conventional methods.","First, the feature selection criterion is based on a statistical measure, so statistically significantfeatures are automatically selected.","Second, according to Equations (10) to (18), the proposed method can be embedded in an original kernel calculation process, which allows us to use the same calculation procedure as the conventional methods. The only difference between the original sequence kernels and the proposed method is that the latter calculates a statistical metric 2","(u) by using a sub-structure mining algorithm in the kernel calculation.","Third, although the kernel calculation, which unifies our proposed method, requires a longer training time because of the feature selection, the selected sub-sequences have a TRIE data structure. This means a fast calculation technique proposed in (Kudo and Matsumoto, 2003) can be simply applied to our method, which yields classificationvery quickly. In the classificationpart, the features (sub-sequences) selected in the learning part must be known. Therefore, we store the TRIE of selected sub-sequences and use them during classification."]},{"title":"5 Proposed Method Applied to Other Convolution Kernels","paragraphs":["We have insufficientspace to discuss this subject in detail in relation to other convolution kernels. However, our proposals can be easily applied to tree kernels (Collins and Duffy, 2001) by using string encoding for trees. We enumerate nodes (labels) of tree in postorder traversal. After that, we can employ a sequential pattern mining technique to select statistically significantsub-trees. This is because we can convert to the original sub-tree form from the string encoding representation. Table 2: Parameter values of proposed kernels and Support Vector Machines","parameter value","soft margin for SVM (C) 1000","decay factor of gap () 0.5","threshold of 2","() 2.7055","3.8415","As a result, we can calculate tree kernels with statistical feature selection by using the original tree kernel calculation with the sequential pattern mining technique introduced in this paper. Moreover, we can expand our proposals to hierarchically structured graph kernels (Suzuki et al., 2003a) by using a simple extension to cover hierarchical structures."]},{"title":"6 Experiments","paragraphs":["We evaluated the performance of the proposed method in actual NLP tasks, namely English question classification(EQC), Japanese question classification(JQC) and sentence modality identification (MI) tasks.","We compared the proposed method (FSSK) with a conventional method (SK), as discussed in Section 3, and with bag-of-words (BOW) Kernel (BOW-K)(Joachims, 1998) as baseline methods.","Support Vector Machine (SVM) was selected as the kernel-based classifier for training and classification. Table 2 shows some of the parameter values that we used in the comparison. We set thresholds of = 2:7055 (FSSK1) and = 3:8415 (FSSK2) for the proposed methods; these values represent the 10% and 5% level of significancein the 2","distribu-tion with one degree of freedom, which used the 2 significanttest. 6.1 Question Classification Question classificationis definedas a task similar to text categorization; it maps a given question into a question type.","We evaluated the performance by using data provided by (Li and Roth, 2002) for English and (Suzuki et al., 2003b) for Japanese question classificationand followed the experimental setting used in these papers; namely we use four typical question types, LOCATION, NUMEX, ORGANIZATION, and TIME TOP for JQA, and “coarse” and “fine” classes for EQC. We used the one-vs-rest classifier of SVM as the multi-class classification method for EQC.","Figure 4 shows examples of the question classification data used here. question types input object : word sequences ([ ]: information of chunk and h i: named entity) ABBREVIATION what,[B-NP] be,[B-VP] the,[B-NP] abbreviation,[I-NP] for,[B-PP] Texas,[B-NP],hB-GPEi ?,[O] DESCRIPTION what,[B-NP] be,[B-VP] Aborigines,[B-NP] ?,[O]","HUMAN who,[B-NP] discover,[B-VP] America,[B-NP],hB-GPEi ?,[O] Figure 4: Examples of English question classificationdata Table 3: Results of the Japanese question classification(F-measure) (a) TIME TOP (b) LOCATION (c) ORGANIZATION (d) NUMEX","n FSSK1 FSSK2 SK BOW-K 1 2 3 4 1 - .961 .958 .957 .956 - .961 .956 .957 .956 - .946 .910 .866 .223",".902 .909 .886 .855 - 1 2 3 4 1 - .795 .793 .798 .792 - .788 .799 .804 .800 - .791 .775 .732 .169",".744 .768 .756 .747 - 1 2 3 4 1 - .709 .720 .720 .723 - .703 .710 .716 .720 - .705 .668 .594 .035",".641 690 .636 .572 - 1 2 3 4 1 - .912 .915 .908 .908 - .913 .916 .911 .913 - .912 .885 .817 .036",".842 .852 .807 .726 - 6.2 Sentence Modality Identification For example, sentence modality identificationtechniques are used in automatic text analysis systems that identify the modality of a sentence, such as “opinion” or “description”.","The data set was created from Mainichi news articles and one of three modality tags, “opinion”, “decision” and “description” was applied to each sentence. The data size was 1135 sentences consisting of 123 sentences of “opinion”, 326 of “decision” and 686 of “description”. We evaluated the results by using 5-fold cross validation."]},{"title":"7 Results and Discussion","paragraphs":["Tables 3 and 4 show the results of Japanese and English question classification, respectively. Table 5 shows the results of sentence modality identification. n in each table indicates the threshold of the sub-sequence size. n = 1 means all possible sub-sequences are used.","First, SK was consistently superior to BOW-K. This indicates that the structural features were quite efficient in performing these tasks. In general we can say that the use of structural features can improve the performance of NLP tasks that require the details of the contents to perform the task.","Most of the results showed that SK achieves its maximum performance when n = 2. The performance deteriorates considerably once n exceeds 4. This implies that SK with larger sub-structures degrade classification performance. These results show the same tendency as the previous studies discussed in Section 3. Table 6 shows the precision and recall of SK when n = 1. As shown in Table 6, the classifieroffered high precision but low recall. This is evidence of over-fittingin learning.","As shown by the above experiments, FSSK pro-Table 6: Precision and recall of SK: n = 1","Precision Recall F MI:Opinion .917 .209 .339","JQA:LOCATION .896 .093 .168 vided consistently better performance than the conventional methods. Moreover, the experiments confirmed one important fact. That is, in some cases maximum performance was achieved with n = 1. This indicates that sub-sequences created using very large structures can be extremely effective. Of course, a larger feature space also includes the smaller feature spaces, n","n+1",". If the performance is improved by using a larger n, this means that significantfeatures do exist. Thus, we can improve the performance of some classificationproblems by dealing with larger substructures. Even if optimum performance was not achieved with n = 1, difference between the performance of smaller n are quite small compared to that of SK. This indicates that our method is very robust as regards sub-structure size; It therefore becomes unnecessary for us to decide sub-structure size carefully. This indicates our approach, using large sub-structures, is better than the conventional approach of eliminating sub-sequences based on size."]},{"title":"8 Conclusion","paragraphs":["This paper proposed a statistical feature selection method for convolution kernels. Our approach can select significant features automatically based on a statistical significance test. Our proposed method can be embedded in the DP based kernel calculation process for convolution kernels by using sub-structure mining algorithms. Table 4: Results of English question classification(Accuracy) (a) coarse (b) fine","n FSSK1 FSSK2 SK BOW-K 1 2 3 4 1 - .908 .914 .916 .912 - .902 .896 .902 .906 - .912 .914 .912 .892",".728 .836 .864 .858 - 1 2 3 4 1 - .852 .854 .852 .850 - .858 .856 .854 .854 - .850 .840 .830 .796",".754 .792 .790 .778 - Table 5: Results of sentence modality identification(F-measure) (a) opinion (b) decision (c) description","n FSSK1 FSSK2 SK BOW-K 1 2 3 4 1 - .734 .743 .746 .751 - .740 .748 .750 .750 - .706 .672 .577 .058",".507 .531 .438 .368 - 1 2 3 4 1 - .828 .858 .854 .857 - .824 .855 .859 .860 - .816 .834 .830 .339",".652 .708 .686 .665 - 1 2 3 4 1 - .896 .906 .910 .910 - .894 .903 .909 .909 - .902 .913 .910 .808",".819 .839 .826 .793 -","Experiments show that our method is superior to conventional methods. Moreover, the results indicate that complex features exist and can be effective. Our method can employ them without over-fitting problems, which yields benefitsin terms of concept and performance."]},{"title":"References","paragraphs":["N. Cancedda, E. Gaussier, C. Goutte, and J.-M. Renders. 2003. Word-Sequence Kernels. Journal of Machine Learning Research, 3:1059–1082.","M. Collins and N. Duffy. 2001. Convolution Kernels for Natural Language. In Proc. of Neural Information Processing Systems (NIPS’2001).","C. Cortes and V. N. Vapnik. 1995. Support Vector Networks. Machine Learning, 20:273–297.","D. Haussler. 1999. Convolution Kernels on Discrete Structures. In Technical Report UCS-CRL-99-10. UC Santa Cruz.","T. Joachims. 1998. Text Categorization with Support Vector Machines: Learning with Many Relevant Features. In Proc. of European Conference on Machine Learning (ECML ’98), pages 137– 142.","T. Kudo and Y. Matsumoto. 2002. Japanese Dependency Analysis Using Cascaded Chunking. In Proc. of the 6th Conference on Natural Language Learning (CoNLL 2002), pages 63–69.","T. Kudo and Y. Matsumoto. 2003. Fast Methods for Kernel-based Text Analysis. In Proc. of the 41st Annual Meeting of the Association for Computational Linguistics (ACL-2003), pages 24–31.","X. Li and D. Roth. 2002. Learning Question Classifiers. In Proc. of the 19th International Conference on Computational Linguistics (COLING 2002), pages 556–562.","H. Lodhi, C. Saunders, J. Shawe-Taylor, N. Cristianini, and C. Watkins. 2002. Text Classification Using String Kernel. Journal of Machine Learning Research, 2:419–444.","S. Morishita and J. Sese. 2000. Traversing Item-set Lattices with Statistical Metric Pruning. In Proc. of ACM SIGACT-SIGMOD-SIGART Symp. on Database Systems (PODS’00), pages 226– 236.","J. Pei, J. Han, B. Mortazavi-Asl, and H. Pinto. 2001. PrefixSpan: Mining Sequential Patterns Efficiently by Prefix-Projected Pattern Growth. In Proc. of the 17th International Conference on Data Engineering (ICDE 2001), pages 215–224.","M. Rogati and Y. Yang. 2002. High-performing Feature Selection for Text Classification. In Proc. of the 2002 ACM CIKM International Conference on Information and Knowledge Management, pages 659–661.","J. Suzuki, T. Hirao, Y. Sasaki, and E. Maeda. 2003a. Hierarchical Directed Acyclic Graph Kernel: Methods for Natural Language Data. In Proc. of the 41st Annual Meeting of the Association for Computational Linguistics (ACL-2003), pages 32–39.","J. Suzuki, Y. Sasaki, and E. Maeda. 2003b. Kernels for Structured Natural Language Data. In Proc. of the 17th Annual Conference on Neural Information Processing Systems (NIPS2003)."]}],"references":[{"authors":[{"first":"N.","last":"Cancedda"},{"first":"E.","last":"Gaussier"},{"first":"C.","last":"Goutte"},{"first":"J.","middle":"-M.","last":"Renders"}],"year":"2003","title":"Word-Sequence Kernels","source":"N. Cancedda, E. Gaussier, C. Goutte, and J.-M. Renders. 2003. Word-Sequence Kernels. Journal of Machine Learning Research, 3:1059–1082."},{"authors":[{"first":"M.","last":"Collins"},{"first":"N.","last":"Duffy"}],"year":"2001","title":"Convolution Kernels for Natural Language","source":"M. Collins and N. Duffy. 2001. Convolution Kernels for Natural Language. In Proc. of Neural Information Processing Systems (NIPS’2001)."},{"authors":[{"first":"C.","last":"Cortes"},{"first":"V.","middle":"N.","last":"Vapnik"}],"year":"1995","title":"Support Vector Networks","source":"C. Cortes and V. N. Vapnik. 1995. Support Vector Networks. Machine Learning, 20:273–297."},{"authors":[{"first":"D.","last":"Haussler"}],"year":"1999","title":"Convolution Kernels on Discrete Structures","source":"D. Haussler. 1999. Convolution Kernels on Discrete Structures. In Technical Report UCS-CRL-99-10. UC Santa Cruz."},{"authors":[{"first":"T.","last":"Joachims"}],"year":"1998","title":"Text Categorization with Support Vector Machines: Learning with Many Relevant Features","source":"T. Joachims. 1998. Text Categorization with Support Vector Machines: Learning with Many Relevant Features. In Proc. of European Conference on Machine Learning (ECML ’98), pages 137– 142."},{"authors":[{"first":"T.","last":"Kudo"},{"first":"Y.","last":"Matsumoto"}],"year":"2002","title":"Japanese Dependency Analysis Using Cascaded Chunking","source":"T. Kudo and Y. Matsumoto. 2002. Japanese Dependency Analysis Using Cascaded Chunking. In Proc. of the 6th Conference on Natural Language Learning (CoNLL 2002), pages 63–69."},{"authors":[{"first":"T.","last":"Kudo"},{"first":"Y.","last":"Matsumoto"}],"year":"2003","title":"Fast Methods for Kernel-based Text Analysis","source":"T. Kudo and Y. Matsumoto. 2003. Fast Methods for Kernel-based Text Analysis. In Proc. of the 41st Annual Meeting of the Association for Computational Linguistics (ACL-2003), pages 24–31."},{"authors":[{"first":"X.","last":"Li"},{"first":"D.","last":"Roth"}],"year":"2002","title":"Learning Question Classifiers","source":"X. Li and D. Roth. 2002. Learning Question Classifiers. In Proc. of the 19th International Conference on Computational Linguistics (COLING 2002), pages 556–562."},{"authors":[{"first":"H.","last":"Lodhi"},{"first":"C.","last":"Saunders"},{"first":"J.","last":"Shawe-Taylor"},{"first":"N.","last":"Cristianini"},{"first":"C.","last":"Watkins"}],"year":"2002","title":"Text Classification Using String Kernel","source":"H. Lodhi, C. Saunders, J. Shawe-Taylor, N. Cristianini, and C. Watkins. 2002. Text Classification Using String Kernel. Journal of Machine Learning Research, 2:419–444."},{"authors":[{"first":"S.","last":"Morishita"},{"first":"J.","last":"Sese"}],"year":"2000","title":"Traversing Item-set Lattices with Statistical Metric Pruning","source":"S. Morishita and J. Sese. 2000. Traversing Item-set Lattices with Statistical Metric Pruning. In Proc. of ACM SIGACT-SIGMOD-SIGART Symp. on Database Systems (PODS’00), pages 226– 236."},{"authors":[{"first":"J.","last":"Pei"},{"first":"J.","last":"Han"},{"first":"B.","last":"Mortazavi-Asl"},{"first":"H.","last":"Pinto"}],"year":"2001","title":"PrefixSpan: Mining Sequential Patterns Efficiently by Prefix-Projected Pattern Growth","source":"J. Pei, J. Han, B. Mortazavi-Asl, and H. Pinto. 2001. PrefixSpan: Mining Sequential Patterns Efficiently by Prefix-Projected Pattern Growth. In Proc. of the 17th International Conference on Data Engineering (ICDE 2001), pages 215–224."},{"authors":[{"first":"M.","last":"Rogati"},{"first":"Y.","last":"Yang"}],"year":"2002","title":"High-performing Feature Selection for Text Classification","source":"M. Rogati and Y. Yang. 2002. High-performing Feature Selection for Text Classification. In Proc. of the 2002 ACM CIKM International Conference on Information and Knowledge Management, pages 659–661."},{"authors":[{"first":"J.","last":"Suzuki"},{"first":"T.","last":"Hirao"},{"first":"Y.","last":"Sasaki"},{"first":"E.","last":"Maeda"}],"year":"2003a","title":"Hierarchical Directed Acyclic Graph Kernel: Methods for Natural Language Data","source":"J. Suzuki, T. Hirao, Y. Sasaki, and E. Maeda. 2003a. Hierarchical Directed Acyclic Graph Kernel: Methods for Natural Language Data. In Proc. of the 41st Annual Meeting of the Association for Computational Linguistics (ACL-2003), pages 32–39."},{"authors":[{"first":"J.","last":"Suzuki"},{"first":"Y.","last":"Sasaki"},{"first":"E.","last":"Maeda"}],"year":"2003b","title":"Kernels for Structured Natural Language Data","source":"J. Suzuki, Y. Sasaki, and E. Maeda. 2003b. Kernels for Structured Natural Language Data. In Proc. of the 17th Annual Conference on Neural Information Processing Systems (NIPS2003)."}],"cites":[{"style":0,"text":"Cortes and Vapnik, 1995","origin":{"pointer":"/sections/1/paragraphs/0","offset":243,"length":23},"authors":[{"last":"Cortes"},{"last":"Vapnik"}],"year":"1995","references":["/references/2"]},{"style":0,"text":"Joachims, 1998","origin":{"pointer":"/sections/1/paragraphs/0","offset":307,"length":14},"authors":[{"last":"Joachims"}],"year":"1998","references":["/references/4"]},{"style":0,"text":"Kudo and Matsumoto, 2002","origin":{"pointer":"/sections/1/paragraphs/0","offset":334,"length":24},"authors":[{"last":"Kudo"},{"last":"Matsumoto"}],"year":"2002","references":["/references/5"]},{"style":0,"text":"Collins and Duffy, 2001","origin":{"pointer":"/sections/1/paragraphs/0","offset":374,"length":23},"authors":[{"last":"Collins"},{"last":"Duffy"}],"year":"2001","references":["/references/1"]},{"style":0,"text":"Lodhi et al., 2002","origin":{"pointer":"/sections/1/paragraphs/1","offset":394,"length":18},"authors":[{"last":"Lodhi"},{"last":"al."}],"year":"2002","references":["/references/8"]},{"style":0,"text":"Collins and Duffy, 2001","origin":{"pointer":"/sections/1/paragraphs/1","offset":429,"length":23},"authors":[{"last":"Collins"},{"last":"Duffy"}],"year":"2001","references":["/references/1"]},{"style":0,"text":"Suzuki et al., 2003a","origin":{"pointer":"/sections/1/paragraphs/1","offset":474,"length":20},"authors":[{"last":"Suzuki"},{"last":"al."}],"year":"2003a","references":["/references/12"]},{"style":0,"text":"Haussler, 1999","origin":{"pointer":"/sections/1/paragraphs/2","offset":59,"length":14},"authors":[{"last":"Haussler"}],"year":"1999","references":["/references/3"]},{"style":0,"text":"Collins and Duffy, 2001","origin":{"pointer":"/sections/1/paragraphs/3","offset":134,"length":23},"authors":[{"last":"Collins"},{"last":"Duffy"}],"year":"2001","references":["/references/1"]},{"style":0,"text":"Cancedda et al., 2003","origin":{"pointer":"/sections/1/paragraphs/3","offset":159,"length":21},"authors":[{"last":"Cancedda"},{"last":"al."}],"year":"2003","references":["/references/0"]},{"style":0,"text":"Suzuki et al., 2003b","origin":{"pointer":"/sections/1/paragraphs/3","offset":182,"length":20},"authors":[{"last":"Suzuki"},{"last":"al."}],"year":"2003b","references":["/references/13"]},{"style":0,"text":"Lodhi et al., 2002","origin":{"pointer":"/sections/2/paragraphs/2","offset":145,"length":18},"authors":[{"last":"Lodhi"},{"last":"al."}],"year":"2002","references":["/references/8"]},{"style":0,"text":"Collins and Duffy, 2001","origin":{"pointer":"/sections/2/paragraphs/2","offset":250,"length":23},"authors":[{"last":"Collins"},{"last":"Duffy"}],"year":"2001","references":["/references/1"]},{"style":0,"text":"Cancedda et al., 2003","origin":{"pointer":"/sections/2/paragraphs/6","offset":153,"length":21},"authors":[{"last":"Cancedda"},{"last":"al."}],"year":"2003","references":["/references/0"]},{"style":0,"text":"Cancedda et al., 2003","origin":{"pointer":"/sections/2/paragraphs/17","offset":202,"length":21},"authors":[{"last":"Cancedda"},{"last":"al."}],"year":"2003","references":["/references/0"]},{"style":0,"text":"Cancedda et al., 2003","origin":{"pointer":"/sections/3/paragraphs/3","offset":23,"length":21},"authors":[{"last":"Cancedda"},{"last":"al."}],"year":"2003","references":["/references/0"]},{"style":0,"text":"Collins and Duffy, 2001","origin":{"pointer":"/sections/3/paragraphs/3","offset":235,"length":23},"authors":[{"last":"Collins"},{"last":"Duffy"}],"year":"2001","references":["/references/1"]},{"style":0,"text":"Rogati and Yang, 2002","origin":{"pointer":"/sections/4/paragraphs/3","offset":326,"length":21},"authors":[{"last":"Rogati"},{"last":"Yang"}],"year":"2002","references":["/references/11"]},{"style":0,"text":"Pei et al., 2001","origin":{"pointer":"/sections/4/paragraphs/28","offset":82,"length":16},"authors":[{"last":"Pei"},{"last":"al."}],"year":"2001","references":["/references/10"]},{"style":0,"text":"Morishita and Sese, 2000","origin":{"pointer":"/sections/4/paragraphs/28","offset":161,"length":24},"authors":[{"last":"Morishita"},{"last":"Sese"}],"year":"2000","references":["/references/9"]},{"style":0,"text":"Morishita and Sese, 2000","origin":{"pointer":"/sections/4/paragraphs/31","offset":45,"length":24},"authors":[{"last":"Morishita"},{"last":"Sese"}],"year":"2000","references":["/references/9"]},{"style":0,"text":"Kudo and Matsumoto, 2003","origin":{"pointer":"/sections/4/paragraphs/71","offset":249,"length":24},"authors":[{"last":"Kudo"},{"last":"Matsumoto"}],"year":"2003","references":["/references/6"]},{"style":0,"text":"Collins and Duffy, 2001","origin":{"pointer":"/sections/5/paragraphs/0","offset":164,"length":23},"authors":[{"last":"Collins"},{"last":"Duffy"}],"year":"2001","references":["/references/1"]},{"style":0,"text":"Suzuki et al., 2003a","origin":{"pointer":"/sections/5/paragraphs/7","offset":277,"length":20},"authors":[{"last":"Suzuki"},{"last":"al."}],"year":"2003a","references":["/references/12"]},{"style":0,"text":"Joachims, 1998","origin":{"pointer":"/sections/6/paragraphs/1","offset":142,"length":14},"authors":[{"last":"Joachims"}],"year":"1998","references":["/references/4"]},{"style":0,"text":"Li and Roth, 2002","origin":{"pointer":"/sections/6/paragraphs/4","offset":56,"length":17},"authors":[{"last":"Li"},{"last":"Roth"}],"year":"2002","references":["/references/7"]},{"style":0,"text":"Suzuki et al., 2003b","origin":{"pointer":"/sections/6/paragraphs/4","offset":92,"length":20},"authors":[{"last":"Suzuki"},{"last":"al."}],"year":"2003b","references":["/references/13"]}]}
