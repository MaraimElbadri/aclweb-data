{"sections":[{"title":"A New Feature Selection Score for Multinomial Naive Bayes Text ClassificationBased on KL-Divergence Karl-Michael Schneider Department of General Linguistics University of Passau 94032 Passau, Germany schneide@phil.uni-passau.de Abstract","paragraphs":["We define a new feature selection score for text classification based on the KL-divergence between the distribution of words in training documents and their classes. The score favors words that have a similar distribution in documents of the same class but different distributions in documents of different classes. Experiments on two standard data sets in-dicate that the new method outperforms mutual information, especially for smaller categories."]},{"title":"1 Introduction","paragraphs":["Text classification is the assignment of predefined categories to text documents. Text classification has many applications in natural language process-ing tasks such as E-mail filtering,prediction of user preferences and organization of web content.","The Naive Bayes classifier is a popular machine learning technique for text classification because it performs well in many domains, despite its simplicity (Domingos and Pazzani, 1997). Naive Bayes as-sumes a stochastic model of document generation. Using Bayes’ rule, the model is inverted in order to predict the most likely class for a new document.","We assume that documents are generated according to a multinomial event model (McCallum and Nigam, 1998). Thus a document is represented as a vector di = (xi1 : : : xijV j) of word counts where V is the vocabulary and each xit 2 f0; 1; 2; : : : g in-dicates how often wt occurs in di. Given model parameters p(wtjcj ) and class prior probabilities p(cj) and assuming independence of the words, the most likely class for a document di is computed as","c (d","i) = argmax j p(cj)p(djcj )","= argmax j p(cj) jV j Y t=1 p(wtjcj)n(wt;di) (1) where n(wt; di) is the number of occurrences of wt in di. p(wtjcj) and p(cj) are estimated from training documents with known classes, using maximum likelihood estimation with a Laplacean prior: p(wtjcj) = 1 + P di2cj n(wt; di) jV j +","PjV j t=1 P di2cj n(wt; di) (2) p(cj) = jcjj","PjCj j′ =1 jcj′j (3)","It is common practice to use only a subset of the words in the training documents for classification to avoid overfitting and make classification more efficient. This is usually done by assigning each word a score f (wt) that measures its usefulness for classification and selecting the N highest scored words. One of the best performing scoring functions for feature selection in text classification is mutual information (Yang and Pedersen, 1997). The mutual information between two random variables, MI(X; Y ), measures the amount of information that the value of one variable gives about the value of the other (Cover and Thomas, 1991).","Note that in the multinomial model, the word variable W takes on values from the vocabulary V . In order to use mutual information with a multinomial model, one defines new random variables Wt 2 f0; 1g with p(Wt = 1) = p(W = wt) (McCallum and Nigam, 1998; Rennie, 2001). Then the mutual information between a word wt and the class variable C is MI(Wt; C) = jCj X j=1 X x=0;1 p(x; cj) log p(x; cj ) p(x)p(cj )","(4) where p(x; cj ) and p(x) are short for p(Wt = x; cj ) and p(Wt = x). p(x; cj ), p(x) and p(cj) are estimated from the training documents by counting how often wt occurs in each class."]},{"title":"2 Naive Bayes and KL-Divergence","paragraphs":["There is a strong connection between Naive Bayes and KL-divergence (Kullback-Leibler divergence, relative entropy). KL-divergence measures how much one probability distribution is different from another (Cover and Thomas, 1991). It is defined(for discrete distributions) by KL(p; q) = X x p(x) log p(x) q(x) : (5) By viewing a document as a probability distribution over words, Naive Bayes can be interpreted in an information-theoretic framework (Dhillon et al., 2002). Let p(wtjd) = n(wt; d)=jdj. Taking logarithms and dividing by the length of d, (1) can be rewritten as","c (d)","= argmax j log p(cj) + jV j X t=1 n(wt; d) log p(wtjcj)","= argmax j 1 jdj log p(cj) + jV j X t=1 p(wtjd) log p(wtjcj) (6) Adding the entropy of p(W jd), we get","c (d)","= argmax j 1 jdj log p(cj) jV j X t=1 p(wtjd) log p(wtjd) p(wtjcj)","= argmin j KL(p(W jd); p(W jcj )) 1 jdj log p(cj) (7) This means that Naive Bayes assigns to a document d the class which is “most similar” to d in terms of the distribution of words. Note also that the prior probabilities are usually dominated by document probabilities except for very short documents."]},{"title":"3 Feature Selection using KL-Divergence","paragraphs":["We define a new scoring function for feature selection based on the following considerations. In the previous section we have seen that Naive Bayes assigns a document d the class c","such that the “distance” between d and c","is minimized. A classification error occurs when a test document is closer to some other class than to its true class, in terms of KL-divergence.","We seek to define a scoring function such that words whose distribution in the individual training documents of a class is much different from the distribution in the class (according to (2)) receive a lower score, while words with a similar distribution in all training documents of the same class receive a higher score. By removing words with a lower score from the vocabulary, the training documents of each class become more similar to each other, and therefore, also to the class, in terms of word distribution. This leads to more homogeneous classes. Assuming that the test documents and training documents come from the same distribution, the similarity between the test documents and their respective classes will be increased as well, thus resulting in higher classification accuracy.","We now make this more precise. Let S = fd1; : : : ; djSjg be the set of training documents, and denote the class of di with c(di). The average KL-divergence for a word wt between the training documents and their classes is given by KLt(S) = 1 jSj X di2S KL(p(wtjdi); p(wtjc(di))):","(8) One problem with (8) is that in addition to the conditional probabilities p(wtjcj) for each word and each class, the computation considers each individual document, thus resulting in a time requirement of O(jSj).1","In order to avoid this additional complexity, instead of KLt(S) we use an approximation fKLt(S), which is based on the following two assumptions: (i) the number of occurrences of wt is the same in all documents that contain wt, (ii) all documents in the same class cj have the same length. Let Njt be the number of documents in cj that contain wt, and let ~pd(wtjcj) = p(wtjcj) jcj j Njt (9) be the average probability of wt in those documents in cj that contain wt (if wt does not occur in cj, set ~pd(wtjcj) = 0). Then KLt(S) reduces to","fKLt(S) = 1 jSj jCj X j=1 Njt ~pd(wtjcj ) log ~pd(wtjcj) p(wtjcj) :","(10) Plugging in (9) and (3) and defining q(wtjcj) = Njt=jcj j, we get fKLt(S) = jCj X j=1 p(cj)p(wtjcj) log q(wtjcj): (11) Note that computing fKLt(S) only requires a statistics of the number of words and documents for each","1","Note that KLt(S) cannot be computed simultaneously with p(wtjcj) in one pass over the documents in (2): KLt(S) requires p(wtjcj) when each document is considered, but computing the latter needs iterating over all documents itself. class, not per document. Thus fKLt(S) can be computed in O(jCj). Typically, jCj is much smaller than jSj.","Another important thing to note is the following. By removing words with an uneven distribution in the documents of the same class, not only the documents in the class, but also the classes themselves may become more similar, which reduces the ability to distinguish between different classes. Let p(wt) be the number of occurrences of wt in all training documents, divided by the total number of words, q(wt) =","PjCj j=1 Njt=jSj and define eKt(S) = p(wt) log q(wt): (12) eKt(S) can be interpreted as an approximation of the average divergence of the distribution of wt in the individual training documents from the global distribution (averaged over all training documents in all classes). If wt is independent of the class, then eKt(S) = fKLt(S). The difference between the two is a measure of the increase in homogeneity of the training documents, in terms of the distribution of wt, when the documents are clustered in their true classes. It is large if the distribution of wt is similar in the training documents of the same class but dis-similar in documents of different classes. In analogy to mutual information, we define our new scoring function as the difference KL(wt) = eKt(S) fKLt(S): (13)","We also use a variant of KL, denoted dKL, where p(wt) is estimated according to (14):","p0 (w t) = jCj X j=1 p(cj)p(wtjcj ) (14) and p(wtjcj) is estimated as in (2)."]},{"title":"4 Experiments","paragraphs":["We compare KL and dKL to mutual information, using two standard data sets: 20 Newsgroups2","and Reuters 21578.3","In tokenizing the data, only words consisting of alphabetic characters are used after conversion to lower case. In addition, all numbers are mapped to a special token NUM. For 20 Newsgroups we remove the newsgroup headers and use a stoplist consisting of the 100 most frequent words of 2 http://www.ai.mit.edu/ jrennie/20Newsgroups/ 3 http://www.daviddlewis.com/resources/testcollections/","reuters21578/ dKLKLMI Vocabulary Size Classification Accuracy 10000010000100010010 1 0.8 0.6 0.4 0.2 0 Figure 1: Classification accuracy for 20 Newsgroups. The curves have small error bars. the British National Corpus.4","We use the ModApte split of Reuters 21578 (Apté et al., 1994) and use only the 10 largest classes. The vocabulary size is 111868 words for 20 Newsgroups and 22430 words for Reuters.","Experiments with 20 Newsgroups are performed with 5-fold cross-validation, using 80% of the data for training and 20% for testing. We build a single classifier for the 20 classes and vary the number of selected words from 20 to 20000. Figure 1 compares classification accuracy for the three scoring functions. dKL slightly outperforms mutual information, especially for smaller vocabulary sizes. The difference is statistically significant for 20 to 200 words at the 99% confidence level, and for 20 to 2000 words at the 95% confidence level, using a one-tailed paired t-test.","For the Reuters dataset we build a binary classifierfor each of the ten topics and set the number of positively classified documents such that precision equals recall. Precision is the percentage of positive documents among all positively classified documents. Recall is the percentage of positive documents that are classifiedas positive.","In Figures 2 and 3 we report microaveraged and macroaveraged recall for each number of selected words. Microaveraged recall is the percentage of all positive documents (in all topics) that are classified as positive. Macroaveraged recall is the average of the recall values of the individual topics. Microaveraged recall gives equal weight to the documents and thus emphasizes larger topics, while macroaveraged recall gives equal weight to the topics and thus emphasizes smaller topics more than microav-4 http://www.itri.brighton.ac.uk/ Adam.Kilgarriff/bnc-","readme.html dKLKLMI Vocabulary Size Precision/Recall Break e v en P oint 10000010000100010010 1 0.95 0.9 0.85 0.8 0.75 0.7 Figure 2: Microaveraged recall on Reuters at breakeven point. dKLKLMI Vocabulary Size Precision/Recall Break e v en P oint 10000010000100010010 1 0.95 0.9 0.85 0.8 0.75 0.7 Figure 3: Macroaveraged recall on Reuters at breakeven point. eraged recall.","Both KL and dKL achieve slightly higher values for microaveraged recall than mutual information, for most vocabulary sizes (Fig. 2). KL performs best at 20000 words with 90.1% microaveraged recall, compared to 89.3% for mutual information. The largest improvement is found for dKL at 100 words with 88.0%, compared to 86.5% for mutual information.","For smaller categories, the difference between the KL-divergence based scores and mutual information is larger, as indicated by the curves for macroaveraged recall (Fig. 3). KL yields the highest recall at 20000 words with 82.2%, an increase of 3.9% compared to mutual information with 78.3%, whereas dKL has its largest value at 100 words with 78.8%, compared to 76.1% for mutual information. We findthe largest improvement at 5000 words with 5.6% for KL and 2.9% for dKL, compared to mutual information."]},{"title":"5 Conclusion","paragraphs":["By interpreting Naive Bayes in an information the-oretic framework, we derive a new scoring method for feature selection in text classification, based on the KL-divergence between training documents and their classes. Our experiments show that it outperforms mutual information, which was one of the best performing methods in previous studies (Yang and Pedersen, 1997). The KL-divergence based scores are especially effective for smaller categories, but additional experiments are certainly required.","In order to keep the computational cost low, we use an approximation instead of the exact KL-divergence. Assessing the error introduced by this approximation is a topic for future work."]},{"title":"References","paragraphs":["Chidanand Apté, Fred Damerau, and Sholom M. Weiss. 1994. Towards language independent automated learning of text categorization models. In Proc. 17th ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR ’94), pages 23–30.","Thomas M. Cover and Joy A. Thomas. 1991. Elements of Information Theory. John Wiley, New York.","Inderjit S. Dhillon, Subramanyam Mallela, and Rahul Kumar. 2002. Enhanced word clustering for hierarchical text classification. In Proc. 8th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 191– 200.","Pedro Domingos and Michael Pazzani. 1997. On the optimality of the simple bayesian classifier under zero-one loss. Machine Learning, 29:103– 130.","Andrew McCallum and Kamal Nigam. 1998. A comparison of event models for Naive Bayes text classification. In Learning for Text Categoriza-tion: Papers from the AAAI Workshop, pages 41– 48. AAAI Press. Technical Report WS-98-05.","Jason D. M. Rennie. 2001. Improving multi-class text classification with Naive Bayes. Master’s thesis, Massachusetts Institute of Technology.","Yiming Yang and Jan O. Pedersen. 1997. A comparative study on feature selection in text categorization. In Proc. 14th International Conference on Machine Learning (ICML-97), pages 412– 420."]}],"references":[{"authors":[{"first":"Chidanand","last":"Apté"},{"first":"Fred","last":"Damerau"},{"first":"Sholom","middle":"M.","last":"Weiss"}],"year":"1994","title":"Towards language independent automated learning of text categorization models","source":"Chidanand Apté, Fred Damerau, and Sholom M. Weiss. 1994. Towards language independent automated learning of text categorization models. In Proc. 17th ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR ’94), pages 23–30."},{"authors":[{"first":"Thomas","middle":"M.","last":"Cover"},{"first":"Joy","middle":"A.","last":"Thomas"}],"year":"1991","title":"Elements of Information Theory","source":"Thomas M. Cover and Joy A. Thomas. 1991. Elements of Information Theory. John Wiley, New York."},{"authors":[{"first":"Inderjit","middle":"S.","last":"Dhillon"},{"first":"Subramanyam","last":"Mallela"},{"first":"Rahul","last":"Kumar"}],"year":"2002","title":"Enhanced word clustering for hierarchical text classification","source":"Inderjit S. Dhillon, Subramanyam Mallela, and Rahul Kumar. 2002. Enhanced word clustering for hierarchical text classification. In Proc. 8th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 191– 200."},{"authors":[{"first":"Pedro","last":"Domingos"},{"first":"Michael","last":"Pazzani"}],"year":"1997","title":"On the optimality of the simple bayesian classifier under zero-one loss","source":"Pedro Domingos and Michael Pazzani. 1997. On the optimality of the simple bayesian classifier under zero-one loss. Machine Learning, 29:103– 130."},{"authors":[{"first":"Andrew","last":"McCallum"},{"first":"Kamal","last":"Nigam"}],"year":"1998","title":"A comparison of event models for Naive Bayes text classification","source":"Andrew McCallum and Kamal Nigam. 1998. A comparison of event models for Naive Bayes text classification. In Learning for Text Categoriza-tion: Papers from the AAAI Workshop, pages 41– 48. AAAI Press. Technical Report WS-98-05."},{"authors":[{"first":"Jason","middle":"D. M.","last":"Rennie"}],"year":"2001","title":"Improving multi-class text classification with Naive Bayes","source":"Jason D. M. Rennie. 2001. Improving multi-class text classification with Naive Bayes. Master’s thesis, Massachusetts Institute of Technology."},{"authors":[{"first":"Yiming","last":"Yang"},{"first":"Jan","middle":"O.","last":"Pedersen"}],"year":"1997","title":"A comparative study on feature selection in text categorization","source":"Yiming Yang and Jan O. Pedersen. 1997. A comparative study on feature selection in text categorization. In Proc. 14th International Conference on Machine Learning (ICML-97), pages 412– 420."}],"cites":[{"style":0,"text":"Domingos and Pazzani, 1997","origin":{"pointer":"/sections/1/paragraphs/1","offset":157,"length":26},"authors":[{"last":"Domingos"},{"last":"Pazzani"}],"year":"1997","references":["/references/3"]},{"style":0,"text":"McCallum and Nigam, 1998","origin":{"pointer":"/sections/1/paragraphs/2","offset":79,"length":24},"authors":[{"last":"McCallum"},{"last":"Nigam"}],"year":"1998","references":["/references/4"]},{"style":0,"text":"Yang and Pedersen, 1997","origin":{"pointer":"/sections/1/paragraphs/8","offset":424,"length":23},"authors":[{"last":"Yang"},{"last":"Pedersen"}],"year":"1997","references":["/references/6"]},{"style":0,"text":"Cover and Thomas, 1991","origin":{"pointer":"/sections/1/paragraphs/8","offset":616,"length":22},"authors":[{"last":"Cover"},{"last":"Thomas"}],"year":"1991","references":["/references/1"]},{"style":0,"text":"McCallum and Nigam, 1998","origin":{"pointer":"/sections/1/paragraphs/9","offset":230,"length":24},"authors":[{"last":"McCallum"},{"last":"Nigam"}],"year":"1998","references":["/references/4"]},{"style":0,"text":"Rennie, 2001","origin":{"pointer":"/sections/1/paragraphs/9","offset":256,"length":12},"authors":[{"last":"Rennie"}],"year":"2001","references":["/references/5"]},{"style":0,"text":"Cover and Thomas, 1991","origin":{"pointer":"/sections/2/paragraphs/0","offset":204,"length":22},"authors":[{"last":"Cover"},{"last":"Thomas"}],"year":"1991","references":["/references/1"]},{"style":0,"text":"Dhillon et al., 2002","origin":{"pointer":"/sections/2/paragraphs/0","offset":448,"length":20},"authors":[{"last":"Dhillon"},{"last":"al."}],"year":"2002","references":["/references/2"]},{"style":0,"text":"Apté et al., 1994","origin":{"pointer":"/sections/4/paragraphs/4","offset":43,"length":17},"authors":[{"last":"Apté"},{"last":"al."}],"year":"1994","references":["/references/0"]},{"style":0,"text":"Yang and Pedersen, 1997","origin":{"pointer":"/sections/5/paragraphs/0","offset":345,"length":23},"authors":[{"last":"Yang"},{"last":"Pedersen"}],"year":"1997","references":["/references/6"]}]}
