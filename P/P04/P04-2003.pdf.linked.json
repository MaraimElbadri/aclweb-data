{"sections":[{"title":"Searching for Topics in a Large Collection of Texts Martin Holub Jiřı́Semecký Jiřı́Diviš Center for Computational Linguistics Charles University, Prague holub|semecky @ufal.mff.cuni.cz jiri.divis@atlas.cz Abstract","paragraphs":["We describe an original method that automatically finds specific topics in a large collection of texts. Each topic is first identified as a specific cluster of texts and then represented as a virtual concept, which is a weighted mixture of words. Our intention is to employ these virtual concepts in document indexing. In this paper we show some preliminary experimental results and discuss directions of future work."]},{"title":"1 Introduction","paragraphs":["In the fieldof information retrieval (for a detailed survey see e.g. (Baeza-Yates and Ribeiro-Neto, 1999)), document indexing and representing documents as vectors belongs among the most successful techniques. Within the framework of the well known vector model, the indexed elements are usually individual words, which leads to high dimensional vectors. However, there are several approaches that try to reduce the high dimension-ality of the vectors in order to improve the effectivity of retrieving. The most famous is probably the method called Latent Semantic Indexing (LSI), introduced by Deerwester et al. (1990), which employs a specific linear transformation of original word-based vectors using a system of “latent semantic concepts”. Other two approaches which inspired us, namely (Dhillon and Modha, 2001) and (Torkkola, 2002), are similar to LSI but different in the way how they project the vectors of documents into a space of a lower dimension.","Our idea is to establish a system of “virtual concepts”, which are linear functions represented by vectors, extracted from automatically discovered “concept-formati ve clusters” of documents. Shortly speaking, concept-formative clusters are semantically coherent and specific sets of documents, which represent specific topics. This idea was originally proposed by Holub (2003), who hypothesizes that concept-oriented vector models of documents based on indexing virtual concepts could improve the effectiveness of both automatic comparison of documents and their matching with queries.","The paper is organized as follows. In section 2 we formalize the notion of concept-formative clusters and give a heuristic method of finding them. Section 3 first introduces virtual concepts in a formal way and shows an algorithm to construct them. Then, some experiments are shown. In sec-tions 4 we compare our model with another approach and give a brief survey of some open ques-tions. Finally, a short summary is given in section 5."]},{"title":"2 Concept-formative clusters 2.1 Graph of a text collection","paragraphs":["Let","be a collection of text documents;","is the size of the collection. Now suppose that we have a function ","",", which gives a degree of document similarity for each pair of documents. Then we represent the collection as a graph.","Definition: A labeled graph","is called graph of","collection","if","    ","","where","","","","","","","","","","","and each edge","","","","","is labeled by","number","","","","","","",", called weight of",";","","","","is a given document similarity threshold","(i.e. a threshold weight of edge).","Now we introduce some terminology and neces-","sary notation. Let","","   ","","be a graph of col-","lection",". Each subset ","is called a cut of",";","","stands for the complement","",". If","","","","are disjoint cuts then","","","","","  ","","","","is a set of edges","within cut",";","","","","",""," ","","","","is called weight of","cut",";","","","","","","  ","","","","","","","","","","","","","","","","is a set","of edges between cuts","and",";","","","","","","","  ","","","","","is called weight","of the connection between cuts","and",";","","","","","     ","","","is the expected weight of","edge in graph",";","","","   "," ","   ","","is the expected weight of cut",";","    "," ","   "," ","","","","","is the expected","weight of the connection between cut X and","the rest of the collection;","","each cut","naturally splits the collection into","three disjoint subsets","","    where ","","","","","","","   ","","","","","",""," and ","  ","","   . 2.2 Quality of cuts","Now we formalize the property of “being concept-","-formative” by a positive real function called qual-","ity of cut. A high value of quality means that a cut","must be specificand extensive. A cut","is called specific if (i) the weight","  ","","is relatively high and (ii) the connec-","tion between","and the rest of the collection","  "," ",""," is relatively small. The first prop-","erty is called compactness of cut, and is defined","as","",""," ","","","","",""," ","  ","",", while the other is","called exhaustivity of cut, which is defined as    "," ","  "," ","","","","",""," ","  . Both functions are positive. Thus, the specificityof cut","can be formalized by the following formula ","  "," ","  ","","  ","  "," "," ","  "," ","  ","","— the greater this value, the more specific the","cut ;","","and","","are positive parameters, which","are used for balancing the two factors.","The extensity of cut","is defined as a positive","function ","","","",""," ","","","where","","","is a","threshold size of cut.","Definition: The total quality of cut  ","","is a pos-","itive real function composed of all factors men-","tioned above and is definedas","","","    ","","","     ","     ","  where the three lambdas are parameters whose purpose is balancing the three factors.","To be concept-formative, a cut (i) must have a sufficiently high quality and (ii) must be locally optimal. 2.3 Local optimization of cuts","A cut","","","is called locally optimal regarding","quality function","if each cut","","which is","only a small modification of the original","does","not have greater quality, i.e.    ","","","",".","Now we describe a local search procedure whose purpose is to optimize any input cut","; if","is not locally optimal, the output of the Local Search procedure is a locally optimal cut","which results from the original","as its local modification. First we need the following definition:","Definition: Potential of document","","","with re-","spect to cut  is a real function","","","",""," : ","","","","","","definedas","","","","","","","  ","","","","","","","","The Local Search procedure is described in Fig. 1. Note that","1. Local Search gradually generates a sequence of cuts             ","so that","Input: the graph of text collection",";","an initial cut",".","Output: locally optimal cut",".","Algorithm:","loop:   if","","","then "," ","","","","  goto loop "," ","","if","then   ","","","","  goto loop ","  "," end Figure 1: The Local Search Algorithm","(i)     ","","   ","","   ","","for ","",", and (ii) cut    always arises from   ","","","by adding or taking away one document into/from it;","2. since the quality of modified cuts cannot in-","crease infinitely, a finite","","","necessarily","exists so that   ","is locally optimal and consequently the program stops at least after the  -th iteration;","3. each output cut","","is locally optimal.","Now we are ready to precisely define concept- -formative clusters:","Definition: A cut ","","is called a concept-","-formative cluster if","(i)","","   ","","where","","is a threshold quality","and","(ii)","","","","where","","is the output of the","Local Search algorithm.","The whole procedure for finding concept-formative clusters consists of two basic stages: first,a set of initial cuts is found within the whole collection, and then each of them is used as a seed for the Local Search algorithm, which locally optimizes the quality function",".","Note that","","","","","","are crucial parameters, which strongly affect the whole process of searching and consequently also the character of resulting concept-formative clusters. We have optimized their values by a sort of machine learning, using a small manually annotated collection of texts. When optimized","-parameters are used, the Local Search procedure tries to simulate the behavior of human annotator who finds topically coherent clusters in a training collection. The task of","-optimization leads to a system of linear inequalities, which we solve via linear programming. As there is no scope for this issue here, we cannot go into details."]},{"title":"3 Virtual concepts","paragraphs":["In this section we first show that concept- -formative clusters can be viewed as fuzzy sets. In this sense, each concept-formative cluster can be characterized by a membership function. Fuzzy clustering allows for some ambiguity in the data, and its main advantage over hard clustering is that it yields much more detailed information on the structure of the data (cf. (Kaufman and Rousseeuw, 1990), chapter 4).","Then we define virtual concepts as linear functions which estimate degree of membership of documents in concept-formative clusters. Since virtual concepts are weighted mixtures of words represented as vectors, they can also be seen as virtual documents representing specifictopics that emerge in the analyzed collection.","Definition: Degree of membership of a document","","","","in a concept-formative cluster","","","is a function","","","  :","","","","","","","","",". For","",""," ","we define","","","  ","","","   ","","","","","where","  is a constant. For","","  we define","","","","","",".","The following holds true for any concept-","-formative cluster","and any document",":","","","","  ","","iff","","",";","","","","  ","","iff","","  .","Now we formalize the notion of virtual con-","cepts. Let","","","   ","","","","","be vector rep-","resentations of documents","",", where","Input: pairs           ","        where    ","  ","; . . . maximal number of words in output concept;  ",". . . quadratic residual error threshold.","Output: "," ",". . . output concept;","  . . . quadratic residual error;  . . . number of words in the output concept.","Algorithm:  ",", ","  ","while    ","     ","","do","","","   for each ","","","  ",""," ","do","  output of MLR                ","           ","   "," if   ","","then",""," ",", ,        ","           end Figure 2: The Greedy Regression Algorithm ","is the number of indexed terms. We look for such a vector    ","so that  ","","","","","","","","","approximately holds for any  ","",". This","vector ","is then called virtual concept corresponding to concept-formative cluster",". The task of finding virtual concepts can be solved using the Greedy Regression Algorithm (GRA), originally suggested by Semecký (2003). 3.1 Greedy Regression Algorithm The GRA is directly based on multiple linear regression (see e.g. (Rice, 1994)). The GRA works in iterations and gradually increases the number of non-zero elements in the resulting vector, i.e. the number of words with non-zero weight in the resulting mixture. So this number can be explicitly restricted by a parameter. This feature of the GRA has been designed for the sake of generalization, in order to not overfitthe input sample.","The input of the GRA consists of (i) a sample set of document vectors with the corresponding values of","","","",", (ii) a maximum number of non-zero elements, and (iii) an error threshold.","The GRA, which is described in Fig. 2, re-","quires a procedure for solving multiple linear re-","gression (MLR) with a limited number of non-","zero elements in the resulting vector. Formally,","","","","","    ","","","","","","","","","gets on input","","a set of","vectors   ",";","","a corresponding set of","values","","","","to be","approximated; and","","a set of indexes  ","","","of the ele-","ments which are allowed to be non-zero in","the output vector. The output of the MLR is a vector","    ","","","","    ","    ","",""," ","where each considered","  ","",""," "," must","fulfill","","","for any"," ","","","","",". Implementation and time complexity","For solving multiple linear regression we use a","public-domain Java package JAMA (2004), devel-","oped by the MathWorks and NIST. The computa-","tion of inverse matrix is based on the LU decom-","position, which makes it faster (Press et al., 1992).","As for the asymptotic time complexity of the","GRA, it is in","","","","","","complexity of the MLR","since the outer loop runs","times at maximum and","the inner loop always runs nearly","times. The","MLR substantially consists of matrix multiplica-","tions in dimension","","","and a matrix inversion","in dimension ","",". Thus the complexity of the","MLR is in     "," ","  ",""," ","","","","because",""," . So the total complexity of the GRA is in","","","  ","","","",".","To reduce this high computational complexity,","we make a term pre-selection using a heuristic","method based on linear programming. Then, the","GRA does not need to deal with high-dimensional","vectors in","",", but works with vectors in dimen-","sion","","","",". Although the acceleration is only","linear, the required time has been reduced more","than ten times, which is practically significant. 3.2 Experiments","The experiments reported here were done on a","small experimental collection of    ","Czech documents. The texts were articles from","two different newspapers and one journal. Each","document was morphologically analyzed and lem-","matized (Hajič, 2000) and then indexed and rep-","resented as a vector. We indexed only lemmas","of nouns, adjectives, verbs, adverbs and numer-","als whose document frequency was greater than","and less than","","","",". Then the number of indexed","terms was","","","","","",". The cosine similarity was","used to compute the document similarity; thresh-","old was","","","","",". There were","","","","","","edges in","the graph of the collection.","We had computed a set of concept-formative","clusters and then approximated the corresponding","membership functions by virtual concepts.","The first thing we have observed was that the","quadratic residual error systematically and progre-","sivelly decreases in each GRA iteration. More-","over, the words in virtual concepts are obviously","intelligible for humans and strongly suggest the","topic. An example is given in Table 1.","words in the concept the weights Czech lemma literally transl.    ","  ","bosenský Bosnian","","   ","Srb Serb","","    ","UNPROFOR UNPROFOR","","   ","OSN UN","","    ","Sarajevo Sarajevo","","","   ","muslimský Muslim (adj) —  ","odvolat withdraw —   ","srbský Serbian —  ","generál general (n) —  ","list paper —  ","","","quadratic residual error:","","    ","Table 1: Two virtual concepts (","","","and","","",")","corresponding to cluster #318.","Another example is cluster #19 focused on","“pension funds”, which was approximated","(","",""," ) by the following words (literally trans-","lated):","pension (adj), pension","(n), fund",", additional insurance",",","inheritance",", payment",", interest","(n), dealer",", regulation",",","lawsuit , August","(adj), measure","(n), approve",",","increase (v), appreciation",", property",", trade","(adj),","attentively , improve",", coupon","(adj).","(The signs after the words indicate their positive","or negative weights in the concept.) Figure 3","shows the approximation of this cluster by virtual","concept.","Figure 3: The approximation of membership func-","tion corresponding to cluster #19 by a virtual con-","cept (the number of words in the concept","","",")."]},{"title":"4 Discussion 4.1 Related work","paragraphs":["A similar approach to searching for topics and employing them for document retrieval has been recently suggested by Xu and Croft (2000), who, however, try to employ the topics in the area of distributed retrieval.","They use document clustering, treat each cluster as a topic, and then define topics as probability distributions of words. They use the Kullback- -Leibler divergence with some modification as a distance metric to determine the closeness of a document to a cluster. Although our virtual concepts cannot be interpreted as probability distributions, in this point both approaches are quite similar.","The substantial difference is in the clustering method used. Xu and Croft have chosen the K-Means algorithm, “for its efficiency”. In contrast to this hard clustering algorithm, (i) our method is consistently based on empirical analysis of a text collection and does not require an a priori given number of topics; (ii) in order to induce permeable topics, our concept-formative clusters are not disjoint; (iii) the specificity of our clusters is driven by training samples given by human.","Xu and Croft suggest that retrieval based on topics may be more robust in comparison with the classic vector technique: Document ranking against a query is based on statistical correlation between query words and words in a document. Since a document is a small sample of text, the statistics in a document are often too sparse to reliably predict how likely the document is relevant to a query. In contrast, we have much more texts for a topic and the statistics are more stable. By excluding clearly unrelated topics, we can avoid retrieving many of the non-relevant documents. 4.2 Future work As our work is still in progress, there are some open questions, which we will concentrate on in the near future. Three main issues are (i) evaluation, (ii) parameters setting (which is closely connected to the previous one), and (iii) an effective implementation of crucial algorithms (the current implementation is still experimental).","As for the evaluation, we are building a manually annotated test collection using which we want to test the capability of our model to estimate inter- -document similarity in comparison with the classic vector model and the LSI model. So far, we have been working with a Czech collection for we also test the impact of morphology and some other NLP methods developed for Czech. Next step will be the evaluation on the English TREC collec-tions, which will enable us to rigorously evaluate if our model really helps to improve IR tasks.","The evaluation will also give us criteria for parameters setting. We expect that a positive value of","","will significantlyaccelerate the computation without loss of quality, but finding the right value must be based on the evaluation. As for the most important parameters of the GRA (i.e. the size of the sample set","and the number of words in concept","), these should be set so that the resulting concept is a good membership estimator also for documents not included in the sample set."]},{"title":"5 Summary","paragraphs":["We have designed and implemented a system that automatically discovers specific topics in a text collection. We try to employ it in document indexing. The main directions for our future work are thorough evaluation of the model and optimization of the parameters."]},{"title":"Acknowledgments","paragraphs":["This work has been supported by the Ministry of Education, project Center for Computational Linguistics (project LN00A063)."]},{"title":"References","paragraphs":["Ricardo A. Baeza-Yates and Berthier A. Ribeiro-Neto. 1999. Modern Information Retrieval. ACM Press / Addison-Wesley.","Scott C. Deerwester, Susan T. Dumais, Thomas K. Landauer, George W. Furnas, and Richard A. Harshman. 1990. Indexing by latent semantic analysis. JASIS, 41(6):391–407.","Inderjit S. Dhillon and D. S. Modha. 2001. Concept decompositions for large sparse text data using clustering. Machine Learning, 42(1/2):143–175.","Jan Hajič. 2000. Morphological tagging: Data vs. dictionaries. In Proceedings of the 6th ANLP Conference, 1st NAACL Meeting, pages 94–101, Seattle.","Martin Holub. 2003. A new approach to conceptual document indexing: Building a hierarchical system of concepts based on document clusters. In M. Aleksy et al. (eds.): ISICT 2003, Proceedings of the International Symposium on Information and Communication Technologies, pages 311–316. Trin-ity College Dublin, Ireland.","JAMA. 2004. JAMA: A Java Matrix Package. Publicdomain, http://math.nist.gov/javanumerics/jama/.","Leonard Kaufman and Peter J. Rousseeuw. 1990. Finding Groups in Data. John Wiley & Sons.","W. H. Press, S. A. Teukolsky, W. T. Vetterling, and B. P. Flannery. 1992. Numerical Recipes in C. Second edition, Cambridge University Press, Cambridge.","John A. Rice. 1994. Mathematical Statistics and Data Analysis. Second edition, Duxbury Press, California.","Jiřı́ Semecký. 2003. Semantic word classes extracted from text clusters. In 12th Annual Conference WDS 2003, Proceeding of Contributed Papers. MATFYZPRESS, Prague.","Kari Torkkola. 2002. Discriminative features for document classification. In Proceedings of the International Conference on Pattern Recognition, Quebec City, Canada, August 11–15.","Jinxi Xu and W. Bruce Croft. 2000. Topic-based language models for distributed retrieval. In W. Bruce Croft (ed.): Advances in Information Retrieval, pages 151–172. Kluwer Academic Publishers."]}],"references":[{"authors":[{"first":"Ricardo","middle":"A.","last":"Baeza-Yates"},{"first":"Berthier","middle":"A.","last":"Ribeiro-Neto"}],"year":"1999","title":"Modern Information Retrieval","source":"Ricardo A. Baeza-Yates and Berthier A. Ribeiro-Neto. 1999. Modern Information Retrieval. ACM Press / Addison-Wesley."},{"authors":[{"first":"Scott","middle":"C.","last":"Deerwester"},{"first":"Susan","middle":"T.","last":"Dumais"},{"first":"Thomas","middle":"K.","last":"Landauer"},{"first":"George","middle":"W.","last":"Furnas"},{"first":"Richard","middle":"A.","last":"Harshman"}],"year":"1990","title":"Indexing by latent semantic analysis","source":"Scott C. Deerwester, Susan T. Dumais, Thomas K. Landauer, George W. Furnas, and Richard A. Harshman. 1990. Indexing by latent semantic analysis. JASIS, 41(6):391–407."},{"authors":[{"first":"Inderjit","middle":"S.","last":"Dhillon"},{"first":"D.","middle":"S.","last":"Modha"}],"year":"2001","title":"Concept decompositions for large sparse text data using clustering","source":"Inderjit S. Dhillon and D. S. Modha. 2001. Concept decompositions for large sparse text data using clustering. Machine Learning, 42(1/2):143–175."},{"authors":[{"first":"Jan","last":"Hajič"}],"year":"2000","title":"Morphological tagging: Data vs","source":"Jan Hajič. 2000. Morphological tagging: Data vs. dictionaries. In Proceedings of the 6th ANLP Conference, 1st NAACL Meeting, pages 94–101, Seattle."},{"authors":[{"first":"Martin","last":"Holub"}],"year":"2003","title":"A new approach to conceptual document indexing: Building a hierarchical system of concepts based on document clusters","source":"Martin Holub. 2003. A new approach to conceptual document indexing: Building a hierarchical system of concepts based on document clusters. In M. Aleksy et al. (eds.): ISICT 2003, Proceedings of the International Symposium on Information and Communication Technologies, pages 311–316. Trin-ity College Dublin, Ireland."},{"authors":[{"last":"JAMA"}],"year":"2004","title":"JAMA: A Java Matrix Package","source":"JAMA. 2004. JAMA: A Java Matrix Package. Publicdomain, http://math.nist.gov/javanumerics/jama/."},{"authors":[{"first":"Leonard","last":"Kaufman"},{"first":"Peter","middle":"J.","last":"Rousseeuw"}],"year":"1990","title":"Finding Groups in Data","source":"Leonard Kaufman and Peter J. Rousseeuw. 1990. Finding Groups in Data. John Wiley & Sons."},{"authors":[{"first":"W.","middle":"H.","last":"Press"},{"first":"S.","middle":"A.","last":"Teukolsky"},{"first":"W.","middle":"T.","last":"Vetterling"},{"first":"B.","middle":"P.","last":"Flannery"}],"year":"1992","title":"Numerical Recipes in C","source":"W. H. Press, S. A. Teukolsky, W. T. Vetterling, and B. P. Flannery. 1992. Numerical Recipes in C. Second edition, Cambridge University Press, Cambridge."},{"authors":[{"first":"John","middle":"A.","last":"Rice"}],"year":"1994","title":"Mathematical Statistics and Data Analysis","source":"John A. Rice. 1994. Mathematical Statistics and Data Analysis. Second edition, Duxbury Press, California."},{"authors":[{"first":"Jiřı́","last":"Semecký"}],"year":"2003","title":"Semantic word classes extracted from text clusters","source":"Jiřı́ Semecký. 2003. Semantic word classes extracted from text clusters. In 12th Annual Conference WDS 2003, Proceeding of Contributed Papers. MATFYZPRESS, Prague."},{"authors":[{"first":"Kari","last":"Torkkola"}],"year":"2002","title":"Discriminative features for document classification","source":"Kari Torkkola. 2002. Discriminative features for document classification. In Proceedings of the International Conference on Pattern Recognition, Quebec City, Canada, August 11–15."},{"authors":[{"first":"Jinxi","last":"Xu"},{"first":"W.","middle":"Bruce","last":"Croft"}],"year":"2000","title":"Topic-based language models for distributed retrieval","source":"Jinxi Xu and W. Bruce Croft. 2000. Topic-based language models for distributed retrieval. In W. Bruce Croft (ed.): Advances in Information Retrieval, pages 151–172. Kluwer Academic Publishers."}],"cites":[{"style":0,"text":"Baeza-Yates and Ribeiro-Neto, 1999","origin":{"pointer":"/sections/1/paragraphs/0","offset":70,"length":34},"authors":[{"last":"Baeza-Yates"},{"last":"Ribeiro-Neto"}],"year":"1999","references":["/references/0"]},{"style":0,"text":"Deerwester et al. (1990)","origin":{"pointer":"/sections/1/paragraphs/0","offset":595,"length":24},"authors":[{"last":"Deerwester"},{"last":"al."}],"year":"1990","references":["/references/1"]},{"style":0,"text":"Dhillon and Modha, 2001","origin":{"pointer":"/sections/1/paragraphs/0","offset":793,"length":23},"authors":[{"last":"Dhillon"},{"last":"Modha"}],"year":"2001","references":["/references/2"]},{"style":0,"text":"Torkkola, 2002","origin":{"pointer":"/sections/1/paragraphs/0","offset":823,"length":14},"authors":[{"last":"Torkkola"}],"year":"2002","references":["/references/10"]},{"style":0,"text":"Holub (2003)","origin":{"pointer":"/sections/1/paragraphs/1","offset":365,"length":12},"authors":[{"last":"Holub"}],"year":"2003","references":["/references/4"]},{"style":0,"text":"Kaufman and Rousseeuw, 1990","origin":{"pointer":"/sections/3/paragraphs/0","offset":370,"length":27},"authors":[{"last":"Kaufman"},{"last":"Rousseeuw"}],"year":"1990","references":["/references/6"]},{"style":0,"text":"Semecký (2003)","origin":{"pointer":"/sections/3/paragraphs/130","offset":122,"length":14},"authors":[{"last":"Semecký"}],"year":"2003","references":["/references/9"]},{"style":0,"text":"Rice, 1994","origin":{"pointer":"/sections/3/paragraphs/130","offset":237,"length":10},"authors":[{"last":"Rice"}],"year":"1994","references":["/references/8"]},{"style":0,"text":"JAMA (2004)","origin":{"pointer":"/sections/3/paragraphs/199","offset":27,"length":11},"authors":[{"last":"JAMA"}],"year":"2004","references":["/references/5"]},{"style":0,"text":"Press et al., 1992","origin":{"pointer":"/sections/3/paragraphs/202","offset":33,"length":18},"authors":[{"last":"Press"},{"last":"al."}],"year":"1992","references":["/references/7"]},{"style":0,"text":"Hajič, 2000","origin":{"pointer":"/sections/3/paragraphs/260","offset":9,"length":11},"authors":[{"last":"Hajič"}],"year":"2000","references":["/references/3"]},{"style":0,"text":"Xu and Croft (2000)","origin":{"pointer":"/sections/4/paragraphs/0","offset":116,"length":19},"authors":[{"last":"Xu"},{"last":"Croft"}],"year":"2000","references":["/references/11"]}]}
