{"sections":[{"title":"The Sentimental Factor: Improving Review Classificationvia Human-Provided Information Philip Beineke and Trevor Hastie Dept. of Statistics Stanford University Stanford, CA 94305 Shivakumar Vaithyanathan IBM Almaden Research Center 650 Harry Rd. San Jose, CA 95120-6099 Abstract","paragraphs":["Sentiment classification is the task of labeling a review document according to the polarity of its prevailing opinion (favorable or unfavorable). In approaching this problem, a model builder often has three sources of information available: a small collection of labeled documents, a large collection of unlabeled documents, and human understanding of language. Ideally, a learning method will utilize all three sources. To accomplish this goal, we generalize an existing procedure that uses the latter two.","We extend this procedure by re-interpreting it as a Naive Bayes model for document sentiment. Viewed as such, it can also be seen to extract a pair of derived features that are linearly combined to predict sentiment. This perspective allows us to improve upon previous methods, primarily through two strategies: incorporating additional derived features into the model and, where possible, using labeled data to estimate their relative influence."]},{"title":"1 Introduction","paragraphs":["Text documents are available in ever-increasing numbers, making automated techniques for information extraction increasingly useful. Traditionally, most research effort has been directed towards “objective” information, such as classification according to topic; however, interest is growing in producing information about the opinions that a document contains; for instance, Morinaga et al. (2002). In March, 2004, the American Association for Artificial Intelligence held a symposium in this area, entitled “Exploring Affect and Attitude in Text.”","One task in opinion extraction is to label a review document d according to its prevailing sentiment s 2 f 1; 1g (unfavorable or favorable). Several previous papers have addressed this problem by building models that rely exclusively upon labeled documents, e.g. Pang et al. (2002), Dave et al. (2003). By learning models from labeled data, one can apply familiar, powerful techniques directly; however, in practice it may be difficult to obtain enough labeled reviews to learn model parameters accurately.","A contrasting approach (Turney, 2002) relies only upon documents whose labels are unknown. This makes it possible to use a large underlying corpus – in this case, the entire Internet as seen through the AltaVista search engine. As a result, estimates for model parameters are subject to a relatively small amount of random variation. The corresponding drawback to such an approach is that its predictions are not validated on actual documents.","In machine learning, it has often been effec-tive to use labeled and unlabeled examples in tandem, e.g. Nigam et al. (2000). Turney’s model introduces the further consideration of incorporating human-provided knowledge about language. In this paper we build models that utilize all three sources: labeled documents, unlabeled documents, and human-provided information.","The basic concept behind Turney’s model is quite simple. The “sentiment orientation” (Hatzivassiloglou and McKeown, 1997) of a pair of words is taken to be known. These words serve as “anchors” for positive and negative sentiment. Words that co-occur more frequently with one anchor than the other are themselves taken to be predictive of sentiment. As a result, information about a pair of words is generalized to many words, and then to documents.","In the following section, we relate this model with Naive Bayes classification, showing that Turney’s classifier is a “pseudo-supervise d” approach: it effectively generates a new corpus of labeled documents, upon which it fitsa Naive Bayes classifier. This insight allows the procedure to be represented as a probability model that is linear on the logistic scale, which in turn suggests generalizations that are developed in subsequent sections."]},{"title":"2 A Logistic Model for Sentiment 2.1 Turney’s Sentiment Classifier","paragraphs":["In Turney’s model, the “sentiment orientation” of word w is estimated as follows. (̂w) = log","N(w;excellent)=Nexcellent N(w;poor)=Npoor (1) Here, Na is the total number of sites on the Internet that contain an occurrence of a – a feature that can be a word type or a phrase. N(w;a) is the number of sites in which features w and a appear “near” each other, i.e. in the same passage of text, within a span of ten words. Both numbers are obtained from the hit count that results from a query of the AltaVista search engine. The rationale for this estimate is that words that express similar sentiment often co-occur, while words that express conflicting sentiment co-occur more rarely. Thus, a word that co-occurs more frequently with excellent than poor is estimated to have a positive sentiment orientation.","To extrapolate from words to documents, the estimated sentiment ŝ 2 f 1; 1g of a review document d is the sign of the average sentiment orientation of its constituent features.1","To represent this estimate formally, we introduce the following notation: W is a “dictionary” of features: (w1; : : : ; wp). Each feature’s respective sentiment orientation is represented as an entry in the vector ̂of length p: ĵ = (̂wj) (2) Given a collection of n review documents, the i-th each di is also represented as a vector of length p, with dij equal to the number of times that feature wj occurs in di. The length of a document is its total number of features, jdij =","Pp","j=1 dij.","Turney’s classifier for the i-th document’s senti-","ment si can now be written: ŝi = sign","Pp j=1 ĵdij jdij ! (3)","Using a carefully chosen collection of features, this classifier produces correct results on 65.8% of a collection of 120 movie reviews, where 60 are labeled positive and 60 negative. Although this is not a particularly encouraging result, movie reviews tend to be a difficult domain. Accuracy on sentiment classification in other domains exceeds 80% (Turney, 2002). 1","Note that not all words or phrases need to be considered as features. In Turney (2002), features are selected according to part-of-speech labels. 2.2 Naive Bayes Classification Bayes’ Theorem provides a convenient framework for predicting a binary response s 2 f 1; 1g from a feature vector x: Pr(s = 1jx) = Pr(xjs = 1)1 P k2f 1;1g Pr(xjs = k)k (4)","For a labeled sample of data (xi; si); i = 1; :::; n, a class’s marginal probability k can be estimated trivially as the proportion of training samples be-longing to the class. Thus the critical aspect of classificationby Bayes’ Theorem is to estimate the conditional distribution of x given s. Naive Bayes simplifies this problem by making a “nai ve” assumption: within a class, the different feature values are taken to be independent of one another. Pr(xjs) = Y j Pr(xjjs) (5) As a result, the estimation problem is reduced to univariate distributions. Naive Bayes for a Multinomial Distribution We consider a “bag of words” model for a document that belongs to class k, where features are assumed to result from a sequence of jdij independent multinomial draws with outcome probability vector qk = (qk1; : : : ; qkp).","Given a collection of documents with labels, (di; si); i = 1; : : : ; n, a natural estimate for qkj is the fraction of all features in documents of class k that equal wj: q̂kj = P i:si=k dij P i:si=k jdij (6)","In the two-class case, the logit transformation provides a revealing representation of the class posterior probabilities of the Naive Bayes model. dlogit(sjd) ≜ log cPr(s = 1jd) cPr(s = 1jd) (7) = log 1̂ ̂1 + p X j=1 dj log q̂1j q̂ 1j (8) = 0̂ + p X j=1 dj ĵ (9) where 0̂ = log 1̂ ̂1 (10) ĵ = log q̂1j q̂ 1j (11) Observe that the estimate for the logit in Equation 9 has a simple structure: it is a linear function of d. Models that take this form are commonplace in classification. 2.3 Turney’s Classifieras Naive Bayes Although Naive Bayes classification requires a labeled corpus of documents, we show in this section that Turney’s approach corresponds to a Naive Bayes model. The necessary documents and their corresponding labels are built from the spans of text that surround the anchor words excellent and poor.","More formally, a labeled corpus may be produced by the following procedure:","1. For a particular anchor ak, locate all of the sites on the Internet where it occurs.","2. From all of the pages within a site, gather the features that occur within ten words of an occurrence of ak, with any particular feature included at most once. This list comprises a new “document, ” representing that site.2","3. Label this document +1 if ak = excellent, -1 if ak = poor.","When a Naive Bayes model is fit to the corpus described above, it results in a vector ̂of length p, consisting of coefficient estimates for all features. In Propositions 1 and 2 below, we show that Turney’s estimates of sentiment orientation ̂are closely related to ,̂ and that both estimates produce identical classifiers. Proposition 1 ̂= C1 ̂(12) where C1 = Nexc:= P i:si=1 jdij Npoor= P i:si= 1 jdij (13) Proof: Because a feature is restricted to at most one occurrence in a document,","X i:si=k dij = N(w;ak) (14) Then from Equations 6 and 11: ĵ = log q̂1j q̂ 1j (15) = log N(w;exc:)= P i:si=1 jdij N(w;poor)= P i:si= 1 jdij (16) = C1 ĵ (17)"," 2 If both anchors occur on a site, then there will actually be","two documents, one for each sentiment Proposition 2 Turney’s classifier is identical to a Naive Bayes classifierfiton this corpus, with 1 = 1 = 0:5. Proof: A Naive Bayes classifiertypically assigns an observation to its most probable class. This is equivalent to classifying according to the sign of the estimated logit. So for any document, we must show that both the logit estimate and the average sentiment orientation are identical in sign.","When 1 = 0:5, 0 = 0. Thus the estimated logit is dlogit(sjd) = p X j=1 ĵdj (18) = C1 p X j=1 ĵdj (19) This is a positive multiple of Turney’s classifier (Equation 3), so they clearly match in sign."]},{"title":"3 A More Versatile Model 3.1 Desired Extensions","paragraphs":["By understanding Turney’s model within a Naive Bayes framework, we are able to interpret its output as a probability model for document classes. In the presence of labeled examples, this insight also makes it possible to estimate the intercept term 0. Further, we are able to view this model as a member of a broad class: linear estimates for the logit. This understanding facilitates further extensions, in particular, utilizing the following: 1. Labeled documents 2. More anchor words","The reason for using labeled documents is straightforward; labels offer validation for any chosen model. Using additional anchors is desirable in part because it is inexpensive to produce lists of words that are believed to reflect positive sentiment, perhaps by reference to a thesaurus. In addition, a single anchor may be at once too general and too specific.","An anchor may be too general in the sense that many common words have multiple meanings, and not all of them reflect a chosen sentiment orientation. For example, poor can refer to an objec-tive economic state that does not necessarily express negative sentiment. As a result, a word such as income appears 4.18 times as frequently with poor as excellent, even though it does not convey negative sentiment. Similarly, excellent has a technical meaning in antiquity trading, which causes it to appear 3.34 times as frequently with f urniture.","An anchor may also be too specific, in the sense that there are a variety of different ways to express sentiment, and a single anchor may not capture them all. So a word like pretentious carries a strong negative sentiment but co-occurs only slightly more frequently (1.23 times) with excellent than poor. Likewise, f ascination generally reflects a positive sentiment, yet it appears slightly more frequently (1.06 times) with poor than excellent. 3.2 Other Sources of Unlabeled Data The use of additional anchors has a drawback in terms of being resource-intensive. A feature set may contain many words and phrases, and each of them requires a separate AltaVista query for every chosen anchor word. In the case of 30,000 features and ten queries per minute, downloads for a single anchor word require over two days of data collection.","An alternative approach is to access a large collection of documents directly. Then all co-occurrences can be counted in a single pass. Although this approach dramatically reduces the amount of data available, it does offer several advantages.","Increased Query Options Search engine queries of the form phrase NEAR anchor may not produce all of the desired co-occurrence counts. For instance, one may wish to run queries that use stemmed words, hyphenated words, or punctuation marks. One may also wish to modify the definition of NEAR, or to count individual co-occurrences, rather than counting sites that contain at least one co-occurrence.","Topic Matching Across the Internet as a whole, features may not exhibit the same correlation structure as they do within a specific domain. By restricting attention to documents within a domain, one may hope to avoid co-occurrences that are primarily relevant to other subjects.","Reproducibility On a fixed corpus, counts of word occurrences produce consistent results. Due to the dynamic nature of the Internet, numbers may fluctuate. 3.3 Co-Occurrences and Derived Features The Naive Bayes coefficient estimate ̂j may itself be interpreted as an intercept term plus a linear combination of features of the form log N(wj;ak).","Num. of Labeled Occurrences Correlation 1 - 5 0.022 6 - 10 0.082 11 - 25 0.113 26 - 50 0.183 51 - 75 0.283 76 - 100 0.316 Figure 1: Correlation between Supervised and Unsupervised CoefficientEstimates ĵ = log N(j;exc:)= P i:si=1 jdij N(j;pr:)= P i:si= 1 jdij (20)","= log C1 + log N(j;exc:) log N(j;pr:) (21)","We generalize this estimate as follows: for a collection of K different anchor words, we consider a general linear combination of logged co-occurrence counts. ĵ = K X k=1 k log N(wj;ak) (22) In the special case of a Naive Bayes model, k = 1 when the k-th anchor word ak conveys positive sentiment, 1 when it conveys negative sentiment.","Replacing the logit estimate in Equation 9 with an estimate of this form, the model becomes: dlogit(sjd) = 0̂ + p X j=1 dj ĵ (23) = 0̂ + p X j=1 K X k=1 djk log N(wj;ak) (24) = 0 + K X k=1 k p X j=1 dj log N(wj;ak) (25) (26)","This model has only K + 1 parameters: 0; 1; : : : ; K . These can be learned straightforwardly from labeled documents by a method such as logistic regression.","Observe that a document receives a score for each anchor word","Pp","j=1 dj log N(wj;ak). Effectively, the","predictor variables in this model are no longer","counts of the original features dj. Rather, they are","−2.0","−1.5","−1.0","−0.5","0.0","0.5","1.0","1.5 −3 −2 −1 0 1 2 3 4 Traditional Naive Bayes Coefs. Turney Naive Bayes Coefs. Unsupervised vs. Supervised Coefficients Figure 2: Unsupervised versus Supervised Coefficient Estimates inner products between the entire feature vector d and the logged co-occurence vector N(w;ak). In this respect, the vector of logged co-occurrences is used to produce derived feature."]},{"title":"4 Data Analysis 4.1 Accuracy of Unsupervised Coefficients","paragraphs":["By means of a Perl script that uses the Lynx browser, Version 2.8.3rel.1, we download AltaVista hit counts for queries of the form “ target NEAR anchor.” The initial list of targets consists of 44,321 word types extracted from the Pang corpus of 1400 labeled movie reviews. After preprocessing, this number is reduced to 28,629.3","In Figure 1, we compare estimates produced by two Naive Bayes procedures. For each feature wj, we estimate j by using Turney’s procedure, and by fitting a traditional Naive Bayes model to the labeled documents. The traditional estimates are smoothed by assuming a Beta prior distribution that is equivalent to having four previous observations of wj in documents of each class. q̂1j q̂ 1j = C2 4 + P i:si=1 dij 4 + P i:si= 1 dij (27) where C2 = 4p + P i:si=1 jdij 4p + P i:si= 1 jdij (28) Here, dij is used to indicate feature presence: dij =  1 if wj appears in di 0 otherwise (29)","3","We eliminate extremely rare words by requiring each target to co-occur at least once with each anchor. In addition, certain types, such as words containing hyphens, apostrophes, or other punctuation marks, do not appear to produce valid counts, so they are discarded. Positive Negative","best awful brilliant bad excellent pathetic spectacular poor wonderful worst Figure 3: Selected Anchor Words We choose this fittingprocedure among several can-didates because it performs well in classifying test documents.","In Figure 1, each entry in the right-hand column is the observed correlation between these two estimates over a subset of features. For features that occur in five documents or fewer, the correlation is very weak (0.022). This is not surpris-ing, as it is difficult to estimate a coefficient from such a small number of labeled examples. Correlations are stronger for more common features, but never strong. As a baseline for comparison, Naive Bayes coefficients can be estimated using a subset of their labeled occurrences. With two independent sets of 51-75 occurrences, Naive Bayes coefficient estimates had a correlation of 0.475.","Figure 2 is a scatterplot of the same coefficient estimates for word types that appear in 51 to 100 documents. The great majority of features do not have large coefficients, but even for the ones that do, there is not a tight correlation. 4.2 Additional Anchors We wish to learn how our model performance depends on the choice and number of anchor words. Selecting from WordNet synonym lists (Fellbaum, 1998), we choose five positive anchor words and five negative (Figure 3). This produces a total of 25 different possible pairs for use in producing coefficientestimates.","Figure 4 shows the classification performance of unsupervised procedures using the 1400 labeled Pang documents as test data. Coefficients ̂j are estimated as described in Equation 22. Several different experimental conditions are applied. The methods labeled ”Count” use the original un-normalized coefficients,while those labeled “Norm. ” have been normalized so that the number of co-occurrences with each anchor have identical variance. Results are shown when rare words (with three or fewer occurrences in the labeled corpus) are included and omitted. The methods “pair” and “10” describe whether all ten anchor coefficientsare used at once, or just the ones that correspond to a single pair of Method Feat. Misclass. St.Dev Count Pair >3 39.6% 2.9% Norm. Pair >3 38.4% 3.0% Count Pair all 37.4% 3.1% Norm. Pair all 37.3% 3.0% Count 10 > 3 36.4% – Norm. 10 > 3 35.4% – Count 10 all 34.6% – Norm. 10 all 34.1% – Figure 4: Classification Error Rates for Different Unsupervised Approaches anchor words. For anchor pairs, the mean error across all 25 pairs is reported, along with its standard deviation.","Patterns are consistent across the different conditions. A relatively large improvement comes from using all ten anchor words. Smaller benefits arise from including rare words and from normalizing model coefficients.","Models that use the original pair of anchor words, excellent and poor, perform slightly better than the average pair. Whereas mean performance ranges from 37.3% to 39.6%, misclassification rates for this pair of anchors ranges from 37.4% to 38.1%. 4.3 A Smaller Unlabeled Corpus As described in Section 3.2, there are several reasons to explore the use of a smaller unlabeled corpus, rather than the entire Internet. In our experiments, we use additional movie reviews as our documents. For this domain, Pang makes available 27,886 reviews.4","Because this corpus offers dramatically fewer in-stances of anchor words, we modify our estimation procedure. Rather than discarding words that rarely co-occur with anchors, we use the same feature set as before and regularize estimates by the same procedure used in the Naive Bayes procedure described earlier.","Using all features, and ten anchor words with normalized scores, test error is 35.0%. This suggests that comparable results can be attained while referring to a considerably smaller unlabeled corpus. Rather than requiring several days of downloads, the count of nearby co-occurrences was completed in under ten minutes.","Because this procedure enables fast access to counts, we explore the possibility of dramatically enlarging our collection of anchor words. We col-4 This corpus is freely available on the following website:","http://www.cs.cornell.edu/people/pabo/movie-review-data/.","100","200","300","400","500","600 0.30 0.32 0.34 0.36 0.38 0.40 Num. of Labeled Documents Classif. Error Misclassification versus Sample Size Figure 5: Misclassification with Labeled Documents. The solid curve represents a latent factor model with estimated coefficients. The dashed curve uses a Naive Bayes classifier. The two horizontal lines represent unsupervised estimates; the upper one is for the original unsupervised classifier, and the lower is for the most successful unsupervised method. lect data for the complete set of WordNet synonyms for the words good, best, bad, boring, and dreadf ul. This yields a total of 83 anchor words, 35 positive and 48 negative. When all of these anchors are used in conjunction, test error increases to 38.3%. One possible difficulty in using this automated procedure is that some synonyms for a word do not carry the same sentiment orientation. For in-stance, intense is listed as a synonym for bad, even though its presence in a movie review is a strongly positive indication.5 4.4 Methods with Supervision As demonstrated in Section 3.3, each anchor word ak is associated with a coefficient k. In unsupervised models, these coefficients are assumed to be known. However, when labeled documents are available, it may be advantageous to estimate them.","Figure 5 compares the performance of a model with estimated coefficient vector , as opposed to unsupervised models and a traditional supervised approach. When a moderate number of labeled documents are available, it offers a noticeable improvement.","The supervised method used for reference in this case is the Naive Bayes model that is described in section 4.1. Naive Bayes classification is of particular interest here because it converges faster to its asymptotic optimum than do discriminative methods (Ng, A. Y. and Jordan, M., 2002). Further, with 5 In the labeled Pang corpus, intense appears in 38 positive","reviews and only 6 negative ones. a larger number of labeled documents, its performance on this corpus is comparable to that of Support Vector Machines and Maximum Entropy models (Pang et al., 2002).","The coefficient vector is estimated by regularized logistic regression. This method has been used in other text classification problems, as in Zhang and Yang (2003). In our case, the regularization6 is introduced in order to enforce the beliefs that: 1 2, if a1, a2 synonyms (30) 1 2, if a1, a2 antonyms (31) For further information on regularized model fitting, see for instance, Hastie et al. (2001)."]},{"title":"5 Conclusion","paragraphs":["In business settings, there is growing interest in learning product reputations from the Internet. For such problems, it is often difficult or expensive to obtain labeled data. As a result, a change in modeling strategies is needed, towards approaches that require less supervision. In this paper we provide a framework for allowing human-provided information to be combined with unlabeled documents and labeled documents. We have found that this framework enables improvements over existing techniques, both in terms of the speed of model estimation and in classification accuracy. As a result, we believe that this is a promising new approach to problems of practical importance."]},{"title":"References","paragraphs":["Kushal Dave, Steve Lawrence, and David M. Pennock. 2003. Mining the peanut gallery: Opinion extraction and semantic classification of product reviews.","C. Fellbaum. 1998. Wordnet an electronic lexical database.","T. Hastie, R. Tibshirani, and J. Friedman. 2001. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer-Verlag.","Vasileios Hatzivassiloglou and Kathleen R. McKeown. 1997. Predicting the semantic orientation of adjectives. In Philip R. Cohen and Wolfgang Wahlster, editors, Proceedings of the Thirty-Fifth Annual Meeting of the Association for Computational Linguistics and Eighth Conference of the European Chapter of the Association for Computational Linguistics, pages 174–181, Somerset, New Jersey. Association for Computational Linguistics. 6 By cross-validation, we choose the regularization term =","1.5/sqrt(n), where n is the number of labeled documents.","Satoshi Morinaga, Kenji Yamanishi, Kenji Tateishi, and Toshikazu Fukushima. 2002. Mining product reputations on the web.","Ng, A. Y. and Jordan, M. 2002. On discriminative vs. generative classifiers: A comparison of logistic regression and naive bayes. Advances in Neural Information Processing Systems, 14.","Kamal Nigam, Andrew K. McCallum, Sebastian Thrun, and Tom M. Mitchell. 2000. Text classification from labeled and unlabeled documents using EM. Machine Learning, 39(2/3):103–134.","Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up? sentiment classification using machine learning techniques. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP).","P.D. Turney and M.L. Littman. 2002. Unsupervised learning of semantic orientation from a hundredbillion-word corpus.","Peter Turney. 2002. Thumbs up or thumbs down? semantic orientation applied to unsupervised classification of reviews. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL’02), pages 417– 424, Philadelphia, Pennsylvania. Association for Computational Linguistics.","Janyce Wiebe. 2000. Learning subjective adjectives from corpora. In Proc. 17th National Conference on Artificial Intelligence (AAAI-2000), Austin, Texas.","Jian Zhang and Yiming Yang. 2003. ”rob ustness of regularized linear classification methods in text categorization”. In Proceedings of the 26th Annual International ACM SIGIR Conference (SIGIR 2003)."]}],"references":[{"authors":[{"first":"Kushal","last":"Dave"},{"first":"Steve","last":"Lawrence"},{"first":"David","middle":"M.","last":"Pennock"}],"year":"2003","title":"Mining the peanut gallery: Opinion extraction and semantic classification of product reviews","source":"Kushal Dave, Steve Lawrence, and David M. Pennock. 2003. Mining the peanut gallery: Opinion extraction and semantic classification of product reviews."},{"authors":[{"first":"C.","last":"Fellbaum"}],"year":"1998","title":"Wordnet an electronic lexical database","source":"C. Fellbaum. 1998. Wordnet an electronic lexical database."},{"authors":[{"first":"T.","last":"Hastie"},{"first":"R.","last":"Tibshirani"},{"first":"J.","last":"Friedman"}],"year":"2001","title":"The Elements of Statistical Learning: Data Mining, Inference, and Prediction","source":"T. Hastie, R. Tibshirani, and J. Friedman. 2001. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer-Verlag."},{"authors":[{"first":"Vasileios","last":"Hatzivassiloglou"},{"first":"Kathleen","middle":"R.","last":"McKeown"}],"year":"1997","title":"Predicting the semantic orientation of adjectives","source":"Vasileios Hatzivassiloglou and Kathleen R. McKeown. 1997. Predicting the semantic orientation of adjectives. In Philip R. Cohen and Wolfgang Wahlster, editors, Proceedings of the Thirty-Fifth Annual Meeting of the Association for Computational Linguistics and Eighth Conference of the European Chapter of the Association for Computational Linguistics, pages 174–181, Somerset, New Jersey. Association for Computational Linguistics. 6 By cross-validation, we choose the regularization term ="},{"authors":[],"source":"1.5/sqrt(n), where n is the number of labeled documents."},{"authors":[{"first":"Satoshi","last":"Morinaga"},{"first":"Kenji","last":"Yamanishi"},{"first":"Kenji","last":"Tateishi"},{"first":"Toshikazu","last":"Fukushima"}],"year":"2002","title":"Mining product reputations on the web","source":"Satoshi Morinaga, Kenji Yamanishi, Kenji Tateishi, and Toshikazu Fukushima. 2002. Mining product reputations on the web."},{"authors":[{"first":"A.","middle":"Y.","last":"Ng"},{"last":"Jordan"},{"last":"M"}],"year":"2002","title":"On discriminative vs","source":"Ng, A. Y. and Jordan, M. 2002. On discriminative vs. generative classifiers: A comparison of logistic regression and naive bayes. Advances in Neural Information Processing Systems, 14."},{"authors":[{"first":"Kamal","last":"Nigam"},{"first":"Andrew","middle":"K.","last":"McCallum"},{"first":"Sebastian","last":"Thrun"},{"first":"Tom","middle":"M.","last":"Mitchell"}],"year":"2000","title":"Text classification from labeled and unlabeled documents using EM","source":"Kamal Nigam, Andrew K. McCallum, Sebastian Thrun, and Tom M. Mitchell. 2000. Text classification from labeled and unlabeled documents using EM. Machine Learning, 39(2/3):103–134."},{"authors":[{"first":"Bo","last":"Pang"},{"first":"Lillian","last":"Lee"},{"first":"Shivakumar","last":"Vaithyanathan"}],"year":"2002","title":"Thumbs up? sentiment classification using machine learning techniques","source":"Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up? sentiment classification using machine learning techniques. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP)."},{"authors":[{"first":"P.","middle":"D.","last":"Turney"},{"first":"M.","middle":"L.","last":"Littman"}],"year":"2002","title":"Unsupervised learning of semantic orientation from a hundredbillion-word corpus","source":"P.D. Turney and M.L. Littman. 2002. Unsupervised learning of semantic orientation from a hundredbillion-word corpus."},{"authors":[{"first":"Peter","last":"Turney"}],"year":"2002","title":"Thumbs up or thumbs down? semantic orientation applied to unsupervised classification of reviews","source":"Peter Turney. 2002. Thumbs up or thumbs down? semantic orientation applied to unsupervised classification of reviews. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL’02), pages 417– 424, Philadelphia, Pennsylvania. Association for Computational Linguistics."},{"authors":[{"first":"Janyce","last":"Wiebe"}],"year":"2000","title":"Learning subjective adjectives from corpora","source":"Janyce Wiebe. 2000. Learning subjective adjectives from corpora. In Proc. 17th National Conference on Artificial Intelligence (AAAI-2000), Austin, Texas."},{"authors":[{"first":"Jian","last":"Zhang"},{"first":"Yiming","last":"Yang"}],"year":"2003","title":"”rob ustness of regularized linear classification methods in text categorization”","source":"Jian Zhang and Yiming Yang. 2003. ”rob ustness of regularized linear classification methods in text categorization”. In Proceedings of the 26th Annual International ACM SIGIR Conference (SIGIR 2003)."}],"cites":[{"style":0,"text":"Morinaga et al. (2002)","origin":{"pointer":"/sections/1/paragraphs/0","offset":376,"length":22},"authors":[{"last":"Morinaga"},{"last":"al."}],"year":"2002","references":["/references/5"]},{"style":0,"text":"March, 2004","origin":{"pointer":"/sections/1/paragraphs/0","offset":403,"length":11},"authors":[{"last":"March"}],"year":"2004","references":[]},{"style":0,"text":"Pang et al. (2002)","origin":{"pointer":"/sections/1/paragraphs/1","offset":263,"length":18},"authors":[{"last":"Pang"},{"last":"al."}],"year":"2002","references":["/references/8"]},{"style":0,"text":"Dave et al. (2003)","origin":{"pointer":"/sections/1/paragraphs/1","offset":283,"length":18},"authors":[{"last":"Dave"},{"last":"al."}],"year":"2003","references":["/references/0"]},{"style":0,"text":"Turney, 2002","origin":{"pointer":"/sections/1/paragraphs/2","offset":24,"length":12},"authors":[{"last":"Turney"}],"year":"2002","references":["/references/10"]},{"style":0,"text":"Nigam et al. (2000)","origin":{"pointer":"/sections/1/paragraphs/3","offset":104,"length":19},"authors":[{"last":"Nigam"},{"last":"al."}],"year":"2000","references":["/references/7"]},{"style":0,"text":"Hatzivassiloglou and McKeown, 1997","origin":{"pointer":"/sections/1/paragraphs/4","offset":86,"length":34},"authors":[{"last":"Hatzivassiloglou"},{"last":"McKeown"}],"year":"1997","references":["/references/3"]},{"style":0,"text":"Turney, 2002","origin":{"pointer":"/sections/2/paragraphs/9","offset":352,"length":12},"authors":[{"last":"Turney"}],"year":"2002","references":["/references/10"]},{"style":0,"text":"Turney (2002)","origin":{"pointer":"/sections/2/paragraphs/10","offset":73,"length":13},"authors":[{"last":"Turney"}],"year":"2002","references":["/references/10"]},{"style":0,"text":"Fellbaum, 1998","origin":{"pointer":"/sections/4/paragraphs/6","offset":393,"length":14},"authors":[{"last":"Fellbaum"}],"year":"1998","references":["/references/1"]},{"style":0,"text":"M., 2002","origin":{"pointer":"/sections/4/paragraphs/21","offset":279,"length":8},"authors":[{"last":"M."}],"year":"2002","references":[]},{"style":0,"text":"Pang et al., 2002","origin":{"pointer":"/sections/4/paragraphs/22","offset":180,"length":17},"authors":[{"last":"Pang"},{"last":"al."}],"year":"2002","references":["/references/8"]},{"style":0,"text":"Zhang and Yang (2003)","origin":{"pointer":"/sections/4/paragraphs/23","offset":143,"length":21},"authors":[{"last":"Zhang"},{"last":"Yang"}],"year":"2003","references":["/references/12"]},{"style":0,"text":"Hastie et al. (2001)","origin":{"pointer":"/sections/4/paragraphs/23","offset":381,"length":20},"authors":[{"last":"Hastie"},{"last":"al."}],"year":"2001","references":["/references/2"]}]}
