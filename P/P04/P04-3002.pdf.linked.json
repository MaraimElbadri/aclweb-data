{"sections":[{"title":"Improving Domain-Specific Word Alignment for Computer Assisted Translation WU Hua, WANG Haifeng","paragraphs":["Toshiba (China) Research and Development Center","5/F., Tower W2, Oriental Plaza","No.1, East Chang An Ave., Dong Cheng District","Beijing, China, 100738 {wuhua, wanghaifeng}@rdc.toshiba.com.cn "]},{"title":"Abstract","paragraphs":["This paper proposes an approach to improve word alignment in a specific domain, in which only a small-scale domain-specific corpus is available, by adapting the word alignment information in the general domain to the specific domain. This approach first trains two statistical word alignment models with the large-scale corpus in the general domain and the small-scale corpus in the specific domain respectively, and then improves the domain-specific word alignment with these two models. Experimental results show a significant improvement in terms of both alignment precision and recall. And the alignment results are applied in a computer assisted translation system to improve human translation efficiency."]},{"title":"1 Introduction","paragraphs":["Bilingual word alignment is first introduced as an intermediate result in statistical machine translation (SMT) (Brown et al., 1993). In previous alignment methods, some researchers modeled the alignments with different statistical models (Wu, 1997; Och and Ney, 2000; Cherry and Lin, 2003). Some researchers use similarity and association measures to build alignment links (Ahrenberg et al., 1998; Tufis and Barbu, 2002). However, All of these methods require a large-scale bilingual corpus for training. When the large-scale bilingual corpus is not available, some researchers use existing dictionaries to improve word alignment (Ker and Chang, 1997). However, few works address the problem of domain-specific word alignment when neither the large-scale domain-specific bilingual corpus nor the domain-specific translation dictionary is available.","This paper addresses the problem of word alignment in a specific domain, where only a small domain-specific corpus is available. In the domain-specific corpus, there are two kinds of words. Some are general words, which are also frequently used in the general domain. Others are domain-specific words, which only occur in the specific domain. In general, it is not quite hard to obtain a large-scale general bilingual corpus while the available domain-specific bilingual corpus is usually quite small. Thus, we use the bilingual corpus in the general domain to improve word alignments for general words and the corpus in the specific domain for domain-specific words. In other words, we will adapt the word alignment information in the general domain to the specific domain.","In this paper, we perform word alignment adaptation from the general domain to a specific domain (in this study, a user manual for a medical system) with four steps. (1) We train a word alignment model using the large-scale bilingual corpus in the general domain; (2) We train another word alignment model using the small-scale bilingual corpus in the specific domain; (3) We build two translation dictionaries according to the alignment results in (1) and (2) respectively; (4) For each sentence pair in the specific domain, we use the two models to get different word alignment results and improve the results according to the translation dictionaries. Experimental results show that our method improves domain-specific word alignment in terms of both precision and recall, achieving a 21.96% relative error rate reduction.","The acquired alignment results are used in a generalized translation memory system (GTMS, a kind of computer assisted translation systems) (Simard and Langlais, 2001). This kind of system facilitates the re-use of existing translation pairs to translate documents. When translating a new sentence, the system tries to provide the pre-translated examples matched with the input and recommends a translation to the human translator, and then the translator edits the suggestion to get a final translation. The conventional TMS can only recommend translation examples on the sentential level while GTMS can work on both sentential and sub-sentential levels by using word alignment results. These GTMS are usually employed to translate various documents such as user manuals, computer operation guides, and mechanical operation manuals."]},{"title":"2 2.1 Word Alignment Adaptation Bi-directional Word Alignment","paragraphs":["In statistical translation models (Brown et al., 1993), only one-to-one and more-to-one word alignment links can be found. Thus, some multi-word units cannot be correctly aligned. In order to deal with this problem, we perform translation in two directions (English to Chinese, and Chinese to English) as described in (Och and Ney, 2000). The GIZA++ toolkit 1","is used to perform statistical word alignment.","For the general domain, we use and to represent the alignment sets obtained with English as the source language and Chinese as the target language or vice versa. For alignment links in both sets, we use i for English words and j for Chinese words. 1SG 2SG }0 },{|),{(1 ≥== jjjj aaAjASG","}0 },{|),{(2 ≥== iiii aaAAiSG","Where, is the position of the source word aligned to the target word in position k. The set","indicates the words aligned to the same source word k. For example, if a Chinese word in position j is connect to an English word in position i, then . And if a Chinese word in position j is","connect to English words in position i and k, then . ),( jikak = ),( jikAk = ia j = },{ kiA j = Based on the above two alignment sets, we","obtain their intersection set, union set 2","and","subtraction set. Intersection: 21 SGSGSG ∩= Union: 21 SGSGPG ∪= Subtraction: SGMG −= PG For the specific domain, we use and","to represent the word alignment sets in the two","directions. The symbols , 1SF 2SF","SF PF and MF represents the intersection set, union set and the subtraction set, respectively."]},{"title":"2.2  Translation Dictionary Acquisition","paragraphs":["When we train the statistical word alignment model with a large-scale bilingual corpus in the general domain, we can get two word alignment results for the training data. By taking the intersection of the two word alignment results, we build a new alignment set. The alignment links in this intersection set are extended by iteratively adding word alignment links into it as described in (Och and Ney, 2000)."," 1 It is located at http://www.isi.edu/~och/GIZA++.html 2 In this paper, the union operation does not remove the replicated elements. For example, if set one includes two elements {1, 2} and set two includes two elements {1, 3}, then the union of these two sets becomes {1, 1, 2, 3}.","Based on the extended alignment links, we build an English to Chinese translation dictionary with translation probabilities. In order to filter some noise caused by the error alignment links, we only retain those translation pairs whose translation probabilities are above a threshold 1D","1δ or co-occurring frequencies are above a threshold 2δ .","When we train the IBM statistical word alignment model with a limited bilingual corpus in the specific domain, we build another translation dictionary with the same method as for the dictionary . But we adopt a different filtering strategy for the translation dictionary . We use log-likelihood ratio to estimate the association strength of each translation pair because Dunning (1993) proved that log-likelihood ratio performed very well on small-scale data. Thus, we get the translation dictionary by keeping those entries whose log-likelihood ratio scores are greater than a threshold 2D 1D 3 2D 2D δ ."]},{"title":"2.3 Word Alignment Adaptation Algorithm","paragraphs":["Based on the bi-directional word alignment, we define as SI SFSGSI ∩= and as UG","SIPFPGUG −∪= . The word alignment links in the set SI are very reliable. Thus, we directly accept them as correct links and add them into the final alignment set . WA Input: Alignment set and SI UG (1) For alignment links in , we directly add","them into the final alignment set . SI","WA","(2) For each English word i in the , we first find its different alignment links, and then do the following: UG","a) If there are alignment links found in dictionary , add the link with the largest probability to . 1D","WA","b) Otherwise, if there are alignment links found in dictionary , add the link with the largest log-likelihood ratio score to . 2D","WA","c) If both a) and b) fail, but three links select the same target words for the English word i, we add this link into . WA","d) Otherwise, if there are two different links for this word: one target is a single word, and the other target is a multi-word unit and the words in the multi-word unit have no link in",", add this multi-word alignment link to",". WA WA","Output: Updated alignment set WA Figure 1. Word Alignment Adaptation Algorithm","For each source word in the set , there are two to four different alignment links. We first use translation dictionaries to select one link among them. We first examine the dictionary and then","to see whether there is at least an alignment link of this word included in these two dictionaries. If it is successful, we add the link with the largest probability or the largest log-likelihood ratio score to the final set . Otherwise, we use two heuristic rules to select word alignment links. The detailed algorithm is described in Figure 1. UG 1D 2D WA  Figure 2. Alignment Example","Figure 2 shows an alignment result obtained with the word alignment adaptation algorithm. For example, for the English word “x-ray”, we have two different links in UG . One is (x-ray, X) and the other is (x-ray, X 射线). And the single Chinese words “射” and “线” have no alignment links in the set . According to the rule d), we select the link (x-ray, X 射线). WA"]},{"title":"3 Evaluation 3.1 3.2","paragraphs":["We compare our method with three other methods. The first method “Gen+Spec” directly combines the corpus in the general domain and in the specific domain as training data. The second method “Gen” only uses the corpus in the general domain as training data. The third method “Spec” only uses the domain-specific corpus as training data. With these training data, the three methods can get their own translation dictionaries. However, each of them can only get one translation dictionary. Thus, only one of the two steps a) and b) in Figure 1 can be applied to these methods. The difference between these three methods and our method is that, for each word, our method has four candidate alignment links while the other three methods only has two candidate alignment links. Thus, the steps c) and d) in Figure 1 should not be applied to these three methods. Training and Testing Data We have a sentence aligned English-Chinese bilingual corpus in the general domain, which includes 320,000 bilingual sentence pairs, and a sentence aligned English-Chinese bilingual corpus in the specific domain (a medical system manual), which includes 546 bilingual sentence pairs. From this domain-specific corpus, we randomly select 180 pairs as testing data. The remained 366 pairs are used as domain-specific training data.","The Chinese sentences in both the training set and the testing set are automatically segmented into words. In order to exclude the effect of the segmentation errors on our alignment results, we correct the segmentation errors in our testing set. The alignments in the testing set are manually annotated, which includes 1,478 alignment links. Overall Performance We use evaluation metrics similar to those in (Och and Ney, 2000). However, we do not classify alignment links into sure links and possible links. We consider each alignment as a sure link. If we use","to represent the alignments identified by the proposed methods and to denote the reference alignments, the methods to calculate the precision, recall, and f-measure are shown in Equation (1), (2) and (3). According to the definition of the alignment error rate (AER) in (Och and Ney, 2000), AER can be calculated with Equation (4). Thus, the higher the f-measure is, the lower the alignment error rate is. Thus, we will only give precision, recall and AER values in the experimental results. GS CS |S| |SS| G","CG ∩ =precision  (1) |S| |SS| C","CG ∩ =recall  (2) |||| ||*2 CG CG SS SS fmeasure","+∩ = (3) fmeasure SS SS AER","CG CG","−=","+∩","−= 1","|||| ||*2","1 (4) ","Method Precision Recall AER Ours 0.8363 0.7673 0.1997 Gen+Spec 0.8276 0.6758 0.2559 Gen 0.8668 0.6428 0.2618 Spec 0.8178 0.4769 0.3974 Table 1. Word Alignment Adaptation Results","We get the alignment results shown in Table 1 by setting the translation probability threshold to","1.01 =δ , the co-occurring frequency threshold to","52 =δ and log-likelihood ratio score to 503 =δ . From the results, it can be seen that our approach performs the best among others, achieving much higher recall and comparable precision. It also achieves a 21.96% relative error rate reduction compared to the method “Gen+Spec”. This indicates that separately modeling the general words and domain-specific words can effectively improve the word alignment in a specific domain."]},{"title":"4 Computer Assisted Translation System","paragraphs":["A direct application of the word alignment result to the GTMS is to get translations for sub-sequences in the input sentence using the pre-translated examples. For each sentence, there are many sub-sequences. GTMS tries to find translation examples that match the longest sub-sequences so as to cover as much of the input sentence as possible without overlapping. Figure 3 shows a sentence translated on the sub-sentential level. The three panels display the input sentence, the example translations and the translation suggestion provided by the system, respectively. The input sentence is segmented to three parts. For each part, the GTMS finds one example to get a translation fragment according to the word alignment result. By combining the three translation fragments, the GTMS produces a correct translation suggestion “系统被认为有 CT 扫描机。” Without the word alignment information, the conventional TMS cannot find translations for the input sentence because there are no examples closely matched with it. Thus, word alignment information can improve the translation accuracy of the GTMS, which in turn reduces editing time of the translators and improves translation efficiency.  Figure 3. A Snapshot of the Translation System"]},{"title":"5 Conclusion","paragraphs":["This paper proposes an approach to improve domain-specific word alignment through alignment adaptation. Our contribution is that our approach improves domain-specific word alignment by adapting word alignment information from the general domain to the specific domain. Our approach achieves it by training two alignment models with a large-scale general bilingual corpus and a small-scale domain-specific corpus. Moreover, with the training data, two translation dictionaries are built to select or modify the word alignment links and further improve the alignment results. Experimental results indicate that our approach achieves a precision of 83.63% and a recall of 76.73% for word alignment on a user manual of a medical system, resulting in a relative error rate reduction of 21.96%. Furthermore, the alignment results are applied to a computer assisted translation system to improve translation efficiency.","Our future work includes two aspects. First, we will seek other adaptation methods to further improve the domain-specific word alignment results. Second, we will use the alignment adaptation results in other applications."]},{"title":"References","paragraphs":["Lars Ahrenberg, Magnus Merkel and Mikael Andersson. 1998. A Simple Hybrid Aligner for Generating Lexical Correspondences in Parallel Tests. In Proc. of the 36th","Annual Meeting of the Association for Computational Linguistics and the 17th","International Conference on Computational Linguistics, pages 29-35.","Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra and Robert L. Mercer. 1993. The Mathematics of Statistical Machine Translation: Parameter Estimation. Computational Linguistics, 19(2): 263-311.","Colin Cherry and Dekang Lin. 2003. A Probability Model to Improve Word Alignment. In Proc. of the 41st","Annual Meeting of the Association for Computational Linguistics, pages 88-95.","Ted Dunning. 1993. Accurate Methods for the Statistics of Surprise and Coincidence. Computational Linguistics, 19(1): 61-74.","Sue J. Ker, Jason S. Chang. 1997. A Class-based Approach to Word Alignment. Computational Linguistics, 23(2): 313-343.","Franz Josef Och and Hermann Ney. 2000. Improved Statistical Alignment Models. In Proc. of the 38th"," Annual Meeting of the Association for Computational Linguistics, pages 440-447.","Michel Simard and Philippe Langlais. 2001. Sub-sentential Exploitation of Translation Memories. In Proc. of MT Summit VIII, pages 335-339.","Dan Tufis and Ana Maria Barbu. 2002. Lexical Token Alignment: Experiments, Results and Application. In Proc. of the Third International Conference on Language Resources and Evaluation, pages 458-465.","Dekai Wu. 1997. Stochastic Inversion Transduction Grammars and Bilingual Parsing of Parallel Corpora. Computational Linguistics, 23(3): 377-403."]}],"references":[{"authors":[{"first":"Lars","last":"Ahrenberg"},{"first":"Magnus","last":"Merkel"},{"first":"Mikael","last":"Andersson"}],"year":"1998","title":"A Simple Hybrid Aligner for Generating Lexical Correspondences in Parallel Tests","source":"Lars Ahrenberg, Magnus Merkel and Mikael Andersson. 1998. A Simple Hybrid Aligner for Generating Lexical Correspondences in Parallel Tests. In Proc. of the 36th"},{"authors":[],"source":"Annual Meeting of the Association for Computational Linguistics and the 17th"},{"authors":[],"source":"International Conference on Computational Linguistics, pages 29-35."},{"authors":[{"first":"Peter","middle":"F.","last":"Brown"},{"first":"Stephen","middle":"A. Della","last":"Pietra"},{"first":"Vincent","middle":"J. Della","last":"Pietra"},{"first":"Robert","middle":"L.","last":"Mercer"}],"year":"1993","title":"The Mathematics of Statistical Machine Translation: Parameter Estimation","source":"Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra and Robert L. Mercer. 1993. The Mathematics of Statistical Machine Translation: Parameter Estimation. Computational Linguistics, 19(2): 263-311."},{"authors":[{"first":"Colin","last":"Cherry"},{"first":"Dekang","last":"Lin"}],"year":"2003","title":"A Probability Model to Improve Word Alignment","source":"Colin Cherry and Dekang Lin. 2003. A Probability Model to Improve Word Alignment. In Proc. of the 41st"},{"authors":[],"source":"Annual Meeting of the Association for Computational Linguistics, pages 88-95."},{"authors":[{"first":"Ted","last":"Dunning"}],"year":"1993","title":"Accurate Methods for the Statistics of Surprise and Coincidence","source":"Ted Dunning. 1993. Accurate Methods for the Statistics of Surprise and Coincidence. Computational Linguistics, 19(1): 61-74."},{"authors":[{"first":"Sue","middle":"J.","last":"Ker"},{"first":"Jason","middle":"S.","last":"Chang"}],"year":"1997","title":"A Class-based Approach to Word Alignment","source":"Sue J. Ker, Jason S. Chang. 1997. A Class-based Approach to Word Alignment. Computational Linguistics, 23(2): 313-343."},{"authors":[{"first":"Franz","middle":"Josef","last":"Och"},{"first":"Hermann","last":"Ney"}],"year":"2000","title":"Improved Statistical Alignment Models","source":"Franz Josef Och and Hermann Ney. 2000. Improved Statistical Alignment Models. In Proc. of the 38th"},{"authors":[],"source":" Annual Meeting of the Association for Computational Linguistics, pages 440-447."},{"authors":[{"first":"Michel","last":"Simard"},{"first":"Philippe","last":"Langlais"}],"year":"2001","title":"Sub-sentential Exploitation of Translation Memories","source":"Michel Simard and Philippe Langlais. 2001. Sub-sentential Exploitation of Translation Memories. In Proc. of MT Summit VIII, pages 335-339."},{"authors":[{"first":"Dan","last":"Tufis"},{"first":"Ana","middle":"Maria","last":"Barbu"}],"year":"2002","title":"Lexical Token Alignment: Experiments, Results and Application","source":"Dan Tufis and Ana Maria Barbu. 2002. Lexical Token Alignment: Experiments, Results and Application. In Proc. of the Third International Conference on Language Resources and Evaluation, pages 458-465."},{"authors":[{"first":"Dekai","last":"Wu"}],"year":"1997","title":"Stochastic Inversion Transduction Grammars and Bilingual Parsing of Parallel Corpora","source":"Dekai Wu. 1997. Stochastic Inversion Transduction Grammars and Bilingual Parsing of Parallel Corpora. Computational Linguistics, 23(3): 377-403."}],"cites":[{"style":0,"text":"Brown et al., 1993","origin":{"pointer":"/sections/2/paragraphs/0","offset":113,"length":18},"authors":[{"last":"Brown"},{"last":"al."}],"year":"1993","references":["/references/3"]},{"style":0,"text":"Wu, 1997","origin":{"pointer":"/sections/2/paragraphs/0","offset":240,"length":8},"authors":[{"last":"Wu"}],"year":"1997","references":["/references/12"]},{"style":0,"text":"Och and Ney, 2000","origin":{"pointer":"/sections/2/paragraphs/0","offset":250,"length":17},"authors":[{"last":"Och"},{"last":"Ney"}],"year":"2000","references":["/references/8"]},{"style":0,"text":"Cherry and Lin, 2003","origin":{"pointer":"/sections/2/paragraphs/0","offset":269,"length":20},"authors":[{"last":"Cherry"},{"last":"Lin"}],"year":"2003","references":["/references/4"]},{"style":0,"text":"Ahrenberg et al., 1998","origin":{"pointer":"/sections/2/paragraphs/0","offset":375,"length":22},"authors":[{"last":"Ahrenberg"},{"last":"al."}],"year":"1998","references":["/references/0"]},{"style":0,"text":"Tufis and Barbu, 2002","origin":{"pointer":"/sections/2/paragraphs/0","offset":399,"length":21},"authors":[{"last":"Tufis"},{"last":"Barbu"}],"year":"2002","references":["/references/11"]},{"style":0,"text":"Ker and Chang, 1997","origin":{"pointer":"/sections/2/paragraphs/0","offset":632,"length":19},"authors":[{"last":"Ker"},{"last":"Chang"}],"year":"1997","references":["/references/7"]},{"style":0,"text":"Simard and Langlais, 2001","origin":{"pointer":"/sections/2/paragraphs/3","offset":140,"length":25},"authors":[{"last":"Simard"},{"last":"Langlais"}],"year":"2001","references":["/references/10"]},{"style":0,"text":"Brown et al., 1993","origin":{"pointer":"/sections/3/paragraphs/0","offset":35,"length":18},"authors":[{"last":"Brown"},{"last":"al."}],"year":"1993","references":["/references/3"]},{"style":0,"text":"Och and Ney, 2000","origin":{"pointer":"/sections/3/paragraphs/0","offset":319,"length":17},"authors":[{"last":"Och"},{"last":"Ney"}],"year":"2000","references":["/references/8"]},{"style":0,"text":"Och and Ney, 2000","origin":{"pointer":"/sections/4/paragraphs/0","offset":389,"length":17},"authors":[{"last":"Och"},{"last":"Ney"}],"year":"2000","references":["/references/8"]},{"style":0,"text":"Dunning (1993)","origin":{"pointer":"/sections/4/paragraphs/4","offset":371,"length":14},"authors":[{"last":"Dunning"}],"year":"1993","references":["/references/6"]},{"style":0,"text":"Och and Ney, 2000","origin":{"pointer":"/sections/6/paragraphs/1","offset":409,"length":17},"authors":[{"last":"Och"},{"last":"Ney"}],"year":"2000","references":["/references/8"]},{"style":0,"text":"Och and Ney, 2000","origin":{"pointer":"/sections/6/paragraphs/2","offset":271,"length":17},"authors":[{"last":"Och"},{"last":"Ney"}],"year":"2000","references":["/references/8"]}]}
