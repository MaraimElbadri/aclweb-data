{"sections":[{"title":"Compiling Boostexter Rules into a Finite-state Transducer Srinivas Bangalore AT&T LabsResearch 180 Park Avenue Florham Park, NJ 07932 Abstract","paragraphs":["A number of NLP tasks have been effectively modeled as classication tasks using a variety of classication techniques. Most of these tasks have been pursued in isolation with the classier assuming unambiguous input. In order for these techniques to be more broadly applicable, they need to be extended to apply on weighted packed representations of ambiguous input. One approach for achieving this is to represent the classication model as a weighted nite-state transducer (WFST). In this paper, we present a compilation procedure to convert the rules resulting from an AdaBoost classier into an WFST. We validate the compilation technique by applying the resulting WFST on a call-routing application."]},{"title":"1 Introduction","paragraphs":["Many problems in Natural Language Processing (NLP) can be modeled as classication tasks either at the word or at the sentence level. For example, part-of-speech tagging, named-entity identication supertagging1",", word sense disambiguation are tasks that have been modeled as classication problems at the word level. In addition, there are problems that classify the entire sentence or document into one of a set of categories. These problems are loosely characterized as semantic classication and have been used in many practical applications including call routing and text classication.","Most of these problems have been addressed in isolation assuming unambiguous (one-best) input. Typically, however, in NLP applications these modules are chained together with each module introducing some amount of error. In order to alleviate the errors introduced by a module, it is typical for a module to provide multiple weighted solutions (ideally as a packed representation) that serve as input to the next module. For example, a speech recognizer provides a lattice of possible recognition outputs that is to be annotated with part-of-speech and 1 associating each word with a label that represents the syn-","tactic information of the word given the context of the sentence. named-entities. Thus classication approaches need to be extended to be applicable on weighted packed representations of ambiguous input represented as a weighted lattice. The research direction we adopt here is to compile the model of a classier into a weighted nite-state transducer (WFST) so that it can compose with the input lattice.","Finite state models have been extensively applied to many aspects of language processing including, speech recognition (Pereira and Riley, 1997), phonology (Kaplan and Kay, 1994), morphology (Koskenniemi, 1984), chunking (Abney, 1991; Bangalore and Joshi, 1999), parsing (Roche, 1999; Oazer, 1999) and machine translation (Vilar et al., 1999; Bangalore and Riccardi, 2000). Finite-state models are attractive mechanisms for language processing since they (a) provide an efcient data structure for representing weighted ambiguous hypotheses (b) generally effective for decoding (c) associated with a calculus for composing models which allows for straightforward integration of constraints from various levels of speech and language processing.2","In this paper, we describe the compilation process for a particular classier model into an WFST and validate the accuracy of the compilation process on a one-best input in a call-routing task. We view this as a rst step toward using a classication model on a lattice input. The outline of the paper is as follows. In Section 2, we review the classication approach to resolving ambiguity in NLP tasks and in Section 3 we discuss the boosting approach to classication. In Section 4 we describe the compilation of the boosting model into an WFST and validate the result of this compilation using a call-routing task."]},{"title":"2 Resolving Ambiguity by Classication","paragraphs":["In general, we can characterize all these tagging problems as search problems formulated as shown 2 Furthermore, software implementing the nite-state calcu-","lus is available for research purposes. in Equation (1). We notate ","to be the input vocab-","ulary,","to be the vocabulary of","tags, an","word","input sequence as (  ) and tag sequence as ( ","). We are interested in",", the most likely tag","sequence out of the possible tag sequences (",") that","can be associated to","."," "," "," (1)","Following the techniques of Hidden Markov Models (HMM) applied to speech recognition, these tagging problems have been previously modeled in-directly through the transformation of the Bayes rule as in Equation 2. The problem is then approximated for sequence classication by a k","-order Markov model as shown in Equation (3).","  "," ","  "," (2) "," ","   "," ","  ","",""," ","","","","(3)","Although the HMM approach to tagging can easily be represented as a WFST, it has a drawback in that the use of large contexts and richer features results in sparseness leading to unreliable estimation of the parameters of the model.","An alternate approach to arriving at","is to model Equation 1 directly. There are many examples in recent literature (Breiman et al., 1984; Freund and Schapire, 1996; Roth, 1998; Lafferty et al., 2001; McCallum et al., 2000) which take this approach and are well equipped to handle large number of features. The general framework for these approaches is to learn a model from pairs of associations of the form (","",") where","","is a feature representation of","and","(",") is one of the members of the tag set. Although these approaches have been more effective than HMMs, there have not been many attempts to represent these models as a WFST, with the exception of the work on compiling decision trees (Sproat and Riley, 1996). In this paper, we consider the boosting (Freund and Schapire, 1996) approach (which outperforms decision trees) to Equation 1 and present a technique for compiling the classier model into a WFST."]},{"title":"3 Boostexter","paragraphs":["Boostexter is a machine learning tool which is based on the boosting family of algorithms rst proposed in (Freund and Schapire, 1996). The basic idea of boosting is to build a highly accurate classier by combining many weak or simple base learner, each one of which may only be moderately accurate. A weak learner or a rule","is a triple","","  ","","  , which tests a predicate (  ) of the input (",") and assigns a","weight","","(",""," ",") for each member (",") of","if ","is true in and assigns a weight (  ",") otherwise. It","is assumed that a pool of such weak learners","  ","can be constructed easily. From the pool of weak learners, the selection","the weak learner to be combined is performed it-","eratively. At each iteration",", a weak learner  is selected that minimizes a prediction error loss function on the training corpus which takes into account the weight  ","assigned to each training example. Intuitively, the weights encode how important it is that ","correctly classies each training example. Generally, the examples that were most often misclassied by the preceding base classiers will be given the most weight so as to force the base learner to focus on the hardest examples. As described in (Schapire and Singer, 1999), Boostexter uses condence rated classiers  that output a real number  ","  ","whose sign (-1 or +1) is inter-","preted as a prediction, and whose magnitude","     is a measure of condence. The iterative algorithm for combining weak learners stops after a prespecied number of iterations or when the training set accuracy saturates. 3.1 Weak Learners In the case of text classication applications, the set of possible weak learners is instantiated from simple","-grams of the input text (","). Thus, if","is a function to produce all","-grams up to","of its argument, then the set of predicates for the weak learners is ","  ","",". For word-level classication problems, which take into account the left and right context, we extend the set of weak learners created from the word features with those created from the left and right context features. Thus features of the left context (   ), features of the right context (   ) and the features of the word itself (  ",") constitute","the features at position . The predicates for the pool","of weak learners are created from these set of fea-","tures and are typically -grams on the feature repre-","sentations. Thus the set of predicates resulting from","the word level features is","","","     , from left context features is ","       and from right context features is ","  ","   ","",". The set","of predicates for the weak learners for word level","classication problems is:",""," ",""," ","   .","3.2 Decoding","The result of training is a set of selected rules","",""," ","","","","(","","). The output of the nal","classier is ","  ","  ","   ","  ",", i.e. the sum of condence of all classiers  . The real-valued","predictions of the nal classier can be converted into probabilities by a logistic function transform; that is"," ","","  ","","  ","","    (4) Thus the most likely tag sequence","is deter-","mined as in Equation 5, where                 is computed using Equation 4."," ","                  "," (5)","To date, decoding using the boosted rule sets is restricted to cases where the test input is unambiguous such as strings or words (not word graphs). By compiling these rule sets into WFSTs, we intend to extend their applicability to packed representations of ambiguous input such as word graphs."]},{"title":"4 Compilation","paragraphs":["We note that the weak learners selected at the end of the training process can be partitioned into one of three types based on the features that the learners test."," ",": test features of the word","   : test features of the left context","   : test features of the right context","We use the representation of context-dependent rewrite rules (Johnson, 1972; Kaplan and Kay, 1994) and their weighted version (Mohri and Sproat, 1996) to represent these weak learners. The (weighted) context-dependent rewrite rules have the general form"," ","(6)","where",",",",","and","are regular expressions on the","alphabet of the rules. The interpretation of these","rules are as follows: Rewrite","by","when it is","preceded by","and followed by",". Furthermore,","can be extended to a rational power series which","are weighted regular expressions where the weights","encode preferences over the paths in","(Mohri and","Sproat, 1996).","Each weak learner can then be viewed as a set","of weighted rewrite rules mapping the input word","into each member","(",") with a weight","","when","the predicate of the weak learner is true and with","weight  ","when the predicate of the weak learner is false. The translation between the three types of weak learners and the weighted context-dependency rules is shown in Table 13",".","We note that these rules apply left to right on an input and do not repeatedly apply at the same point in an input since the output vocabulary","would typically be disjoint from the input vocabulary ",".","We use the technique described in (Mohri and Sproat, 1996) to compile each weighted context-dependency rules into an WFST. The compilation is accomplished by the introduction of context symbols which are used as markers to identify locations for rewrites of","with",". After the rewrites, the markers are deleted. The compilation process is represented as a composition of ve transducers.","The WFSTs resulting from the compilation of each selected weak learner (","",") are unioned to create the WFST to be used for decoding. The weights of paths with the same input and output labels are added during the union operation.","","   ","(7)","We note that the due to the difference in the nature of the learning algorithm, compiling decision trees results in a composition of WFSTs representing the rules on the path from the root to a leaf node (Sproat and Riley, 1996), while compiling boosted rules results in a union of WFSTs, which is expected to result in smaller transducers.","In order to apply the WFST for decoding, we simply compose the model with the input represented as an WFST (  ) and search for the best path (if we are","interested in the single best classication result).  ","         "," (8)","We have compiled the rules resulting from boostexter trained on transcriptions of speech utterances from a call routing task with a vocabulary ( ","",") of 2912 and 40 classes (","","). There were a to-tal of 1800 rules comprising of 900 positive rules and their negative counterparts. The WFST resulting from compiling these rules has a 14372 states and 5.7 million arcs. The accuracy of the WFST on a random set of 7013 sentences was the same (85% accuracy) as the accuracy with the decoder that accompanies the boostexter program. This validates the compilation procedure."]},{"title":"5 Conclusions","paragraphs":["Classication techniques have been used to effectively resolve ambiguity in many natural language","3","For ease of exposition, we show the positive and negative sides of a rule each resulting in a context dependency rule. However, we can represent them in the form of a single context dependency rule which is ommitted here due to space constraints. Type of Weak Learner Weak Learner Weighted Context Dependency Rule ",": if WORD==  then ","","","","   ","","","","","","","  else","","","   ","","   ","      : if LeftContext==  then ","","","","","","","   ","","","","","","","","","","  else","","","",""," ","   ","","","   ","","  ","    : if RightContext==  then ","","","","   ","","","","","","","  else","","","   ","","   ","     Table 1: Translation of the three types of weak learners into weighted context-dependency rules. processing tasks. However, most of these tasks have been solved in isolation and hence assume an unambiguous input. In this paper, we extend the utility of the classication based techniques so as to be applicable on packed representations such as word graphs. We do this by compiling the rules resulting from an AdaBoost classier into a nite-state transducer. The resulting nite-state transducer can then be used as one part of a nite-state decoding chain."]},{"title":"References","paragraphs":["S. Abney. 1991. Parsing by chunks. In Robert Berwick, Steven Abney, and Carol Tenny, editors, Principle-based parsing. Kluwer Academic Publishers.","S. Bangalore and A. K. Joshi. 1999. Supertagging: An approach to almost parsing. Computational Linguistics, 25(2).","S. Bangalore and G. Riccardi. 2000. Stochastic nite-state models for spoken language machine translation. In Proceedings of the Workshop on Embedded Machine Translation Systems.","L. Breiman, J.H. Friedman, R.A. Olshen, and C.J. Stone. 1984. Classication and Regression Trees. Wadsworth & Brooks, Pacic Grove, CA.","Y. Freund and R. E. Schapire. 1996. Experiments with a new boosting alogrithm. In Machine Learning: Proceedings of the Thirteenth International Conference, pages 148156.","C.D. Johnson. 1972. Formal Aspects of Phonological Description. Mouton, The Hague.","R. M. Kaplan and M. Kay. 1994. Regular models of phonological rule systems. Computational Linguistics, 20(3):331378.","K. K. Koskenniemi. 1984. Two-level morphology: a general computation model for word-form recognition and production. Ph.D. thesis, University of Helsinki.","J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional random elds: Probabilistic models for segmenting and labeling sequence data. In In Proceedings of ICML, San Francisco, CA.","A. McCallum, D. Freitag, and F. Pereira. 2000. Maximum entropy markov models for information extraction and segmentation. In In Proceedings of ICML, Stanford, CA.","M. Mohri and R. Sproat. 1996. An efcient compiler for weighted rewrite rules. In Proceedings of ACL, pages 231238.","K. Oazer. 1999. Dependency parsing with an extended nite state approach. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, Maryland, USA, June.","F.C.N. Pereira and M.D. Riley. 1997. Speech recognition by composition of weighted nite automata. In E. Roche and Schabes Y., editors, Finite State Devices for Natural Language Processing, pages 431456. MIT Press, Cambridge, Massachusetts.","E. Roche. 1999. Finite state transducers: parsing free and frozen sentences. In Andr·as Kornai, editor, Extended Finite State Models of Language. Cambridge University Press.","D. Roth. 1998. Learning to resolve natural language ambiguities: A unied approach. In Proceedings of AAAI.","R.E. Schapire and Y. Singer. 1999. Improved boosting algorithms using condence-rated predictions. Machine Learning, 37(3):297336, December.","R. Sproat and M. Riley. 1996. Compilation of weighted nite-state transducers from decision trees. In Proceedings of ACL, pages 215222.","J. Vilar, V.M. Jim·enez, J. Amengual, A. Castellanos, D. Llorens, and E. Vidal. 1999. Text and speech translation by means of subsequential transducers. In Andr·as Kornai, editor, Extened Finite State Models of Language. Cambridge University Press."]}],"references":[{"authors":[{"first":"S.","last":"Abney"}],"year":"1991","title":"Parsing by chunks","source":"S. Abney. 1991. Parsing by chunks. In Robert Berwick, Steven Abney, and Carol Tenny, editors, Principle-based parsing. Kluwer Academic Publishers."},{"authors":[{"first":"S.","last":"Bangalore"},{"first":"A.","middle":"K.","last":"Joshi"}],"year":"1999","title":"Supertagging: An approach to almost parsing","source":"S. Bangalore and A. K. Joshi. 1999. Supertagging: An approach to almost parsing. Computational Linguistics, 25(2)."},{"authors":[{"first":"S.","last":"Bangalore"},{"first":"G.","last":"Riccardi"}],"year":"2000","title":"Stochastic nite-state models for spoken language machine translation","source":"S. Bangalore and G. Riccardi. 2000. Stochastic nite-state models for spoken language machine translation. In Proceedings of the Workshop on Embedded Machine Translation Systems."},{"authors":[{"first":"L.","last":"Breiman"},{"first":"J.","middle":"H.","last":"Friedman"},{"first":"R.","middle":"A.","last":"Olshen"},{"first":"C.","middle":"J.","last":"Stone"}],"year":"1984","title":"Classication and Regression Trees","source":"L. Breiman, J.H. Friedman, R.A. Olshen, and C.J. Stone. 1984. Classication and Regression Trees. Wadsworth & Brooks, Pacic Grove, CA."},{"authors":[{"first":"Y.","last":"Freund"},{"first":"R.","middle":"E.","last":"Schapire"}],"year":"1996","title":"Experiments with a new boosting alogrithm","source":"Y. Freund and R. E. Schapire. 1996. Experiments with a new boosting alogrithm. In Machine Learning: Proceedings of the Thirteenth International Conference, pages 148156."},{"authors":[{"first":"C.","middle":"D.","last":"Johnson"}],"year":"1972","title":"Formal Aspects of Phonological Description","source":"C.D. Johnson. 1972. Formal Aspects of Phonological Description. Mouton, The Hague."},{"authors":[{"first":"R.","middle":"M.","last":"Kaplan"},{"first":"M.","last":"Kay"}],"year":"1994","title":"Regular models of phonological rule systems","source":"R. M. Kaplan and M. Kay. 1994. Regular models of phonological rule systems. Computational Linguistics, 20(3):331378."},{"authors":[{"first":"K.","middle":"K.","last":"Koskenniemi"}],"year":"1984","title":"Two-level morphology: a general computation model for word-form recognition and production","source":"K. K. Koskenniemi. 1984. Two-level morphology: a general computation model for word-form recognition and production. Ph.D. thesis, University of Helsinki."},{"authors":[{"first":"J.","last":"Lafferty"},{"first":"A.","last":"McCallum"},{"first":"F.","last":"Pereira"}],"year":"2001","title":"Conditional random elds: Probabilistic models for segmenting and labeling sequence data","source":"J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional random elds: Probabilistic models for segmenting and labeling sequence data. In In Proceedings of ICML, San Francisco, CA."},{"authors":[{"first":"A.","last":"McCallum"},{"first":"D.","last":"Freitag"},{"first":"F.","last":"Pereira"}],"year":"2000","title":"Maximum entropy markov models for information extraction and segmentation","source":"A. McCallum, D. Freitag, and F. Pereira. 2000. Maximum entropy markov models for information extraction and segmentation. In In Proceedings of ICML, Stanford, CA."},{"authors":[{"first":"M.","last":"Mohri"},{"first":"R.","last":"Sproat"}],"year":"1996","title":"An efcient compiler for weighted rewrite rules","source":"M. Mohri and R. Sproat. 1996. An efcient compiler for weighted rewrite rules. In Proceedings of ACL, pages 231238."},{"authors":[{"first":"K.","last":"Oazer"}],"year":"1999","title":"Dependency parsing with an extended nite state approach","source":"K. Oazer. 1999. Dependency parsing with an extended nite state approach. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, Maryland, USA, June."},{"authors":[{"first":"F.","middle":"C. N.","last":"Pereira"},{"first":"M.","middle":"D.","last":"Riley"}],"year":"1997","title":"Speech recognition by composition of weighted nite automata","source":"F.C.N. Pereira and M.D. Riley. 1997. Speech recognition by composition of weighted nite automata. In E. Roche and Schabes Y., editors, Finite State Devices for Natural Language Processing, pages 431456. MIT Press, Cambridge, Massachusetts."},{"authors":[{"first":"E.","last":"Roche"}],"year":"1999","title":"Finite state transducers: parsing free and frozen sentences","source":"E. Roche. 1999. Finite state transducers: parsing free and frozen sentences. In Andr·as Kornai, editor, Extended Finite State Models of Language. Cambridge University Press."},{"authors":[{"first":"D.","last":"Roth"}],"year":"1998","title":"Learning to resolve natural language ambiguities: A unied approach","source":"D. Roth. 1998. Learning to resolve natural language ambiguities: A unied approach. In Proceedings of AAAI."},{"authors":[{"first":"R.","middle":"E.","last":"Schapire"},{"first":"Y.","last":"Singer"}],"year":"1999","title":"Improved boosting algorithms using condence-rated predictions","source":"R.E. Schapire and Y. Singer. 1999. Improved boosting algorithms using condence-rated predictions. Machine Learning, 37(3):297336, December."},{"authors":[{"first":"R.","last":"Sproat"},{"first":"M.","last":"Riley"}],"year":"1996","title":"Compilation of weighted nite-state transducers from decision trees","source":"R. Sproat and M. Riley. 1996. Compilation of weighted nite-state transducers from decision trees. In Proceedings of ACL, pages 215222."},{"authors":[{"first":"J.","last":"Vilar"},{"first":"V.","middle":"M.","last":"Jim·enez"},{"first":"J.","last":"Amengual"},{"first":"A.","last":"Castellanos"},{"first":"D.","last":"Llorens"},{"first":"E.","last":"Vidal"}],"year":"1999","title":"Text and speech translation by means of subsequential transducers","source":"J. Vilar, V.M. Jim·enez, J. Amengual, A. Castellanos, D. Llorens, and E. Vidal. 1999. Text and speech translation by means of subsequential transducers. In Andr·as Kornai, editor, Extened Finite State Models of Language. Cambridge University Press."}],"cites":[{"style":0,"text":"Pereira and Riley, 1997","origin":{"pointer":"/sections/1/paragraphs/4","offset":120,"length":23},"authors":[{"last":"Pereira"},{"last":"Riley"}],"year":"1997","references":["/references/12"]},{"style":0,"text":"Kaplan and Kay, 1994","origin":{"pointer":"/sections/1/paragraphs/4","offset":157,"length":20},"authors":[{"last":"Kaplan"},{"last":"Kay"}],"year":"1994","references":["/references/6"]},{"style":0,"text":"Koskenniemi, 1984","origin":{"pointer":"/sections/1/paragraphs/4","offset":192,"length":17},"authors":[{"last":"Koskenniemi"}],"year":"1984","references":["/references/7"]},{"style":0,"text":"Abney, 1991","origin":{"pointer":"/sections/1/paragraphs/4","offset":222,"length":11},"authors":[{"last":"Abney"}],"year":"1991","references":["/references/0"]},{"style":0,"text":"Bangalore and Joshi, 1999","origin":{"pointer":"/sections/1/paragraphs/4","offset":235,"length":25},"authors":[{"last":"Bangalore"},{"last":"Joshi"}],"year":"1999","references":["/references/1"]},{"style":0,"text":"Roche, 1999","origin":{"pointer":"/sections/1/paragraphs/4","offset":272,"length":11},"authors":[{"last":"Roche"}],"year":"1999","references":["/references/13"]},{"style":0,"text":"Oazer, 1999","origin":{"pointer":"/sections/1/paragraphs/4","offset":285,"length":11},"authors":[{"last":"Oazer"}],"year":"1999","references":["/references/11"]},{"style":0,"text":"Vilar et al., 1999","origin":{"pointer":"/sections/1/paragraphs/4","offset":323,"length":18},"authors":[{"last":"Vilar"},{"last":"al."}],"year":"1999","references":["/references/17"]},{"style":0,"text":"Bangalore and Riccardi, 2000","origin":{"pointer":"/sections/1/paragraphs/4","offset":343,"length":28},"authors":[{"last":"Bangalore"},{"last":"Riccardi"}],"year":"2000","references":["/references/2"]},{"style":0,"text":"Breiman et al., 1984","origin":{"pointer":"/sections/2/paragraphs/36","offset":79,"length":20},"authors":[{"last":"Breiman"},{"last":"al."}],"year":"1984","references":["/references/3"]},{"style":0,"text":"Freund and Schapire, 1996","origin":{"pointer":"/sections/2/paragraphs/36","offset":101,"length":25},"authors":[{"last":"Freund"},{"last":"Schapire"}],"year":"1996","references":["/references/4"]},{"style":0,"text":"Roth, 1998","origin":{"pointer":"/sections/2/paragraphs/36","offset":128,"length":10},"authors":[{"last":"Roth"}],"year":"1998","references":["/references/14"]},{"style":0,"text":"Lafferty et al., 2001","origin":{"pointer":"/sections/2/paragraphs/36","offset":140,"length":21},"authors":[{"last":"Lafferty"},{"last":"al."}],"year":"2001","references":["/references/8"]},{"style":0,"text":"McCallum et al., 2000","origin":{"pointer":"/sections/2/paragraphs/36","offset":163,"length":21},"authors":[{"last":"McCallum"},{"last":"al."}],"year":"2000","references":["/references/9"]},{"style":0,"text":"Sproat and Riley, 1996","origin":{"pointer":"/sections/2/paragraphs/43","offset":233,"length":22},"authors":[{"last":"Sproat"},{"last":"Riley"}],"year":"1996","references":["/references/16"]},{"style":0,"text":"Freund and Schapire, 1996","origin":{"pointer":"/sections/2/paragraphs/43","offset":299,"length":25},"authors":[{"last":"Freund"},{"last":"Schapire"}],"year":"1996","references":["/references/4"]},{"style":0,"text":"Freund and Schapire, 1996","origin":{"pointer":"/sections/3/paragraphs/0","offset":107,"length":25},"authors":[{"last":"Freund"},{"last":"Schapire"}],"year":"1996","references":["/references/4"]},{"style":0,"text":"Schapire and Singer, 1999","origin":{"pointer":"/sections/3/paragraphs/24","offset":244,"length":25},"authors":[{"last":"Schapire"},{"last":"Singer"}],"year":"1999","references":["/references/15"]},{"style":0,"text":"Johnson, 1972","origin":{"pointer":"/sections/4/paragraphs/5","offset":62,"length":13},"authors":[{"last":"Johnson"}],"year":"1972","references":["/references/5"]},{"style":0,"text":"Kaplan and Kay, 1994","origin":{"pointer":"/sections/4/paragraphs/5","offset":77,"length":20},"authors":[{"last":"Kaplan"},{"last":"Kay"}],"year":"1994","references":["/references/6"]},{"style":0,"text":"Mohri and Sproat, 1996","origin":{"pointer":"/sections/4/paragraphs/5","offset":127,"length":22},"authors":[{"last":"Mohri"},{"last":"Sproat"}],"year":"1996","references":["/references/10"]},{"style":0,"text":"Sproat, 1996","origin":{"pointer":"/sections/4/paragraphs/24","offset":0,"length":12},"authors":[{"last":"Sproat"}],"year":"1996","references":[]},{"style":0,"text":"Mohri and Sproat, 1996","origin":{"pointer":"/sections/4/paragraphs/39","offset":35,"length":22},"authors":[{"last":"Mohri"},{"last":"Sproat"}],"year":"1996","references":["/references/10"]},{"style":0,"text":"Sproat and Riley, 1996","origin":{"pointer":"/sections/4/paragraphs/48","offset":204,"length":22},"authors":[{"last":"Sproat"},{"last":"Riley"}],"year":"1996","references":["/references/16"]}]}
