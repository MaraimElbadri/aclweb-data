{"sections":[{"title":"Dual Decomposition for Natural Language Processing","paragraphs":["Alexander M. Rush and Michael Collins"]},{"title":"Decoding complexity focus:","paragraphs":["decoding problem for natural language tasks y ∗ = arg maxy f (y ) motivation: • richer model structure often leads to improved accuracy • exact decoding for complex models tends to be intractable"]},{"title":"Decoding tasks","paragraphs":["many common problems are intractable to decode exactly","high complexity • combined parsing and part-of-speech tagging (Rush et al.,","2010) • “loopy” HMM part-of-speech tagging • syntactic machine translation (Rush and Collins, 2011)","NP-Hard • symmetric HMM alignment (DeNero and Macherey, 2011) • phrase-based translation • higher-order non-projective dependency parsing (Koo et al.,","2010)","in practice: • approximate decoding methods (coarse-to-fine, beam search,","cube pruning, gibbs sampling, belief propagation) • approximate models (mean field, variational models)"]},{"title":"Motivation","paragraphs":["cannot hope to find exact algorithms (particularly when NP-Hard) aim: develop decoding algorithms with formal guarantees method: • derive fast algorithms that provide certificates of optimality","• show that for practical instances, these algorithms often yield exact solutions","• provide strategies for improving solutions or finding approximate solutions when no certificate is found dual decomposition helps us develop algorithms of this form"]},{"title":"Dual Decomposition","paragraphs":["(Komodakis et al., 2010; Lemaréchal, 2001) goal: solve complicated optimization problem y ∗ = arg maxy f (y ) method: decompose into subproblems, solve iteratively benefit: can choose decomposition to provide “easy” subproblems aim for simple and efficient combinatorial algorithms • dynamic programming • minimum spanning tree • shortest path • min-cut • bipartite match • etc."]},{"title":"Related work","paragraphs":["there are related methods used NLP with similar motivation related methods:","• belief propagation (particularly max-product) (Smith and Eisner, 2008) • factored A* search (Klein and Manning, 2003) • exact coarse-to-fine (Raphael, 2001) aim to find exact solutions without exploring the full search space"]},{"title":"Tutorial outline focus:","paragraphs":["• developing dual decomposition algorithms for new NLP tasks • understanding formal guarantees of the algorithms • extensions to improve exactness and select solutions outline: 1. worked algorithm for combined parsing and tagging 2. important theorems and formal derivation 3. more examples from parsing, sequence labeling, MT 4. practical considerations for implementing dual decomposition 5. relationship to linear programming relaxations 6. further variations and advanced examples"]},{"title":"1. Worked example aim:","paragraphs":["walk through a dual decomposition algorithm for combined parsing and part-of-speech tagging • introduce formal notation for parsing and tagging • give assumptions necessary for decoding • step through a run of the dual decomposition algorithm"]},{"title":"Combined parsing and part-of-speech tagging","paragraphs":["S NP N United VP V flies NP D some A large N jet goal: find parse tree that optimizes score (S → NP VP) + score (VP → V NP) + ... + score (United1, N) + score (V, N) + ..."]},{"title":"Constituency parsing notation:","paragraphs":["• Y is set of constituency parses for input • y ∈ Y is a valid parse • f (y ) scores a parse tree goal: arg maxy ∈Y f (y ) example: a context-free grammar for constituency parsing S NP N United VP V flies NP D some A large N jet"]},{"title":"Part-of-speech tagging notation:","paragraphs":["• Z is set of tag sequences for input • z ∈ Z is a valid tag sequence • g (z ) scores of a tag sequence goal: arg maxz∈Z g (z ) example: an HMM for part-of speech tagging United1 flies2 some3 large4 jet5 N V D A N"]},{"title":"Identifying tags notation:","paragraphs":["identify the tag labels selected by each model • y (i , t ) = 1 when parse y selects tag t at position i • z (i , t ) = 1 when tag sequence z selects tag t at position i example: a parse and tagging with y (4, A) = 1 and z (4, A) = 1 S NP N United VP V flies NP D some A large N jet y United1 flies2 some3 large4 jet5 N V D A N z"]},{"title":"Combined optimization goal:","paragraphs":["arg maxy ∈Y,z∈Z f (y ) + g (z ) such that for all i = 1 . . . n, t ∈ T , y (i , t ) = z (i , t ) i.e. find the best parse and tagging pair that agree on tag labels equivalent formulation: arg maxy ∈Y f (y ) + g (l (y )) where l : Y → Z extracts the tag sequence from a parse tree"]},{"title":"Dynamic programming intersection","paragraphs":["can solve by solving the product of the two models example: • parsing model is a context-free grammar • tagging model is a first-order HMM • can solve as CFG and finite-state automata intersection replace S → NP VP with SN,N → NPN,V VP V ,N S NP N United VP V flies NP D some A large N jet"]},{"title":"Parsing assumption","paragraphs":["the structure of Y is open (could be CFG, TAG, etc.) assumption: optimization with u can be solved efficiently arg maxy ∈Y f (y ) + ∑ i ,t u(i , t )y (i , t ) generally benign since u can be incorporated into the structure of f example: CFG with rule scoring function h f (y ) = ∑ X →Y Z ∈y h(X → Y Z ) + ∑ (i ,X )∈y h(X → wi ) where arg maxy ∈Y f (y ) + ∑ i ,t u(i , t )y (i , t ) = arg maxy ∈Y ∑ X →Y Z ∈y h(X → Y Z ) + ∑ (i ,X )∈y (h(X → wi ) + u(i , X ))"]},{"title":"Tagging assumption","paragraphs":["we make a similar assumption for the set Z assumption: optimization with u can be solved efficiently arg maxz∈Z g (z ) − ∑ i ,t u(i , t )z (i , t ) example: HMM with scores for transitions T and observations O g (z ) = ∑ t→t′∈z T (t → t ′ ) + ∑ (i ,t)∈z O (t → wi ) where arg maxz∈Z g (z ) − ∑ i ,t u(i , t )z (i , t ) = arg maxz∈Z ∑ t→t′∈z T (t → t ′ ) + ∑ (i ,t)∈z (O (t → wi ) − u(i , t ))"]},{"title":"Dual decomposition algorithm","paragraphs":["Set u (1) (i , t ) = 0 for all i , t ∈ T For k = 1 to K y (k) ← arg maxy ∈Y f (y ) + ∑ i ,t u (k) (i , t )y (i , t ) [Parsing] z (k) ← arg maxz∈Z g (z ) − ∑ i ,t u (k) (i , t )z (i , t ) [Tagging] If y (k) (i , t ) = z (k) (i , t ) for all i , t Return (y (k) , z (k) ) Else u (k+1) (i , t ) ← u (k) (i , t ) − αk (y (k) (i , t ) − z (k) (i , t ))"]},{"title":"Algorithm step-by-step","paragraphs":["[Animation]"]},{"title":"Main theorem theorem:","paragraphs":["if at any iteration, for all i , t ∈ T y (k) (i , t ) = z (k) (i , t ) then (y (k) , z (k) ) is the global optimum proof: focus of the next section"]},{"title":"2. Formal properties aim:","paragraphs":["formal derivation of the algorithm given in the previous section • derive Lagrangian dual • prove three properties ▶ upper bound ▶ convergence ▶ optimality • describe subgradient method"]},{"title":"Lagrangian goal:","paragraphs":["arg maxy ∈Y,z∈Z f (y ) + g (z ) such that y (i , t ) = z (i , t ) Lagrangian: L(u, y , z ) = f (y ) + g (z ) + ∑ i ,t u(i , t ) (y (i , t ) − z (i , t )) redistribute terms L(u, y , z ) =  f (y ) + ∑ i ,t u(i , t )y (i , t )   +  g (z ) − ∑ i ,t u(i , t )z (i , t )  "]},{"title":"Lagrangian dual Lagrangian:","paragraphs":["L(u, y , z ) =  f (y ) + ∑ i ,t u(i , t )y (i , t )   +  g (z ) − ∑ i ,t u(i , t )z (i , t )   Lagrangian dual: L(u) = maxy ∈Y,z∈Z L(u, y , z ) = maxy ∈Y  f (y ) + ∑ i ,t u(i , t )y (i , t )   + maxz∈Z  g (z ) − ∑ i ,t u(i , t )z (i , t )  "]},{"title":"Theorem 1. Upper bound define:","paragraphs":["• y ∗ , z ∗ is the optimal combined parsing and tagging solution with y ∗ (i , t ) = z ∗ (i , t ) for all i , t theorem: for any value of u L(u) ≥ f (y ∗ ) + g (z ∗ ) L(u) provides an upper bound on the score of the optimal solution note: upper bound may be useful as input to branch and bound or A* search"]},{"title":"Theorem 1. Upper bound (proof ) theorem:","paragraphs":["for any value of u, L(u) ≥ f (y ∗ ) + g (z ∗ ) proof: L(u) = maxy ∈Y,z∈Z L(u, y , z ) (1) ≥ maxy ∈Y,z∈Z:y =z L(u, y , z ) (2) = maxy ∈Y,z∈Z:y =z f (y ) + g (z ) (3) = f (y ∗ ) + g (z ∗ ) (4)"]},{"title":"Formal algorithm (reminder)","paragraphs":["Set u (1) (i , t ) = 0 for all i , t ∈ T For k = 1 to K y (k) ← arg maxy ∈Y f (y ) + ∑ i ,t u (k) (i , t )y (i , t ) [Parsing] z (k) ← arg maxz∈Z g (z ) − ∑ i ,t u (k) (i , t )z (i , t ) [Tagging] If y (k) (i , t ) = z (k) (i , t ) for all i , t Return (y (k) , z (k) ) Else u (k+1) (i , t ) ← u (k) (i , t ) − αk (y (k) (i , t ) − z (k) (i , t ))"]},{"title":"Theorem 2. Convergence notation:","paragraphs":["• u(k+1) (i , t ) ← u (k) (i , t ) + αk (y (k) (i , t ) − z (k) (i , t )) is update • u(k) is the penalty vector at iteration k • αk is the update rate at iteration k theorem: for any sequence α 1 , α 2 , α 3 , . . . such that limt→∞ α t = 0 and ∞∑ t=1 α t = ∞, we have limt→∞ L(u t ) = minu L(u) i.e. the algorithm converges to the tightest possible upper bound proof: by subgradient convergence (next section)"]},{"title":"Dual solutions define:","paragraphs":["• for any value of u yu = arg maxy ∈Y  f (y ) + ∑ i ,t u(i , t )y (i , t )   and zu = arg maxz∈Z  g (z ) − ∑ i ,t u(i , t )z (i , t )   • yu and zu are the dual solutions for a given u"]},{"title":"Theorem 3. Optimality theorem:","paragraphs":["if there exists u such that yu (i , t ) = zu (i , t ) for all i , t then f (yu ) + g (zu ) = f (y ∗ ) + g (z ∗ ) i.e. if the dual solutions agree, we have an optimal solution (yu , zu )"]},{"title":"Theorem 3. Optimality (proof ) theorem:","paragraphs":["if u such that yu (i , t ) = zu (i , t ) for all i , t then f (yu ) + g (zu ) = f (y ∗ ) + g (z ∗ ) proof: by the definitions of yu and zu L(u) = f (yu ) + g (zu ) + ∑ i ,t u(i , t )(yu (i , t ) − zu (i , t )) = f (yu ) + g (zu ) since L(u) ≥ f (y ∗ ) + g (z ∗ ) for all values of u f (yu ) + g (zu ) ≥ f (y ∗ ) + g (z ∗ ) but y ∗ and z ∗ are optimal f (yu ) + g (zu ) ≤ f (y ∗ ) + g (z ∗ )"]},{"title":"Dual optimization Lagrangian dual:","paragraphs":["L(u) = maxy ∈Y,z∈Z L(u, y , z ) = maxy ∈Y  f (y ) + ∑ i ,t u(i , t )y (i , t )   + maxz∈Z  g (z ) − ∑ i ,t u(i , t )z (i , t )   goal: dual problem is to find the tightest upper bound minu L(u)"]},{"title":"Dual subgradient","paragraphs":["L(u) = maxy ∈Y  f (y ) + ∑ i,t u(i , t )y (i , t )   + maxz∈Z  g (z ) − ∑ i,t u(i , t )z (i , t )  ","properties: • L(u) is convex in u (no local minima) • L(u) is not differentiable (because of max operator) handle non-differentiability by using subgradient descent define: a subgradient of L(u) at u is a vector gu such that for all v L(v ) ≥ L(u) + gu · (v − u)"]},{"title":"Subgradient algorithm","paragraphs":["L(u) = maxy ∈Y  f (y ) + ∑ i,t u(i , t )y (i , t )   + maxz∈Z  g (z ) − ∑ i,j u(i , t )z (i , t )   recall, yu and zu are the argmax’s of the two terms subgradient: gu (i , t ) = yu (i , t ) − zu (i , t ) subgradient descent: move along the subgradient u ′ (i , t ) = u(i , t ) − α (yu (i , t ) − zu (i , t )) guaranteed to find a minimum with conditions given earlier for α"]},{"title":"3. More examples aim:","paragraphs":["demonstrate similar algorithms that can be applied to other decoding applications • context-free parsing combined with dependency parsing • corpus-level part-of-speech tagging • combined translation alignment"]},{"title":"Combined constituency and dependency parsing setup:","paragraphs":["assume separate models trained for constituency and dependency parsing problem: find constituency parse that maximizes the sum of the two models example: • combine lexicalized CFG with second-order dependency parser"]},{"title":"Lexicalized constituency parsing notation:","paragraphs":["• Y is set of lexicalized constituency parses for input • y ∈ Y is a valid parse • f (y ) scores a parse tree goal: arg maxy ∈Y f (y ) example: a lexicalized context-free grammar S(flies) NP(United) N United VP(flies) V flies NP(jet) D some A large N jet"]},{"title":"Dependency parsing define:","paragraphs":["• Z is set of dependency parses for input • z ∈ Z is a valid dependency parse • g (z ) scores a dependency parse example: *0 United1 flies2 some3 large4 jet5"]},{"title":"Identifying dependencies notation:","paragraphs":["identify the dependencies selected by each model • y (i , j ) = 1 when constituency parse y selects word i as a modifier of word j • z (i , j ) = 1 when dependency parse z selects word i as a modifier of word j example: a constituency and dependency parse with y (3, 5) = 1 and z (3, 5) = 1 S(flies) NP(United) N United VP(flies) V flies NP(jet) D some A large N jet y *0 United1 flies2 some3 large4 jet5 z"]},{"title":"Combined optimization goal:","paragraphs":["arg maxy ∈Y,z∈Z f (y ) + g (z ) such that for all i = 1 . . . n, j = 0 . . . n, y (i , j ) = z (i , j )"]},{"title":"Algorithm step-by-step","paragraphs":["[Animation]"]},{"title":"Corpus-level tagging setup:","paragraphs":["given a corpus of sentences and a trained sentence-level tagging model problem: find best tagging for each sentence, while at the same time enforcing inter-sentence soft constraints example: • test-time decoding with a trigram tagger • constraint that each word type prefer a single POS tag"]},{"title":"Corpus-level tagging","paragraphs":["full model for corpus-level tagging He saw an American man The smart man stood outside Man is the best measure N"]},{"title":"Sentence-level decoding notation:","paragraphs":["• Yi is set of tag sequences for input sentence i • Y = Y1 × . . . × Ym is set of tag sequences for the input corpus • Y ∈ Y is a valid tag sequence for the corpus • F (Y ) = ∑ i f (Yi ) is the score for tagging the whole corpus goal: arg maxY ∈Y F (Y ) example: decode each sentence with a trigram tagger He P saw V an D American A man N The D smart A man N stood V outside R"]},{"title":"Inter-sentence constraints notation:","paragraphs":["• Z is set of possible assignments of tags to word types • z ∈ Z is a valid tag assignment • g (z ) is a scoring function for assignments to word types","(e.g. a hard constraint - all word types only have one tag) example: an MRF model that encourages words of the same type to choose the same tag z1 man N man N man N N z2 man N man N man A N g (z1) > g (z2)"]},{"title":"Identifying word tags notation:","paragraphs":["identify the tag labels selected by each model","• Ys (i , t ) = 1 when the tagger for sentence s at position i selects tag t","• z (s , i , t ) = 1 when the constraint assigns at sentence s position i the tag t example: a parse and tagging with Y1(5, N ) = 1 and z (1, 5, N ) = 1 He saw an American man The smart man stood outside Y man man man z"]},{"title":"Combined optimization goal:","paragraphs":["arg maxY ∈Y,z∈Z F (Y ) + g (z ) such that for all s = 1 . . . m, i = 1 . . . n, t ∈ T , Ys (i , t ) = z (s , i , t )"]},{"title":"Algorithm step-by-step","paragraphs":["[Animation]"]},{"title":"Combined alignment","paragraphs":["(DeNero and Macherey, 2011) setup: assume separate models trained for English-to-French and French-to-English alignment problem: find an alignment that maximizes the score of both models with soft agreement example:","• HMM models for both directional alignments (assume correct alignment is one-to-one for simplicity)"]},{"title":"English-to-French alignment define:","paragraphs":["• Y is set of all possible English-to-French alignments • y ∈ Y is a valid alignment • f (y ) scores of the alignment example: HMM alignment The1 ugly2 dog3 has4 red5 fur6 1 3 2 4 6 5"]},{"title":"French-to-English alignment define:","paragraphs":["• Z is set of all possible French-to-English alignments • z ∈ Z is a valid alignment • g (z ) scores of an alignment example: HMM alignment Le1 chien2 laid3 a4 fourrure5 rouge6 1 2 3 4 6 5"]},{"title":"Identifying word alignments notation:","paragraphs":["identify the tag labels selected by each model","• y (i , j ) = 1 when e-to-f alignment y selects French word i to align with English word j","• z (i , j ) = 1 when f-to-e alignment z selects French word i to align with English word j example: two HMM alignment models with y (6, 5) = 1 and z (6, 5) = 1 The1 ugly2 dog3 has4 red5 fur6 1 3 2 4 6 5 y Le1 chien2 laid3 a4 fourrure5 rouge6 1 2 3 4 6 5 z"]},{"title":"Combined optimization goal:","paragraphs":["arg maxy ∈Y,z∈Z f (y ) + g (z ) such that for all i = 1 . . . n, j = 1 . . . n, y (i , j ) = z (i , j )"]},{"title":"Algorithm step-by-step","paragraphs":["[Animation]"]},{"title":"4. Practical issues aim:","paragraphs":["overview of practical dual decomposition techniques • tracking the progress of the algorithm • extracting solutions if algorithm does not converge • lazy update of dual solutions"]},{"title":"Tracking progress","paragraphs":["at each stage of the algorithm there are several useful values","track: • y (k) , z (k) are current dual solutions • L(u(k) ) is the current dual value • y (k) , l (y (k) ) is a potential primal feasible solution • f (y (k) ) + g (l (y (k) )) is the potential primal value","useful signals: • L(u(k) ) − L(u (k−1) ) is the dual change (may be positive) • mink L(u (k) ) is the best dual value (tightest upper bound) • maxk f (y (k) ) + g (l (y (k) )) is the best primal value the optimal value must be between the best dual and primal values"]},{"title":"Approximate solution","paragraphs":["upon agreement the solution is exact, but this may not occur otherwise, there is an easy way to find an approximate solution choose: the structure y (k′) where k ′ = arg maxk f (y (k) ) + g (l (y (k) )) is the iteration with the best primal score guarantee: the solution y k′ is non-optimal by at most (mint L(u t )) − (f (y (k′) ) + g (l (y (k′) ))) there are other methods to estimate solutions, for instance by averaging solutions (see Nedić and Ozdaglar (2009))"]},{"title":"Lazy decoding idea:","paragraphs":["don’t recompute y (k) or z (k) from scratch each iteration lazy decoding: if subgradient u (k) is sparse, then y (k) may be very easy to compute from y (k−1) use: • very helpful if y or z factors naturally into several parts • decompositions with this property are very fast in practice example:","• in corpus-level tagging, only need to recompute sentences with a word type that received an update"]},{"title":"5. Linear programming aim:","paragraphs":["explore the connections between dual decomposition and linear programming • basic optimization over the simplex • formal properties of linear programming • full example with fractional optimal solutions • tightening linear program relaxations"]},{"title":"Simplex define:","paragraphs":["• ∆y is the simplex over Y where α ∈ ∆y implies αy ≥ 0 and ∑ y αy = 1 • ∆z is the simplex over Z • δy : Y → ∆y maps elements to the simplex example: Y = {y1, y2, y3} vertices • δy (y1) = (1, 0, 0) • δy (y2) = (0, 1, 0) • δy (y3) = (0, 0, 1) δy (y1) δy (y2) δy (y 3) ∆y"]},{"title":"Linear programming","paragraphs":["optimize over the simplices ∆y and ∆z instead of the discrete sets Y and Z goal: optimize linear program maxα∈∆y ,β∈∆z ∑ y αy f (y ) + ∑ z βz g (z ) such that for all i , t ∑ y αy y (i , t ) = ∑ z βz z (i , t )"]},{"title":"Lagrangian Lagrangian:","paragraphs":["M (u, α, β) = ∑ y αy f (y ) + ∑ z βz g (z ) + ∑ i,t u(i , t) ( ∑ y αy y (i , t) − ∑ z βz z (i , t) ) = ( ∑ y αy f (y ) + ∑ i,t u(i , t) ∑ y αy y (i , t) ) + ( ∑ z βz g (z ) − ∑ i,t u(i , t) ∑ z βz z (i , t) ) Lagrangian dual: M (u) = maxα∈∆y ,β∈∆z M (u, α, β)"]},{"title":"Strong duality define:","paragraphs":["• α∗ , β ∗ is the optimal assignment to α, β in the linear program theorem: minu M (u) = ∑ y α ∗ y f (y ) + ∑ z β ∗ z g (z ) proof: by linear programming duality"]},{"title":"Dual relationship theorem:","paragraphs":["for any value of u, M (u) = L(u) note: solving the original Lagrangian dual also solves dual of the linear program"]},{"title":"Primal relationship define:","paragraphs":["• Q ⊆ ∆y × ∆z corresponds to feasible solutions of the original problem Q = {(δy (y ), δz (z )): y ∈ Y , z ∈ Z , y (i , t ) = z (i , t ) for all (i , t )} • Q′ ⊆ ∆y × ∆z is the set of feasible solutions to the LP Q′","= {(α, β): α ∈ ∆Y , β ∈ ∆Z , ∑","y αy y (i , t ) =","∑ z βz z (i , t ) for all (i , t )} • Q ⊆ Q′ solutions: maxq∈Q h(q) ≤ maxq∈Q′ h(q) for any h"]},{"title":"Concrete example","paragraphs":["• Y = {y1, y2, y3} • Z = {z1, z2, z3} • ∆y ⊂ R3 , ∆z ⊂ R3"]},{"title":"Y","paragraphs":["x a He a is y1 x b He b is y2 x c He c is y3"]},{"title":"Z","paragraphs":["a He b is z1 b He a is z2 c He c is z3"]},{"title":"Simple solution Y","paragraphs":["x a He a is y1 x b He b is y2 x c He c is y3"]},{"title":"Z","paragraphs":["a He b is z1 b He a is z2 c He c is z3","choose: • α(1) = (0, 0, 1) ∈ ∆y is representation of y3 • β(1)","= (0, 0, 1) ∈ ∆z is representation of z3 confirm: ∑ y α (1) y y (i , t ) = ∑ z β (1) z z (i , t ) α (1) and β (1) satisfy agreement constraint"]},{"title":"Fractional solution Y","paragraphs":["x a He a is y1 x b He b is y2 x c He c is y3"]},{"title":"Z","paragraphs":["a He b is z1 b He a is z2 c He c is z3","choose: • α(2) = (0.5, 0.5, 0) ∈ ∆y is combination of y1 and y2 • β(2)","= (0.5, 0.5, 0) ∈ ∆z is combination of z1 and z2 confirm: ∑ y α (2) y y (i , t ) = ∑ z β (2) z z (i , t ) α (2) and β (2) satisfy agreement constraint, but not integral"]},{"title":"Optimal solution weights:","paragraphs":["• the choice of f and g determines the optimal solution • if (f , g ) favors (α(2) , β (2) ), the optimal solution is fractional","example: f = [1 1 2] and g = [1 1 − 2] • f · α(1) + g · β (1) = 0 vs f · α (2) + g · β (2) = 2 • α(2) , β (2) is optimal, even though it is fractional"]},{"title":"Algorithm run","paragraphs":["[Animation]"]},{"title":"Tightening","paragraphs":["(Sherali and Adams, 1994; Sontag et al., 2008)","modify: • extend Y , Z to identify bigrams of part-of-speech tags • y (i , t1, t2) = 1 ↔ y (i , t1) = 1 and y (i + 1, t2) = 1 • z (i , t1, t2) = 1 ↔ z (i , t1) = 1 and z (i + 1, t2) = 1","all bigram constraints: valid to add for all i , t1, t2 ∈ T ∑ y αy y (i , t1, t2) = ∑ z βz z (i , t1, t2) however this would make decoding expensive","single bigram constraint: cheaper to implement ∑ y αy y (1, a, b) = ∑ z βz z (1, a, b) the solution α (1) , β (1) trivially passes this constraint, while α (2) , β (2) violates it"]},{"title":"Dual decomposition with tightening","paragraphs":["tightened decomposition includes an additional Lagrange multiplier yu,v = arg maxy ∈Y f (y ) + ∑ i ,t u(i , t )y (i , t ) + v (1, a, b)y (1, a, b) zu,v = arg maxz∈Z g (z ) − ∑ i ,t u(i , t )z (i , t ) − v (1, a, b)z (1, a, b) in general, this term can make the decoding problem more difficult example: • for small examples, these penalties are easy to compute","• for CFG parsing, need to include extra states that maintain tag bigrams (still faster than full intersection)"]},{"title":"Tightening step-by-step","paragraphs":["[Animation]"]},{"title":"6. Advanced examples aim:","paragraphs":["demonstrate some different relaxation techniques • higher-order non-projective dependency parsing • syntactic machine translation"]},{"title":"Higher-order non-projective dependency parsing setup:","paragraphs":["given a model for higher-order non-projective dependency parsing (sibling features) problem: find non-projective dependency parse that maximizes the score of this model difficulty: • model is NP-hard to decode","• complexity of the model comes from enforcing combinatorial constraints strategy: design a decomposition that separates combinatorial constraints from direct implementation of the scoring function"]},{"title":"Non-projective dependency parsing structure:","paragraphs":["• starts at the root symbol * • each word has a exactly one parent word • produces a tree structure (no cycles) • dependencies can cross example: *0 John1 saw2 a3 movie4 today5 that6 he7 liked8 *0 John1 saw2 a3 movie4 today5 that6 he7 liked8"]},{"title":"Arc-Factored","paragraphs":["*0 John1 saw2 a3 movie4 today5 that6 he7 liked8 f (y ) = score (head =∗0, mod =saw2) +score (saw2, John1) +score (saw2, movie4) +score (saw2, today5) +score (movie4, a3) + ... e.g. score (∗0, saw2) = log p(saw2|∗0) (generative model) or score (∗0, saw2) = w · φ(saw2, ∗0) (CRF/perceptron model) y ∗ = arg maxy f (y ) ⇐ Minimum Spanning Tree Algorithm"]},{"title":"Sibling models","paragraphs":["*0 John1 saw2 a3 movie4 today5 that6 he7 liked8 f (y ) = score (head = ∗0, prev = NULL, mod = saw2) +score (saw2, NULL, John1)+score (saw2, NULL, movie4) +score (saw2,movie4, today5) + ... e.g. score (saw2, movie4, today5) = log p(today5|saw2, movie4) or score (saw2, movie4, today5) = w · φ(saw2, movie4, today5) y ∗ = arg maxy f (y ) ⇐ NP-Hard"]},{"title":"Thought experiment: individual decoding","paragraphs":["*0 John1 saw2 a3 movie4 today5 that6 he7 liked8 score (saw2, NULL, John1) + score (saw2, NULL, movie4) +score (saw2, movie4, today5) score (saw2, NULL, John1) + score (saw2, NULL, that6) score (saw2, NULL, a3) + score (saw2, a3, he7) 2 n−1 possibilities under sibling model, can solve for each word with Viterbi decoding."]},{"title":"Thought experiment continued","paragraphs":["*0 John1 saw2 a3 movie4 today5 that6 he7 liked8 idea: do individual decoding for each head word using dynamic programming if we’re lucky, we’ll end up with a valid final tree but we might violate some constraints"]},{"title":"Dual decomposition structure goal:","paragraphs":["y ∗ = arg maxy ∈Y f (y ) rewrite:","arg max y ∈ Y z∈ Z , f (y ) + g (z ) such that for all i , j y (i , j ) = z (i , j )"]},{"title":"Algorithm step-by-step","paragraphs":["[Animation]"]},{"title":"Syntactic translation decoding setup:","paragraphs":["assume a trained model for syntactic machine translation problem: find best derivation that maximizes the score of this model difficulty: • need to incorporate language model in decoding","• empirically, relaxation is often not tight, so dual decomposition does not always converge strategy: • use a different relaxation to handle language model • incrementally add constraints to find exact solution"]},{"title":"Syntactic translation example","paragraphs":["[Animation]"]},{"title":"Summary","paragraphs":["presented dual decomposition as a method for decoding in NLP formal guarantees • gives certificate or approximate solution • can improve approximate solutions by tightening relaxation efficient algorithms • uses fast combinatorial algorithms • can improve speed with lazy decoding widely applicable","• demonstrated algorithms for a wide range of NLP tasks (parsing, tagging, alignment, mt decoding)"]},{"title":"References I","paragraphs":["J. DeNero and K. Macherey. Model-Based Aligner Combination Using Dual Decomposition. In Proc. ACL, 2011.","D. Klein and C.D. Manning. Factored A* Search for Models over Sequences and Trees. In Proc IJCAI, volume 18, pages 1246–1251. Citeseer, 2003.","N. Komodakis, N. Paragios, and G. Tziritas. Mrf energy minimization and beyond via dual decomposition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2010. ISSN 0162-8828.","Terry Koo, Alexander M. Rush, Michael Collins, Tommi Jaakkola, and David Sontag. Dual decomposition for parsing with non-projective head automata. In EMNLP, 2010. URL http://www.aclweb.org/anthology/D10-1125.","B.H. Korte and J. Vygen. Combinatorial Optimization: Theory and Algorithms. Springer Verlag, 2008."]},{"title":"References II","paragraphs":["C. Lemaréchal. Lagrangian Relaxation. In Computational Combinatorial Optimization, Optimal or Provably Near-Optimal Solutions [based on a Spring School], pages 112–156, London, UK, 2001. Springer-Verlag. ISBN 3-540-42877-1.","Angelia Nedić and Asuman Ozdaglar. Approximate primal solutions and rate analysis for dual subgradient methods. SIAM Journal on Optimization, 19(4):1757–1780, 2009.","Christopher Raphael. Coarse-to-fine dynamic programming. IEEE Transactions on Pattern Analysis and Machine Intelligence, 23: 1379–1390, 2001.","A.M. Rush and M. Collins. Exact Decoding of Syntactic Translation Models through Lagrangian Relaxation. In Proc. ACL, 2011.","A.M. Rush, D. Sontag, M. Collins, and T. Jaakkola. On Dual Decomposition and Linear Programming Relaxations for Natural Language Processing. In Proc. EMNLP, 2010."]},{"title":"References III","paragraphs":["Hanif D. Sherali and Warren P. Adams. A hierarchy of relaxations and convex hull characterizations for mixed-integer zero–one programming problems. Discrete Applied Mathematics, 52(1):83 – 106, 1994.","D.A. Smith and J. Eisner. Dependency Parsing by Belief Propagation. In Proc. EMNLP, pages 145–156, 2008. URL http://www.aclweb.org/anthology/D08-1016.","D. Sontag, T. Meltzer, A. Globerson, T. Jaakkola, and Y. Weiss. Tightening LP relaxations for MAP using message passing. In Proc. UAI, 2008."]}],"references":[{"authors":[],"source":"J. DeNero and K. Macherey. Model-Based Aligner Combination Using Dual Decomposition. In Proc. ACL, 2011."},{"authors":[{"first":"D.","last":"Klein"},{"first":"C.","middle":"D. Manning. Factored A* Search for Models over","last":"Sequences"},{"first":"Trees.","middle":"In Proc","last":"IJCAI"},{"first":"volume","last":"18"},{"last":"pages"}],"year":"1246–1251","title":"Citeseer, 2003","source":"D. Klein and C.D. Manning. Factored A* Search for Models over Sequences and Trees. In Proc IJCAI, volume 18, pages 1246–1251. Citeseer, 2003."},{"authors":[{"first":"N.","last":"Komodakis"},{"first":"N.","last":"Paragios"},{"first":"G.","middle":"Tziritas. Mrf energy","last":"minimization"},{"first":"beyond","middle":"via dual decomposition. IEEE Transactions on Pattern","last":"Analysis"},{"first":"Machine","last":"Intelligence"}],"year":"2010","title":"ISSN 0162-8828","source":"N. Komodakis, N. Paragios, and G. Tziritas. Mrf energy minimization and beyond via dual decomposition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2010. ISSN 0162-8828."},{"authors":[{"first":"Terry","last":"Koo"},{"first":"Alexander","middle":"M.","last":"Rush"},{"first":"Michael","last":"Collins"},{"first":"Tommi","last":"Jaakkola"},{"first":"David","middle":"Sontag. Dual decomposition for parsing with non-projective head automata. In","last":"EMNLP"}],"year":"2010","title":"URL http://www","source":"Terry Koo, Alexander M. Rush, Michael Collins, Tommi Jaakkola, and David Sontag. Dual decomposition for parsing with non-projective head automata. In EMNLP, 2010. URL http://www.aclweb.org/anthology/D10-1125."},{"authors":[],"source":"B.H. Korte and J. Vygen. Combinatorial Optimization: Theory and Algorithms. Springer Verlag, 2008."},{"authors":[{"first":"C.","middle":"Lemaréchal. Lagrangian Relaxation. In Computational Combinatorial","last":"Optimization"},{"first":"Optimal","middle":"or Provably Near-Optimal Solutions [based on a Spring","last":"School]"},{"first":"pages","last":"112–156"},{"last":"London"},{"last":"UK"}],"year":"2001","title":"Springer-Verlag","source":"C. Lemaréchal. Lagrangian Relaxation. In Computational Combinatorial Optimization, Optimal or Provably Near-Optimal Solutions [based on a Spring School], pages 112–156, London, UK, 2001. Springer-Verlag. ISBN 3-540-42877-1."},{"authors":[],"source":"Angelia Nedić and Asuman Ozdaglar. Approximate primal solutions and rate analysis for dual subgradient methods. SIAM Journal on Optimization, 19(4):1757–1780, 2009."},{"authors":[],"source":"Christopher Raphael. Coarse-to-fine dynamic programming. IEEE Transactions on Pattern Analysis and Machine Intelligence, 23: 1379–1390, 2001."},{"authors":[],"source":"A.M. Rush and M. Collins. Exact Decoding of Syntactic Translation Models through Lagrangian Relaxation. In Proc. ACL, 2011."},{"authors":[],"source":"A.M. Rush, D. Sontag, M. Collins, and T. Jaakkola. On Dual Decomposition and Linear Programming Relaxations for Natural Language Processing. In Proc. EMNLP, 2010."},{"authors":[],"source":"Hanif D. Sherali and Warren P. Adams. A hierarchy of relaxations and convex hull characterizations for mixed-integer zero–one programming problems. Discrete Applied Mathematics, 52(1):83 – 106, 1994."},{"authors":[{"first":"D.","middle":"A.","last":"Smith"},{"first":"J.","middle":"Eisner. Dependency Parsing by Belief Propagation. In Proc.","last":"EMNLP"},{"first":"pages","last":"145–156"}],"year":"2008","title":"URL http://www","source":"D.A. Smith and J. Eisner. Dependency Parsing by Belief Propagation. In Proc. EMNLP, pages 145–156, 2008. URL http://www.aclweb.org/anthology/D08-1016."},{"authors":[],"source":"D. Sontag, T. Meltzer, A. Globerson, T. Jaakkola, and Y. Weiss. Tightening LP relaxations for MAP using message passing. In Proc. UAI, 2008."}],"cites":[{"style":0,"text":"Rush and Collins, 2011","origin":{"pointer":"/sections/2/paragraphs/2","offset":76,"length":22},"authors":[{"last":"Rush"},{"last":"Collins"}],"year":"2011","references":[]},{"style":0,"text":"DeNero and Macherey, 2011","origin":{"pointer":"/sections/2/paragraphs/3","offset":35,"length":25},"authors":[{"last":"DeNero"},{"last":"Macherey"}],"year":"2011","references":[]},{"style":0,"text":"Komodakis et al., 2010","origin":{"pointer":"/sections/4/paragraphs/0","offset":1,"length":22},"authors":[{"last":"Komodakis"},{"last":"al."}],"year":"2010","references":["/references/2"]},{"style":0,"text":"Lemaréchal, 2001","origin":{"pointer":"/sections/4/paragraphs/0","offset":25,"length":16},"authors":[{"last":"Lemaréchal"}],"year":"2001","references":[]},{"style":0,"text":"Smith and Eisner, 2008","origin":{"pointer":"/sections/5/paragraphs/1","offset":49,"length":22},"authors":[{"last":"Smith"},{"last":"Eisner"}],"year":"2008","references":[]},{"style":0,"text":"Klein and Manning, 2003","origin":{"pointer":"/sections/5/paragraphs/1","offset":95,"length":23},"authors":[{"last":"Klein"},{"last":"Manning"}],"year":"2003","references":[]},{"style":0,"text":"Raphael, 2001","origin":{"pointer":"/sections/5/paragraphs/1","offset":144,"length":13},"authors":[{"last":"Raphael"}],"year":"2001","references":[]},{"style":0,"text":"DeNero and Macherey, 2011","origin":{"pointer":"/sections/46/paragraphs/0","offset":1,"length":25},"authors":[{"last":"DeNero"},{"last":"Macherey"}],"year":"2011","references":[]},{"style":0,"text":"Nedić and Ozdaglar (2009)","origin":{"pointer":"/sections/54/paragraphs/0","offset":439,"length":25},"authors":[{"last":"Nedić"},{"last":"Ozdaglar"}],"year":"2009","references":[]},{"style":0,"text":"Sherali and Adams, 1994","origin":{"pointer":"/sections/72/paragraphs/0","offset":1,"length":23},"authors":[{"last":"Sherali"},{"last":"Adams"}],"year":"1994","references":[]},{"style":0,"text":"Sontag et al., 2008","origin":{"pointer":"/sections/72/paragraphs/0","offset":26,"length":19},"authors":[{"last":"Sontag"},{"last":"al."}],"year":"2008","references":[]}]}
