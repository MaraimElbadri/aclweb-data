{"sections":[{"title":"","paragraphs":["Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 335–340, Portland, Oregon, June 19-24, 2011. c⃝2011 Association for Computational Linguistics"]},{"title":"Modeling Wisdom of Crowds Using Latent Mixture of Discriminative Experts Derya Ozkan and Louis-Philippe Morency Institute for Creative Technologies University of Southern California {ozkan,morency}@ict.usc.edu Abstract","paragraphs":["In many computational linguistic scenarios, training labels are subjectives making it necessary to acquire the opinions of multiple an-notators/experts, which is referred to as ”wisdom of crowds”. In this paper, we propose a new approach for modeling wisdom of crowds based on the Latent Mixture of Discriminative Experts (LMDE) model that can automatically learn the prototypical patterns and hidden dynamic among different experts. Experiments show improvement over state-of-the-art approaches on the task of listener backchannel prediction in dyadic conversations."]},{"title":"1 Introduction","paragraphs":["In many real life scenarios, it is hard to collect the actual labels for training, because it is expensive or the labeling is subjective. To address this issue, a new direction of research appeared in the last decade, taking full advantage of the ”wisdom of crowds” (Surowiecki, 2004). In simple words, wisdom of crowds enables parallel acquisition of opinions from multiple annotators/experts.","In this paper, we propose a new method to fuse wisdom of crowds. Our approach is based on the Latent Mixture of Discriminative Experts (LMDE) model originally introduced for multimodal fusion (Ozkan et al., 2010). In our Wisdom-LMDE model, a discriminative expert is trained for each crowd member. The key advantage of our computational model is that it can automatically discover the prototypical patterns of experts and learn the dynamic between these patterns. An overview of our approach is depicted in Figure 1.","We validate our model on the challenging task of listener backchannel feedback prediction in dyadic conversations. Backchannel feedback includes the nods and paraverbals such as ”uh-huh” and ”mmhmm” that listeners produce as they are speaking. Backchannels play a significant role in determining the nature of a social exchange by showing rapport and engagement (Gratch et al., 2007). When these signals are positive, coordinated and reciprocated, they can lead to feelings of rapport and promote beneficial outcomes in diverse areas such as negotiations and conflict resolution (Drolet and Morris, 2000), psychotherapeutic effectiveness (Tsui and Schultz, 1985), improved test performance in classrooms (Fuchs, 1987) and improved quality of child care (Burns, 1984). Supporting such fluid interactions has become an important topic of virtual human research. In particular, backchannel feedback has received considerable interest due to its pervasiveness across languages and conversational contexts. By correctly predicting backchannel feedback, virtual agent and robots can have stronger sense of rapport.","What makes backchannel prediction task well-suited for our model is that listener feedback varies between people and is often optional (listeners can always decide to give feedback or not). A successful computational model of backchannel must be able to learn these variations among listeners. Wisdom-LMDE is a generic approach designed to integrate opinions from multiple listeners.","In our experiments, we validate the performance of our approach using a dataset of 43 storytelling dyadic interactions. Our analysis suggests three pro-335"]},{"title":"Latent Mixture of Discriminative Experts h 1 h 2 h 3 h n y 2y 1 y 3 y n x 1 x 1 Wisdom of crowds","paragraphs":["(listener backchannel) Speaker","x","1 x","2 x 3","x","n PitchWords Gaze Look at listener h 1 Time Figure 1: Left: Our approach applied to backchannel prediction: (1) multiple listeners experience the same series of stimuli (pre-recorded speakers) and (2) a Wisdom-LMDE model is learned using this wisdom of crowds, associating one expert for each listener. Right: Baseline models used in our experiments: a) Conditional Random Fields (CRF), b) Latent Dynamic Conditional Random Fields (LDCRF), c) CRF Mixture of Experts (no latent variable) totypical patterns for backchannel feedback. By automatically identifying these prototypical patterns and learning the dynamic, our Wisdom-LMDE model outperforms the previous approaches for listener backchannel prediction. 1.1 Previous Work Several researchers have developed models to predict when backchannel should happen. Ward and Tsukahara (2000) propose a unimodal approach where backchannels are associated with a region of low pitch lasting 110ms during speech. Nishimura et al. (2007) present a unimodal decision-tree approach for producing backchannels based on prosodic features. Cathcart et al. (2003) propose a unimodal model based on pause duration and trigram part-of-speech frequency.","Wisdom of crowds was first defined and used in business world by Surowiecki (2004). Later, it has been applied to other research areas as well. Raykar et. al. (2010) proposed a probabilistic approach for supervised learning tasks for which multiple annotators provide labels but not an absolute gold standard. Snow et. al. (2008) show that using non-expert labels for training machine learning algorithms can be as effective as using a gold standard annotation.","In this paper, we present a computational approach for listener backchannel prediction that exploits multiple listeners. Our model takes into account the differences in people’s reactions, and automatically learns the hidden structure among them.","The rest of the paper is organized as follows. In Section 2, we present the wisdom acquisition process. Then, we describe our Wisdom-LMDE model in Section 3. Experimentals are presented in Section 4. Finally, we conclude with discussions and future works in Section 5."]},{"title":"2 Wisdom Acquisition","paragraphs":["It is known that culture, age and gender affect people’s nonverbal behaviors (Linda L. Carli and Loeber, 1995; Matsumoto, 2006). Therefore, there might be variations among people’s reactions even when experiencing the same situation. To efficiently acquire responses from multiple listeners, we employ the Parasocial Consensus Sampling (PCS) paradigm (Huang et al., 2010), which is based on the theory that people behave similarly when interacting through a media (e.g., video conference). Huang et al. (2010) showed that a virtual human driven by PCS approach creates significantly more rapport and is perceived as more believable than the virtual human driven by face-to-face interaction data (from actual listener). This result indicates that the parasocial paradigm is a viable source of information for wisdom of crowds.","In practice, PCS is applied by having participants watch pre-recorded speaker videos drawn from a 336 Listener1 Listener2 Listener3 Listener4 Listener5 Listener6 Listener7 Listener8 Listener9 pause label:sub POS:NN POS:NN pause label:pmod pause POS:NN label:nmod pause POS:NN low pitch pause dirdist:L1 low pitch POS:NN pause low pitch Eyebrow up dirdist:L8+ POS:NN eye gaze dirdist:R1 POS:JJ lowness eye gaze pause Table 1: Most predictive features for each listener from our wisdom dataset. This analysis suggests three prototypical patterns for backchannel feedback. dyadic story-telling dataset. In our experiments, we used 43 video-recorded dyadic interactions from the RAPPORT1","dataset (Gratch et al., 2006). This dataset was drawn from a study of face-to-face narrative discourse (’quasi-monologic’ storytelling). The videos of the actual listeners were manually annotated for backchannel feedback. For PCS wisdom acquisition, we recruited 9 participants, who were told to pretend they are an active listener and press the keyboard whenever they felt like provid-ing backchannel feedback. This provides us the responses from multiple listeners all interacting with the same speaker, hence the wisdom necessary to model the variability among listeners."]},{"title":"3 Modeling Wisdom of Crowds","paragraphs":["Given the wisdom of multiple listeners, our goal is to create a computational model of backchannel feedback. Although listener responses vary among individuals, we expect some patterns in these responses. Therefore, we first analyze the most predictive features for each listener and search for prototypical patterns (in Section 3.1). Then, we present our Wisdom-LMDE that allows to automatically learn the hidden structure within listener responses. 3.1 Wisdom Analysis We analyzed our wisdom data to see the most relevant speaker features when predicting responses from each individual listener. (The complete list of speaker features are described in Section 4.1.) We used a feature ranking scheme based on a sparse regularization technique, as described in (Ozkan and Morency, 2010). It allows us to identify the speaker features most predictive of each listener backchannel feedback. The top 3 features for all 9 listeners are listed in Table 1.","This analysis suggests three prototypical patterns. For the first 3 listeners, pause in speech and syntac-","1","http://rapport.ict.usc.edu/ tic information (POS:NN) are more important. The next 3 experts include a prosodic feature, low pitch, which is coherent with earlier findings (Nishimura et al., 2007; Ward and Tsukahara, 2000). It is interesting to see that the last 3 experts incorporate visual information when predicting backchannel feedback. This is in line with Burgoon et al. (Burgoon et al., 1995) work showing that speaker gestures are often correlated with listener feedback. These results clearly suggest that variations be present among listeners and some prototypical patterns may exist. Based on these observations, we propose new computational model for listener backchannel. 3.2 Computational Model: Wisdom-LMDE The goals of our computational model are to automatically discover the prototypical patterns of backchannel feedback and learn the dynamic between these patterns. This will allow the computational model to accurately predict the responses of a new listener even if he/she changes her backchannel patterns in the middle of the interaction. It will also improve generalization by allowing mixtures of these prototypical patterns.","To achieve these goals, we propose a variant of the Latent Mixture of Discriminative Experts (Ozkan et al., 2010) which takes full advantage of the wisdom of crowds. Our Wisdom-LMDE model is based on a two step process: a Conditional Random Field (CRF, see Figure 1a) is learned for each wisdom listener, and the outputs of these expert models are used as input to a Latent Dynamic Conditional Random Field (LDCRF, see Figure 1b) model, which is capable of learning the hidden structure within the experts. In our Wisdom-LMDE, each expert corresponds to a different listener from the wisdom of crowds. More details about training and inference of LMDE can be found in Ozkan et al. (2010). 337"]},{"title":"4 Experiments","paragraphs":["To confirm the validity of our Wisdom-LMDE model, we compare its performance with computational models previously proposed. As motivated earlier, we focus our experiments on predicting listener backchannel since it is a well-suited task where variability exists among listeners. 4.1 Multimodal Speaker Features The speaker videos were transcribed and annotated to extract the following features: Lexical: Some studies have suggested an association between lexical features and listener feedback (Cathcart et al., 2003). Therefore, we use all the words (i.e., unigrams) spoken by the speaker. Syntactic structure: Using a CRF part-of-speech (POS) tagger and a data-driven left-to-right shiftreduce dependency parser (Sagae and Tsujii, 2007) we extract four types of features from a syntactic dependency structure corresponding to the utterance: POS tags and grammatical function for each word, POS tag of the syntactic head, distance and direction from each word to its syntactic head. Prosody: Prosody refers to the rhythm, pitch and intonation of speech. Several studies have demonstrated that listener feedback is correlated with a speaker’s prosody (Ward and Tsukahara, 2000; Nishimura et al., 2007). Following this, we use downslope in pitch, pitch regions lower than 26th percentile, drop/rise and fast drop/rise in energy of speech, vowel volume, pause. Visual gestures: Gestures performed by the speaker are often correlated with listener feedback (Burgoon et al., 1995). Eye gaze, in particular, has often been implicated as eliciting listener feedback. Thus, we encode the following contextual features: speaker looking at listener, smiling, moving eyebrows up and frowning.","Although our current method for extracting these features requires that the entire utterance to be available for processing, this provides us with a first step towards integrating information about syntactic structure in multimodal prediction models. Many of these features could in principle be computed incrementally with only a slight degradation in accuracy, with the exception of features that require dependency links where a word’s syntactic head is to the right of the word itself. We leave an investiga-tion that examines only syntactic features that can be produced incrementally in real time as future work. 4.2 Baseline Models Consensus Classifier In our first baseline model, we use consensus labels to train a CRF model, which are constructed by a similar approach presented in (Huang et al., 2010). The consensus threshold is set to 3 (at least 3 listeners agree to give feedback at a point) so that it contains approximately the same number of head nods as the actual listener. See Figure 1 for a graphical representation of CRF model. CRF Mixture of Experts To show the importance of latent variable in our Wisdom-LMDE model, we trained a CRF-based mixture of discriminative experts. This model is similar to the Logarithmic Opinion Pool (LOP) CRF suggested by Smith et al. (2005). Similar to our Wisdom-LMDE model, the training is performed in two steps. A graphical representation of a CRF Mixture of experts is given in the Figure 1. Actual Listener (AL) Classifiers This baseline model consists of two models: CRF and LDCRF chains (See Figure 1). To train these models, we use the labels of the ”Actual Listeners” (AL) from the RAP-PORT dataset. Multimodal LMDE In this baseline model, we compare our Wisdom LMDE to a multimodal LMDE, where each expert refers to one of 5 different set of multimodal features as presented in (Ozkan et al., 2010): lexical, prosodic, part-of-speech, syntactic, and visual. Random Classifier Our last baseline model is a random backchannel generator as desribed by Ward and Tsukahara (2000). This model randomly generates backchannels whenever some pre-defined conditions in the prosody of the speech is purveyed. 4.3 Methodolgy We performed hold-out testing on a randomly selected subset of 10 interactions. The training set contains the remaining 33 interactions. Model parameters were validated by using a 3-fold crossvalidation strategy on the training set. Regulariza-338 Table 2: Comparison of our Wisdom-LMDE model with previously proposed models. The last column shows the paired one tailed t-test results comparing Wisdom LMDE to each model. tion values used are 10k for k = -1,0,..,3. Numbers of hidden states used in the LDCRF models were 2, 3 and 4. We use the hCRF library2","for training of CRFs and LDCRFs. Our Wisdom-LMDE model was implemented in Matlab based on the hCRF library. Following (Morency et al., 2008), we use an encoding dictionary to represent our features. The performance is measured by using the F-score, which is the weighted harmonic mean of precision and recall. A backchannel is predicted correctly if a peak happens during an actual listener backchannel with high enough probability. The threshold was selected automatically during validation. 4.4 Results and Discussion Before reviewing the prediction results, is it important to remember that backchannel feedback is an optional phenomena, where the actual listener may or may not decide on giving feedback (Ward and Tsukahara, 2000). Therefore, results from prediction tasks are expected to have lower accuracies as opposed to recognition tasks where labels are directly observed (e.g., part-of-speech tagging).","Table 2 summarizes our experiments comparing our Wisdom-LMDE model with state-of-the-art approaches for behavior prediction (see Section 4.2). Our Wisdom-LMDE model achieves the best F1 score. Statistical t-test analysis show that Wisdom-LMDE is significantly better than Consensus Classifier, AL Classifier (LDCRF), Multimodel LMDE and Random Classifier.","The second best F1 score is achieved by CRF Mixture of experts, which is the only model among other baseline models that combines different listener labels in a late fusion manner. This result","2","http://sourceforge.net/projects/hrcf/ supports our claim that wisdom of clouds improves learning of prediction models. CRF Mixture model is a linear combination of the experts, whereas Wisdom-LMDE enables different weighting of experts at different point in time. By using hidden states, Wisdom-LMDE can automatically learn the prototypical patterns between listeners.","One really interesting result is that the optimal number of hidden states in the Wisdom-LMDE model (after cross-validation) is 3. This is coherent with our qualitative analysis in Section 3.1, where we observed 3 prototypical patterns."]},{"title":"5 Conclusions","paragraphs":["In this paper, we proposed a new approach called Wisdom-LMDE for modeling wisdom of crowds, which automatically learns the hidden structure in listener responses. We applied this method on the task of listener backchannel feedback prediction, and showed improvement over previous approaches. Both our qualitative analysis and experimental results suggest that prototypical patterns exist when predicting listener backchannel feedback. The Wisdom-LMDE is a generic model applicable to multiple sequence labeling tasks (such as emotion analysis and dialogue intent recognition), where labels are subjective (i.e. small inter-coder reliability)."]},{"title":"Acknowledgements","paragraphs":["This material is based upon work supported by the National Science Foundation under Grant No. 0917321 and the U.S. Army Research, Development, and Engineering Command (RDE-COM). The content does not necessarily reflect the position or the policy of the Government, and no official endorsement should be inferred. 339"]},{"title":"References","paragraphs":["Judee K. Burgoon, Lesa A. Stern, and Leesa Dillman. 1995. Interpersonal adaptation: Dyadic interaction patterns. Cambridge University Press, Cambridge.","M. Burns. 1984. Rapport and relationships: The basis of child care. Journal of Child Care, 4:47–57.","N. Cathcart, Jean Carletta, and Ewan Klein. 2003. A shallow model of backchannel continuers in spoken dialogue. In European Chapter of the Association for Computational Linguistics. 51–58.","Aimee L. Drolet and Michael W. Morris. 2000. Rapport in conflict resolution: Accounting for how face-to-face contact fosters mutual cooperation in mixedmotive conflicts. Journal of Experimental Social Psychology, 36(1):26–50.","D. Fuchs. 1987. Examiner familiarity effects on test performance: Implications for training and practice. Topics in Early Childhood Special Education, 7:90–104.","J. Gratch, A. Okhmatovskaia, F. Lamothe, S. Marsella, M. Morales, R.J. Werf, and L.-P. Morency. 2006. Virtual rapport. Proceedings of International Conference on Intelligent Virtual Agents (IVA), Marina del Rey, CA.","Jonathan Gratch, Ning Wang, Jillian Gerten, and Edward Fast. 2007. Creating rapport with virtual agents. In IVA.","L. Huang, L.-P. Morency, and J. Gratch:. 2010. Parasocial consensus sampling: combining multiple perspectives to learn virtual human behavior. In International Conference on Autonomous Agents and Multiagent Systems (AAMAS).","Suzanne J. LaFleur Linda L. Carli and Christopher C. Loeber. 1995. Nonverbal behavior, gender, and influence. Journal of Personality and Social Psychology. 68, 1030-1041.","D. Matsumoto. 2006. Culture and Nonverbal Behavior. The Sage Handbook of Nonverbal Communica-tion, Sage Publications Inc.","L.-P. Morency, I. de Kok, and J. Gratch. 2008. Predict-ing listener backchannels: A probabilistic multimodal approach. In Proceedings of the Conference on Intelligent Virutal Agents (IVA).","Ryota Nishimura, Norihide Kitaoka, and Seiichi Nakagawa. 2007. A spoken dialog system for chat-like conversations considering response timing. International Conference on Text, Speech and Dialog. 599-606.","D. Ozkan and L.-P. Morency. 2010. Concensus of self-features for nonverbal behavior analysis. In Human Behavior Understanding in conjucion with International Conference in Pattern Recognition.","D. Ozkan, K. Sagae, and L.-P. Morency. 2010. Latent mixture of discriminative experts for multimodal prediction modeling. In International Conference on Computational Linguistics (COLING).","Vikas C. Raykar, Shipeng Yu, Linda H. Zhao, Gerardo Hermosillo Valadez, Charles Florin, Luca Bogoni, Linda Moy, and David Blei. 2010. Learning from crowds.","Kenji Sagae and Jun’ichi Tsujii. 2007. Dependency pars-ing and domain adaptation with LR models and parser ensembles. In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pages 1044–1050, Prague, Czech Republic, June. Association for Computational Linguistics.","A. Smith, T. Cohn, and M. Osborne. 2005. Logarithmic opinion pools for conditional random fields. In ACL, pages 18–25.","Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2008. Cheap and fast - but is it good? Evaluating non-expert annotations for natural language tasks.","James Surowiecki. 2004. The Wisdom of Crowds: Why the Many Are Smarter Than the Few and How Collective Wisdom Shapes Business, Economies, Societies and Nations. Doubleday.","P. Tsui and G.L. Schultz. 1985. Failure of rapport: Why psychotheraputic engagement fails in the treatment of asian clients. American Journal of Orthopsychiatry, 55:561–569.","N. Ward and W. Tsukahara. 2000. Prosodic features which cue back-channel responses in english and japanese. Journal of Pragmatics. 23, 1177–1207. 340"]}],"references":[{"authors":[{"first":"Judee","middle":"K.","last":"Burgoon"},{"first":"Lesa","middle":"A.","last":"Stern"},{"first":"Leesa","last":"Dillman"}],"year":"1995","title":"Interpersonal adaptation: Dyadic interaction patterns","source":"Judee K. Burgoon, Lesa A. Stern, and Leesa Dillman. 1995. Interpersonal adaptation: Dyadic interaction patterns. Cambridge University Press, Cambridge."},{"authors":[{"first":"M.","last":"Burns"}],"year":"1984","title":"Rapport and relationships: The basis of child care","source":"M. Burns. 1984. Rapport and relationships: The basis of child care. Journal of Child Care, 4:47–57."},{"authors":[{"first":"N.","last":"Cathcart"},{"first":"Jean","last":"Carletta"},{"first":"Ewan","last":"Klein"}],"year":"2003","title":"A shallow model of backchannel continuers in spoken dialogue","source":"N. Cathcart, Jean Carletta, and Ewan Klein. 2003. A shallow model of backchannel continuers in spoken dialogue. In European Chapter of the Association for Computational Linguistics. 51–58."},{"authors":[{"first":"Aimee","middle":"L.","last":"Drolet"},{"first":"Michael","middle":"W.","last":"Morris"}],"year":"2000","title":"Rapport in conflict resolution: Accounting for how face-to-face contact fosters mutual cooperation in mixedmotive conflicts","source":"Aimee L. Drolet and Michael W. Morris. 2000. Rapport in conflict resolution: Accounting for how face-to-face contact fosters mutual cooperation in mixedmotive conflicts. Journal of Experimental Social Psychology, 36(1):26–50."},{"authors":[{"first":"D.","last":"Fuchs"}],"year":"1987","title":"Examiner familiarity effects on test performance: Implications for training and practice","source":"D. Fuchs. 1987. Examiner familiarity effects on test performance: Implications for training and practice. Topics in Early Childhood Special Education, 7:90–104."},{"authors":[{"first":"J.","last":"Gratch"},{"first":"A.","last":"Okhmatovskaia"},{"first":"F.","last":"Lamothe"},{"first":"S.","last":"Marsella"},{"first":"M.","last":"Morales"},{"first":"R.","middle":"J.","last":"Werf"},{"first":"L.","middle":"-P.","last":"Morency"}],"year":"2006","title":"Virtual rapport","source":"J. Gratch, A. Okhmatovskaia, F. Lamothe, S. Marsella, M. Morales, R.J. Werf, and L.-P. Morency. 2006. Virtual rapport. Proceedings of International Conference on Intelligent Virtual Agents (IVA), Marina del Rey, CA."},{"authors":[{"first":"Jonathan","last":"Gratch"},{"first":"Ning","last":"Wang"},{"first":"Jillian","last":"Gerten"},{"first":"Edward","last":"Fast"}],"year":"2007","title":"Creating rapport with virtual agents","source":"Jonathan Gratch, Ning Wang, Jillian Gerten, and Edward Fast. 2007. Creating rapport with virtual agents. In IVA."},{"authors":[{"first":"L.","last":"Huang"},{"first":"L.","middle":"-P.","last":"Morency"},{"first":"J.","last":"Gratch:"}],"year":"2010","title":"Parasocial consensus sampling: combining multiple perspectives to learn virtual human behavior","source":"L. Huang, L.-P. Morency, and J. Gratch:. 2010. Parasocial consensus sampling: combining multiple perspectives to learn virtual human behavior. In International Conference on Autonomous Agents and Multiagent Systems (AAMAS)."},{"authors":[{"first":"Suzanne","middle":"J. LaFleur Linda L.","last":"Carli"},{"first":"Christopher","middle":"C.","last":"Loeber"}],"year":"1995","title":"Nonverbal behavior, gender, and influence","source":"Suzanne J. LaFleur Linda L. Carli and Christopher C. Loeber. 1995. Nonverbal behavior, gender, and influence. Journal of Personality and Social Psychology. 68, 1030-1041."},{"authors":[{"first":"D.","last":"Matsumoto"}],"year":"2006","title":"Culture and Nonverbal Behavior","source":"D. Matsumoto. 2006. Culture and Nonverbal Behavior. The Sage Handbook of Nonverbal Communica-tion, Sage Publications Inc."},{"authors":[{"first":"L.","middle":"-P.","last":"Morency"},{"first":"I.","last":"de Kok"},{"first":"J.","last":"Gratch"}],"year":"2008","title":"Predict-ing listener backchannels: A probabilistic multimodal approach","source":"L.-P. Morency, I. de Kok, and J. Gratch. 2008. Predict-ing listener backchannels: A probabilistic multimodal approach. In Proceedings of the Conference on Intelligent Virutal Agents (IVA)."},{"authors":[{"first":"Ryota","last":"Nishimura"},{"first":"Norihide","last":"Kitaoka"},{"first":"Seiichi","last":"Nakagawa"}],"year":"2007","title":"A spoken dialog system for chat-like conversations considering response timing","source":"Ryota Nishimura, Norihide Kitaoka, and Seiichi Nakagawa. 2007. A spoken dialog system for chat-like conversations considering response timing. International Conference on Text, Speech and Dialog. 599-606."},{"authors":[{"first":"D.","last":"Ozkan"},{"first":"L.","middle":"-P.","last":"Morency"}],"year":"2010","title":"Concensus of self-features for nonverbal behavior analysis","source":"D. Ozkan and L.-P. Morency. 2010. Concensus of self-features for nonverbal behavior analysis. In Human Behavior Understanding in conjucion with International Conference in Pattern Recognition."},{"authors":[{"first":"D.","last":"Ozkan"},{"first":"K.","last":"Sagae"},{"first":"L.","middle":"-P.","last":"Morency"}],"year":"2010","title":"Latent mixture of discriminative experts for multimodal prediction modeling","source":"D. Ozkan, K. Sagae, and L.-P. Morency. 2010. Latent mixture of discriminative experts for multimodal prediction modeling. In International Conference on Computational Linguistics (COLING)."},{"authors":[{"first":"Vikas","middle":"C.","last":"Raykar"},{"first":"Shipeng","last":"Yu"},{"first":"Linda","middle":"H.","last":"Zhao"},{"first":"Gerardo","middle":"Hermosillo","last":"Valadez"},{"first":"Charles","last":"Florin"},{"first":"Luca","last":"Bogoni"},{"first":"Linda","last":"Moy"},{"first":"David","last":"Blei"}],"year":"2010","title":"Learning from crowds","source":"Vikas C. Raykar, Shipeng Yu, Linda H. Zhao, Gerardo Hermosillo Valadez, Charles Florin, Luca Bogoni, Linda Moy, and David Blei. 2010. Learning from crowds."},{"authors":[{"first":"Kenji","last":"Sagae"},{"first":"Jun’ichi","last":"Tsujii"}],"year":"2007","title":"Dependency pars-ing and domain adaptation with LR models and parser ensembles","source":"Kenji Sagae and Jun’ichi Tsujii. 2007. Dependency pars-ing and domain adaptation with LR models and parser ensembles. In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pages 1044–1050, Prague, Czech Republic, June. Association for Computational Linguistics."},{"authors":[{"first":"A.","last":"Smith"},{"first":"T.","last":"Cohn"},{"first":"M.","last":"Osborne"}],"year":"2005","title":"Logarithmic opinion pools for conditional random fields","source":"A. Smith, T. Cohn, and M. Osborne. 2005. Logarithmic opinion pools for conditional random fields. In ACL, pages 18–25."},{"authors":[{"first":"Rion","last":"Snow"},{"first":"Daniel","last":"Jurafsky"},{"first":"Andrew","middle":"Y.","last":"Ng"}],"year":"2008","title":"Cheap and fast - but is it good? Evaluating non-expert annotations for natural language tasks","source":"Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2008. Cheap and fast - but is it good? Evaluating non-expert annotations for natural language tasks."},{"authors":[{"first":"James","last":"Surowiecki"}],"year":"2004","title":"The Wisdom of Crowds: Why the Many Are Smarter Than the Few and How Collective Wisdom Shapes Business, Economies, Societies and Nations","source":"James Surowiecki. 2004. The Wisdom of Crowds: Why the Many Are Smarter Than the Few and How Collective Wisdom Shapes Business, Economies, Societies and Nations. Doubleday."},{"authors":[{"first":"P.","last":"Tsui"},{"first":"G.","middle":"L.","last":"Schultz"}],"year":"1985","title":"Failure of rapport: Why psychotheraputic engagement fails in the treatment of asian clients","source":"P. Tsui and G.L. Schultz. 1985. Failure of rapport: Why psychotheraputic engagement fails in the treatment of asian clients. American Journal of Orthopsychiatry, 55:561–569."},{"authors":[{"first":"N.","last":"Ward"},{"first":"W.","last":"Tsukahara"}],"year":"2000","title":"Prosodic features which cue back-channel responses in english and japanese","source":"N. Ward and W. Tsukahara. 2000. Prosodic features which cue back-channel responses in english and japanese. Journal of Pragmatics. 23, 1177–1207. 340"}],"cites":[{"style":0,"text":"Surowiecki, 2004","origin":{"pointer":"/sections/2/paragraphs/0","offset":267,"length":16},"authors":[{"last":"Surowiecki"}],"year":"2004","references":["/references/18"]},{"style":0,"text":"Ozkan et al., 2010","origin":{"pointer":"/sections/2/paragraphs/1","offset":193,"length":18},"authors":[{"last":"Ozkan"},{"last":"al."}],"year":"2010","references":["/references/13"]},{"style":0,"text":"Gratch et al., 2007","origin":{"pointer":"/sections/2/paragraphs/2","offset":363,"length":19},"authors":[{"last":"Gratch"},{"last":"al."}],"year":"2007","references":["/references/6"]},{"style":0,"text":"Drolet and Morris, 2000","origin":{"pointer":"/sections/2/paragraphs/2","offset":580,"length":23},"authors":[{"last":"Drolet"},{"last":"Morris"}],"year":"2000","references":["/references/3"]},{"style":0,"text":"Tsui and Schultz, 1985","origin":{"pointer":"/sections/2/paragraphs/2","offset":639,"length":22},"authors":[{"last":"Tsui"},{"last":"Schultz"}],"year":"1985","references":["/references/19"]},{"style":0,"text":"Fuchs, 1987","origin":{"pointer":"/sections/2/paragraphs/2","offset":705,"length":11},"authors":[{"last":"Fuchs"}],"year":"1987","references":["/references/4"]},{"style":0,"text":"Burns, 1984","origin":{"pointer":"/sections/2/paragraphs/2","offset":754,"length":11},"authors":[{"last":"Burns"}],"year":"1984","references":["/references/1"]},{"style":0,"text":"Ward and Tsukahara (2000)","origin":{"pointer":"/sections/3/paragraphs/5","offset":813,"length":25},"authors":[{"last":"Ward"},{"last":"Tsukahara"}],"year":"2000","references":["/references/20"]},{"style":0,"text":"Nishimura et al. (2007)","origin":{"pointer":"/sections/3/paragraphs/5","offset":957,"length":23},"authors":[{"last":"Nishimura"},{"last":"al."}],"year":"2007","references":["/references/11"]},{"style":0,"text":"Cathcart et al. (2003)","origin":{"pointer":"/sections/3/paragraphs/5","offset":1078,"length":22},"authors":[{"last":"Cathcart"},{"last":"al."}],"year":"2003","references":["/references/2"]},{"style":0,"text":"Surowiecki (2004)","origin":{"pointer":"/sections/3/paragraphs/6","offset":65,"length":17},"authors":[{"last":"Surowiecki"}],"year":"2004","references":["/references/18"]},{"style":0,"text":"Carli and Loeber, 1995","origin":{"pointer":"/sections/4/paragraphs/0","offset":87,"length":22},"authors":[{"last":"Carli"},{"last":"Loeber"}],"year":"1995","references":["/references/8"]},{"style":0,"text":"Matsumoto, 2006","origin":{"pointer":"/sections/4/paragraphs/0","offset":111,"length":15},"authors":[{"last":"Matsumoto"}],"year":"2006","references":["/references/9"]},{"style":0,"text":"Huang et al., 2010","origin":{"pointer":"/sections/4/paragraphs/0","offset":352,"length":18},"authors":[{"last":"Huang"},{"last":"al."}],"year":"2010","references":["/references/7"]},{"style":0,"text":"Huang et al. (2010)","origin":{"pointer":"/sections/4/paragraphs/0","offset":490,"length":19},"authors":[{"last":"Huang"},{"last":"al."}],"year":"2010","references":["/references/7"]},{"style":0,"text":"Gratch et al., 2006","origin":{"pointer":"/sections/4/paragraphs/2","offset":9,"length":19},"authors":[{"last":"Gratch"},{"last":"al."}],"year":"2006","references":["/references/5"]},{"style":0,"text":"Ozkan and Morency, 2010","origin":{"pointer":"/sections/5/paragraphs/0","offset":762,"length":23},"authors":[{"last":"Ozkan"},{"last":"Morency"}],"year":"2010","references":["/references/12"]},{"style":0,"text":"Nishimura et al., 2007","origin":{"pointer":"/sections/5/paragraphs/3","offset":172,"length":22},"authors":[{"last":"Nishimura"},{"last":"al."}],"year":"2007","references":["/references/11"]},{"style":0,"text":"Ward and Tsukahara, 2000","origin":{"pointer":"/sections/5/paragraphs/3","offset":196,"length":24},"authors":[{"last":"Ward"},{"last":"Tsukahara"}],"year":"2000","references":["/references/20"]},{"style":0,"text":"Burgoon et al., 1995","origin":{"pointer":"/sections/5/paragraphs/3","offset":378,"length":20},"authors":[{"last":"Burgoon"},{"last":"al."}],"year":"1995","references":["/references/0"]},{"style":0,"text":"Ozkan et al., 2010","origin":{"pointer":"/sections/5/paragraphs/4","offset":94,"length":18},"authors":[{"last":"Ozkan"},{"last":"al."}],"year":"2010","references":["/references/13"]},{"style":0,"text":"Ozkan et al. (2010)","origin":{"pointer":"/sections/5/paragraphs/4","offset":668,"length":19},"authors":[{"last":"Ozkan"},{"last":"al."}],"year":"2010","references":["/references/13"]},{"style":0,"text":"Cathcart et al., 2003","origin":{"pointer":"/sections/6/paragraphs/0","offset":496,"length":21},"authors":[{"last":"Cathcart"},{"last":"al."}],"year":"2003","references":["/references/2"]},{"style":0,"text":"Sagae and Tsujii, 2007","origin":{"pointer":"/sections/6/paragraphs/0","offset":716,"length":22},"authors":[{"last":"Sagae"},{"last":"Tsujii"}],"year":"2007","references":["/references/15"]},{"style":0,"text":"Ward and Tsukahara, 2000","origin":{"pointer":"/sections/6/paragraphs/0","offset":1153,"length":24},"authors":[{"last":"Ward"},{"last":"Tsukahara"}],"year":"2000","references":["/references/20"]},{"style":0,"text":"Nishimura et al., 2007","origin":{"pointer":"/sections/6/paragraphs/0","offset":1179,"length":22},"authors":[{"last":"Nishimura"},{"last":"al."}],"year":"2007","references":["/references/11"]},{"style":0,"text":"Burgoon et al., 1995","origin":{"pointer":"/sections/6/paragraphs/0","offset":1456,"length":20},"authors":[{"last":"Burgoon"},{"last":"al."}],"year":"1995","references":["/references/0"]},{"style":0,"text":"Huang et al., 2010","origin":{"pointer":"/sections/6/paragraphs/1","offset":793,"length":18},"authors":[{"last":"Huang"},{"last":"al."}],"year":"2010","references":["/references/7"]},{"style":0,"text":"Smith et al. (2005)","origin":{"pointer":"/sections/6/paragraphs/1","offset":1278,"length":19},"authors":[{"last":"Smith"},{"last":"al."}],"year":"2005","references":["/references/16"]},{"style":0,"text":"Ozkan et al., 2010","origin":{"pointer":"/sections/6/paragraphs/1","offset":1847,"length":18},"authors":[{"last":"Ozkan"},{"last":"al."}],"year":"2010","references":["/references/13"]},{"style":0,"text":"Ward and Tsukahara (2000)","origin":{"pointer":"/sections/6/paragraphs/1","offset":2017,"length":25},"authors":[{"last":"Ward"},{"last":"Tsukahara"}],"year":"2000","references":["/references/20"]},{"style":0,"text":"Morency et al., 2008","origin":{"pointer":"/sections/6/paragraphs/2","offset":119,"length":20},"authors":[{"last":"Morency"},{"last":"al."}],"year":"2008","references":["/references/10"]},{"style":0,"text":"Ward and Tsukahara, 2000","origin":{"pointer":"/sections/6/paragraphs/2","offset":709,"length":24},"authors":[{"last":"Ward"},{"last":"Tsukahara"}],"year":"2000","references":["/references/20"]}]}
