{"sections":[{"title":"","paragraphs":["1 1"]},{"title":"Automatic Summarization","paragraphs":["Ani Nenkova University of Pennsylvania Sameer Maskey IBM Research Yang Liu University of Texas at Dallas 22"]},{"title":"Why summarize?","paragraphs":["2 3"]},{"title":"Text summarization","paragraphs":["News articles Scientific Articles Emails Books Websites Social Media Streams 4"]},{"title":"Speech summarization","paragraphs":["Meeting Phone Conversation Classroom Radio News Broadcast News Talk Shows Lecture Chat 3 5","How to summarize","Text & Speech? -Algorithms -Issues -Challenges -Systems Tutorial 6 Motivation & Definition Topic Models Graph Based Methods Supervised Techniques Optimization Methods Speech Summarization Evaluation","Manual (Pyramid), Automatic (Rouge, F-Measure) Fully Automatic Frequency, Lexical chains, TF*IDF, Topic Words, Topic Models [LSA, EM, Bayesian]","Features, Discriminative Training Sampling, Data, Co-training","Iterative, Greedy, Dynamic Programming ILP, Sub-Modular Selection","Segmentation, ASR Acoustic Information, Disfluency 4 7"]},{"title":"Motivation: where does summarization help? ","paragraphs":["Single document summarization Simulate the work of intelligence analyst Judge if a document is relevant to a topic of interest","“Summaries as short as 17% of the full text length speed up decision making twice, with no significant degradation in accuracy.”","“Query-focused summaries enable users to find more relevant documents more accurately, with less need to consult the full text of the document.” [Mani et al., 2002] 8"]},{"title":"Motivation: multi-document summarization helps in compiling and presenting ","paragraphs":["Reduce search time, especially when the goal of the user is to find as much information as possible about a given topic Writing better reports, finding more relevant information,","quicker","Cluster similar articles and provide a multi-document summary of the similarities","Single document summary of the information unique to an article [Roussinov and Chen, 2001; Mana-Lopez et al., 2004; McKeown et al., 2005 ] 5 9"]},{"title":"Benefits from speech summarization Voicemail ","paragraphs":["Shorter time spent on listening (call centers) "]},{"title":"Meetings ","paragraphs":["Easier to find main points "]},{"title":"Broadcast News ","paragraphs":["Summary of story from mulitiple channels "]},{"title":"Lectures ","paragraphs":["Useful for reviewing of course materials [He et al., 2000; Tucker and Whittaker, 2008; Murray et al., 2009] 10"]},{"title":"Assessing summary quality: overview ","paragraphs":["Responsiveness Assessor directly rate each summary on a scale In official evaluations but rarely reported in papers","Pyramid Assessors create model summaries Assessors identifies semantic overlap between summary","and models","ROUGE Assessors create model summaries ROUGE automatically computes word overlap 6 11"]},{"title":"Tasks in summarization","paragraphs":["Content (sentence) selection Extractive summarization","Information ordering In what order to present the selected sentences, especially","in multi-document summarization","Automatic editing, information fusion and compression Abstractive summaries 12"]},{"title":"Extractive (multi-document) summarization","paragraphs":["Input text2Input text1 Input text3 Summary 1. Selection 2. Ordering 3. Fusion Compute Informativeness 7 13"]},{"title":"Computing informativeness ","paragraphs":["Topic models (unsupervised) Figure out what the topic of the input Frequency, Lexical chains, TF*IDF LSA, content models (EM, Bayesian) Select informative sentences based on the topic "]},{"title":"Graph models (unsupervised) ","paragraphs":["Sentence centrality "]},{"title":"Supervised approaches ","paragraphs":["Ask people which sentences should be in a summary","Use any imaginable feature to learn to predict human choices 14 Motivation & Definition Topic Models Graph Based Methods Supervised Techniques Global Optimization Methods Speech Summarization Evaluation Frequency, Lexical chains, TF*IDF, Topic Words,Topic Models [LSA, EM, Bayesian]","Manual (Pyramid), Automatic (Rouge, F-Measure) Fully Automatic","Features, Discriminative Training Sampling, Data, Co-training","Iterative, Greedy, Dynamic Programming ILP, Sub-Modular Selection","Segmentation, ASR Acoustic Information, Disfluency 8 15"]},{"title":"Frequency as document topic proxy 10 incarnations of an intuition ","paragraphs":["Simple intuition, look only at the document(s) Words that repeatedly appear in the document are likely to","be related to the topic of the document Sentences that repeatedly appear in different input","documents represent themes in the input","But what appears in other documents is also helpful in determining the topic Background corpus probabilities/weights for word 16"]},{"title":"What is an article about? Word probability/frequency ","paragraphs":["Proposed by Luhn in 1958 [Luhn 1958]","Frequent content words would be indicative of the topic of the article "]},{"title":"In multi-document summarization, words or facts repeated in the input are more likely to appear in human summaries","paragraphs":["[Nenkova et al., 2006] 9 1717"]},{"title":"Word probability/weights","paragraphs":["Libya bombing trail Gadafhi suspects Libya refuses to surrender two Pan Am bombing suspects Pan Am INPUT SUMMARY WORD PROBABILITY TABLE Word Probability pan 0.0798 am 0.0825 libya 0.0096 suspects 0.0341 gadafhi 0.0911 trail 0.0002 .... usa 0.0007 HOW? UK and USA 1818"]},{"title":"HOW: Main steps in sentence selection according to word probabilities Step 1","paragraphs":["Estimate word weights (probabilities) Step 2 Estimate sentence weights Step 3 Choose best sentence Step 4 Update word weights Step 5 Go to 2 if desired length not reached )()( SentwCFSentWeight i ∈= 10 19"]},{"title":"More specific choices","paragraphs":["[Vanderwende et al., 2007; Yih et al., 2007; Haghighi and Vanderwende, 2009] Select highest scoring sentence Update word probabilities for the selected sentence to reduce redundancy Repeat until desired summary length"]},{"title":"∑","paragraphs":["∈"]},{"title":"=","paragraphs":["Sw"]},{"title":"wp SSScore )( || 1 )( p","paragraphs":["new"]},{"title":"(w) = p","paragraphs":["old"]},{"title":"(w).p","paragraphs":["old"]},{"title":"(w)","paragraphs":["2020"]},{"title":"Is this a reasonable approach: yes, people seem to be doing something similar ","paragraphs":["Simple test Compute word probability table from the input Get a batch of summaries written by H(umans) and S(ystems) Compute the likelihood of the summaries given the word","probability table","Results Human summaries have higher likelihood"]},{"title":"HSSSSSSSSSSHSSSHSSHHSHHHHH","paragraphs":["HIGH LIKELIHOODLOW 11 21"]},{"title":"Obvious shortcomings of the pure frequency approaches Does not take account of related words ","paragraphs":["suspects -- trail Gadhafi – Libya "]},{"title":"Does not take into account evidence from other documents ","paragraphs":["Function words: prepositions, articles, etc. Domain words: “cell” in cell biology articles "]},{"title":"Does not take into account many other aspects","paragraphs":["22"]},{"title":"Two easy fixes Lexical chains","paragraphs":["[Barzilay and Elhadad, 1999, Silber and McCoy, 2002, Gurevych and Nahnsen, 2005] Exploits existing lexical resources (WordNet) "]},{"title":"TF*IDF weights","paragraphs":["[most summarizers] Incorporates evidence from a background corpus 12 23"]},{"title":"Lexical chains and WordNet relations ","paragraphs":["Lexical chains Word sense disambiguation is performed Then topically related words represent a topic","Synonyms, hyponyms, hypernyms Importance is determined by frequency of the words in a","topic rather than a single word One sentence per topic is selected","Concepts based on WordNet [Schiffman et al., 2002, Ye et al., 2007]","No word sense disambiguation is performed {war, campaign, warfare, effort, cause, operation} {concern, carrier, worry, fear, scare} 24"]},{"title":"TF*IDF weights for words","paragraphs":["Combining evidence for document topics from the input and from a background corpus "]},{"title":"Term Frequency (TF) ","paragraphs":["Times a word occurs in the input "]},{"title":"Inverse Document Frequency (IDF) ","paragraphs":["Number of documents (df) from a background corpus of N documents that contain the word"]},{"title":")/log(* dfNtfIDFTF ×=","paragraphs":["13 25 Motivation & Definition Topic Models Graph Based Methods Supervised Techniques Global Optimization Methods Speech Summarization Evaluation Frequency, TF*IDF, Topic Words Topic Models [LSA, EM, Bayesian]","Manual (Pyramid), Automatic (Rouge, F-Measure) Fully Automatic","Features, Discriminative Training Sampling, Data, Co-training","Iterative, Greedy, Dynamic Programming ILP, Sub-Modular Selection","Segmentation, ASR Acoustic Information, Disfluency 26"]},{"title":"Topic words (topic signatures) ","paragraphs":["Which words in the input are most descriptive?","Instead of assigning probabilities or weights to all words, divide words into two classes: descriptive or not","For iterative sentence selection approach, the binary distinction is key to the advantage over frequency and TF*IDF","Systems based on topic words have proven to be the most successful in official summarization evaluations 14 27"]},{"title":"Example input and associated topic words ","paragraphs":["Input for summarization: articles relevant to the following user need Title: Human Toll of Tropical","Storms Narrative: What has been the human toll in death or injury of tropical storms in recent years? Where and when have each of the storms caused human casualties? What are the approximate total number of casualties attributed to each of the storms? ahmed, allison, andrew, bahamas, bangladesh, bn, caribbean, carolina, caused, cent, coast, coastal, croix, cyclone, damage, destroyed, devastated, disaster, dollars, drowned, flood, flooded, flooding, floods, florida, gulf, ham, hit, homeless, homes, hugo, hurricane, insurance, insurers, island, islands, lloyd, losses, louisiana, manila, miles, nicaragua, north, port, pounds, rain, rains, rebuild, rebuilding, relief, remnants, residents, roared, salt, st, storm, storms, supplies, tourists, trees, tropical, typhoon, virgin, volunteers, weather, west, winds, yesterday. Topic Words 28"]},{"title":"Formalizing the problem of identifying topic words ","paragraphs":["Given t: a word that appears in the input T: cluster of articles on a given topic (input) NT: articles not on topic T (background corpus) Decide if t is a topic word or not","Words that have (almost) the same probability in T and NT are not topic words 15 29"]},{"title":"Computing probabilities ","paragraphs":["View a text as a sequence of Bernoulli trails A word is either our term of interest t or not The likelihood of observing term t which occurs with","probability p in a text consisting of N words is given by","Estimate the probability of t in three ways Input + background corpus combines Input only Background only t 30"]},{"title":"Testing which hypothesis is more likely: log-likelihood ratio test","paragraphs":["has a known statistical distribution: chi-square At a given significance level, we can decide if a word is descriptive of the input or not. This feature is used in the best performing systems for multi-document summarization of news [Lin and Hovy, 2000; Conroy et al., 2006] Likelihood of the data given H1 Likelihood of the data given H2"]},{"title":"λ =","paragraphs":["-2 log λ 16 31 Motivation & Definition Topic Models Graph Based Methods Supervised Techniques Global Optimization Methods Speech Summarization Evaluation Frequency, TF*IDF, Topic Words Topic Models [LSA, EM, Bayesian]","Manual (Pyramid), Automatic (Rouge, F-Measure) Fully Automatic","Features, Discriminative Training Sampling, Data, Co-training","Iterative, Greedy, Dynamic Programming ILP, Sub-Modular Selection","Segmentation, ASR Acoustic Information, Disfluency 32"]},{"title":"The background corpus takes more central stage Learn topics from the background corpus ","paragraphs":["topic ~ themes often discusses in the background topic representation ~ word probability tables Usually one time training step "]},{"title":"To summarize an input ","paragraphs":["Select sentences from the input that correspond to the most prominent topics 17 33"]},{"title":"Latent semantic analysis (LSA)","paragraphs":["[Gong and Liu, 2001, Hachey et al., 2006, Steinberger et al., 2007]","Discover topics from the background corpus with n unique words and d documents Represent the background corpus as nxd matrix A Rows correspond to words Aij=number of times word I appears in document j Use standard change of coordinate system and dimensionality","reduction techniques In the new space each row corresponds to the most important","topics in the corpus Select the best sentence to cover each topic T"]},{"title":"UPVA =","paragraphs":["34"]},{"title":"Notes on LSA and other approaches The original article that introduced LSA for single document summarization of news did not find significant difference with TF*IDF For multi-document summarization of news LSA approaches have not outperformed topic words or extensions of frequency approaches Other topic/content models have been much more influential","paragraphs":["18 35"]},{"title":"Domain dependent content models Get sample documents from the domain ","paragraphs":["background corpus "]},{"title":"Cluster sentences from these documents ","paragraphs":["Implicit topics "]},{"title":"Obtain a word probability table for each topic ","paragraphs":["Counts only from the cluster representing the topic "]},{"title":"Select sentences from the input with highest probability for main topics","paragraphs":["36"]},{"title":"Text structure can be learnt ","paragraphs":["Human-written examples from a domain Location, time relief efforts magnitude damage 36 19 37"]},{"title":"Topic = cluster of similar sentences from the background corpus ","paragraphs":["Sentences cluster from earthquake articles Topic “earthquake location”","The Athens seismological institute said the temblor’s epicenter was located 380 kilometers (238 miles) south of the capital.","Seismologists in Pakistan’s Northwest Frontier Province said the temblor’s epicenter was about 250 kilometers (155 miles) north of the provincial capital Peshawar.","The temblor was centered 60 kilometers (35 miles) north- west of the provincial capital of Kunming, about 2,200 kilometers (1,300 miles) southwest of Beijing, a bureau seismologist said. 38"]},{"title":"Content model","paragraphs":["[Barzilay and Lee, 2004, Pascale et al., 2003]","Hidden Markov Model (HMM)-based States - clusters of related sentences “topics” Transition prob. - sentence precedence in corpus Emission prob. - bigram language model location, magnitude casualties relief efforts )|()|(),|,( 11111 +++++ ⋅=><>< iieiitiiii hsphhphshsp Earthquake reports Transition from previous topic Generating sentence in current topic 38 20 39"]},{"title":"Learning the content model ","paragraphs":["Many articles from the same domain","Cluster sentences: each cluster represents a topic from the domain Word probability tables for each topic","Transitions between clusters can be computed from sentence adjacencies in the original articles Probabilities of going from one topic to another","Iterate between clustering and transition probability estimation to obtain domain model 40"]},{"title":"To select a summary Find main topics in the domain ","paragraphs":["using a small collection of summary-input pairs "]},{"title":"Find the most likely topic for each sentence in the input Select the best sentence per main topic","paragraphs":["21 41"]},{"title":"Historical note Some early approaches to multi-document summarization relied on clustering the sentences in the input alone","paragraphs":["[McKeown et al., 1999, Siddharthan et al., 2004]","Clusters of similar sentences represent a theme in the input Clusters with more sentences are more important Select one sentence per important cluster 42"]},{"title":"Example cluster","paragraphs":["Choose one sentence to represent the cluster","1. PAL was devastated by a pilots' strike in June and by the region's currency crisis.","2. In June, PAL was embroiled in a crippling three-week pilots' strike.","3. Tan wants to retain the 200 pilots because they stood by him when the majority of PAL's pilots staged a devastating strike in June. 22 43"]},{"title":"Bayesian content models ","paragraphs":["Takes a batch of inputs for summarization","Many word probability tables One for general English One for each of the inputs to be summarized One for each document in any input To select a summary S with L words from document collection D given as input The goal is to select the summary, not a sentence. Greedy selection vs. global will be discussed in detail later S* = minS:words(S)≤LKL(PD||PS) 44"]},{"title":"KL divergence ","paragraphs":["Distance between two probability distributions: P, Q P, Q: Input and summary word distributions KL (P || Q) = pP (w) log2 pP (w) pQ (w)w"]},{"title":"∑","paragraphs":["23 45"]},{"title":"Intriguing side note In the full Bayesian topic models, word probabilities for all words is more important than binary distinctions of topic and non-topic word Haghighi and Vanderwende report that a system that chooses the summary with highest expected number of topic words performs as SumBasic","paragraphs":["46"]},{"title":"Review Frequency based informativeness has been used in building summarizers Topic words probably more useful Topic models ","paragraphs":["Latent Semantic Analysis Domain dependent content model Bayesian content model 24 47 Motivation & Definition Topic Models Graph Based Methods Supervised Techniques Optimization Methods Speech Summarization Evaluation Frequency, TF*IDF, Topic Words Topic Models [LSA, EM, Bayesian]","Manual (Pyramid), Automatic (Rouge, F-Measure) Fully Automatic","Features, Discriminative Training Sampling, Data, Co-training","Iterative, Greedy, Dynamic Programming ILP, Sub-Modular Selection","Segmentation, ASR Acoustic Information, Disfluency 48"]},{"title":"Using graph representations","paragraphs":["[Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Leskovec et al., 2005 ] "]},{"title":"Nodes ","paragraphs":["Sentences Discourse entities "]},{"title":"Edges ","paragraphs":["Between similar sentences Between syntactically related entities "]},{"title":"Computing sentence similarity ","paragraphs":["Distance between their TF*IDF weighted vector representations 25 49 50","Sentence : Iraqi vice president...","Sentence : Ivanov contended... Sim(d1s1, d3s2) 26 51"]},{"title":"Advantages of the graph model Combines word frequency and sentence clustering Gives a formal model for computing importance: random walks ","paragraphs":["Normalize weights of edges to sum to 1","They now represent probabilities of transitioning from one node to another 52"]},{"title":"Random walks for summarization Represent the input text as graph Start traversing from node to node ","paragraphs":["following the transition probabilities occasionally hopping to a new node "]},{"title":"What is the probability that you are in any particular node after doing this process for a certain time? ","paragraphs":["Standard solution (stationary distribution) This probability is the weight of the sentence 27 53 Motivation & Definition Topic Models Graph Based Methods Supervised Techniques Global Optimization Methods Speech Summarization Evaluation Frequency, TF*IDF, Topic Words Topic Models [LSA, EM, Bayesian]","Manual (Pyramid), Automatic (Rouge, F-Measure) Fully Automatic","Features, Discriminative Training Sampling, Data, Co-training","Iterative, Greedy, Dynamic Programming ILP, Sub-Modular Selection","Segmentation, ASR Acoustic Information, Disfluency 54"]},{"title":"Supervised methods ","paragraphs":["For extractive summarization, the task can be represented as binary classification A sentence is in the summary or not","Use statistical classifiers to determine the score of a sentence: how likely it’s included in the summary Feature representation for each sentence Classification models trained from annotated data","Select the sentences with highest scores (greedy for now, see other selection methods later) 28 55"]},{"title":"Features Sentence length ","paragraphs":["long sentences tend to be more important "]},{"title":"Sentence weight ","paragraphs":["cosine similarity with documents sum of term weights for all words in a sentence calculate term weight after applying LSA 56"]},{"title":"Features Sentence position ","paragraphs":["beginning is often more important","some sections are more important (e.g., in conclusion section) "]},{"title":"Cue words/phrases ","paragraphs":["frequent n-grams cue phrases (e.g., in summary, as a conclusion) named entities 29 57"]},{"title":"Features Contextual features ","paragraphs":["features from context sentences difference of a sentence and its neighboring ones "]},{"title":"Speech related features (more later): ","paragraphs":["acoustic/prosodic features","speaker information (who said the sentence, is the speaker dominant?) speech recognition confidence measure 58"]},{"title":"Classifiers Can classify each sentence individually, or use sequence modeling ","paragraphs":["Maximum entropy [Osborne, 2002] Condition random fields (CRF) [Galley, 2006] Classic Bayesian Method [Kupiec et al., 1995] HMM [Conroy and O'Leary, 2001; Maskey, 2006 ] Bayesian networks SVMs [Xie and Liu, 2010] Regression [Murray et al., 2005] Others 30 59"]},{"title":"So that is it with supervised methods? It seems it is a straightforward classification problem What are the issues with this method? ","paragraphs":["How to get good quality labeled training data How to improve learning "]},{"title":"Some recent research has explored a few directions ","paragraphs":["Discriminative training, regression, sampling, co-training, active learning 60 Motivation & Definition Topic Models Graph Based Methods Supervised Techniques Global Optimization Methods Speech Summarization Evaluation Frequency, TF*IDF, Topic Words Topic Models [LSA, EM, Bayesian]","Manual (Pyramid), Automatic (Rouge, F-Measure) Fully Automatic","Features, Discriminative Training Sampling, Data, Co-training","Iterative, Greedy, Dynamic Programming ILP, Sub-Modular Selection","Segmentation, ASR Acoustic Information, Disfluency 31 61"]},{"title":"Improving supervised methods: different training approaches What are the problems with standard training methods? ","paragraphs":["Classifiers learn to determine a sentence’s label (in summary or not)","Sentence-level accuracy is different from summarization evaluation criterion (e.g., summary-level ROUGE scores) Training criterion is not optimal","Sentences’ labels used in training may be too strict (binary classes) 62"]},{"title":"Improving supervised methods: MERT discriminative training Discriminative training based on MERT","paragraphs":["[Aker et al., 2010]","In training, generate multiple summary candidates (using A* search algorithm)","Adjust model parameters (feature weights) iteratively to optimize ROUGE scores  32 63"]},{"title":"Improving supervised methods: ranking approaches Ranking approaches","paragraphs":["[Lin et al. 2010]","Pair-wise training Not classify each sentence individually Input to learner is a pair of sentences Use Rank SVM to learn the order of two sentences","Direct optimization Learns how to correctly order/rank summary candidates","(a set of sentences) Use AdaRank [Xu and Li 2007] to combine weak rankers 64"]},{"title":"Improving supervised methods: regression model Use regression model","paragraphs":["[Xie and Liu, 2010] In training, a sentence’s label is not +1 and -1","Each one is labeled with numerical values to represent their importance Keep +1 for summary sentence For non-summary sentences (-1), use their similarity to","the summary as labels","Train a regression model to better discriminate sentence candidates 33 65"]},{"title":"Improving supervised methods: sampling Problems -- in binary classification setup for summarization, the two classes are imbalanced ","paragraphs":["Summary sentences are minority class. Imbalanced data can hurt classifier training "]},{"title":"How can we address this? ","paragraphs":["Sampling to make distribution more balanced to train classifiers Has been studied a lot in machine learning 66"]},{"title":"Improving supervised methods: sampling Upsampling: increase minority samples ","paragraphs":["Replicate existing minority samples","Generate synthetic examples (e.g., by some kind of interpolation) "]},{"title":"Downsampling: reduce majority samples ","paragraphs":["Often randomly select from existing majority samples 34 67"]},{"title":"Improving supervised methods: sampling ","paragraphs":["Sampling for summarization [Xie and Liu, 2010] Different from traditional upsampling and downsampling Upsampling select non-summary sentences that are like summary","sentences based on cosine similarity or ROUGE scores change their label to positive","Downsampling: select those that are different from summary sentences","These also address some human annotation disagreement The instances whose labels are changed are often the ones","that humans have problems with 68 Motivation & Definition Topic Models Graph Based Methods Supervised Techniques Global Optimization Methods Speech Summarization Evaluation Frequency, TF*IDF, Topic Words Topic Models [LSA, EM, Bayesian]","Manual (Pyramid), Automatic (Rouge, F-Measure) Fully Automatic Features, Discriminative Training Sampling, Data, Co-raining","Iterative, Greedy, Dynamic Programming ILP, Sub-Modular Selection","Segmentation, ASR Acoustic Information, Disfluency 35 69"]},{"title":"Supervised methods: data issues Need labeled data for model training How do we get good quality training data? ","paragraphs":["Can ask human annotators to select extractive summary sentences However, human agreement is generally low "]},{"title":"What if data is not labeled at all? or it only has abstractive summary?","paragraphs":["7070 Distributions of content units and words are similar","Few units are expressed by everyone; many units are expressed by only one person"]},{"title":"Do humans agree on summary sentence selection?","paragraphs":["Human agreement on word/sentence/fact selection 36 71"]},{"title":"Supervised methods: semi-supervised learning Question – can we use unlabeled data to help supervised methods? A lot of research has been done on semisupervised learning for various tasks Co-training and active learning have been used in summarization","paragraphs":["72"]},{"title":"Co-training Use co-training to leverage unlabeled data ","paragraphs":["Feature sets represent different views","They are conditionally independent given the class label Each is sufficient for learning","Select instances based on one view, to help the other classifier 37 73"]},{"title":"Co-training in summarization In text summarization","paragraphs":["[Wong et al., 2008]","Two classifiers (SVM, naïve Bayes) are used on the same feature set "]},{"title":"In speech summarization","paragraphs":["[Xie et al., 2010] Two different views: acoustic and lexical features","They use both sentence and document as selection units 74"]},{"title":"Active learning in summarization Select samples for humans to label ","paragraphs":["Typically hard samples, machines are not confident, informative ones "]},{"title":"Active learning in lecture summarization","paragraphs":["[Zhang et al. 2009]","Criterion: similarity scores between the extracted summary sentences and the sentences in the lecture slides are high 38 75"]},{"title":"Supervised methods: using labeled abstractive summaries Question -- what if I only have abstractive summaries, but not extractive summaries? No labeled sentences to use for classifier training in extractive summarization Can use reference abstract summary to automatically create labels for sentences ","paragraphs":["Use similarity of a sentence to the human written abstract (or ROUGE scores, other metrics) 76"]},{"title":"Comment on supervised performance Easier to incorporate more information At the cost of requiring a large set of human annotated training data Human agreement is low, therefore labeled training data is noisy Need matched training/test conditions ","paragraphs":["may not easily generalize to different domains "]},{"title":"Effective features vary for different domains ","paragraphs":["e.g., position is important for news articles 39 77"]},{"title":"Comments on supervised performance Seems supervised methods are more successful in speech summarization than in text ","paragraphs":["Speech summarization is almost never multi-document","There are fewer indications about the topic of the input in speech domains","Text analysis techniques used in speech summarization are relatively simpler 78 Motivation & Definition Topic Models Graph Based Methods Supervised Techniques Optimization Methods Speech Summarization Evaluation Frequency, TF*IDF, Topic Words Topic Models [LSA, EM, Bayesian]","Manual (Pyramid), Automatic (Rouge, F-Measure) Fully Automatic","Features, Discriminative Training Sampling, Data, Co-training","Iterative, Greedy, Dynamic Programming ILP, Sub-Modular Selection","Segmentation, ASR Acoustic Information, Disfluency 40 79"]},{"title":"Parameters to optimize In summarization methods we try to find","paragraphs":["1. Most significant sentences 2. Remove redundant ones 3. Keep the summary under given length "]},{"title":"Can we combine all 3 steps in one? ","paragraphs":["Optimize all 3 parameters at once 80"]},{"title":"Summarization as an optimization problem ","paragraphs":["Knapsack Optimization Problem Select boxes such that amount of money is maximized while keeping total weight under X Kg Summarization Problem Select sentences such that summary relevance is maximized while keeping total length under X words Many other similar optimization problems","General Idea: Maximize a function given a set of constraints 41 81"]},{"title":"Optimization methods for summarization ","paragraphs":["Different flavors of solutions Greedy Algorithm Choose highest valued boxes Choose the most relevant sentence","Dynamic Programming algorithm Save intermediate computations Look at both relevance and length","Integer Linear Programming Exact Inference Scaling Issues We will now discuss these 3 types of optimization solutions 82 Motivation & Definition Topic Models Graph Based Methods Supervised Techniques Optimization Methods Speech Summarization Evaluation Frequency, TF*IDF, Topic Words Topic Models [LSA, EM, Bayesian]","Manual (Pyramid), Automatic (Rouge, F-Measure) Fully Automatic","Features, Discriminative Training Sampling, Data, Co-training Greedy, Dynamic Programming ILP, Sub-Modular Selection","Segmentation, ASR Acoustic Information, Disfluency 42 83"]},{"title":"Greedy optimization algorithms ","paragraphs":["Greedy solution is an approximate algorithm which may not be optimal","Choose the most relevant + least redundant sentence if the total length does not exceed the summary length Maximal Marginal Relevance is one such greedy algorithm","proposed by [Carbonell et al., 1998] 84"]},{"title":"Maximal Marginal Relevance (MMR)","paragraphs":["[Carbonell et al., 1998]","Summary: relevant and non-redundant information Many summaries are built based on sentences ranked by","relevance E.g. Extract most relevant 30% of sentences"]},{"title":"Relevance Redundancy","paragraphs":["vs.","Summary should maximize relevant information as well as reduce redundancy 43 85"]},{"title":"Marginal relevance ","paragraphs":["“Marginal Relevance” or “Relevant Novelty” Measure relevance and novelty separately Linearly combine these two measures","High Marginal relevance if Sentence is relevant to story (significant information) Contains minimal similarity to previously selected sentences","(new novel information)","Maximize Marginal Relevance to get summary that has significant non-redundant information 86"]},{"title":"Relevance with query or centroid We can compute relevance of text snippet with respect to query or centroid Centroid as defined in","paragraphs":["[Radev, 2004] based on the content words of a document TF*IDF vector of all documents in corpus","Select words above a threshold : remaining vector is a centroid vector 44 87"]},{"title":"Maximal Marginal Relevance (MMR)","paragraphs":["[Carbonell et al., 1998] Q – document centroid/user query D – document collection R – ranked listed S – subset of documents in R already selected Sim – similarity metric Lambda =1 produces most significant ranked list Lambda = 0 produces most diverse ranked list MMR ≈ Argmax(Di∈R−S)[λ(Sim1(Di, Q))−(1−λ)max(Dj∈S)Sim2(Di, Dj)] 88"]},{"title":"MMR based Summarization","paragraphs":["[Zechner, 2000] Iteratively select next sentence Next Sentence = Frequency Vector of all content words centroid 45 89"]},{"title":"MMR based summarization Why this iterative sentence selection process works? ","paragraphs":["1st Term: Find relevant sentences similar to centroid of the document","2nd Term: Find redundancy ─ sentences that are similar to already selected sentences are not selected 90 "]},{"title":"MMR is an iterative sentence selection process ","paragraphs":["decision made for each sentence Is this selected sentence globally optimal?"]},{"title":"Sentence selection in MMR","paragraphs":["Sentence with same level of relevance but shorter may not be selected if a longer relevant sentence is already selected 46 91 Motivation & Definition Topic Models Graph Based Methods Supervised Techniques Optimization Methods Speech Summarization Evaluation Frequency, TF*IDF, Topic Words Topic Models [LSA, EM, Bayesian]","Manual (Pyramid), Automatic (Rouge, F-Measure) Fully Automatic","Features, Discriminative Training Sampling, Data, Co-training","Iterative, Greedy, Dynamic Programming ILP, Sub-Modular Selection","Segmentation, ASR Acoustic Information, Disfluency 92"]},{"title":"Global inference D=t1 , t2 , , tn−1 , tn Modify our greedy algorithm ","paragraphs":["add constraints for sentence length as well "]},{"title":"Let us define document D with tn textual units","paragraphs":["47 93"]},{"title":"Global inference Let us define Relevance of ti to be in the summary Redundancy between ti and tj Length of til(i) Red(i,j) Rel(i)","paragraphs":["94"]},{"title":"Inference problem","paragraphs":["[McDonald, 2007] "]},{"title":"Let us define inference problem as","paragraphs":["Summary Score Pairwise Redundancy Maximum Length 48 95"]},{"title":"Greedy solution","paragraphs":["[McDonald, 2007] Sort by Relevance Select Sentence Sorted list may have longer sentences at the top Solve it using dynamic programming","Create table and fill it based on length and redundancy requirements No consideration of sentence length 96"]},{"title":"Dynamic programming solution","paragraphs":["[McDonald, 2007]","High scoring summary","of length k and i-1","text unitsHigh scoring","summary of","length k-l(i) +","ti Higher ? 49 97 "]},{"title":"Better than the previously shown greedy algorithm Maximizes the space utilization by not inserting longer sentences These are still approximate algorithms: performance loss? Dynamic programming algorithm","paragraphs":["[McDonald, 2007] 98"]},{"title":"Inference algorithms comparison","paragraphs":["[McDonald, 2007] System 50 100 200 Baseline 26.6/5.3 33.0/6.8 39.4/9.6 Greedy 26.8/5.1 33.5/6.9 40.1/9.5 Dynamic Program 27.9/5.9 34.8/7.3 41.2/10.0 Summarization results: Rouge-1/Rouge-2 Sentence Length 50 99 Motivation & Definition Topic Models Graph Based Methods Supervised Techniques Optimization Methods Speech Summarization Evaluation Frequency, TF*IDF, Topic Words Topic Models [LSA, EM, Bayesian]","Manual (Pyramid), Automatic (Rouge, F-Measure) Fully Automatic","Features, Discriminative Training Sampling, Data, Co-training","Iterative, Greedy, Dynamic Programming ILP, Sub-Modular Selection","Segmentation, ASR Acoustic Information, Disfluency 100"]},{"title":"Integer Linear Programming (ILP)","paragraphs":["[Gillick and Favre, 2009; Gillick et al., 2009; McDonald, 2007] Greedy algorithm is an approximate solution Use exact solution algorithm with ILP (scaling issues though) ILP is constrained optimization problem Cost and constraints are linear in a set of integer variables Many solvers on the web Define the constraints based on relevance and redundancy for summarization Sentence based ILP N-gram based ILP 51 101"]},{"title":"Sentence-level ILP formulation","paragraphs":["[McDonald, 2007] 1 if ti in summary Constraints Optimization Function 102"]},{"title":"N-gram ILP formulation","paragraphs":["[Gillick and Favre, 2009; Gillick et al., 2009] "]},{"title":"Sentence-ILP constraint on redundancy is based on sentence pairs Improve by modeling n-gram-level redundancy Redundancy implicitly defined","paragraphs":["Ci indicates presence of n-gram i in summary and its weight is wi"]},{"title":"∑ i wi ci","paragraphs":["52 103"]},{"title":"N-gram ILP formulation","paragraphs":["[Gillick and Favre, 2009] Constraints","Optimization Function n-gram level ILP has different optimization function than one shown before 104"]},{"title":"Sentence vs. n-gram ILP System ROUGE-2 Pyramid Baseline 0.058 0.186 Sentence ILP","paragraphs":["[McDonald, 2007]"]},{"title":"0.072 0.295 N-gram ILP","paragraphs":["[Gillick and Favre, 2009]"]},{"title":"0.110 0.345","paragraphs":["53 105"]},{"title":"Other optimization based summarization algorithms Submodular selection","paragraphs":["[Lin et al., 2009] Submodular set functions for optimization "]},{"title":"Modified greedy algorithm","paragraphs":["[Filatova, 2004] Event based features "]},{"title":"Stack decoding algorithm","paragraphs":["[Yih et al., 2007]","Multiple stacks, each stack represents hypothesis of different length "]},{"title":"A* Search","paragraphs":["[Aker et al., 2010] Use scoring and heuristic functions 106"]},{"title":"Submodular selection for summarization","paragraphs":["[Lin et al., 2009]","Summarization Setup V – set of all sentences in document S – set of extraction sentences f(.) scores the quality of the summary","Submodularity been used in solving many optimization problems in near polynomial time For summarization:","Select subset S (sentences) representative of V given the constraint |S| =< K (budget) 54 107"]},{"title":"Submodular selection","paragraphs":["[Lin et al., 2009]","If V are nodes in a Graph G=(V,E) representing sentences","And E represents edges (i,j) such that w(i,j) represents similarity between sentences i and j","Introduce submodular set functions which measures “representative” S of entire set V [Lin et al., 2009] presented 4 submodular set functions 108"]},{"title":"Submodular selection for summarization","paragraphs":["[Lin et al., 2009]"]},{"title":"Comparison of results using different methods","paragraphs":["55 109"]},{"title":"Review: optimization methods ","paragraphs":["Global optimization methods have shown to be superior than 2-step selection process and reduce redundancy","3 parameters are optimized together Relevance Redundancy Length","Various Algorithms for Global Inference Greedy Dynamic Programming Integer Linear Programming Submodular Selection 110 Motivation & Definition Topic Models Graph Based Methods Supervised Techniques Optimization Methods Speech Summarization Evaluation Frequency, TF*IDF, Topic Words Topic Models [LSA, EM, Bayesian]","Manual (Pyramid), Automatic (Rouge, F-Measure) Fully Automatic","Features, Discriminative Training Sampling, Data, Co-training","Iterative, Greedy, Dynamic Programming ILP, Sub-Modular Selection","Segmentation, ASR Acoustic Information, Disfluency 56 111"]},{"title":"Speech summarization Increasing amount of data available in speech form ","paragraphs":["meetings, lectures, broadcast, youtube, voicemail "]},{"title":"Browsing is not as easy as for text domains ","paragraphs":["users need to listen to the entire audio "]},{"title":"Summarization can help effective information access Summary output can be in the format of text or speech","paragraphs":["112"]},{"title":"Domains Broadcast news Lectures/presentations Multiparty meetings Telephone conversations Voicemails","paragraphs":["57 113"]},{"title":"Example","paragraphs":["Meeting transcripts and summary sentences (in red)                  Broadcast news transcripts and summary (in red)                  114"]},{"title":"Speech vs. text summarization: similarities When high quality transcripts are available ","paragraphs":["Not much different from text summarization Many similar approaches have been used Some also incorporate acoustic information "]},{"title":"For genres like broadcast news, style is also similar to text domains","paragraphs":["58 115"]},{"title":"Speech vs. text summarization: differences Challenges in speech summarization ","paragraphs":["Speech recognition errors can be very high","Sentences are not as well formed as in most text domains: disfluencies, ungrammatical There are not clearly defined sentences","Information density is also low (off-topic discussions, chit chat, etc.) Multiple participants 116 Motivation & Definition Topic Models Graph Based Methods Supervised Techniques Optimization Methods Speech Summarization Evaluation Frequency, TF*IDF, Topic Words Topic Models [LSA, EM, Bayesian]","Manual (Pyramid), Automatic (Rouge, F-Measure) Fully Automatic","Features, Discriminative Training Sampling, Data, Co-training","Iterative, Greedy, Dynamic Programming ILP, Sub-Modular Selection","Segmentation, ASR Acoustic Information, Disfluency 59 117"]},{"title":"What should be extraction units in speech summarization? Text domain ","paragraphs":["Typically use sentences (based on punctuation marks) "]},{"title":"Speech domain ","paragraphs":["Sentence information is not available Sentences are not as clearly defined Utterance from previous example:   118"]},{"title":"Automatic sentence segmentation (side note) ","paragraphs":["For a word boundary, determine whether it’s a sentence boundary","Different approaches: Generative: HMM Discriminative: SVM, boosting, maxent, CRF Information used: word n-gram, part-of-speech, parsing","information, acoustic info (pause, pitch, energy) 60 119"]},{"title":"What is the effect of different units/segmentation on summarization? Research has used different units in speech summarization ","paragraphs":["Human annotated sentences or dialog acts Automatic sentence segmentation Pause-based segments Adjacency pairs Intonational phrases Words 120"]},{"title":"What is the effect of different units/segmentation on summarization? Findings from previous studies ","paragraphs":["Using intonational phrases (IP) is better than automatic sentence segmentation, pause-based segmentation [Maskey, 2008 ] IPs are generally smaller than sentences, also","linguistically meaningful","Using sentences is better than words, between filler segments [Furui et al., 2004]","Using human annotated dialog acts is better than automatically generated ones [Liu and Xie, 2008] 61 121 Motivation & Definition Topic Models Graph Based Methods Supervised Techniques Optimization Methods Speech Summarization Evaluation Frequency, TF*IDF, Topic Words Topic Models [LSA, EM, Bayesian]","Manual (Pyramid), Automatic (Rouge, F-Measure) Fully Automatic","Features, Discriminative Training Sampling, Data, Co-training","Iterative, Greedy, Dynamic Programming ILP, Sub-Modular Selection","Segmentation, ASR Acoustic Information, Disfluency 122"]},{"title":"Using acoustic information in summarization Acoustic/prosodic features: ","paragraphs":["F0 (max, min, mean, median, range) Energy (max, min, mean, median, range) Sentence duration Speaking rate (# of words or letters) Need proper normalization "]},{"title":"Widely used in supervised methods, in combination with textual features","paragraphs":["62 123"]},{"title":"Using acoustic information in summarization Are acoustic features useful when combining it with lexical information? Results vary depending on the tasks and domains ","paragraphs":["Often lexical features are ranked higher","But acoustic features also contribute to overall system performance","Some studies showed little impact when adding speech information to textual features [Penn and Zhu, 2008] 124"]},{"title":"Using acoustic information in summarization ","paragraphs":["Can we use acoustic information only for speech summarization? Transcripts may not be available Another way to investigate contribution of acoustic","information","Studies showed using just acoustic information can achieve similar performance to using lexical information [Maskey and Hirschberg, 2005; Xie et al., 2009; Zhu et al., 2009]","Caveat: in some experiments, lexical information is used (e.g., define the summarization units) 63 125"]},{"title":"Speech recognition errors ASR is not perfect, often high word error rate ","paragraphs":["10-20% for read speech 40% or even higher for conversational speech "]},{"title":"Recognition errors generally have negative impact on summarization performance ","paragraphs":["Important topic indicative words are incorrectly recognized Can affect term weighting and sentence scores 126"]},{"title":"Speech recognition errors Some studies evaluated effect of recognition errors on summarization by varying word error rate","paragraphs":["[Christensen et al., 2003; Penn and Zhu, 2008; Lin et al., 2009] "]},{"title":"Degradation is not much when word error rate is not too low (similar to spoken document retrieval) ","paragraphs":["Reason: better recognition accuracy in summary sentences than overall 64 127"]},{"title":"What can we do about ASR errors? Deliver summary using original speech ","paragraphs":["Can avoid showing recognition errors in the delivered text summary","But still need to correctly identify summary sentences/segments "]},{"title":"Use recognition confidence measure and multiple candidates to help better summarize","paragraphs":["128"]},{"title":"Address problems due to ASR errors Re-define summarization task: select sentences that are most informative, at the same time have high recognition accuracy ","paragraphs":["Important words tend to have high recognition accuracy "]},{"title":"Use ASR confidence measure or n-gram language model scores in summarization ","paragraphs":["Unsupervised methods [Zechner, 2002; Kikuchi et al., 2003; Maskey, 2008] Use as a feature in supervised methods 65 129"]},{"title":"Address problems due to ASR errors Use multiple recognition candidates ","paragraphs":["n-best lists [Liu et al., 2010]","Lattices [Lin et al., 2010]","Confusion network [Xie and Liu, 2010] Use in MMR framework Summarization segment/unit contains all the word","candidates (or pruned ones based on probabilities) Term weights (TF, IDF) use candidate’s posteriors Improved performance over using 1-best recognition","output 130 Motivation & Definition Topic Models Graph Based Methods Supervised Techniques Optimization Methods Speech Summarization Evaluation Frequency, TF*IDF, Topic Words Topic Models [LSA, EM, Bayesian]","Manual (Pyramid), Automatic (Rouge, F-Measure) Fully Automatic","Features, Discriminative Training Sampling, Data, Co-training","Iterative, Greedy, Dynamic Programming ILP, Sub-Modular Selection","Segmentation, ASR Acoustic Information, Disfluency 66 131"]},{"title":"Disfluencies and summarization ","paragraphs":["Disfluencies (filler words, repetitions, revisions, restart, etc) are frequent in conversational speech Example from meeting transcript: "," ","Existence may hurt summarization systems, also affect human readability of the summaries 132"]},{"title":"Disfluencies and summarization Natural thought: remove disfluenices Word-based selection can avoid disfluent words Using n-gram scores tends to select fluent parts","paragraphs":["[Hori and Furui, 2001] "]},{"title":"Remove disfluencies first, then perform summarization Does it work? not consistent results ","paragraphs":["Small improvement [Maskey, 2008; Zechner, 2002] No improvement [Liu et al., 2007] 67 133"]},{"title":"Disfluencies and summarization ","paragraphs":["In supervised classification, information related to disfluencies can be used as features for summarization Small improvement on Switchboard data [Zhu and Penn, 2006]","Going beyond disfluency removal, can perform sentence compression in conversational speech to remove un-necessary words [Liu and Liu, 2010] Help improve sentence readability Output is more like abstractive summaries Compression helps summarization 134"]},{"title":"Review on speech summarization Speech summarization has been performed for different domains A lot of text-based approaches have been adopted Some speech specific issues have been investigated ","paragraphs":["Segmentation ASR errors Disfluencies Use acoustic information 68 135 Motivation & Definition Topic Models Graph Based Methods Supervised Techniques Global Optimization Methods Speech Summarization Evaluation Frequency, TF*IDF, Topic Words Topic Models [LSA, EM, Bayesian]","Manual (Pyramid), Automatic (Rouge, F-Measure) Fully Automatic","Features, Discriminative Training Sampling, Data, Co-training","Iterative, Greedy, Dynamic Programming ILP, Sub-Modular Selection","Segmentation, ASR Acoustic Information, Disfluency 136"]},{"title":"Manual evaluations Task-based evaluations ","paragraphs":["too expensive Bad decisions possible, hard to fix "]},{"title":"Assessors rate summaries on a scale ","paragraphs":["Responsiveness "]},{"title":"Assessors compare with gold-standards ","paragraphs":["Pyramid 69 137"]},{"title":"Automatic and fully automatic evaluation Automatically compare with gold-standard ","paragraphs":["Precision/recall (sentence level) ROUGE (word level) "]},{"title":"No human gold-standard is used ","paragraphs":["Automatically compare input and summary 138"]},{"title":"Precision and recall for extractive summaries Ask a person to select the most important sentences Recall: system-human choice overlap/sentences chosen by human Precision: system-human choice overlap/sentences chosen by system","paragraphs":["70 139"]},{"title":"Problems? Different people choose different sentences The same summary can obtain a recall score that is between 25% and 50% different depending on which of two available human extracts is used for evaluation Recall more important/informative than precision?","paragraphs":["140"]},{"title":"More problems? Granularity","paragraphs":["We need help. Fires have spread in the nearby forest and threaten several villages in this remote area. "]},{"title":"Semantic equivalence ","paragraphs":["Especially in multi-document summarization","Two sentences convey almost the same information: only one will be chosen in the human summary 71 141 Pyramid Responsiveness ROUGE Fully automatic Model summaries Manual comparison/ ratings"]},{"title":"Evaluation methods for content","paragraphs":["142"]},{"title":"Pyramid method","paragraphs":["[Nenkova and Passonneau, 2004; Nenkova et al., 2007] "]},{"title":"Based on Semantic Content Units (SCU) Emerge from the analysis of several texts Link different surface realizations with the same meaning","paragraphs":["72 143"]},{"title":"SCU example","paragraphs":["S1 Pinochet arrested in London on Oct 16 at a Spanish judge’s request for atrocities against Spaniards in Chile.","S2 Former Chilean dictator Augusto Pinochet has been arrested in London at the request of the Spanish government.","S3 Britain caused international controversy and Chilean turmoil by arresting former Chilean dictator Pinochet in London. 144"]},{"title":"SCU: label, weight, contributors Label London was where Pinochet was arrested Weight=3","paragraphs":["S1 Pinochet arrested in London on Oct 16 at a Spanish judge’s request for atrocities against Spaniards in Chile.","S2 Former Chilean dictator Augusto Pinochet has been arrested in London at the request of the Spanish government.","S3 Britain caused international controversy and Chilean turmoil by arresting former Chilean dictator Pinochet in London. 73 145"]},{"title":"Ideally informative summary Does not include an SCU from a lower tier unless all SCUs from higher tiers are included as well","paragraphs":["146"]},{"title":"Ideally informative summary Does not include an SCU from a lower tier unless all SCUs from higher tiers are included as well","paragraphs":["74 147"]},{"title":"Ideally informative summary Does not include an SCU from a lower tier unless all SCUs from higher tiers are included as well","paragraphs":["148"]},{"title":"Ideally informative summary Does not include an SCU from a lower tier unless all SCUs from higher tiers are included as well","paragraphs":["75 149"]},{"title":"Ideally informative summary Does not include an SCU from a lower tier unless all SCUs from higher tiers are included as well","paragraphs":["150"]},{"title":"Ideally informative summary Does not include an SCU from a lower tier unless all SCUs from higher tiers are included as well","paragraphs":["76 151"]},{"title":"Different equally good summaries ","paragraphs":["Pinochet arrested Arrest in London","Pinochet is a former Chilean dictator","Accused of atrocities against Spaniards 152"]},{"title":"Different equally good summaries ","paragraphs":["Pinochet arrested Arrest in London On Spanish warrant Chile protests 77 153"]},{"title":"Diagnostic ─ why is a summary bad? ","paragraphs":["Good Less relevant summary 154"]},{"title":"Importance of content Can observe distribution in human summaries ","paragraphs":["Assign relative importance Empirical rather than subjective "]},{"title":"The more people agree, the more important","paragraphs":["78 155"]},{"title":"Pyramid score for evaluation New summary with n content units Estimates the percentage of information that is maximally important","paragraphs":["IdealWightightObservedWe Ideal Weight n i i n i i ="]},{"title":"∑ ∑","paragraphs":["= = 1 1 156"]},{"title":"ROUGE","paragraphs":["[Lin, 2004] "]},{"title":"De facto standard for evaluation in text summarization ","paragraphs":["High correlation with manual evaluations in that domain "]},{"title":"More problematic for some other domains, particularly speech ","paragraphs":["Not highly correlated with manual evaluations","May fail to distinguish human and machine summaries 79 157"]},{"title":"ROUGE details ","paragraphs":["In fact a suite of evaluation metrics Unigram Bigram Skip bigram Longest common subsequence","Many settings concerning Stopwords Stemming Dealing with multiple models 158"]},{"title":"How to evaluate without human involvement?","paragraphs":["[Louis and Nenkova, 2009] "]},{"title":"A good summary should be similar to the input Multiple ways to measure similarity ","paragraphs":["Cosine similarity KL divergence JS divergence "]},{"title":"Not all work!","paragraphs":["80 159 "]},{"title":"Distance between two distributions as average KL divergence from their mean distribution JS divergence between input and summary","paragraphs":[")]||()||([)||( 21 ASummKLAInpKLSummInpJS += SummaryandInputofondistributimean SummInp","A , 2 + = 160"]},{"title":"Summary likelihood given the input ","paragraphs":["Probability that summary is generated according to term distribution in the input Higher likelihood ~ better summary Unigram Model Multinomial Model ii n rInp n Inp n Inp wwordofsummaryincountn vocabularysummaryr wpwpwp r =−",")()()( 21 21 K sizesummarynN wpwpwp i i n rInp n Inp n","Inpnn N r r =="]},{"title":"∑","paragraphs":[")()()( 21","1 21!! ! KK 81 161 Fraction of summary = input’s topic words","% of input’s topic words also appearing in summary Capture variety","Cosine similarity: input’s topic words and all summary words Fewer dimensions, more specific vectors"]},{"title":"Topic words identified by log-likelihood test","paragraphs":["162"]},{"title":"How good are these metrics?","paragraphs":["48 inputs, 57 systems JSD -0.880 -0.736 0.795 0.627 -0.763 -0.694 0.712 0.647 0.712 0.602 -0.688 -0.585 -0.188 -0.101 0.222 0.235 % input’s topic in summary KL div summ-input Cosine similarity % of summary = topic words KL div input-summ Unigram summ prob. Multinomial summ prob. -0.699 0.629Topic word similarity Pyramid Responsiveness Spearman correlation on macro level for the query focused task. 82 163","JSD correlations with pyramid scores even better than R1-recall","R2-recall is consistently better Can extend features using higher order n-grams"]},{"title":"How good are these metrics?","paragraphs":["0.870.90R2-recall 0.800.85R1-recall -0.73-0.88JSD Resp.Pyramid 164 Motivation & Definition Topic Models Graph Based Methods Supervised Techniques Global Optimization Methods Speech Summarization Evaluation Frequency, TF*IDF, Topic Words Topic Models [LSA, EM, Bayesian]","Manual (Pyramid), Automatic (Rouge, F-Measure) Fully Automatic","Features, Discriminative Training Sampling, Data, Co-training","Iterative, Greedy, Dynamic Programming ILP, Sub-Modular Selection","Segmentation, ASR Acoustic Information, Disfluency 83 165"]},{"title":"Current summarization research ","paragraphs":["Summarization for various new genres Scientific articles Biography Social media (blog, twitter) Other text and speech data","New task definition Update summarization Opinion summarization","New summarization approaches Incorporate more information (deep linguistic knowledge, information","from the web) Adopt more complex machine learning techniques","Evaluation issues Better automatic metrics Extrinsic evaluations And more... 166 "]},{"title":"Check out summarization papers at ACL this year Workshop at ACL-HLT 2011: ","paragraphs":["Automatic summarization for different genres, media, and languages [June 23, 2011] http://www.summarization2011.org/ 84 167"]},{"title":"References ","paragraphs":["Ahmet Aker, Trevor Cohn, Robert Gaizauska. 2010. Multi-document summarization using A* search and discriminative training. Proc. of EMNLP."," R. Barzilay and M. Elhadad. 2009. Text summarizations with lexical chains. In: I. Mani and M. Maybury (eds.): Advances in Automatic Text Summarization."," Jaime Carbonell and Jade Goldstein. 1998. The Use of MMR, Diversity-Based reranking for Reordering Documents and Producing Summaries. Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval."," H. Christensen, Y. Gotoh, B. Killuru, and S. Renals. 2003. Are Extractive Text Summarization Techniques Portable to Broadcast News? Proc. of ASRU."," John Conroy and Dianne O'Leary. 2001. Text Summarization via Hidden Markov Models. Proc. of SIGIR."," J. M. Conroy, J. D. Schlesinger, and D. P. OLeary. 2006. Topic-Focused Multi-Document Summarization Using an Approximate Oracle Score. Proc. COLING/ACL 2006. pp. 152-159."," Thomas Cormen, Charles E. Leiserson, and Ronald L. Rivest.1990. Introduction to algorithms. MIT Press."," G. Erkan and D. R. Radev.2004. LexRank: Graph-based Centrality as Salience in Text Summarization. Journal of Artificial Intelligence Research (JAIR)."," Pascale Fung, Grace Ngai, and Percy Cheung. 2003. Combining optimal clustering and hidden Markov models for extractive summarization. Proceedings of ACL Workshop on Multilingual Summarization.Sadoki Furui, T. Kikuchi, Y. Shinnaka, and C. Hori. 2004. Speech-to-text and Speech-to-speech Summarization of Spontaneous Speech. IEEE Transactions on Audio, Speech, and Language Processing. 12(4), pages 401-408."," Michel Galley. 2006. A Skip-Chain Conditional Random Field for Ranking Meeting Utterances by Importance. Proc. of EMNLP."," Dan Gillick, Benoit Favre. 2009. A scalable global model for summarization. Proceedings of the Workshop on Integer Linear Programming for Natural Language Processing."," Dan Gillick, Korbinian Riedhammer, Benoit Favre, Dilek Hakkani-Tur. 2009. A global optimization framework for meeting summarization. Proceedings of ICASSP."," Jade Goldstein, Vibhu Mittal, Jaime Carbonell, and Mark Kantrowitz. 2000. Multi-document summarization by sentence extraction. Proceedings of the 2000 NAACL-ANLP Workshop on Automatic Summarization. 168"]},{"title":"References ","paragraphs":["Y. Gong and X. Liu. 2001. Generic text summarization using relevance measure and latent semantic analysis. Proc. ACM SIGIR."," I. Gurevych and T. Nahnsen. 2005. Adapting Lexical Chaining to Summarize Conversational Dialogues. Proc. RANLP."," B. Hachey, G. Murray, and D. Reitter.2006. Dimensionality reduction aids term co-occurrence based multi-document summarization. In: SumQA 06: Proceedings of the Workshop on Task-Focused Summarization and Question Answering."," Aria Haghighi and Lucy Vanderwende. 2009. Exploring content models for multi-document summarization. Proc. of NAACL-HLT."," L. He, E. Sanocki, A. Gupta, and J. Grudin. 2000. Comparing presentation summaries: Slides vs. reading vs. listening. Proc. of SIGCHI on Human factors in computing systems."," C. Hori and Sadaoki Furui. 2001. Advances in Automatic Speech Summarization. Proc. of Eurospeech."," T. Kikuchi, S. Furui, and C. Hori. 2003. Automatic Speech Summarization based on Sentence Extractive and Compaction. Proc. of ICSLP."," Julian Kupiec, Jan Pedersen, and Francine Chen. 1995. A Trainable Document Summarizer. Proc. of SIGIR."," J. Leskovec, N. Milic-frayling, and M. Grobelnik. 2005. Impact of Linguistic Analysis on the Semantic Graph Coverage and Learning of Document Extracts. Proc. AAAI."," Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries, Workshop on Text Summarization Branches Out."," C.Y. Lin and E. Hovy. 2000. The automated acquisition of topic signatures for text summarization. Proc. COLING."," Hui Lin and Jeff Bilmes. 2010. Multi-document summarization via budgeted maximization of submodular functions. Proc. of NAACL."," Hui Lin and Jeff Bilmes and Shasha Xie. 2009. Graph-based Submodular Selection for Extractive Summarization. Proceedings of ASRU."," Shih-Hsiang Lin and Berlin Chen. 2009. Improved Speech Summarization with Multiple-hypothesis Representations and Kullback-Leibler Divergence Measures. Proc. of Interspeech."," Shih-Hsiang Lin, Berlin Chen, and H. Min Wang. 2009. A Comparative Study of Probabilistic Ranking Models for Chinese Spoken Document Summarization. ACM Transactions on Asian Language Information Processing. 85 169"]},{"title":"References ","paragraphs":["Shih Hsiang Lin, Yu Mei Chang, Jia Wen Liu, Berlin Chen. 2010 Leveraging Evaluation Metric-related Training Criteria for Speech Summarization. Proc. of ICASSP."," Fei Liu and Yang Liu. 2009. From Extractive to Abstractive Meeting Summaries: Can it be done by sentence compression? Proc. of ACL."," Fei Liu and Yang Liu. 2010. Using Spoken Utterance Compression for Meeting Summarization: A pilot study. Proc. of IEEE SLT."," Yang Liu and Shasha Xie. 2008. Impact of Automatic Sentence Segmentation on Meeting Summarization. Proc. of ICASSP."," Yang Liu, Feifan Liu, Bin Li, and Shasha Xie. 2007. Do Disfluencies Affect Meeting Summarization: A pilot study on the impact of disfluencies. Poster at MLMI."," Yang Liu, Shasha Xie, and Fei Liu. 2010. Using n-best Recognition Output for Extractive Summarization and Keyword Extraction in Meeting Speech. Proc. of ICASSP."," Annie Louis and Ani Nenkova. 2009. Automatically evaluating content selection in summarization without human models. Proceedings of EMNLP"," H.P. Luhn. 1958. The Automatic Creation of Literature Abstracts. IBM Journal of Research and Development 2(2)."," Inderjeet Mani, Gary Klein, David House, Lynette Hirschman, Therese Firmin, and Beth Sundheim. 2002. SUMMAC: a text summarization evaluation. Natual Language Engineering. 8,1 (March 2002), 43-68."," Manuel J. Mana-Lopez, Manuel De Buenaga, and Jose M. Gomez-Hidalgo. 2004. Multidocument summarization: An added value to clustering in interactive retrieval. ACM Trans. Inf. Systems."," Sameer Maskey. 2008. Automatic Broadcast News Summarization. Ph.D thesis. Columbia University."," Sameer Maskey and Julia Hirschberg. 2005. Comparing lexical, acoustic/prosodic, discourse and structural features for speech summarization. Proceedings of Interspeech."," Sameer Maskey and Julia Hirschberg. 2006. Summarizing Speech Without Text Using Hidden Markov Models. Proc. of HLT-NAACL."," Ryan McDonald. 2007. A Study of Global Inference Algorithms in Multi-document Summarization. Lecture Notes in Computer Science. Advances in Information Retrieval."," Kathleen McKeown, Rebecca J. Passonneau, David K. Elson, Ani Nenkova, and Julia Hirschberg. 2005. Do summaries help?. Proc. of SIGIR."," K. McKeown, J. L. Klavans, V. Hatzivassiloglou, R. Barzilay, and E. Eskin.1999. Towards multidocument summarization by reformulation: progress and prospects. Proc. AAAI 1999. 170"]},{"title":"References ","paragraphs":["R. Mihalcea and P. Tarau .2004. Textrank: Bringing order into texts. Proc. of EMNLP 2004."," G. Murray, S. Renals, J. Carletta, J. Moore. 2005. Evaluating Automatic Summaries of Meeting Recordings. Proc. of ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation."," G. Murray, T. Kleinbauer, P. Poller, T. Becker, S. Renals, and J. Kilgour. 2009. Extrinsic Summarization Evaluation: A Decision Audit Task. ACM Transactions on Speech and Language Processing."," A. Nenkova and R. Passonneau. 2004. Evaluating Content Selection in Summarization: The Pyramid Method. Proc. HLT-NAACL."," A. Nenkova, L. Vanderwende, and K. McKeown. 2006. A compositional context sensitive multi-document summarizer: exploring the factors that influence summarization. Proc. ACM SIGIR."," A. Nenkova, R. Passonneau, and K. McKeown. 2007. The Pyramid Method: Incorporating human content selection variation in summarization evaluation. ACM Trans. Speech Lang. Processing."," Miles Osborne. 2002. Using maximum entropy for sentence extraction. Proc. of ACL Workshop on Automatic Summarization."," Gerald Penn and Xiaodan Zhu. 2008. A critical Reassessement of Evaluation Baselines for Speech Summarization. Proc. of ACL-HLT."," Dmitri G. Roussinov and Hsinchun Chen. 2001. Information navigation on the web by clustering and summarizing query results. Inf. Process. Manage. 37, 6 (October 2001), 789-816."," B. Schiffman, A. Nenkova, and K. McKeown. 2002. Experiments in Multidocument Summarization. Proc. HLT."," A. Siddharthan, A. Nenkova, and K. Mckeown.2004. Syntactic Simplification for Improving Content Selection in Multi-Document Summarization. Proc. COLING."," H. Grogory Silber and Kathleen F. McCoy. 2002. Efficiently computed lexical chains as an intermediate representation for automatic text summarization. Computational. Linguist. 28, 4 (December 2002), 487-496."," J. Steinberger, M. Poesio, M. A. Kabadjov, and K. Jeek. 2007. Two uses of anaphora resolution in summarization. Inf. Process. Manage. 43(6)."," S. Tucker and S. Whittaker. 2008. Temporal compression of speech: an evaluation. IEEE Transactions on Audio, Speech and Language Processing, pages 790-796."," L. Vanderwende, H. Suzuki, C. Brockett, and A. Nenkova. 2007. Beyond SumBasic: Task-focused summarization with sentence simplification and lexical expansion. Information Processing and Management 43. 86 171"]},{"title":"References ","paragraphs":["Kam-Fai Wong, Mingli Wu, and Wenjie Li. 2008. Extractive Summarization using Supervised and Semi-supervised learning. Proc. of ACL."," Shasha Xie and Yang Liu. 2010. Improving Supervised Learning for Meeting Summarization using Sampling and Regression. Computer Speech and Language. V24, pages 495-514."," Shasha Xie and Yang Liu. 2010. Using Confusion Networks for Speech Summarization. Proc. of NAACL."," Shasha Xie, Dilek Hakkani-Tur, Benoit Favre, and Yang Liu. 2009. Integrating Prosodic Features in Extractive Meeting Summarization. Proc. of ASRU."," Shasha Xie, Hui Lin, and Yang Liu. 2010. Semi-supervised Extractive Speech Summarization via Co-training Algorithm. Proc. of Interspeech."," S. Ye, T.-S. Chua, M.-Y. Kan, and L. Qiu. 2007. Document concept lattice for text understanding and summarization. Information Processing and Management 43(6)."," W. Yih, J. Goodman, L. Vanderwende, and H. Suzuki. 2007. Multi-Document Summarization by Maximizing Informative Content-Words. Proc. IJCAI 2007."," Klaus Zechner. 2002. Automatic Summarization of Open-domain Multiparty Dialogues in Diverse Genres. Computational Linguistics. V28, pages 447-485."," Klaus Zechner and Alex Waibel. 2000. Minimizing word error rate in textual summaries of spoken language. Proceedings of the 1st North American chapter of the Association for Computational Linguistics conference."," Justin Zhang and Pascale Fung. 2009. Extractive Speech Summarization by Active Learning. Proc. of ASRU."," Xiaodan Zhu and Gerald Penn. 2006. Comparing the Roles of Textual, Acoustic and Spoken-language Features on Spontaneous Conversation Summarization. Proc. of HLT-NAACL."," Xiaodan Zhu, Gerald Penn, and F. Rudzicz. 2009. Summarizing Multiple Spoken Documents: Finding Evidence from Untranscribed Audio. Proc. of ACL."]}],"references":[{"authors":[{"first":"Ahmet","last":"Aker"},{"first":"Trevor","last":"Cohn"},{"first":"Robert","last":"Gaizauska"}],"year":"2010","title":"Multi-document summarization using A* search and discriminative training","source":"Ahmet Aker, Trevor Cohn, Robert Gaizauska. 2010. Multi-document summarization using A* search and discriminative training. Proc. of EMNLP."},{"authors":[{"first":"R.","last":"Barzilay"},{"first":"M.","last":"Elhadad"}],"year":"2009","title":"Text summarizations with lexical chains","source":" R. Barzilay and M. Elhadad. 2009. Text summarizations with lexical chains. In: I. Mani and M. Maybury (eds.): Advances in Automatic Text Summarization."},{"authors":[{"first":"Jaime","last":"Carbonell"},{"first":"Jade","last":"Goldstein"}],"year":"1998","title":"The Use of MMR, Diversity-Based reranking for Reordering Documents and Producing Summaries","source":" Jaime Carbonell and Jade Goldstein. 1998. The Use of MMR, Diversity-Based reranking for Reordering Documents and Producing Summaries. Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval."},{"authors":[{"first":"H.","last":"Christensen"},{"first":"Y.","last":"Gotoh"},{"first":"B.","last":"Killuru"},{"first":"S.","last":"Renals"}],"year":"2003","title":"Are Extractive Text Summarization Techniques Portable to Broadcast News? Proc","source":" H. Christensen, Y. Gotoh, B. Killuru, and S. Renals. 2003. Are Extractive Text Summarization Techniques Portable to Broadcast News? Proc. of ASRU."},{"authors":[{"first":"John","last":"Conroy"},{"first":"Dianne","last":"O'Leary"}],"year":"2001","title":"Text Summarization via Hidden Markov Models","source":" John Conroy and Dianne O'Leary. 2001. Text Summarization via Hidden Markov Models. Proc. of SIGIR."},{"authors":[{"first":"J.","middle":"M.","last":"Conroy"},{"first":"J.","middle":"D.","last":"Schlesinger"},{"first":"D.","middle":"P.","last":"OLeary"}],"year":"2006","title":"Topic-Focused Multi-Document Summarization Using an Approximate Oracle Score","source":" J. M. Conroy, J. D. Schlesinger, and D. P. OLeary. 2006. Topic-Focused Multi-Document Summarization Using an Approximate Oracle Score. Proc. COLING/ACL 2006. pp. 152-159."},{"authors":[{"first":"Thomas","last":"Cormen"},{"first":"Charles","middle":"E.","last":"Leiserson"},{"first":"Ronald","middle":"L.","last":"Rivest"}],"year":"1990","title":"Introduction to algorithms","source":" Thomas Cormen, Charles E. Leiserson, and Ronald L. Rivest.1990. Introduction to algorithms. MIT Press."},{"authors":[{"first":"G.","last":"Erkan"},{"first":"D.","middle":"R.","last":"Radev"}],"year":"2004","title":"LexRank: Graph-based Centrality as Salience in Text Summarization","source":" G. Erkan and D. R. Radev.2004. LexRank: Graph-based Centrality as Salience in Text Summarization. Journal of Artificial Intelligence Research (JAIR)."},{"authors":[{"first":"Pascale","last":"Fung"},{"first":"Grace","last":"Ngai"},{"first":"Percy","last":"Cheung"}],"year":"2003","title":"Combining optimal clustering and hidden Markov models for extractive summarization","source":" Pascale Fung, Grace Ngai, and Percy Cheung. 2003. Combining optimal clustering and hidden Markov models for extractive summarization. Proceedings of ACL Workshop on Multilingual Summarization.Sadoki Furui, T. Kikuchi, Y. Shinnaka, and C. Hori. 2004. Speech-to-text and Speech-to-speech Summarization of Spontaneous Speech. IEEE Transactions on Audio, Speech, and Language Processing. 12(4), pages 401-408."},{"authors":[{"first":"Michel","last":"Galley"}],"year":"2006","title":"A Skip-Chain Conditional Random Field for Ranking Meeting Utterances by Importance","source":" Michel Galley. 2006. A Skip-Chain Conditional Random Field for Ranking Meeting Utterances by Importance. Proc. of EMNLP."},{"authors":[{"first":"Dan","last":"Gillick"},{"first":"Benoit","last":"Favre"}],"year":"2009","title":"A scalable global model for summarization","source":" Dan Gillick, Benoit Favre. 2009. A scalable global model for summarization. Proceedings of the Workshop on Integer Linear Programming for Natural Language Processing."},{"authors":[{"first":"Dan","last":"Gillick"},{"first":"Korbinian","last":"Riedhammer"},{"first":"Benoit","last":"Favre"},{"first":"Dilek","last":"Hakkani-Tur"}],"year":"2009","title":"A global optimization framework for meeting summarization","source":" Dan Gillick, Korbinian Riedhammer, Benoit Favre, Dilek Hakkani-Tur. 2009. A global optimization framework for meeting summarization. Proceedings of ICASSP."},{"authors":[{"first":"Jade","last":"Goldstein"},{"first":"Vibhu","last":"Mittal"},{"first":"Jaime","last":"Carbonell"},{"first":"Mark","last":"Kantrowitz"}],"year":"2000","title":"Multi-document summarization by sentence extraction","source":" Jade Goldstein, Vibhu Mittal, Jaime Carbonell, and Mark Kantrowitz. 2000. Multi-document summarization by sentence extraction. Proceedings of the 2000 NAACL-ANLP Workshop on Automatic Summarization. 168"},{"authors":[{"first":"Y.","last":"Gong"},{"first":"X.","last":"Liu"}],"year":"2001","title":"Generic text summarization using relevance measure and latent semantic analysis","source":"Y. Gong and X. Liu. 2001. Generic text summarization using relevance measure and latent semantic analysis. Proc. ACM SIGIR."},{"authors":[{"first":"I.","last":"Gurevych"},{"first":"T.","last":"Nahnsen"}],"year":"2005","title":"Adapting Lexical Chaining to Summarize Conversational Dialogues","source":" I. Gurevych and T. Nahnsen. 2005. Adapting Lexical Chaining to Summarize Conversational Dialogues. Proc. RANLP."},{"authors":[{"first":"B.","last":"Hachey"},{"first":"G.","last":"Murray"},{"first":"D.","last":"Reitter"}],"year":"2006","title":"Dimensionality reduction aids term co-occurrence based multi-document summarization","source":" B. Hachey, G. Murray, and D. Reitter.2006. Dimensionality reduction aids term co-occurrence based multi-document summarization. In: SumQA 06: Proceedings of the Workshop on Task-Focused Summarization and Question Answering."},{"authors":[{"first":"Aria","last":"Haghighi"},{"first":"Lucy","last":"Vanderwende"}],"year":"2009","title":"Exploring content models for multi-document summarization","source":" Aria Haghighi and Lucy Vanderwende. 2009. Exploring content models for multi-document summarization. Proc. of NAACL-HLT."},{"authors":[{"first":"L.","last":"He"},{"first":"E.","last":"Sanocki"},{"first":"A.","last":"Gupta"},{"first":"J.","last":"Grudin"}],"year":"2000","title":"Comparing presentation summaries: Slides vs","source":" L. He, E. Sanocki, A. Gupta, and J. Grudin. 2000. Comparing presentation summaries: Slides vs. reading vs. listening. Proc. of SIGCHI on Human factors in computing systems."},{"authors":[{"first":"C.","last":"Hori"},{"first":"Sadaoki","last":"Furui"}],"year":"2001","title":"Advances in Automatic Speech Summarization","source":" C. Hori and Sadaoki Furui. 2001. Advances in Automatic Speech Summarization. Proc. of Eurospeech."},{"authors":[{"first":"T.","last":"Kikuchi"},{"first":"S.","last":"Furui"},{"first":"C.","last":"Hori"}],"year":"2003","title":"Automatic Speech Summarization based on Sentence Extractive and Compaction","source":" T. Kikuchi, S. Furui, and C. Hori. 2003. Automatic Speech Summarization based on Sentence Extractive and Compaction. Proc. of ICSLP."},{"authors":[{"first":"Julian","last":"Kupiec"},{"first":"Jan","last":"Pedersen"},{"first":"Francine","last":"Chen"}],"year":"1995","title":"A Trainable Document Summarizer","source":" Julian Kupiec, Jan Pedersen, and Francine Chen. 1995. A Trainable Document Summarizer. Proc. of SIGIR."},{"authors":[{"first":"J.","last":"Leskovec"},{"first":"N.","last":"Milic-frayling"},{"first":"M.","last":"Grobelnik"}],"year":"2005","title":"Impact of Linguistic Analysis on the Semantic Graph Coverage and Learning of Document Extracts","source":" J. Leskovec, N. Milic-frayling, and M. Grobelnik. 2005. Impact of Linguistic Analysis on the Semantic Graph Coverage and Learning of Document Extracts. Proc. AAAI."},{"authors":[{"first":"Chin-Yew","last":"Lin"}],"year":"2004","title":"ROUGE: A Package for Automatic Evaluation of Summaries, Workshop on Text Summarization Branches Out","source":" Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries, Workshop on Text Summarization Branches Out."},{"authors":[{"first":"C.","middle":"Y.","last":"Lin"},{"first":"E.","last":"Hovy"}],"year":"2000","title":"The automated acquisition of topic signatures for text summarization","source":" C.Y. Lin and E. Hovy. 2000. The automated acquisition of topic signatures for text summarization. Proc. COLING."},{"authors":[{"first":"Hui","last":"Lin"},{"first":"Jeff","last":"Bilmes"}],"year":"2010","title":"Multi-document summarization via budgeted maximization of submodular functions","source":" Hui Lin and Jeff Bilmes. 2010. Multi-document summarization via budgeted maximization of submodular functions. Proc. of NAACL."},{"authors":[{"first":"Hui","last":"Lin"},{"first":"Jeff","last":"Bilmes"},{"first":"Shasha","last":"Xie"}],"year":"2009","title":"Graph-based Submodular Selection for Extractive Summarization","source":" Hui Lin and Jeff Bilmes and Shasha Xie. 2009. Graph-based Submodular Selection for Extractive Summarization. Proceedings of ASRU."},{"authors":[{"first":"Shih-Hsiang","last":"Lin"},{"first":"Berlin","last":"Chen"}],"year":"2009","title":"Improved Speech Summarization with Multiple-hypothesis Representations and Kullback-Leibler Divergence Measures","source":" Shih-Hsiang Lin and Berlin Chen. 2009. Improved Speech Summarization with Multiple-hypothesis Representations and Kullback-Leibler Divergence Measures. Proc. of Interspeech."},{"authors":[{"first":"Shih-Hsiang","last":"Lin"},{"first":"Berlin","last":"Chen"},{"first":"H.","middle":"Min","last":"Wang"}],"year":"2009","title":"A Comparative Study of Probabilistic Ranking Models for Chinese Spoken Document Summarization","source":" Shih-Hsiang Lin, Berlin Chen, and H. Min Wang. 2009. A Comparative Study of Probabilistic Ranking Models for Chinese Spoken Document Summarization. ACM Transactions on Asian Language Information Processing. 85 169"},{"authors":[],"source":"Shih Hsiang Lin, Yu Mei Chang, Jia Wen Liu, Berlin Chen. 2010 Leveraging Evaluation Metric-related Training Criteria for Speech Summarization. Proc. of ICASSP."},{"authors":[{"first":"Fei","last":"Liu"},{"first":"Yang","last":"Liu"}],"year":"2009","title":"From Extractive to Abstractive Meeting Summaries: Can it be done by sentence compression? Proc","source":" Fei Liu and Yang Liu. 2009. From Extractive to Abstractive Meeting Summaries: Can it be done by sentence compression? Proc. of ACL."},{"authors":[{"first":"Fei","last":"Liu"},{"first":"Yang","last":"Liu"}],"year":"2010","title":"Using Spoken Utterance Compression for Meeting Summarization: A pilot study","source":" Fei Liu and Yang Liu. 2010. Using Spoken Utterance Compression for Meeting Summarization: A pilot study. Proc. of IEEE SLT."},{"authors":[{"first":"Yang","last":"Liu"},{"first":"Shasha","last":"Xie"}],"year":"2008","title":"Impact of Automatic Sentence Segmentation on Meeting Summarization","source":" Yang Liu and Shasha Xie. 2008. Impact of Automatic Sentence Segmentation on Meeting Summarization. Proc. of ICASSP."},{"authors":[{"first":"Yang","last":"Liu"},{"first":"Feifan","last":"Liu"},{"first":"Bin","last":"Li"},{"first":"Shasha","last":"Xie"}],"year":"2007","title":"Do Disfluencies Affect Meeting Summarization: A pilot study on the impact of disfluencies","source":" Yang Liu, Feifan Liu, Bin Li, and Shasha Xie. 2007. Do Disfluencies Affect Meeting Summarization: A pilot study on the impact of disfluencies. Poster at MLMI."},{"authors":[{"first":"Yang","last":"Liu"},{"first":"Shasha","last":"Xie"},{"first":"Fei","last":"Liu"}],"year":"2010","title":"Using n-best Recognition Output for Extractive Summarization and Keyword Extraction in Meeting Speech","source":" Yang Liu, Shasha Xie, and Fei Liu. 2010. Using n-best Recognition Output for Extractive Summarization and Keyword Extraction in Meeting Speech. Proc. of ICASSP."},{"authors":[{"first":"Annie","last":"Louis"},{"first":"Ani","last":"Nenkova"}],"year":"2009","title":"Automatically evaluating content selection in summarization without human models","source":" Annie Louis and Ani Nenkova. 2009. Automatically evaluating content selection in summarization without human models. Proceedings of EMNLP"},{"authors":[{"first":"H.","middle":"P.","last":"Luhn"}],"year":"1958","title":"The Automatic Creation of Literature Abstracts","source":" H.P. Luhn. 1958. The Automatic Creation of Literature Abstracts. IBM Journal of Research and Development 2(2)."},{"authors":[{"first":"Inderjeet","last":"Mani"},{"first":"Gary","last":"Klein"},{"first":"David","last":"House"},{"first":"Lynette","last":"Hirschman"},{"first":"Therese","last":"Firmin"},{"first":"Beth","last":"Sundheim"}],"year":"2002","title":"SUMMAC: a text summarization evaluation","source":" Inderjeet Mani, Gary Klein, David House, Lynette Hirschman, Therese Firmin, and Beth Sundheim. 2002. SUMMAC: a text summarization evaluation. Natual Language Engineering. 8,1 (March 2002), 43-68."},{"authors":[{"first":"Manuel","middle":"J.","last":"Mana-Lopez"},{"first":"Manuel","last":"De Buenaga"},{"first":"Jose","middle":"M.","last":"Gomez-Hidalgo"}],"year":"2004","title":"Multidocument summarization: An added value to clustering in interactive retrieval","source":" Manuel J. Mana-Lopez, Manuel De Buenaga, and Jose M. Gomez-Hidalgo. 2004. Multidocument summarization: An added value to clustering in interactive retrieval. ACM Trans. Inf. Systems."},{"authors":[{"first":"Sameer","last":"Maskey"}],"year":"2008","title":"Automatic Broadcast News Summarization","source":" Sameer Maskey. 2008. Automatic Broadcast News Summarization. Ph.D thesis. Columbia University."},{"authors":[{"first":"Sameer","last":"Maskey"},{"first":"Julia","last":"Hirschberg"}],"year":"2005","title":"Comparing lexical, acoustic/prosodic, discourse and structural features for speech summarization","source":" Sameer Maskey and Julia Hirschberg. 2005. Comparing lexical, acoustic/prosodic, discourse and structural features for speech summarization. Proceedings of Interspeech."},{"authors":[{"first":"Sameer","last":"Maskey"},{"first":"Julia","last":"Hirschberg"}],"year":"2006","title":"Summarizing Speech Without Text Using Hidden Markov Models","source":" Sameer Maskey and Julia Hirschberg. 2006. Summarizing Speech Without Text Using Hidden Markov Models. Proc. of HLT-NAACL."},{"authors":[{"first":"Ryan","last":"McDonald"}],"year":"2007","title":"A Study of Global Inference Algorithms in Multi-document Summarization","source":" Ryan McDonald. 2007. A Study of Global Inference Algorithms in Multi-document Summarization. Lecture Notes in Computer Science. Advances in Information Retrieval."},{"authors":[{"first":"Kathleen","last":"McKeown"},{"first":"Rebecca","middle":"J.","last":"Passonneau"},{"first":"David","middle":"K.","last":"Elson"},{"first":"Ani","last":"Nenkova"},{"first":"Julia","last":"Hirschberg"}],"year":"2005","title":"Do summaries help?","source":" Kathleen McKeown, Rebecca J. Passonneau, David K. Elson, Ani Nenkova, and Julia Hirschberg. 2005. Do summaries help?. Proc. of SIGIR."},{"authors":[{"first":"K.","last":"McKeown"},{"first":"J.","middle":"L.","last":"Klavans"},{"first":"V.","last":"Hatzivassiloglou"},{"first":"R.","last":"Barzilay"},{"first":"E.","last":"Eskin"}],"year":"1999","title":"Towards multidocument summarization by reformulation: progress and prospects","source":" K. McKeown, J. L. Klavans, V. Hatzivassiloglou, R. Barzilay, and E. Eskin.1999. Towards multidocument summarization by reformulation: progress and prospects. Proc. AAAI 1999. 170"},{"authors":[{"first":"R.","last":"Mihalcea"},{"first":"P.","last":"Tarau"}],"year":"2004","title":"Textrank: Bringing order into texts","source":"R. Mihalcea and P. Tarau .2004. Textrank: Bringing order into texts. Proc. of EMNLP 2004."},{"authors":[{"first":"G.","last":"Murray"},{"first":"S.","last":"Renals"},{"first":"J.","last":"Carletta"},{"first":"J.","last":"Moore"}],"year":"2005","title":"Evaluating Automatic Summaries of Meeting Recordings","source":" G. Murray, S. Renals, J. Carletta, J. Moore. 2005. Evaluating Automatic Summaries of Meeting Recordings. Proc. of ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation."},{"authors":[{"first":"G.","last":"Murray"},{"first":"T.","last":"Kleinbauer"},{"first":"P.","last":"Poller"},{"first":"T.","last":"Becker"},{"first":"S.","last":"Renals"},{"first":"J.","last":"Kilgour"}],"year":"2009","title":"Extrinsic Summarization Evaluation: A Decision Audit Task","source":" G. Murray, T. Kleinbauer, P. Poller, T. Becker, S. Renals, and J. Kilgour. 2009. Extrinsic Summarization Evaluation: A Decision Audit Task. ACM Transactions on Speech and Language Processing."},{"authors":[{"first":"A.","last":"Nenkova"},{"first":"R.","last":"Passonneau"}],"year":"2004","title":"Evaluating Content Selection in Summarization: The Pyramid Method","source":" A. Nenkova and R. Passonneau. 2004. Evaluating Content Selection in Summarization: The Pyramid Method. Proc. HLT-NAACL."},{"authors":[{"first":"A.","last":"Nenkova"},{"first":"L.","last":"Vanderwende"},{"first":"K.","last":"McKeown"}],"year":"2006","title":"A compositional context sensitive multi-document summarizer: exploring the factors that influence summarization","source":" A. Nenkova, L. Vanderwende, and K. McKeown. 2006. A compositional context sensitive multi-document summarizer: exploring the factors that influence summarization. Proc. ACM SIGIR."},{"authors":[{"first":"A.","last":"Nenkova"},{"first":"R.","last":"Passonneau"},{"first":"K.","last":"McKeown"}],"year":"2007","title":"The Pyramid Method: Incorporating human content selection variation in summarization evaluation","source":" A. Nenkova, R. Passonneau, and K. McKeown. 2007. The Pyramid Method: Incorporating human content selection variation in summarization evaluation. ACM Trans. Speech Lang. Processing."},{"authors":[{"first":"Miles","last":"Osborne"}],"year":"2002","title":"Using maximum entropy for sentence extraction","source":" Miles Osborne. 2002. Using maximum entropy for sentence extraction. Proc. of ACL Workshop on Automatic Summarization."},{"authors":[{"first":"Gerald","last":"Penn"},{"first":"Xiaodan","last":"Zhu"}],"year":"2008","title":"A critical Reassessement of Evaluation Baselines for Speech Summarization","source":" Gerald Penn and Xiaodan Zhu. 2008. A critical Reassessement of Evaluation Baselines for Speech Summarization. Proc. of ACL-HLT."},{"authors":[{"first":"Dmitri","middle":"G.","last":"Roussinov"},{"first":"Hsinchun","last":"Chen"}],"year":"2001","title":"Information navigation on the web by clustering and summarizing query results","source":" Dmitri G. Roussinov and Hsinchun Chen. 2001. Information navigation on the web by clustering and summarizing query results. Inf. Process. Manage. 37, 6 (October 2001), 789-816."},{"authors":[{"first":"B.","last":"Schiffman"},{"first":"A.","last":"Nenkova"},{"first":"K.","last":"McKeown"}],"year":"2002","title":"Experiments in Multidocument Summarization","source":" B. Schiffman, A. Nenkova, and K. McKeown. 2002. Experiments in Multidocument Summarization. Proc. HLT."},{"authors":[{"first":"A.","last":"Siddharthan"},{"first":"A.","last":"Nenkova"},{"first":"K.","last":"Mckeown"}],"year":"2004","title":"Syntactic Simplification for Improving Content Selection in Multi-Document Summarization","source":" A. Siddharthan, A. Nenkova, and K. Mckeown.2004. Syntactic Simplification for Improving Content Selection in Multi-Document Summarization. Proc. COLING."},{"authors":[{"first":"H.","middle":"Grogory","last":"Silber"},{"first":"Kathleen","middle":"F.","last":"McCoy"}],"year":"2002","title":"Efficiently computed lexical chains as an intermediate representation for automatic text summarization","source":" H. Grogory Silber and Kathleen F. McCoy. 2002. Efficiently computed lexical chains as an intermediate representation for automatic text summarization. Computational. Linguist. 28, 4 (December 2002), 487-496."},{"authors":[{"first":"J.","last":"Steinberger"},{"first":"M.","last":"Poesio"},{"first":"M.","middle":"A.","last":"Kabadjov"},{"first":"K.","last":"Jeek"}],"year":"2007","title":"Two uses of anaphora resolution in summarization","source":" J. Steinberger, M. Poesio, M. A. Kabadjov, and K. Jeek. 2007. Two uses of anaphora resolution in summarization. Inf. Process. Manage. 43(6)."},{"authors":[{"first":"S.","last":"Tucker"},{"first":"S.","last":"Whittaker"}],"year":"2008","title":"Temporal compression of speech: an evaluation","source":" S. Tucker and S. Whittaker. 2008. Temporal compression of speech: an evaluation. IEEE Transactions on Audio, Speech and Language Processing, pages 790-796."},{"authors":[{"first":"L.","last":"Vanderwende"},{"first":"H.","last":"Suzuki"},{"first":"C.","last":"Brockett"},{"first":"A.","last":"Nenkova"}],"year":"2007","title":"Beyond SumBasic: Task-focused summarization with sentence simplification and lexical expansion","source":" L. Vanderwende, H. Suzuki, C. Brockett, and A. Nenkova. 2007. Beyond SumBasic: Task-focused summarization with sentence simplification and lexical expansion. Information Processing and Management 43. 86 171"},{"authors":[{"first":"Kam-Fai","last":"Wong"},{"first":"Mingli","last":"Wu"},{"first":"Wenjie","last":"Li"}],"year":"2008","title":"Extractive Summarization using Supervised and Semi-supervised learning","source":"Kam-Fai Wong, Mingli Wu, and Wenjie Li. 2008. Extractive Summarization using Supervised and Semi-supervised learning. Proc. of ACL."},{"authors":[{"first":"Shasha","last":"Xie"},{"first":"Yang","last":"Liu"}],"year":"2010","title":"Improving Supervised Learning for Meeting Summarization using Sampling and Regression","source":" Shasha Xie and Yang Liu. 2010. Improving Supervised Learning for Meeting Summarization using Sampling and Regression. Computer Speech and Language. V24, pages 495-514."},{"authors":[{"first":"Shasha","last":"Xie"},{"first":"Yang","last":"Liu"}],"year":"2010","title":"Using Confusion Networks for Speech Summarization","source":" Shasha Xie and Yang Liu. 2010. Using Confusion Networks for Speech Summarization. Proc. of NAACL."},{"authors":[{"first":"Shasha","last":"Xie"},{"first":"Dilek","last":"Hakkani-Tur"},{"first":"Benoit","last":"Favre"},{"first":"Yang","last":"Liu"}],"year":"2009","title":"Integrating Prosodic Features in Extractive Meeting Summarization","source":" Shasha Xie, Dilek Hakkani-Tur, Benoit Favre, and Yang Liu. 2009. Integrating Prosodic Features in Extractive Meeting Summarization. Proc. of ASRU."},{"authors":[{"first":"Shasha","last":"Xie"},{"first":"Hui","last":"Lin"},{"first":"Yang","last":"Liu"}],"year":"2010","title":"Semi-supervised Extractive Speech Summarization via Co-training Algorithm","source":" Shasha Xie, Hui Lin, and Yang Liu. 2010. Semi-supervised Extractive Speech Summarization via Co-training Algorithm. Proc. of Interspeech."},{"authors":[{"first":"S.","last":"Ye"},{"first":"T.","middle":"-S.","last":"Chua"},{"first":"M.","middle":"-Y.","last":"Kan"},{"first":"L.","last":"Qiu"}],"year":"2007","title":"Document concept lattice for text understanding and summarization","source":" S. Ye, T.-S. Chua, M.-Y. Kan, and L. Qiu. 2007. Document concept lattice for text understanding and summarization. Information Processing and Management 43(6)."},{"authors":[{"first":"W.","last":"Yih"},{"first":"J.","last":"Goodman"},{"first":"L.","last":"Vanderwende"},{"first":"H.","last":"Suzuki"}],"year":"2007","title":"Multi-Document Summarization by Maximizing Informative Content-Words","source":" W. Yih, J. Goodman, L. Vanderwende, and H. Suzuki. 2007. Multi-Document Summarization by Maximizing Informative Content-Words. Proc. IJCAI 2007."},{"authors":[{"first":"Klaus","last":"Zechner"}],"year":"2002","title":"Automatic Summarization of Open-domain Multiparty Dialogues in Diverse Genres","source":" Klaus Zechner. 2002. Automatic Summarization of Open-domain Multiparty Dialogues in Diverse Genres. Computational Linguistics. V28, pages 447-485."},{"authors":[{"first":"Klaus","last":"Zechner"},{"first":"Alex","last":"Waibel"}],"year":"2000","title":"Minimizing word error rate in textual summaries of spoken language","source":" Klaus Zechner and Alex Waibel. 2000. Minimizing word error rate in textual summaries of spoken language. Proceedings of the 1st North American chapter of the Association for Computational Linguistics conference."},{"authors":[{"first":"Justin","last":"Zhang"},{"first":"Pascale","last":"Fung"}],"year":"2009","title":"Extractive Speech Summarization by Active Learning","source":" Justin Zhang and Pascale Fung. 2009. Extractive Speech Summarization by Active Learning. Proc. of ASRU."},{"authors":[{"first":"Xiaodan","last":"Zhu"},{"first":"Gerald","last":"Penn"}],"year":"2006","title":"Comparing the Roles of Textual, Acoustic and Spoken-language Features on Spontaneous Conversation Summarization","source":" Xiaodan Zhu and Gerald Penn. 2006. Comparing the Roles of Textual, Acoustic and Spoken-language Features on Spontaneous Conversation Summarization. Proc. of HLT-NAACL."},{"authors":[{"first":"Xiaodan","last":"Zhu"},{"first":"Gerald","last":"Penn"},{"first":"F.","last":"Rudzicz"}],"year":"2009","title":"Summarizing Multiple Spoken Documents: Finding Evidence from Untranscribed Audio","source":" Xiaodan Zhu, Gerald Penn, and F. Rudzicz. 2009. Summarizing Multiple Spoken Documents: Finding Evidence from Untranscribed Audio. Proc. of ACL."}],"cites":[{"style":0,"text":"Mani et al., 2002","origin":{"pointer":"/sections/5/paragraphs/2","offset":146,"length":17},"authors":[{"last":"Mani"},{"last":"al."}],"year":"2002","references":["/references/36"]},{"style":0,"text":"Roussinov and Chen, 2001","origin":{"pointer":"/sections/6/paragraphs/3","offset":65,"length":24},"authors":[{"last":"Roussinov"},{"last":"Chen"}],"year":"2001","references":["/references/52"]},{"style":0,"text":"Mana-Lopez et al., 2004","origin":{"pointer":"/sections/6/paragraphs/3","offset":91,"length":23},"authors":[{"last":"Mana-Lopez"},{"last":"al."}],"year":"2004","references":["/references/37"]},{"style":0,"text":"McKeown et al., 2005","origin":{"pointer":"/sections/6/paragraphs/3","offset":116,"length":20},"authors":[{"last":"McKeown"},{"last":"al."}],"year":"2005","references":["/references/42"]},{"style":0,"text":"He et al., 2000","origin":{"pointer":"/sections/10/paragraphs/0","offset":42,"length":15},"authors":[{"last":"He"},{"last":"al."}],"year":"2000","references":["/references/17"]},{"style":0,"text":"Tucker and Whittaker, 2008","origin":{"pointer":"/sections/10/paragraphs/0","offset":59,"length":26},"authors":[{"last":"Tucker"},{"last":"Whittaker"}],"year":"2008","references":["/references/57"]},{"style":0,"text":"Murray et al., 2009","origin":{"pointer":"/sections/10/paragraphs/0","offset":87,"length":19},"authors":[{"last":"Murray"},{"last":"al."}],"year":"2009","references":["/references/46"]},{"style":0,"text":"Nenkova et al., 2006","origin":{"pointer":"/sections/19/paragraphs/0","offset":1,"length":20},"authors":[{"last":"Nenkova"},{"last":"al."}],"year":"2006","references":["/references/48"]},{"style":0,"text":"Vanderwende et al., 2007","origin":{"pointer":"/sections/22/paragraphs/0","offset":1,"length":24},"authors":[{"last":"Vanderwende"},{"last":"al."}],"year":"2007","references":["/references/58"]},{"style":0,"text":"Yih et al., 2007","origin":{"pointer":"/sections/22/paragraphs/0","offset":27,"length":16},"authors":[{"last":"Yih"},{"last":"al."}],"year":"2007","references":["/references/65"]},{"style":0,"text":"Haghighi and Vanderwende, 2009","origin":{"pointer":"/sections/22/paragraphs/0","offset":45,"length":30},"authors":[{"last":"Haghighi"},{"last":"Vanderwende"}],"year":"2009","references":["/references/16"]},{"style":0,"text":"Barzilay and Elhadad, 1999","origin":{"pointer":"/sections/34/paragraphs/0","offset":1,"length":26},"authors":[{"last":"Barzilay"},{"last":"Elhadad"}],"year":"1999","references":[]},{"style":0,"text":"Silber and McCoy, 2002","origin":{"pointer":"/sections/34/paragraphs/0","offset":29,"length":22},"authors":[{"last":"Silber"},{"last":"McCoy"}],"year":"2002","references":["/references/55"]},{"style":0,"text":"Gurevych and Nahnsen, 2005","origin":{"pointer":"/sections/34/paragraphs/0","offset":53,"length":26},"authors":[{"last":"Gurevych"},{"last":"Nahnsen"}],"year":"2005","references":["/references/14"]},{"style":0,"text":"Schiffman et al., 2002","origin":{"pointer":"/sections/36/paragraphs/3","offset":27,"length":22},"authors":[{"last":"Schiffman"},{"last":"al."}],"year":"2002","references":["/references/53"]},{"style":0,"text":"Ye et al., 2007","origin":{"pointer":"/sections/36/paragraphs/3","offset":51,"length":15},"authors":[{"last":"Ye"},{"last":"al."}],"year":"2007","references":["/references/64"]},{"style":0,"text":"Lin and Hovy, 2000","origin":{"pointer":"/sections/45/paragraphs/0","offset":234,"length":18},"authors":[{"last":"Lin"},{"last":"Hovy"}],"year":"2000","references":["/references/23"]},{"style":0,"text":"Conroy et al., 2006","origin":{"pointer":"/sections/45/paragraphs/0","offset":254,"length":19},"authors":[{"last":"Conroy"},{"last":"al."}],"year":"2006","references":["/references/5"]},{"style":0,"text":"Gong and Liu, 2001","origin":{"pointer":"/sections/49/paragraphs/0","offset":1,"length":18},"authors":[{"last":"Gong"},{"last":"Liu"}],"year":"2001","references":["/references/13"]},{"style":0,"text":"Hachey et al., 2006","origin":{"pointer":"/sections/49/paragraphs/0","offset":21,"length":19},"authors":[{"last":"Hachey"},{"last":"al."}],"year":"2006","references":["/references/15"]},{"style":0,"text":"Steinberger et al., 2007","origin":{"pointer":"/sections/49/paragraphs/0","offset":42,"length":24},"authors":[{"last":"Steinberger"},{"last":"al."}],"year":"2007","references":["/references/56"]},{"style":0,"text":"Barzilay and Lee, 2004","origin":{"pointer":"/sections/58/paragraphs/0","offset":1,"length":22},"authors":[{"last":"Barzilay"},{"last":"Lee"}],"year":"2004","references":[]},{"style":0,"text":"Pascale et al., 2003","origin":{"pointer":"/sections/58/paragraphs/0","offset":25,"length":20},"authors":[{"last":"Pascale"},{"last":"al."}],"year":"2003","references":[]},{"style":0,"text":"McKeown et al., 1999","origin":{"pointer":"/sections/62/paragraphs/0","offset":1,"length":20},"authors":[{"last":"McKeown"},{"last":"al."}],"year":"1999","references":["/references/43"]},{"style":0,"text":"Siddharthan et al., 2004","origin":{"pointer":"/sections/62/paragraphs/0","offset":23,"length":24},"authors":[{"last":"Siddharthan"},{"last":"al."}],"year":"2004","references":["/references/54"]},{"style":0,"text":"Erkan and Radev, 2004","origin":{"pointer":"/sections/69/paragraphs/0","offset":1,"length":21},"authors":[{"last":"Erkan"},{"last":"Radev"}],"year":"2004","references":["/references/7"]},{"style":0,"text":"Mihalcea and Tarau, 2004","origin":{"pointer":"/sections/69/paragraphs/0","offset":24,"length":24},"authors":[{"last":"Mihalcea"},{"last":"Tarau"}],"year":"2004","references":["/references/44"]},{"style":0,"text":"Leskovec et al., 2005","origin":{"pointer":"/sections/69/paragraphs/0","offset":50,"length":21},"authors":[{"last":"Leskovec"},{"last":"al."}],"year":"2005","references":["/references/21"]},{"style":0,"text":"Osborne, 2002","origin":{"pointer":"/sections/83/paragraphs/0","offset":17,"length":13},"authors":[{"last":"Osborne"}],"year":"2002","references":["/references/50"]},{"style":0,"text":"Galley, 2006","origin":{"pointer":"/sections/83/paragraphs/0","offset":63,"length":12},"authors":[{"last":"Galley"}],"year":"2006","references":["/references/9"]},{"style":0,"text":"Kupiec et al., 1995","origin":{"pointer":"/sections/83/paragraphs/0","offset":102,"length":19},"authors":[{"last":"Kupiec"},{"last":"al."}],"year":"1995","references":["/references/20"]},{"style":0,"text":"Conroy and O'Leary, 2001","origin":{"pointer":"/sections/83/paragraphs/0","offset":128,"length":24},"authors":[{"last":"Conroy"},{"last":"O'Leary"}],"year":"2001","references":["/references/4"]},{"style":0,"text":"Maskey, 2006","origin":{"pointer":"/sections/83/paragraphs/0","offset":154,"length":12},"authors":[{"last":"Maskey"}],"year":"2006","references":[]},{"style":0,"text":"Xie and Liu, 2010","origin":{"pointer":"/sections/83/paragraphs/0","offset":193,"length":17},"authors":[{"last":"Xie"},{"last":"Liu"}],"year":"2010","references":["/references/60","/references/61"]},{"style":0,"text":"Murray et al., 2005","origin":{"pointer":"/sections/83/paragraphs/0","offset":224,"length":19},"authors":[{"last":"Murray"},{"last":"al."}],"year":"2005","references":["/references/45"]},{"style":0,"text":"Aker et al., 2010","origin":{"pointer":"/sections/87/paragraphs/0","offset":1,"length":17},"authors":[{"last":"Aker"},{"last":"al."}],"year":"2010","references":["/references/0"]},{"style":0,"text":"Xie and Liu, 2010","origin":{"pointer":"/sections/89/paragraphs/0","offset":1,"length":17},"authors":[{"last":"Xie"},{"last":"Liu"}],"year":"2010","references":["/references/60","/references/61"]},{"style":0,"text":"Xie and Liu, 2010","origin":{"pointer":"/sections/94/paragraphs/0","offset":28,"length":17},"authors":[{"last":"Xie"},{"last":"Liu"}],"year":"2010","references":["/references/60","/references/61"]},{"style":0,"text":"Wong et al., 2008","origin":{"pointer":"/sections/100/paragraphs/0","offset":1,"length":17},"authors":[{"last":"Wong"},{"last":"al."}],"year":"2008","references":["/references/59"]},{"style":0,"text":"Xie et al., 2010","origin":{"pointer":"/sections/101/paragraphs/0","offset":1,"length":16},"authors":[{"last":"Xie"},{"last":"al."}],"year":"2010","references":["/references/63"]},{"style":0,"text":"Carbonell et al., 1998","origin":{"pointer":"/sections/112/paragraphs/2","offset":13,"length":22},"authors":[{"last":"Carbonell"},{"last":"al."}],"year":"1998","references":[]},{"style":0,"text":"Carbonell et al., 1998","origin":{"pointer":"/sections/113/paragraphs/0","offset":1,"length":22},"authors":[{"last":"Carbonell"},{"last":"al."}],"year":"1998","references":[]},{"style":0,"text":"Radev, 2004","origin":{"pointer":"/sections/116/paragraphs/0","offset":1,"length":11},"authors":[{"last":"Radev"}],"year":"2004","references":[]},{"style":0,"text":"Carbonell et al., 1998","origin":{"pointer":"/sections/117/paragraphs/0","offset":1,"length":22},"authors":[{"last":"Carbonell"},{"last":"al."}],"year":"1998","references":[]},{"style":0,"text":"Zechner, 2000","origin":{"pointer":"/sections/118/paragraphs/0","offset":1,"length":13},"authors":[{"last":"Zechner"}],"year":"2000","references":[]},{"style":0,"text":"McDonald, 2007","origin":{"pointer":"/sections/125/paragraphs/0","offset":1,"length":14},"authors":[{"last":"McDonald"}],"year":"2007","references":["/references/41"]},{"style":0,"text":"McDonald, 2007","origin":{"pointer":"/sections/127/paragraphs/0","offset":1,"length":14},"authors":[{"last":"McDonald"}],"year":"2007","references":["/references/41"]},{"style":0,"text":"McDonald, 2007","origin":{"pointer":"/sections/128/paragraphs/0","offset":1,"length":14},"authors":[{"last":"McDonald"}],"year":"2007","references":["/references/41"]},{"style":0,"text":"McDonald, 2007","origin":{"pointer":"/sections/129/paragraphs/0","offset":1,"length":14},"authors":[{"last":"McDonald"}],"year":"2007","references":["/references/41"]},{"style":0,"text":"McDonald, 2007","origin":{"pointer":"/sections/130/paragraphs/0","offset":1,"length":14},"authors":[{"last":"McDonald"}],"year":"2007","references":["/references/41"]},{"style":0,"text":"Gillick and Favre, 2009","origin":{"pointer":"/sections/131/paragraphs/0","offset":1,"length":23},"authors":[{"last":"Gillick"},{"last":"Favre"}],"year":"2009","references":["/references/10"]},{"style":0,"text":"Gillick et al., 2009","origin":{"pointer":"/sections/131/paragraphs/0","offset":26,"length":20},"authors":[{"last":"Gillick"},{"last":"al."}],"year":"2009","references":["/references/11"]},{"style":0,"text":"McDonald, 2007","origin":{"pointer":"/sections/131/paragraphs/0","offset":48,"length":14},"authors":[{"last":"McDonald"}],"year":"2007","references":["/references/41"]},{"style":0,"text":"McDonald, 2007","origin":{"pointer":"/sections/132/paragraphs/0","offset":1,"length":14},"authors":[{"last":"McDonald"}],"year":"2007","references":["/references/41"]},{"style":0,"text":"Gillick and Favre, 2009","origin":{"pointer":"/sections/133/paragraphs/0","offset":1,"length":23},"authors":[{"last":"Gillick"},{"last":"Favre"}],"year":"2009","references":["/references/10"]},{"style":0,"text":"Gillick et al., 2009","origin":{"pointer":"/sections/133/paragraphs/0","offset":26,"length":20},"authors":[{"last":"Gillick"},{"last":"al."}],"year":"2009","references":["/references/11"]},{"style":0,"text":"Gillick and Favre, 2009","origin":{"pointer":"/sections/136/paragraphs/0","offset":1,"length":23},"authors":[{"last":"Gillick"},{"last":"Favre"}],"year":"2009","references":["/references/10"]},{"style":0,"text":"McDonald, 2007","origin":{"pointer":"/sections/137/paragraphs/0","offset":1,"length":14},"authors":[{"last":"McDonald"}],"year":"2007","references":["/references/41"]},{"style":0,"text":"Gillick and Favre, 2009","origin":{"pointer":"/sections/138/paragraphs/0","offset":1,"length":23},"authors":[{"last":"Gillick"},{"last":"Favre"}],"year":"2009","references":["/references/10"]},{"style":0,"text":"Lin et al., 2009","origin":{"pointer":"/sections/140/paragraphs/0","offset":1,"length":16},"authors":[{"last":"Lin"},{"last":"al."}],"year":"2009","references":["/references/25","/references/27"]},{"style":0,"text":"Filatova, 2004","origin":{"pointer":"/sections/141/paragraphs/0","offset":1,"length":14},"authors":[{"last":"Filatova"}],"year":"2004","references":[]},{"style":0,"text":"Yih et al., 2007","origin":{"pointer":"/sections/142/paragraphs/0","offset":1,"length":16},"authors":[{"last":"Yih"},{"last":"al."}],"year":"2007","references":["/references/65"]},{"style":0,"text":"Aker et al., 2010","origin":{"pointer":"/sections/143/paragraphs/0","offset":1,"length":17},"authors":[{"last":"Aker"},{"last":"al."}],"year":"2010","references":["/references/0"]},{"style":0,"text":"Lin et al., 2009","origin":{"pointer":"/sections/144/paragraphs/0","offset":1,"length":16},"authors":[{"last":"Lin"},{"last":"al."}],"year":"2009","references":["/references/25","/references/27"]},{"style":0,"text":"Lin et al., 2009","origin":{"pointer":"/sections/145/paragraphs/0","offset":1,"length":16},"authors":[{"last":"Lin"},{"last":"al."}],"year":"2009","references":["/references/25","/references/27"]},{"style":0,"text":"Lin et al., 2009","origin":{"pointer":"/sections/145/paragraphs/3","offset":86,"length":16},"authors":[{"last":"Lin"},{"last":"al."}],"year":"2009","references":["/references/25","/references/27"]},{"style":0,"text":"Lin et al., 2009","origin":{"pointer":"/sections/146/paragraphs/0","offset":1,"length":16},"authors":[{"last":"Lin"},{"last":"al."}],"year":"2009","references":["/references/25","/references/27"]},{"style":0,"text":"Maskey, 2008","origin":{"pointer":"/sections/161/paragraphs/0","offset":106,"length":12},"authors":[{"last":"Maskey"}],"year":"2008","references":["/references/38"]},{"style":0,"text":"Furui et al., 2004","origin":{"pointer":"/sections/161/paragraphs/2","offset":63,"length":18},"authors":[{"last":"Furui"},{"last":"al."}],"year":"2004","references":[]},{"style":0,"text":"Liu and Xie, 2008","origin":{"pointer":"/sections/161/paragraphs/3","offset":79,"length":17},"authors":[{"last":"Liu"},{"last":"Xie"}],"year":"2008","references":["/references/31"]},{"style":0,"text":"Penn and Zhu, 2008","origin":{"pointer":"/sections/164/paragraphs/2","offset":86,"length":18},"authors":[{"last":"Penn"},{"last":"Zhu"}],"year":"2008","references":["/references/51"]},{"style":0,"text":"Maskey and Hirschberg, 2005","origin":{"pointer":"/sections/165/paragraphs/2","offset":109,"length":27},"authors":[{"last":"Maskey"},{"last":"Hirschberg"}],"year":"2005","references":["/references/39"]},{"style":0,"text":"Xie et al., 2009","origin":{"pointer":"/sections/165/paragraphs/2","offset":138,"length":16},"authors":[{"last":"Xie"},{"last":"al."}],"year":"2009","references":["/references/62"]},{"style":0,"text":"Zhu et al., 2009","origin":{"pointer":"/sections/165/paragraphs/2","offset":156,"length":16},"authors":[{"last":"Zhu"},{"last":"al."}],"year":"2009","references":["/references/70"]},{"style":0,"text":"Christensen et al., 2003","origin":{"pointer":"/sections/168/paragraphs/0","offset":1,"length":24},"authors":[{"last":"Christensen"},{"last":"al."}],"year":"2003","references":["/references/3"]},{"style":0,"text":"Penn and Zhu, 2008","origin":{"pointer":"/sections/168/paragraphs/0","offset":27,"length":18},"authors":[{"last":"Penn"},{"last":"Zhu"}],"year":"2008","references":["/references/51"]},{"style":0,"text":"Lin et al., 2009","origin":{"pointer":"/sections/168/paragraphs/0","offset":47,"length":16},"authors":[{"last":"Lin"},{"last":"al."}],"year":"2009","references":["/references/25","/references/27"]},{"style":0,"text":"Zechner, 2002","origin":{"pointer":"/sections/173/paragraphs/0","offset":22,"length":13},"authors":[{"last":"Zechner"}],"year":"2002","references":["/references/66"]},{"style":0,"text":"Kikuchi et al., 2003","origin":{"pointer":"/sections/173/paragraphs/0","offset":37,"length":20},"authors":[{"last":"Kikuchi"},{"last":"al."}],"year":"2003","references":["/references/19"]},{"style":0,"text":"Maskey, 2008","origin":{"pointer":"/sections/173/paragraphs/0","offset":59,"length":12},"authors":[{"last":"Maskey"}],"year":"2008","references":["/references/38"]},{"style":0,"text":"Liu et al., 2010","origin":{"pointer":"/sections/174/paragraphs/0","offset":14,"length":16},"authors":[{"last":"Liu"},{"last":"al."}],"year":"2010","references":["/references/33"]},{"style":0,"text":"Lin et al., 2010","origin":{"pointer":"/sections/174/paragraphs/1","offset":10,"length":16},"authors":[{"last":"Lin"},{"last":"al."}],"year":"2010","references":[]},{"style":0,"text":"Xie and Liu, 2010","origin":{"pointer":"/sections/174/paragraphs/2","offset":19,"length":17},"authors":[{"last":"Xie"},{"last":"Liu"}],"year":"2010","references":["/references/60","/references/61"]},{"style":0,"text":"Hori and Furui, 2001","origin":{"pointer":"/sections/176/paragraphs/0","offset":1,"length":20},"authors":[{"last":"Hori"},{"last":"Furui"}],"year":"2001","references":["/references/18"]},{"style":0,"text":"Maskey, 2008","origin":{"pointer":"/sections/177/paragraphs/0","offset":19,"length":12},"authors":[{"last":"Maskey"}],"year":"2008","references":["/references/38"]},{"style":0,"text":"Zechner, 2002","origin":{"pointer":"/sections/177/paragraphs/0","offset":33,"length":13},"authors":[{"last":"Zechner"}],"year":"2002","references":["/references/66"]},{"style":0,"text":"Liu et al., 2007","origin":{"pointer":"/sections/177/paragraphs/0","offset":64,"length":16},"authors":[{"last":"Liu"},{"last":"al."}],"year":"2007","references":["/references/32"]},{"style":0,"text":"Zhu and Penn, 2006","origin":{"pointer":"/sections/178/paragraphs/0","offset":147,"length":18},"authors":[{"last":"Zhu"},{"last":"Penn"}],"year":"2006","references":["/references/69"]},{"style":0,"text":"Liu and Liu, 2010","origin":{"pointer":"/sections/178/paragraphs/1","offset":121,"length":17},"authors":[{"last":"Liu"},{"last":"Liu"}],"year":"2010","references":["/references/30"]},{"style":0,"text":"Nenkova and Passonneau, 2004","origin":{"pointer":"/sections/190/paragraphs/0","offset":1,"length":28},"authors":[{"last":"Nenkova"},{"last":"Passonneau"}],"year":"2004","references":["/references/47"]},{"style":0,"text":"Nenkova et al., 2007","origin":{"pointer":"/sections/190/paragraphs/0","offset":31,"length":20},"authors":[{"last":"Nenkova"},{"last":"al."}],"year":"2007","references":["/references/49"]},{"style":0,"text":"Lin, 2004","origin":{"pointer":"/sections/207/paragraphs/0","offset":1,"length":9},"authors":[{"last":"Lin"}],"year":"2004","references":["/references/22"]},{"style":0,"text":"Louis and Nenkova, 2009","origin":{"pointer":"/sections/211/paragraphs/0","offset":1,"length":23},"authors":[{"last":"Louis"},{"last":"Nenkova"}],"year":"2009","references":["/references/34"]}]}
