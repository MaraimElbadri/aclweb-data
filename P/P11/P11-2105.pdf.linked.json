{"sections":[{"title":"","paragraphs":["Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 598–602, Portland, Oregon, June 19-24, 2011. c⃝2011 Association for Computational Linguistics"]},{"title":"Hierarchical Text Classification with Latent Concepts Xipeng Qiu, Xuanjing Huang, Zhao Liu and Jinlong Zhou School of Computer Science, Fudan University {xpqiu,xjhuang}@fudan.edu.cn, {zliu.fd,abc9703}@gmail.com Abstract","paragraphs":["Recently, hierarchical text classification has become an active research topic. The essential idea is that the descendant classes can share the information of the ancestor classes in a predefined taxonomy. In this paper, we claim that each class has several latent concepts and its subclasses share information with these different concepts respectively. Then, we propose a variant Passive-Aggressive (PA) algorithm for hierarchical text classification with latent concepts. Experimental results show that the performance of our algorithm is competitive with the recently proposed hierarchical classification algorithms."]},{"title":"1 Introduction","paragraphs":["Text classification is a crucial and well-proven method for organizing the collection of large scale documents. The predefined categories are formed by different criterions, e.g. “Entertainment”, “Sports” and “Education” in news classification, “Junk Email” and “Ordinary Email” in email classification. In the literature, many algorithms (Sebastiani, 2002; Yang and Liu, 1999; Yang and Pedersen, 1997) have been proposed, such as Support Vector Machines (SVM), k-Nearest Neighbor (kNN), Naı̈ve Bayes (NB) and so on. Empirical evaluations have shown that most of these methods are quite effective in traditional text classification applications.","In past serval years, hierarchical text classification has become an active research topic in database area (Koller and Sahami, 1997; Weigend et al., 1999) and machine learning area (Rousu et al., 2006; Cai and Hofmann, 2007). Different with traditional classification, the document collections are organized as hierarchical class structure in many application fields: web taxonomies (i.e. the Yahoo! Directory http://dir.yahoo.com/ and the Open Directory Project (ODP) http://dmoz.org/), email folders and product catalogs.","The approaches of hierarchical text classification can be divided in three ways: flat, local and global approaches.","The flat approach is traditional multi-class classification in flat fashion without hierarchical class information, which only uses the classes in leaf nodes in taxonomy(Yang and Liu, 1999; Yang and Pedersen, 1997; Qiu et al., 2011).","The local approach proceeds in a top-down fashion, which firstly picks the most relevant categories of the top level and then recursively making the choice among the low-level categories(Sun and Lim, 2001; Liu et al., 2005).","The global approach builds only one classifier to discriminate all categories in a hierarchy(Cai and Hofmann, 2004; Rousu et al., 2006; Miao and Qiu, 2009; Qiu et al., 2009). The essential idea of global approach is that the close classes have some common underlying factors. Especially, the descendant classes can share the characteristics of the ancestor classes, which is similar with multi-task learning(Caruana, 1997; Xue et al., 2007).","Because the global hierarchical categorization can avoid the drawbacks about those high-level irrecoverable error, it is more popular in the machine learning domain.","However, the taxonomy is defined artificially and is usually very difficult to organize for large scale taxonomy. The subclasses of the same parent class may be dissimilar and can be grouped in different concepts, so it bring great challenge to hierarchi-598 Sports Football Basketball Swimming Surfing Sports Water Football Basketball Swimming Surfing Ball (a) (b) College High School College","High","School","Acade","my Figure 1: Example of latent nodes in taxonomy cal classification. For example, the “Sports” node in a taxonomy have six subclasses (Fig. 1a), but these subclass can be grouped into three unobservable concepts (Fig. 1b). These concepts can show the underlying factors more clearly.","In this paper, we claim that each class may have several latent concepts and its subclasses share information with these different concepts respectively. Then we propose a variant Passive-Aggressive (PA) algorithm to maximizes the margins between latent paths.","The rest of the paper is organized as follows. Section 2 describes the basic model of hierarchical classification. Then we propose our algorithm in section 3. Section 4 gives experimental analysis. Section 5 concludes the paper."]},{"title":"2 Hierarchical Text Classification","paragraphs":["In text classification, the documents are often represented with vector space model (VSM) (Salton et al., 1975). Following (Cai and Hofmann, 2007), we incorporate the hierarchical information in feature representation. The basic idea is that the notion of class attributes will allow generalization to take place across (similar) categories and not just across training examples belonging to the same category.","Assuming that the categories is Ω = [ω1, · · · , ωm], where m is the number of the categories, which are organized in hierarchical structure, such as tree or DAG.","Give a sample x with its class path in the taxonomy y, we define the feature is Φ(x, y) = Λ(y) ⊗ x, (1)","where Λ(y) = (λ1(y), · · · , λm(y))T","∈ Rm","and ⊗","is the Kronecker product. We can define λi(y) = { ti if ωi ∈ y 0 otherwise , (2) where ti >= 0 is the attribute value for node v. In the simplest case, ti can be set to a constant, like 1.","Thus, we can classify x with a score function,","ŷ = arg max y F (w, Φ(x, y)), (3) where w is the parameter of F (·)."]},{"title":"3 Hierarchical Text Classification with Latent Concepts","paragraphs":["In this section, we first extent the Passive-Aggressive (PA) algorithm to the hierarchical classification (HPA), then we modify it to incorporate latent concepts (LHPA). 3.1 Hierarchical Passive-Aggressive Algorithm The PA algorithm is an online learning algorithm, which aims to find the new weight vector wt+1 to be the solution to the following constrained optimiza-tion problem in round t.","wt+1 = arg min w∈Rn 1 2","||w − wt||2 + Cξ s.t. l(w; (xt, yt)) <= ξ and ξ >= 0. (4) where l(w; (xt, yt)) is the hinge-loss function and ξ is slack variable.","Since the hierarchical text classification is losssensitive based on the hierarchical structure. We need discriminate the misclassification from “nearly correct” to “clearly incorrect”. Here we use tree induced error ∆(y, y′","), which is the shortest path connecting the nodes yleaf and y′","leaf . yleaf represents the leaf node in path y.","Given a example (x, y), we look for the w to maximize the separation margin γ(w; (x, y)) between the score of the correct path y and the closest error path ŷ.","γ(w; (x, y)) = wT Φ(x, y) − wT","Φ(x, ŷ), (5) 599 where ŷ = arg maxz̸=y wT","Φ(x, z) and Φ is a feature function.","Unlike the standard PA algorithm, which achieve a margin of at least 1 as often as possible, we wish the margin is related to tree induced error ∆(y, ŷ).","This loss is defined by the following function, l(w; (x, y)) = { 0, γ(w; (x, y)) > ∆(y, ŷ) ∆(y, ŷ) − γ(w; (x, y)), otherwise (6)","We abbreviate l(w; (x, y)) to l. If l = 0 then wt itself satisfies the constraint in Eq. (4) and is clearly the optimal solution. We therefore concentrate on the case where l > 0.","First, we define the Lagrangian of the optimiza-tion problem in Eq. (4) to be, L(w, ξ, α, β) = 1 2","||w−wt||2 +Cξ +α(l−ξ)−βξ s.t. α, β >= 0. (7)","where α, β is a Lagrange multiplier. We set the gradient of Eq. (7) respect to ξ to zero. α + β = C. (8) The gradient of w should be zero. w − wt − α(Φ(x, y) − Φ(x, ŷ)) = 0 (9) Then we get, w = wt + α(Φ(x, y) − Φ(x, ŷ)). (10)","Substitute Eq. (8) and Eq. (10) to objective function Eq. (7), we get L(α) = − 1 2","α2 ||Φ(x, y) − Φ(x, ŷ)||2 + αwt(Φ(x, y) − Φ(x, ŷ))) − α∆(y, ŷ) (11)","Differentiate Eq. (11 with α, and set it to zero, we get","α∗ = ∆(y, ŷ) − wt(Φ(x, y) − Φ(x, ŷ)))","||Φ(x, y) − Φ(x, ŷ)||2 (12) From α + β = C, we know that α < C, so","α∗","= min(C, ∆(y, ŷ) − wt(Φ(x, y) − Φ(x, ŷ)))","||Φ(x, y) − Φ(x, ŷ)||2 ).","(13)","3.2 Hierarchical Passive-Aggressive Algorithm with Latent Concepts For the hierarchical taxonomy Ω = (ω1, · · · , ωc), we define that each class ωi has a set Hωi = h1 ωi, · · · , hm","ωi with m latent concepts, which are unobservable.","Given a label path y, it has a set of several latent paths Hy. For a latent path z ∈ Hy, a function P roj(z)",".","= y is the projection from a latent path z","to its corresponding path y.","Then we can define the predict latent path h∗","and","the most correct latent path ĥ:","ĥ = arg max proj(z)̸=y","wT Φ(x, z), (14)","h∗ = arg max","proj(z)=y","wT Φ(x, z). (15)","Similar to the above analysis of HPA, we re-define the margin","γ(w; (x, y) = wT","Φ(x, h∗",") − wT","Φ(x, ĥ), (16) then we get the optimal update step α∗ L = min(C, l(wt; (x, y))","||Φ(x, h∗",") − Φ(x, ĥ)||2 ). (17) Finally, we get update strategy,","w = wt + α∗ L(Φ(x, h∗",") − Φ(x, ĥ)). (18)","Our hierarchical passive-aggressive algorithm with latent concepts (LHPA) is shown in Algorithm 1. In this paper, we use two latent concepts for each class."]},{"title":"4 Experiment 4.1 Datasets","paragraphs":["We evaluate our proposed algorithm on two datasets with hierarchical category structure.","WIPO-alpha dataset The dataset1","consisted of the 1372 training and 358 testing document comprising the D section of the hierarchy. The number of nodes in the hierarchy was 188, with maximum depth 3. The dataset was processed into bag-of-words representation with TF·IDF","1 World Intellectual Property Organization, http://www.","wipo.int/classifications/en 600 input : training data set: (xn, yn), n = 1, · · · , N ,","and parameters: C, K output: w","Initialize: cw ← 0,;","for k = 0 · · · K − 1 do","w0 ← 0 ;","for t = 0 · · · T − 1 do get (xt, yt) from data set; predict ĥ, h∗","; calculate γ(w; (x, y)) and∆(yt, ŷt); if γ(w; (x, y)) ≤ ∆(yt, ŷt) then","calculate α∗","L by Eq. (17);","update wt+1 by Eq. (18). ; end end cw = cw + wT ; end w = cw/K ; Algorithm 1: Hierarchical PA algorithm with latent concepts weighting. No word stemming or stop-word removal was performed. This dataset is used in (Rousu et al., 2006).","LSHTC dataset The dataset2","has been constructed by crawling web pages that are found in the Open Directory Project (ODP) and translating them into feature vectors (content vectors) and splitting the set of Web pages into a training, a validation and a test set, per ODP category. Here, we use the dry-run dataset(task 1). 4.2 Performance Measurement Macro Precision, Macro Recall and Macro F 1 are the most widely used performance measurements for text classification problems nowadays. The macro strategy computes macro precision and recall scores by averaging the precision/recall of each category, which is preferred because the categories are usually unbalanced and give more challenges to classifiers. The Macro F 1 score is computed using the standard formula applied to the macro-level precision and recall scores. M acroF 1 = P × R P + R , (19) 2 Large Scale Hierarchical Text classification Pascal Chal-","lenge, http://lshtc.iit.demokritos.gr","Table 1: Results on WIPO-alpha Dataset.“-” means that","the result is not available in the author’s paper.","Accuracy F1 Precision Recall TIE PA 49.16 40.71 43.27 38.44 2.06 HPA 50.84 40.26 43.23 37.67 1.92 LHPA 51.96 41.84 45.56 38.69 1.87 HSVM 23.8 - - - - HM3 35.0 - - - - Table 2: Results on LSHTC dry-run Dataset","Accuracy F1 Precision Recall TIE PA 47.36 44.63 52.64 38.73 3.68 HPA 46.88 43.78 51.26 38.2 3.73 LHPA 48.39 46.26 53.82 40.56 3.43 where P is the Macro Precision and R is the Macro Recall. We also use tree induced error (TIE) in the experiments. 4.3 Results We implement three algorithms3",": PA(Flat PA), HPA(Hierarchical PA) and LHPA(Hierarchical PA with latent concepts). The results are shown in Table 1 and 2. For WIPO-alpha dataset, we also compared LHPA with two algorithms used in (Rousu et al., 2006): HSVM and HM3.","We can see that LHPA has better performances than the other methods. From Table 2, we can see that it is not always useful to incorporate the hierarchical information. Though the subclasses can share information with their parent class, the shared information may be different for each subclass. So we should decompose the underlying factors into different latent concepts."]},{"title":"5 Conclusion","paragraphs":["In this paper, we propose a variant Passive-Aggressive algorithm for hierarchical text classification with latent concepts. In the future, we will investigate our method in the larger and more noisy data."]},{"title":"Acknowledgments","paragraphs":["This work was (partially) funded by NSFC (No. 61003091 and No. 61073069), 973 Program (No. 3 Source codes are available in FudanNLP toolkit, http:","//code.google.com/p/fudannlp/ 601 2010CB327906) and Shanghai Committee of Science and Technology(No. 10511500703)."]},{"title":"References","paragraphs":["L. Cai and T. Hofmann. 2004. Hierarchical document categorization with support vector machines. In Proceedings of CIKM.","L. Cai and T. Hofmann. 2007. Exploiting known taxonomies in learning overlapping concepts. In Proceedings of International Joint Conferences on Artificial Intelligence.","R. Caruana. 1997. Multi-task learning. Machine Learning, 28(1):41–75.","D. Koller and M Sahami. 1997. Hierarchically classify-ing documents using very few words. In Proceedings of the International Conference on Machine Learning (ICML).","T.Y. Liu, Y. Yang, H. Wan, H.J. Zeng, Z. Chen, and W.Y. Ma. 2005. Support vector machines classification with a very large-scale taxonomy. ACM SIGKDD Explorations Newsletter, 7(1):43.","Youdong Miao and Xipeng Qiu. 2009. Hierarchical centroid-based classifier for large scale text classification. In Large Scale Hierarchical Text classification (LSHTC) Pascal Challenge.","Xipeng Qiu, Wenjun Gao, and Xuanjing Huang. 2009. Hierarchical multi-class text categorization with global margin maximization. In Proceedings of the ACL-IJCNLP 2009 Conference, pages 165–168, Suntec, Singapore, August. Association for Computational Linguistics.","Xipeng Qiu, Jinlong Zhou, and Xuanjing Huang. 2011. An effective feature selection method for text categorization. In Proceedings of the 15th Pacific-Asia Conference on Knowledge Discovery and Data Mining.","Juho Rousu, Craig Saunders, Sandor Szedmak, and John Shawe-Taylor. 2006. Kernel-based learning of hierarchical multilabel classification models. In Journal of Machine Learning Research.","G. Salton, A. Wong, and CS Yang. 1975. A vector space model for automatic indexing. Communications of the ACM, 18(11):613–620.","F. Sebastiani. 2002. Machine learning in automated text categorization. ACM computing surveys, 34(1):1–47.","A. Sun and E.-P Lim. 2001. Hierarchical text classification and evaluation. In Proceedings of the IEEE International Conference on Data Mining.","A. Weigend, E. Wiener, and J Pedersen. 1999. Exploiting hierarchy in text categorization. In Information Retrieval.","Y. Xue, X. Liao, L. Carin, and B. Krishnapuram. 2007. Multi-task learning for classification with dirichlet process priors. The Journal of Machine Learning Research, 8:63.","Y. Yang and X. Liu. 1999. A re-examination of text categorization methods. In Proc. of SIGIR. ACM Press New York, NY, USA.","Y. Yang and J.O. Pedersen. 1997. A comparative study on feature selection in text categorization. In Proc. of Int. Conf. on Mach. Learn. (ICML), volume 97. 602"]}],"references":[{"authors":[{"first":"L.","last":"Cai"},{"first":"T.","last":"Hofmann"}],"year":"2004","title":"Hierarchical document categorization with support vector machines","source":"L. Cai and T. Hofmann. 2004. Hierarchical document categorization with support vector machines. In Proceedings of CIKM."},{"authors":[{"first":"L.","last":"Cai"},{"first":"T.","last":"Hofmann"}],"year":"2007","title":"Exploiting known taxonomies in learning overlapping concepts","source":"L. Cai and T. Hofmann. 2007. Exploiting known taxonomies in learning overlapping concepts. In Proceedings of International Joint Conferences on Artificial Intelligence."},{"authors":[{"first":"R.","last":"Caruana"}],"year":"1997","title":"Multi-task learning","source":"R. Caruana. 1997. Multi-task learning. Machine Learning, 28(1):41–75."},{"authors":[{"first":"D.","last":"Koller"},{"first":"M","last":"Sahami"}],"year":"1997","title":"Hierarchically classify-ing documents using very few words","source":"D. Koller and M Sahami. 1997. Hierarchically classify-ing documents using very few words. In Proceedings of the International Conference on Machine Learning (ICML)."},{"authors":[{"first":"T.","middle":"Y.","last":"Liu"},{"first":"Y.","last":"Yang"},{"first":"H.","last":"Wan"},{"first":"H.","middle":"J.","last":"Zeng"},{"first":"Z.","last":"Chen"},{"first":"W.","middle":"Y.","last":"Ma"}],"year":"2005","title":"Support vector machines classification with a very large-scale taxonomy","source":"T.Y. Liu, Y. Yang, H. Wan, H.J. Zeng, Z. Chen, and W.Y. Ma. 2005. Support vector machines classification with a very large-scale taxonomy. ACM SIGKDD Explorations Newsletter, 7(1):43."},{"authors":[{"first":"Youdong","last":"Miao"},{"first":"Xipeng","last":"Qiu"}],"year":"2009","title":"Hierarchical centroid-based classifier for large scale text classification","source":"Youdong Miao and Xipeng Qiu. 2009. Hierarchical centroid-based classifier for large scale text classification. In Large Scale Hierarchical Text classification (LSHTC) Pascal Challenge."},{"authors":[{"first":"Xipeng","last":"Qiu"},{"first":"Wenjun","last":"Gao"},{"first":"Xuanjing","last":"Huang"}],"year":"2009","title":"Hierarchical multi-class text categorization with global margin maximization","source":"Xipeng Qiu, Wenjun Gao, and Xuanjing Huang. 2009. Hierarchical multi-class text categorization with global margin maximization. In Proceedings of the ACL-IJCNLP 2009 Conference, pages 165–168, Suntec, Singapore, August. Association for Computational Linguistics."},{"authors":[{"first":"Xipeng","last":"Qiu"},{"first":"Jinlong","last":"Zhou"},{"first":"Xuanjing","last":"Huang"}],"year":"2011","title":"An effective feature selection method for text categorization","source":"Xipeng Qiu, Jinlong Zhou, and Xuanjing Huang. 2011. An effective feature selection method for text categorization. In Proceedings of the 15th Pacific-Asia Conference on Knowledge Discovery and Data Mining."},{"authors":[{"first":"Juho","last":"Rousu"},{"first":"Craig","last":"Saunders"},{"first":"Sandor","last":"Szedmak"},{"first":"John","last":"Shawe-Taylor"}],"year":"2006","title":"Kernel-based learning of hierarchical multilabel classification models","source":"Juho Rousu, Craig Saunders, Sandor Szedmak, and John Shawe-Taylor. 2006. Kernel-based learning of hierarchical multilabel classification models. In Journal of Machine Learning Research."},{"authors":[{"first":"G.","last":"Salton"},{"first":"A.","last":"Wong"},{"first":"CS","last":"Yang"}],"year":"1975","title":"A vector space model for automatic indexing","source":"G. Salton, A. Wong, and CS Yang. 1975. A vector space model for automatic indexing. Communications of the ACM, 18(11):613–620."},{"authors":[{"first":"F.","last":"Sebastiani"}],"year":"2002","title":"Machine learning in automated text categorization","source":"F. Sebastiani. 2002. Machine learning in automated text categorization. ACM computing surveys, 34(1):1–47."},{"authors":[{"first":"A.","last":"Sun"},{"first":"E.","middle":"-P","last":"Lim"}],"year":"2001","title":"Hierarchical text classification and evaluation","source":"A. Sun and E.-P Lim. 2001. Hierarchical text classification and evaluation. In Proceedings of the IEEE International Conference on Data Mining."},{"authors":[{"first":"A.","last":"Weigend"},{"first":"E.","last":"Wiener"},{"first":"J","last":"Pedersen"}],"year":"1999","title":"Exploiting hierarchy in text categorization","source":"A. Weigend, E. Wiener, and J Pedersen. 1999. Exploiting hierarchy in text categorization. In Information Retrieval."},{"authors":[{"first":"Y.","last":"Xue"},{"first":"X.","last":"Liao"},{"first":"L.","last":"Carin"},{"first":"B.","last":"Krishnapuram"}],"year":"2007","title":"Multi-task learning for classification with dirichlet process priors","source":"Y. Xue, X. Liao, L. Carin, and B. Krishnapuram. 2007. Multi-task learning for classification with dirichlet process priors. The Journal of Machine Learning Research, 8:63."},{"authors":[{"first":"Y.","last":"Yang"},{"first":"X.","last":"Liu"}],"year":"1999","title":"A re-examination of text categorization methods","source":"Y. Yang and X. Liu. 1999. A re-examination of text categorization methods. In Proc. of SIGIR. ACM Press New York, NY, USA."},{"authors":[{"first":"Y.","last":"Yang"},{"first":"J.","middle":"O.","last":"Pedersen"}],"year":"1997","title":"A comparative study on feature selection in text categorization","source":"Y. Yang and J.O. Pedersen. 1997. A comparative study on feature selection in text categorization. In Proc. of Int. Conf. on Mach. Learn. (ICML), volume 97. 602"}],"cites":[{"style":0,"text":"Sebastiani, 2002","origin":{"pointer":"/sections/2/paragraphs/0","offset":340,"length":16},"authors":[{"last":"Sebastiani"}],"year":"2002","references":["/references/10"]},{"style":0,"text":"Yang and Liu, 1999","origin":{"pointer":"/sections/2/paragraphs/0","offset":358,"length":18},"authors":[{"last":"Yang"},{"last":"Liu"}],"year":"1999","references":["/references/14"]},{"style":0,"text":"Yang and Pedersen, 1997","origin":{"pointer":"/sections/2/paragraphs/0","offset":378,"length":23},"authors":[{"last":"Yang"},{"last":"Pedersen"}],"year":"1997","references":["/references/15"]},{"style":0,"text":"Koller and Sahami, 1997","origin":{"pointer":"/sections/2/paragraphs/1","offset":109,"length":23},"authors":[{"last":"Koller"},{"last":"Sahami"}],"year":"1997","references":["/references/3"]},{"style":0,"text":"Weigend et al., 1999","origin":{"pointer":"/sections/2/paragraphs/1","offset":134,"length":20},"authors":[{"last":"Weigend"},{"last":"al."}],"year":"1999","references":["/references/12"]},{"style":0,"text":"Rousu et al., 2006","origin":{"pointer":"/sections/2/paragraphs/1","offset":183,"length":18},"authors":[{"last":"Rousu"},{"last":"al."}],"year":"2006","references":["/references/8"]},{"style":0,"text":"Cai and Hofmann, 2007","origin":{"pointer":"/sections/2/paragraphs/1","offset":203,"length":21},"authors":[{"last":"Cai"},{"last":"Hofmann"}],"year":"2007","references":["/references/1"]},{"style":0,"text":"Yang and Liu, 1999","origin":{"pointer":"/sections/2/paragraphs/3","offset":170,"length":18},"authors":[{"last":"Yang"},{"last":"Liu"}],"year":"1999","references":["/references/14"]},{"style":0,"text":"Yang and Pedersen, 1997","origin":{"pointer":"/sections/2/paragraphs/3","offset":190,"length":23},"authors":[{"last":"Yang"},{"last":"Pedersen"}],"year":"1997","references":["/references/15"]},{"style":0,"text":"Qiu et al., 2011","origin":{"pointer":"/sections/2/paragraphs/3","offset":215,"length":16},"authors":[{"last":"Qiu"},{"last":"al."}],"year":"2011","references":["/references/7"]},{"style":0,"text":"Sun and Lim, 2001","origin":{"pointer":"/sections/2/paragraphs/4","offset":187,"length":17},"authors":[{"last":"Sun"},{"last":"Lim"}],"year":"2001","references":["/references/11"]},{"style":0,"text":"Liu et al., 2005","origin":{"pointer":"/sections/2/paragraphs/4","offset":206,"length":16},"authors":[{"last":"Liu"},{"last":"al."}],"year":"2005","references":["/references/4"]},{"style":0,"text":"Cai and Hofmann, 2004","origin":{"pointer":"/sections/2/paragraphs/5","offset":93,"length":21},"authors":[{"last":"Cai"},{"last":"Hofmann"}],"year":"2004","references":["/references/0"]},{"style":0,"text":"Rousu et al., 2006","origin":{"pointer":"/sections/2/paragraphs/5","offset":116,"length":18},"authors":[{"last":"Rousu"},{"last":"al."}],"year":"2006","references":["/references/8"]},{"style":0,"text":"Miao and Qiu, 2009","origin":{"pointer":"/sections/2/paragraphs/5","offset":136,"length":18},"authors":[{"last":"Miao"},{"last":"Qiu"}],"year":"2009","references":["/references/5"]},{"style":0,"text":"Qiu et al., 2009","origin":{"pointer":"/sections/2/paragraphs/5","offset":156,"length":16},"authors":[{"last":"Qiu"},{"last":"al."}],"year":"2009","references":["/references/6"]},{"style":0,"text":"Caruana, 1997","origin":{"pointer":"/sections/2/paragraphs/5","offset":408,"length":13},"authors":[{"last":"Caruana"}],"year":"1997","references":["/references/2"]},{"style":0,"text":"Xue et al., 2007","origin":{"pointer":"/sections/2/paragraphs/5","offset":423,"length":16},"authors":[{"last":"Xue"},{"last":"al."}],"year":"2007","references":["/references/13"]},{"style":0,"text":"Salton et al., 1975","origin":{"pointer":"/sections/3/paragraphs/0","offset":91,"length":19},"authors":[{"last":"Salton"},{"last":"al."}],"year":"1975","references":["/references/9"]},{"style":0,"text":"Cai and Hofmann, 2007","origin":{"pointer":"/sections/3/paragraphs/0","offset":124,"length":21},"authors":[{"last":"Cai"},{"last":"Hofmann"}],"year":"2007","references":["/references/1"]},{"style":0,"text":"Rousu et al., 2006","origin":{"pointer":"/sections/5/paragraphs/13","offset":214,"length":18},"authors":[{"last":"Rousu"},{"last":"al."}],"year":"2006","references":["/references/8"]},{"style":0,"text":"Rousu et al., 2006","origin":{"pointer":"/sections/5/paragraphs/21","offset":199,"length":18},"authors":[{"last":"Rousu"},{"last":"al."}],"year":"2006","references":["/references/8"]}]}
