{"sections":[{"title":"","paragraphs":["Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 590–599, Portland, Oregon, June 19-24, 2011. c⃝2011 Association for Computational Linguistics"]},{"title":"Learning Dependency-Based Compositional Semantics Percy Liang UC Berkeley pliang@cs.berkeley.edu Michael I. Jordan UC Berkeley jordan@cs.berkeley.edu Dan Klein UC Berkeley klein@cs.berkeley.edu Abstract","paragraphs":["Compositional question answering begins by mapping questions to logical forms, but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms. In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs. In tackling this challenging learning problem, we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms. On two standard semantic parsing benchmarks (GEO and JOBS), our system obtains the highest published accuracies, despite requiring no annotated logical forms."]},{"title":"1 Introduction","paragraphs":["What is the total population of the ten largest capitals in the US? Answering these types of complex questions compositionally involves first mapping the questions into logical forms (semantic parsing). Supervised semantic parsers (Zelle and Mooney, 1996; Tang and Mooney, 2001; Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Kwiatkowski et al., 2010) rely on manual annotation of logical forms, which is expensive. On the other hand, existing unsupervised semantic parsers (Poon and Domingos, 2009) do not handle deeper linguistic phenomena such as quantification, negation, and superlatives.","As in Clarke et al. (2010), we obviate the need for annotated logical forms by considering the end-to-end problem of mapping questions to answers. However, we still model the logical form (now as a latent variable) to capture the complexities of language. Figure 1 shows our probabilistic model: (parameters) (world) θ w x z y (question) (logical form) (answer) state with the largest area x1","x1 1 1 cc argmax area state ∗∗ Alaska z ∼ pθ(z | x) y = JzKw Semantic Parsing Evaluation Figure 1: Our probabilistic model: a question x is mapped to a latent logical form z, which is then evaluated with respect to a world w (database of facts), producing an answer y. We represent logical forms z as labeled trees, induced automatically from (x, y) pairs. We want to induce latent logical forms z (and parameters θ) given only question-answer pairs (x, y), which is much cheaper to obtain than (x, z) pairs.","The core problem that arises in this setting is program induction: finding a logical form z (over an exponentially large space of possibilities) that produces the target answer y. Unlike standard semantic parsing, our end goal is only to generate the correct y, so we are free to choose the representation for z. Which one should we use?","The dominant paradigm in compositional semantics is Montague semantics, which constructs lambda calculus forms in a bottom-up manner. CCG is one instantiation (Steedman, 2000), which is used by many semantic parsers, e.g., Zettlemoyer and Collins (2005). However, the logical forms there can become quite complex, and in the context of program induction, this would lead to an unwieldy search space. At the same time, representations such as FunQL (Kate et al., 2005), which was used in 590 Clarke et al. (2010), are simpler but lack the full expressive power of lambda calculus.","The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2). The logical forms in this framework are trees, which is desirable for two reasons: (i) they parallel syntactic dependency trees, which facilitates parsing and learning; and (ii) evaluating them to obtain the answer is computationally efficient.","We trained our model using an EM-like algorithm (Section 3) on two benchmarks, GEO and JOBS (Section 4). Our system outperforms all existing systems despite using no annotated logical forms."]},{"title":"2 Semantic Representation","paragraphs":["We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics. We then introduce the full version (Section 2.2), which handles linguistic phenomena such as quantification, where syntactic and semantic scope diverge.","We start with some definitions, using US geography as an example domain. Let V be the set of all values, which includes primitives (e.g., 3, CA ∈ V) as well as sets and tuples formed from other values (e.g., 3, {3, 4, 7}, (CA, {5}) ∈ V). Let P be a set of predicates (e.g., state, count ∈ P), which are just symbols.","A world w is mapping from each predicate p ∈ P to a set of tuples; for example, w(state) = {(CA), (OR), . . . }. Conceptually, a world is a rela-tional database where each predicate is a relation (possibly infinite). Define a special predicate ø with w(ø) = V. We represent functions by a set of inputoutput pairs, e.g., w(count) = {(S, n) : n = |S|}. As another example, w(average) = {(S, x̄) : x̄ = |S1|−1 ∑","x∈S1 S(x)}, where a set of pairs S is treated as a set-valued function S(x) = {y : (x, y) ∈ S} with domain S1 = {x : (x, y) ∈ S}.","The logical forms in DCS are called DCS trees, where nodes are labeled with predicates, and edges are labeled with relations. Formally: Definition 1 (DCS trees) Let Z be the set of DCS trees, where each z ∈ Z consists of (i) a predicate Relations R j j′ (join) E (extract) Σ (aggregate) Q (quantify) Xi (execute) C (compare)","Table 1: Possible relations appearing on the edges of a","DCS tree. Here, j, j′","∈ {1, 2, . . . } and i ∈ {1, 2, . . . }∗",". z.p ∈ P and (ii) a sequence of edges z.e1, . . . , z.em, each edge e consisting of a relation e.r ∈ R (see Table 1) and a child tree e.c ∈ Z. We write a DCS tree z as ⟨p; r1 : c1; . . . ; rm : cm⟩. Figure 2(a) shows an example of a DCS tree. Although a DCS tree is a logical form, note that it looks like a syntactic dependency tree with predicates in place of words. It is this transparency between syntax and semantics provided by DCS which leads to a simple and streamlined compositional semantics suitable for program induction. 2.1 Basic Version The basic version of DCS restricts R to join and aggregate relations (see Table 1). Let us start by considering a DCS tree z with only join relations. Such a z defines a constraint satisfaction problem (CSP) with nodes as variables. The CSP has two types of constraints: (i) x ∈ w(p) for each node x labeled with predicate p ∈ P; and (ii) xj = yj′ (the j-th component of x must equal the j′","-th component of y) for each edge (x, y) labeled with","j","j′ ∈ R.","A solution to the CSP is an assignment of nodes to values that satisfies all the constraints. We say a value v is consistent for a node x if there exists a solution that assigns v to x. The denotation zw (z evaluated on w) is the set of consistent values of the root node (see Figure 2 for an example). Computation We can compute the denotation zw of a DCS tree z by exploiting dynamic programming on trees (Dechter, 2003). The recurrence is as follows: 〈 p; j1 j′ 1 : c1; · · · ; jm j′ m : cm 〉  w (1) = w(p) ∩ m ⋂","i=1{v : vji = tj′ i, t ∈ ciw}. At each node, we compute the set of tuples v consistent with the predicate at that node (v ∈ w(p)), and 591 Example: major city in California","z = ⟨city; 1","1 : ⟨major⟩ ; 1","1 : ⟨loc; 2","1 : ⟨CA⟩⟩⟩ 1 1 1","1 major 2 1 CA loc city","λc ∃m ∃l ∃s . city(c) ∧ major(m)∧ loc(l) ∧ CA(s)∧ c1 = m1 ∧ c1 = l1 ∧ l2 = s1 (a) DCS tree (b) Lambda calculus formula (c) Denotation: JzKw = {SF, LA, . . . } Figure 2: (a) An example of a DCS tree (written in both the mathematical and graphical notation). Each node is labeled with a predicate, and each edge is labeled with a relation. (b) A DCS tree z with only join relations encodes a constraint satisfaction problem. (c) The denotation of z is the set of consistent values for the root node. for each child i, the ji-th component of v must equal the j′","i-th component of some t in the child’s denotation (t ∈ ciw). This algorithm is linear in the number of nodes times the size of the denotations.1","Now the dual importance of trees in DCS is clear: We have seen that trees parallel syntactic dependency structure, which will facilitate parsing. In addition, trees enable efficient computation, thereby establishing a new connection between dependency syntax and efficient semantic evaluation. Aggregate relation DCS trees that only use join relations can represent arbitrarily complex compositional structures, but they cannot capture higher-order phenomena in language. For example, consider the phrase number of major cities, and suppose that number corresponds to the count predicate. It is impossible to represent the semantics of this phrase with just a CSP, so we introduce a new aggregate relation, notated Σ. Consider a tree ⟨Σ : c⟩, whose root is connected to a child c via Σ. If the denotation of c is a set of values s, the parent’s denotation is then a singleton set containing s. Formally: ⟨Σ : c⟩w = {cw}. (2)","Figure 3(a) shows the DCS tree for our running example. The denotation of the middle node is {s},","1","Infinite denotations (such as<w) are represented as implicit sets on which we can perform membership queries. The intersection of two sets can be performed as long as at least one of the sets is finite. number of major cities 1 2 1 1 ΣΣ 1 1 major city ∗∗ count ∗∗","average population of major cities 1 2 1 1 ΣΣ 1 1 1 1 major city population ∗∗ average ∗∗ (a) Counting (b) Averaging Figure 3: Examples of DCS trees that use the aggregate relation (Σ) to (a) compute the cardinality of a set and (b) take the average over a set. where s is all major cities. Having instantiated s as a value, everything above this node is an ordinary CSP: s constrains the count node, which in turns constrains the root node to |s|.","A DCS tree that contains only join and aggregate relations can be viewed as a collection of tree-structured CSPs connected via aggregate relations. The tree structure still enables us to compute denotations efficiently based on (1) and (2). 2.2 Full Version The basic version of DCS described thus far handles a core subset of language. But consider Figure 4: (a) is headed by borders, but states needs to be extracted; in (b), the quantifier no is syntactically dominated by the head verb borders but needs to take wider scope. We now present the full version of DCS which handles this type of divergence between syntactic and semantic scope.","The key idea that allows us to give semanticallyscoped denotations to syntactically-scoped trees is as follows: We mark a node low in the tree with a mark relation (one of E, Q, or C). Then higher up in the tree, we invoke it with an execute relation Xi to create the desired semantic scope.2","This mark-execute construct acts non-locally, so to maintain compositionality, we must augment the 2","Our mark-execute construct is analogous to Montague’s quantifying in, Cooper storage, and Carpenter’s scoping constructor (Carpenter, 1998). 592 California borders which states? x1 x1 2 1 1 1 CA ee ∗∗ state border ∗∗ Alaska borders no states. x1 x1 2 1 1 1 AK qq no state border ∗∗ Some river traverses every city. x12 x12 2 1 1 1 qq some river qq every city traverse ∗∗ x21 x21 2 1 1 1 qq some river qq every city traverse ∗∗ (narrow) (wide) city traversed by no rivers x12 x12 1 2ee ∗∗ 1 1 qq no river traverse city ∗∗ (a) Extraction (e) (b) Quantification (q) (c) Quantifier ambiguity (q, q) (d) Quantification (q, e) state bordering the most states x12 x12 1 1ee ∗∗ 2 1 cc argmax state border state ∗∗","state bordering more states than Texas x12 x12 1 1ee ∗∗ 2 1 cc 3 1 TX more state border state ∗∗ state bordering the largest state 1 1 2 1 x12 x12 1 1ee ∗∗ cc argmax size state ∗∗ border state x12 x12 1 1ee ∗∗ 2 1 1 1 cc argmax size state border state ∗∗ (absolute) (relative)","Every state’s largest city is major. x1 x1 x2 x2 1 1 1 1 2 1 qq every state loc cc argmax size city major ∗∗ (e) Superlative (c) (f) Comparative (c) (g) Superlative ambiguity (c) (h) Quantification+Superlative (q, c) Figure 4: Example DCS trees for utterances in which syntactic and semantic scope diverge. These trees reflect the syntactic structure, which facilitates parsing, but importantly, these trees also precisely encode the correct semantic scope. The main mechanism is using a mark relation (E, Q, or C) low in the tree paired with an execute relation (Xi) higher up at the desired semantic point. denotation d = zw to include any information about the marked nodes in z that can be accessed by an execute relation later on. In the basic version, d was simply the consistent assignments to the root. Now d contains the consistent joint assignments to the active nodes (which include the root and all marked nodes), as well as information stored about each marked node. Think of d as consisting of n columns, one for each active node according to a pre-order traversal of z. Column 1 always corresponds to the root node. Formally, a denotation is defined as follows (see Figure 5 for an example): Definition 2 (Denotations) Let D be the set of denotations, where each d ∈ D consists of","• a set of arrays d.A, where each array a = [a1, . . . , an] ∈ d.A is a sequence of n tuples (ai ∈ V∗","); and • a list of n stores d.α = (d.α1, . . . , d.αn), where each store α contains a mark relation α.r ∈ {E, Q, C, ø}, a base denotation α.b ∈ D ∪ {ø}, and a child denotation α.c ∈ D ∪ {ø}. We write d as ⟨⟨A; (r1, b1, c1); . . . ; (rn, bn, cn)⟩⟩. We use d{ri = x} to mean d with d.ri = d.αi.r = x (similar definitions apply ford{αi = x}, d{bi = x}, and d{ci = x}).","The denotation of a DCS tree can now be defined recursively: ⟨p⟩w = ⟨⟨{[v] : v ∈ w(p)}; ø⟩⟩, (3)","〈 p; e;","j","j′ : c〉","","w = p; ew ▷◁j,j′ cw, (4) ⟨p; e; Σ : c⟩w = p; ew ▷◁∗,∗ Σ (cw) , (5) ⟨p; e; Xi : c⟩w = p; ew ▷◁∗,∗ Xi(cw), (6) ⟨p; e; E : c⟩w = M(p; ew, E, c), (7) ⟨p; e; C : c⟩w = M(p; ew, C, c), (8) ⟨p; Q : c; e⟩w = M(p; ew, Q, c). (9) 593 1 1 2 1 1 1 cc argmax size state border state J·Kw column 1 column 2 A: (OK) (NM) (NV) · · · (TX,2.7e5) (TX,2.7e5) (CA,1.6e5)","· · ·","r: ø c","b: ø J⟨size⟩Kw","c: ø J⟨argmax⟩Kw DCS tree Denotation Figure 5: Example of the denotation for a DCS tree with a compare relation C. This denotation has two columns, one for each active node—the root node state and the marked node size.","The base case is defined in (3): if z is a single node with predicate p, then the denotation of z has one column with the tuples w(p) and an empty store. The other six cases handle different edge relations. These definitions depend on several operations (▷◁j,j′, Σ, Xi, M) which we will define shortly, but let us first get some intuition.","Let z be a DCS tree. If the last child c of z’s root is a join (","j","j′), aggregate (Σ), or execute (Xi) relation ((4)–(6)), then we simply recurse on z with c removed and join it with some transformation (identity, Σ, or Xi) of c’s denotation. If the last (or first) child is connected via a mark relation E, C (or Q), then we strip off that child and put the appropriate information in the store by invoking M.","We now define the operations ▷◁j,j′, Σ, Xi, M. Some helpful notation: For a sequence v = (v1, . . . , vn) and indices i = (i1, . . . , ik), let vi = (vi1, . . . , vik ) be the projection of v onto i; we write v−i to mean v[1,...,n]\\i. Extending this notation to denotations, let ⟨⟨A; α⟩⟩[i] = ⟨⟨{ai : a ∈ A}; αi⟩⟩. Let d[−ø] = d[−i], where i are the columns with empty stores. For example, for d in Figure 5, d[1] keeps column 1, d[−ø] keeps column 2, and d[2, −2] swaps the two columns. Join The join of two denotations d and d′","with respect to components j and j′","(∗ means all components) is formed by concatenating all arrays a of d with all compatible arrays a′","of d′",", where compatibility means a1j = a′","1j′. The stores are also concatenated (α + α′","). Non-initial columns with empty stores are projected away by applying ·[1,−ø]. The","full definition of join is as follows:","⟨⟨A; α⟩⟩ ▷◁j,j′ ⟨⟨A′ ; α′ ⟩⟩ = ⟨⟨A′′","; α + α′","⟩⟩[1,−ø],","A′′","= {a + a′",": a ∈ A, a′","∈ A′",", a1j = a′","1j′}. (10)","Aggregate The aggregate operation takes a deno-","tation and forms a set out of the tuples in the first","column for each setting of the rest of the columns:","Σ (⟨⟨A; α⟩⟩) = ⟨⟨A′","∪ A′′","; α⟩⟩ (11)","A′","= {[S(a), a2, . . . , an] : a ∈ A}","S(a) = {a′ 1 : [a′","1, a2, . . . , an] ∈ A}","A′′ = {[∅, a2, . . . , an] : ¬∃a1, a ∈ A, ∀2 ≤ i ≤ n, [ai] ∈ d.bi[1].A}. 2.2.1 Mark and Execute","Now we turn to the mark (M) and execute (Xi) operations, which handles the divergence between syntactic and semantic scope. In some sense, this is the technical core of DCS. Marking is simple: When a node (e.g., size in Figure 5) is marked (e.g., with relation C), we simply put the relation r, current denotation d and child c’s denotation into the store of column 1: M(d, r, c) = d{r1 = r, b1 = d, c1 = cw}. (12)","The execute operation Xi(d) processes columns i in reverse order. It suffices to defineXi(d) for a single column i. There are three cases: Extraction (d.ri = E) In the basic version, the denotation of a tree was always the set of consistent values of the root node. Extraction allows us to return the set of consistent values of a marked non-root node. Formally, extraction simply moves the i-th column to the front: Xi(d) = d[i, −(i, ø)]{α1 = ø}. For example, in Figure 4(a), before execution, the denotation of the DCS tree is ⟨⟨{[(CA, OR), (OR)], . . . }; ø; (E, ⟨state⟩w, ø)⟩⟩; after applying X1, we have ⟨⟨{[(OR)], . . . }; ø⟩⟩. Generalized Quantification (d.ri = Q) Generalized quantifiers are predicates on two sets, a restrictor A and a nuclear scope B. For example, w(no) = {(A, B) : A ∩ B = ∅} and w(most) = {(A, B) : |A ∩ B| > 1","2 |A|}.","In a DCS tree, the quantifier appears as the child of a Q relation, and the restrictor is the parent (see Figure 4(b) for an example). This information is retrieved from the store when the 594 quantifier in column i is executed. In particular, the restrictor is A = Σ (d.bi) and the nuclear scope is B = Σ (d[i, −(i, ø)]). We then apply d.ci to these two sets (technically, denotations) and project away the first column: Xi(d) = ((d.ci ▷◁1,1 A) ▷◁2,1 B) [−1].","For the example in Figure 4(b), the denotation of the DCS tree before execution is ⟨⟨∅; ø; (Q, ⟨state⟩w, ⟨no⟩w)⟩⟩. The restrictor set (A) is the set of all states, and the nuclear scope (B) is the empty set. Since (A, B) exists in no, the final denotation, which projects away the actual pair, is ⟨⟨{[ ]}⟩⟩ (our representation of true).","Figure 4(c) shows an example with two interact-ing quantifiers. The quantifier scope ambiguity is resolved by the choice of execute relation; X12 gives the narrow reading and X21 gives the wide reading. Figure 4(d) shows how extraction and quantification work together. Comparatives and Superlatives (d.ri = C) To compare entities, we use a set S of (x, y) pairs, where x is an entity and y is a number. For superlatives, the argmax predicate denotes pairs of sets and the set’s largest element(s): w(argmax) = {(S, x∗",") : x∗","∈ argmaxx∈S1 max S(x)}. For comparatives, w(more) contains triples (S, x, y), where x is “more than” y as measured by S; formally: w(more) = {(S, x, y) : max S(x) > max S(y)}.","In a superlative/comparative construction, the root x of the DCS tree is the entity to be compared, the child c of a C relation is the comparative or superlative, and its parent p contains the information used for comparison (see Figure 4(e) for an example). If d is the denotation of the root, its i-th column contains this information. There are two cases: (i) if the i-th column of d contains pairs (e.g., size in Figure 5), then let d′","= ⟨ø⟩w ▷◁1,2 d[i, −i], which reads out the second components of these pairs; (ii) otherwise (e.g., state in Figure 4(e)), let d′","= ⟨ø⟩w ▷◁1,2 ⟨count⟩w ▷◁1,1 Σ (d[i, −i]), which counts the number of things (e.g., states) that occur with each value of the root x. Given d′",", we construct a denotation S by concatenating (+i) the second and first columns of d′","(S = Σ (+","2,1 (d′ {α","2 = ø}))) and apply the superlative/comparative: Xi(d) = (⟨ø⟩w ▷◁1,2 (d.ci ▷◁1,1 S)){α1 = d.α1}. Figure 4(f) shows that comparatives are handled using the exact same machinery as superlatives. Figure 4(g) shows that we can naturally account for superlative ambiguity based on where the scope-determining execute relation is placed."]},{"title":"3 Semantic Parsing","paragraphs":["We now turn to the task of mapping natural language utterances to DCS trees. Our first question is: given an utterance x, what trees z ∈ Z are permissible? To define the search space, we first assume a fixed set of lexical triggers L. Each trigger is a pair (x, p), where x is a sequence of words (usually one) and p is a predicate (e.g., x = California and p = CA). We use L(x) to denote the set of predicates p triggered by x ((x, p) ∈ L). Let L(ε) be the set of trace predicates, which can be introduced without an overt lexical trigger.","Given an utterance x = (x1, . . . , xn), we define ZL(x) ⊂ Z, the set of permissible DCS trees for x. The basic approach is reminiscent of projective labeled dependency parsing: For each span i..j, we build a set of trees Ci,j and set ZL(x) = C0,n. Each set Ci,j is constructed recursively by combining the trees of its subspans Ci,k and Ck′",",j for each pair of split points k, k′","(words between k and k′","are ignored). These combinations are then augmented via a function A and filtered via a functionF , to be specified later. Formally, Ci,j is defined recursively as follows: Ci,j = F ( A ( L(xi+1..j) ∪ ⋃ i≤k≤k′","<j a∈Ci,k b∈Ck′",",j","T1(a, b)))) . (13) In (13), L(xi+1..j) is the set of predicates triggered by the phrase under span i..j (the base case), and Td(a, b) = ⃗Td(a, b) ∪ ⃗T d(b, a), which returns all ways of combining trees a and b where b is a descendant of a ( ⃗Td) or vice-versa ( ⃗T d). The former is defined recursively as follows: ⃗T0(a, b) = ∅, and ⃗Td(a, b) = ⋃ r∈R p∈L(ε) {⟨a; r : b⟩} ∪ ⃗Td−1(a, ⟨p; r : b⟩). The latter ( ⃗T k) is defined similarly. Essentially, ⃗Td(a, b) allows us to insert up to d trace predicates between the roots of a and b. This is use-ful for modeling relations in noun compounds (e.g., 595 California cities), and it also allows us to underspecify L. In particular, our L will not include verbs or prepositions; rather, we rely on the predicates corresponding to those words to be triggered by traces.","The augmentation function A takes a set of trees and optionally attaches E and Xi relations to the root (e.g., A(⟨city⟩) = {⟨city⟩ , ⟨city; E : ø⟩}). The filtering functionF rules out improperly-typed trees such as ⟨city; 0","0 : ⟨state⟩⟩. To further reduce the search space, F imposes a few additional constraints, e.g., limiting the number of marked nodes to 2 and only allowing trace predicates between arity 1 predicates. Model We now present our discriminative semantic parsing model, which places a log-linear distribution over z ∈ ZL(x) given an utterance x. Formally, pθ(z | x) ∝ eφ(x,z)⊤","θ",", where θ and φ(x, z) are parameter and feature vectors, respectively. As a running example, consider x = city that is in California and z = ⟨city; 1","1 : ⟨loc; 2","1 : ⟨CA⟩⟩⟩, where city triggers city and California triggers CA.","To define the features, we technically need to augment each tree z ∈ ZL(x) with alignment information—namely, for each predicate in z, the span in x (if any) that triggered it. This extra information is already generated from the recursive definition in (13).","The feature vector φ(x, z) is defined by sums of five simple indicator feature templates: (F1) a word triggers a predicate (e.g., [city, city]); (F2) a word is under a relation (e.g., [that, 1","1]); (F3) a word is under a trace predicate (e.g., [in, loc]); (F4) two predicates are linked via a relation in the left or right direction (e.g., [city, 1","1, loc, RIGHT]); and (F5) a predicate has a child relation (e.g., [city, 1","1]). Learning Given a training dataset D containing (x, y) pairs, we define the regularized marginal log-likelihood objective O(θ) =","∑","(x,y)∈D log pθ(zw = y | x, z ∈ ZL(x)) − λ∥θ∥2","2, which sums over all DCS trees z that evaluate to the target answer y.","Our model is arc-factored, so we can sum over all DCS trees in ZL(x) using dynamic programming. However, in order to learn, we need to sum over {z ∈ ZL(x) : zw = y}, and unfortunately, the additional constraint zw = y does not factorize. We therefore resort to beam search. Specifically, we truncate each Ci,j to a maximum of K candidates sorted by decreasing score based on parameters θ. Let Z̃L,θ(x) be this approximation of ZL(x).","Our learning algorithm alternates between (i) using the current parameters θ to generate the K-best set Z̃L,θ(x) for each training example x, and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped. Formally, let Õ(θ, θ′",") be the objective function O(θ) with Z","L(x) replaced with Z̃L,θ′(x). We optimize Õ(θ, θ′",") by setting θ(0)","= ⃗0 and iteratively solving θ(t+1)","= argmaxθ Õ(θ, θ(t)",") using L-BFGS until t = T . In all experiments, we set λ = 0.01, T = 5, and K = 100. After training, given a new utterance x, our system outputs the most likely y, summing out the latent logical form z: argmaxy pθ(T)(y | x, z ∈ Z̃L,θ(T))."]},{"title":"4 Experiments","paragraphs":["We tested our system on two standard datasets, GEO and JOBS. In each dataset, each sentence x is annotated with a Prolog logical form, which we use only to evaluate and get an answer y. This evaluation is done with respect to a world w. Recall that a world w maps each predicate p ∈ P to a set of tuples w(p). There are three types of predicates in P: generic (e.g., argmax), data (e.g., city), and value (e.g., CA). GEO has 48 non-value predicates and JOBS has 26. For GEO, w is the standard US geography database that comes with the dataset. For JOBS, if we use the standard Jobs database, close to half the y’s are empty, which makes it uninteresting. We therefore generated a random Jobs database instead as follows: we created 100 job IDs. For each data predicate p (e.g., language), we add each possible tuple (e.g., (job37, Java)) to w(p) independently with probability 0.8.","We used the same training-test splits as Zettlemoyer and Collins (2005) (600+280 for GEO and 500+140 for JOBS). During development, we further held out a random 30% of the training sets for validation.","Our lexical triggers L include the following: (i) predicates for a small set of ≈ 20 function words (e.g., (most, argmax)), (ii) (x, x) for each value 596 System Accuracy Clarke et al. (2010) w/answers 73.2 Clarke et al. (2010) w/logical forms 80.4 Our system (DCS with L) 78.9 Our system (DCS with L+",") 87.2 Table 2: Results on GEO with 250 training and 250 test examples. Our results are averaged over 10 random 250+250 splits taken from our 600 training examples. Of the three systems that do not use logical forms, our two systems yield significant improvements. Our better system even outperforms the system that uses logical forms. predicate x in w (e.g., (Boston, Boston)), and (iii) predicates for each POS tag in {JJ, NN, NNS} (e.g., (JJ, size), (JJ, area), etc.).3","Predicates corresponding to verbs and prepositions (e.g., traverse) are not included as overt lexical triggers, but rather in the trace predicates L(ε).","We also define an augmented lexicon L+","which includes a prototype word x for each predicate appearing in (iii) above (e.g., (large, size)), which cancels the predicates triggered by x’s POS tag. For GEO, there are 22 prototype words; for JOBS, there are 5. Specifying these triggers requires minimal domain-specific supervision. Results We first compare our system with Clarke et al. (2010) (henceforth, SEMRESP), which also learns a semantic parser from question-answer pairs. Table 2 shows that our system using lexical triggers L (henceforth, DCS) outperforms SEMRESP (78.9% over 73.2%). In fact, although neither DCS nor SEMRESP uses logical forms, DCS uses even less supervision than SEMRESP. SEMRESP requires a lexicon of 1.42 words per non-value predicate, Word-Net features, and syntactic parse trees; DCS requires only words for the domain-independent predicates (overall, around 0.5 words per non-value predicate), POS tags, and very simple indicator features. In fact, DCS performs comparably to even the version of SEMRESP trained using logical forms. If we add prototype triggers (use L+","), the resulting system (DCS+",") outperforms both versions of SEMRESP by a significant margin (87.2% over 73.2% and 80.4%). 3","We used the Berkeley Parser (Petrov et al., 2006) to perform POS tagging. The triggers L(x) for a word x thus include L(t) where t is the POS tag of x. System GEO JOBS Tang and Mooney (2001) 79.4 79.8 Wong and Mooney (2007) 86.6 – Zettlemoyer and Collins (2005) 79.3 79.3 Zettlemoyer and Collins (2007) 81.6 – Kwiatkowski et al. (2010) 88.2 – Kwiatkowski et al. (2010) 88.9 – Our system (DCS with L) 88.6 91.4 Our system (DCS with L+",") 91.1 95.0 Table 3: Accuracy (recall) of systems on the two benchmarks. The systems are divided into three groups. Group 1 uses 10-fold cross-validation; groups 2 and 3 use the in-dependent test set. Groups 1 and 2 measure accuracy of logical form; group 3 measures accuracy of the answer; but there is very small difference between the two as seen from the Kwiatkowski et al. (2010) numbers. Our best system improves substantially over past work, despite using no logical forms as training data.","Next, we compared our systems (DCS and DCS+",") with the state-of-the-art semantic parsers on the full dataset for both GEO and JOBS (see Table 3). All other systems require logical forms as training data, whereas ours does not. Table 3 shows that even DCS, which does not use prototypes, is comparable to the best previous system (Kwiatkowski et al., 2010), and by adding a few prototypes, DCS+","offers a decisive edge (91.1% over 88.9% on GEO). Rather than using lexical triggers, several of the other systems use IBM word alignment models to produce an initial word-predicate mapping. This option is not available to us since we do not have annotated logical forms, so we must instead rely on lexical triggers to define the search space. Note that having lexical triggers is a much weaker requirement than having a CCG lexicon, and far easier to obtain than logical forms. Intuitions How is our system learning? Initially, the weights are zero, so the beam search is essentially unguided. We find that only for a small frac-tion of training examples do the K-best sets contain any trees yielding the correct answer (29% for DCS on GEO). However, training on just these examples is enough to improve the parameters, and this 29% increases to 66% and then to 95% over the next few iterations. This bootstrapping behavior occurs naturally: The “easy” examples are processed first, where easy is defined by the ability of the current 597 model to generate the correct answer using any tree.","Our system learns lexical associations between words and predicates. For example, area (by virtue of being a noun) triggers many predicates: city, state, area, etc. Inspecting the final parameters (DCS on GEO), we find that the feature[area, area] has a much higher weight than [area, city]. Trace predicates can be inserted anywhere, but the features favor some insertions depending on the words present (for example, [in, loc] has high weight).","The errors that the system makes stem from multiple sources, including errors in the POS tags (e.g., states is sometimes tagged as a verb, which triggers no predicates), confusion of Washington state with Washington D.C., learning the wrong lexical associations due to data sparsity, and having an insufficiently large K."]},{"title":"5 Discussion","paragraphs":["A major focus of this work is on our semantic representation, DCS, which offers a new perspective on compositional semantics. To contrast, consider CCG (Steedman, 2000), in which semantic parsing is driven from the lexicon. The lexicon encodes information about how each word can used in context; for example, the lexical entry for borders is S\\NP/NP : λy.λx.border(x, y), which means borders looks right for the first argument and left for the second. These rules are often too stringent, and for complex utterances, especially in free word-order languages, either disharmonic combinators are employed (Zettlemoyer and Collins, 2007) or words are given multiple lexical entries (Kwiatkowski et al., 2010).","In DCS, we start with lexical triggers, which are more basic than CCG lexical entries. A trigger for borders specifies only thatborder can be used, but not how. The combination rules are encoded in the features as soft preferences. This yields a more factorized and flexible representation that is easier to search through and parametrize using features. It also allows us to easily add new lexical triggers without becoming mired in the semantic formalism.","Quantifiers and superlatives significantly complicate scoping in lambda calculus, and often type rais-ing needs to be employed. In DCS, the mark-execute construct provides a flexible framework for dealing with scope variation. Think of DCS as a higher-level programming language tailored to natural language, which results in programs (DCS trees) which are much simpler than the logically-equivalent lambda calculus formulae.","The idea of using CSPs to represent semantics is inspired by Discourse Representation Theory (DRT) (Kamp and Reyle, 1993; Kamp et al., 2005), where variables are discourse referents. The restriction to trees is similar to economical DRT (Bos, 2009).","The other major focus of this work is program induction—inferring logical forms from their denotations. There has been a fair amount of past work on this topic: Liang et al. (2010) induces combinatory logic programs in a non-linguistic setting. Eisenstein et al. (2009) induces conjunctive formulae and uses them as features in another learning problem. Piantadosi et al. (2008) induces first-order formulae using CCG in a small domain assuming observed lexical semantics. The closest work to ours is Clarke et al. (2010), which we discussed earlier.","The integration of natural language with denotations computed against a world (grounding) is becoming increasingly popular. Feedback from the world has been used to guide both syntactic parsing (Schuler, 2003) and semantic parsing (Popescu et al., 2003; Clarke et al., 2010). Past work has also focused on aligning text to a world (Liang et al., 2009), using text in reinforcement learning (Branavan et al., 2009; Branavan et al., 2010), and many others. Our work pushes the grounded language agenda towards deeper representations of language—think grounded compositional semantics."]},{"title":"6 Conclusion","paragraphs":["We built a system that interprets natural language utterances much more accurately than existing systems, despite using no annotated logical forms. Our system is based on a new semantic representation, DCS, which offers a simple and expressive alternative to lambda calculus. Free from the burden of annotating logical forms, we hope to use our techniques in developing even more accurate and broader-coverage language understanding systems. Acknowledgments We thank Luke Zettlemoyer and Tom Kwiatkowski for providing us with data and answering questions. 598"]},{"title":"References","paragraphs":["J. Bos. 2009. A controlled fragment of DRT. In Workshop on Controlled Natural Language, pages 1–5.","S. Branavan, H. Chen, L. S. Zettlemoyer, and R. Barzilay. 2009. Reinforcement learning for mapping instructions to actions. In Association for Computational Linguistics and International Joint Conference on Natural Language Processing (ACL-IJCNLP), Singapore. Association for Computational Linguistics.","S. Branavan, L. Zettlemoyer, and R. Barzilay. 2010. Reading between the lines: Learning to map high-level instructions to commands. In Association for Computational Linguistics (ACL). Association for Computational Linguistics.","B. Carpenter. 1998. Type-Logical Semantics. MIT Press.","J. Clarke, D. Goldwasser, M. Chang, and D. Roth. 2010. Driving semantic parsing from the world’s response. In Computational Natural Language Learning (CoNLL).","R. Dechter. 2003. Constraint Processing. Morgan Kaufmann.","J. Eisenstein, J. Clarke, D. Goldwasser, and D. Roth. 2009. Reading to learn: Constructing features from semantic abstracts. In Empirical Methods in Natural Language Processing (EMNLP), Singapore.","R. Ge and R. J. Mooney. 2005. A statistical semantic parser that integrates syntax and semantics. In Computational Natural Language Learning (CoNLL), pages 9–16, Ann Arbor, Michigan.","H. Kamp and U. Reyle. 1993. From Discourse to Logic: An Introduction to the Model-theoretic Semantics of Natural Language, Formal Logic and Discourse Representation Theory. Kluwer, Dordrecht.","H. Kamp, J. v. Genabith, and U. Reyle. 2005. Discourse representation theory. In Handbook of Philosophical Logic.","R. J. Kate and R. J. Mooney. 2007. Learning language semantics from ambiguous supervision. In Association for the Advancement of Artificial Intelligence (AAAI), pages 895–900, Cambridge, MA. MIT Press.","R. J. Kate, Y. W. Wong, and R. J. Mooney. 2005. Learning to transform natural to formal languages. In Association for the Advancement of Artificial Intelligence (AAAI), pages 1062–1068, Cambridge, MA. MIT Press.","T. Kwiatkowski, L. Zettlemoyer, S. Goldwater, and M. Steedman. 2010. Inducing probabilistic CCG grammars from logical form with higher-order unification. In Empirical Methods in Natural Language Processing (EMNLP).","P. Liang, M. I. Jordan, and D. Klein. 2009. Learning semantic correspondences with less supervision. In Association for Computational Linguistics and International Joint Conference on Natural Language Processing (ACL-IJCNLP), Singapore. Association for Computational Linguistics.","P. Liang, M. I. Jordan, and D. Klein. 2010. Learning programs: A hierarchical Bayesian approach. In International Conference on Machine Learning (ICML). Omnipress.","S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In International Conference on Computational Linguistics and Association for Computational Linguistics (COLING/ACL), pages 433–440. Association for Computational Linguistics.","S. T. Piantadosi, N. D. Goodman, B. A. Ellis, and J. B. Tenenbaum. 2008. A Bayesian model of the acquisi-tion of compositional semantics. In Proceedings of the Thirtieth Annual Conference of the Cognitive Science Society.","H. Poon and P. Domingos. 2009. Unsupervised semantic parsing. In Empirical Methods in Natural Language Processing (EMNLP), Singapore.","A. Popescu, O. Etzioni, and H. Kautz. 2003. Towards a theory of natural language interfaces to databases. In International Conference on Intelligent User Interfaces (IUI).","W. Schuler. 2003. Using model-theoretic semantic interpretation to guide statistical parsing and word recognition in a spoken language interface. In Association for Computational Linguistics (ACL). Association for Computational Linguistics.","M. Steedman. 2000. The Syntactic Process. MIT Press.","L. R. Tang and R. J. Mooney. 2001. Using multiple clause constructors in inductive logic programming for semantic parsing. In European Conference on Machine Learning, pages 466–477.","Y. W. Wong and R. J. Mooney. 2007. Learning synchronous grammars for semantic parsing with lambda calculus. In Association for Computational Linguistics (ACL), pages 960–967, Prague, Czech Republic. Association for Computational Linguistics.","M. Zelle and R. J. Mooney. 1996. Learning to parse database queries using inductive logic proramming. In Association for the Advancement of Artificial Intelligence (AAAI), Cambridge, MA. MIT Press.","L. S. Zettlemoyer and M. Collins. 2005. Learning to map sentences to logical form: Structured classifica-tion with probabilistic categorial grammars. In Uncertainty in Artificial Intelligence (UAI), pages 658–666.","L. S. Zettlemoyer and M. Collins. 2007. Online learning of relaxed CCG grammars for parsing to logical form. In Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP/CoNLL), pages 678–687. 599"]}],"references":[{"authors":[{"first":"J.","last":"Bos"}],"year":"2009","title":"A controlled fragment of DRT","source":"J. Bos. 2009. A controlled fragment of DRT. In Workshop on Controlled Natural Language, pages 1–5."},{"authors":[{"first":"S.","last":"Branavan"},{"first":"H.","last":"Chen"},{"first":"L.","middle":"S.","last":"Zettlemoyer"},{"first":"R.","last":"Barzilay"}],"year":"2009","title":"Reinforcement learning for mapping instructions to actions","source":"S. Branavan, H. Chen, L. S. Zettlemoyer, and R. Barzilay. 2009. Reinforcement learning for mapping instructions to actions. In Association for Computational Linguistics and International Joint Conference on Natural Language Processing (ACL-IJCNLP), Singapore. Association for Computational Linguistics."},{"authors":[{"first":"S.","last":"Branavan"},{"first":"L.","last":"Zettlemoyer"},{"first":"R.","last":"Barzilay"}],"year":"2010","title":"Reading between the lines: Learning to map high-level instructions to commands","source":"S. Branavan, L. Zettlemoyer, and R. Barzilay. 2010. Reading between the lines: Learning to map high-level instructions to commands. In Association for Computational Linguistics (ACL). Association for Computational Linguistics."},{"authors":[{"first":"B.","last":"Carpenter"}],"year":"1998","title":"Type-Logical Semantics","source":"B. Carpenter. 1998. Type-Logical Semantics. MIT Press."},{"authors":[{"first":"J.","last":"Clarke"},{"first":"D.","last":"Goldwasser"},{"first":"M.","last":"Chang"},{"first":"D.","last":"Roth"}],"year":"2010","title":"Driving semantic parsing from the world’s response","source":"J. Clarke, D. Goldwasser, M. Chang, and D. Roth. 2010. Driving semantic parsing from the world’s response. In Computational Natural Language Learning (CoNLL)."},{"authors":[{"first":"R.","last":"Dechter"}],"year":"2003","title":"Constraint Processing","source":"R. Dechter. 2003. Constraint Processing. Morgan Kaufmann."},{"authors":[{"first":"J.","last":"Eisenstein"},{"first":"J.","last":"Clarke"},{"first":"D.","last":"Goldwasser"},{"first":"D.","last":"Roth"}],"year":"2009","title":"Reading to learn: Constructing features from semantic abstracts","source":"J. Eisenstein, J. Clarke, D. Goldwasser, and D. Roth. 2009. Reading to learn: Constructing features from semantic abstracts. In Empirical Methods in Natural Language Processing (EMNLP), Singapore."},{"authors":[{"first":"R.","last":"Ge"},{"first":"R.","middle":"J.","last":"Mooney"}],"year":"2005","title":"A statistical semantic parser that integrates syntax and semantics","source":"R. Ge and R. J. Mooney. 2005. A statistical semantic parser that integrates syntax and semantics. In Computational Natural Language Learning (CoNLL), pages 9–16, Ann Arbor, Michigan."},{"authors":[{"first":"H.","last":"Kamp"},{"first":"U.","last":"Reyle"}],"year":"1993","title":"From Discourse to Logic: An Introduction to the Model-theoretic Semantics of Natural Language, Formal Logic and Discourse Representation Theory","source":"H. Kamp and U. Reyle. 1993. From Discourse to Logic: An Introduction to the Model-theoretic Semantics of Natural Language, Formal Logic and Discourse Representation Theory. Kluwer, Dordrecht."},{"authors":[{"first":"H.","last":"Kamp"},{"first":"J.","middle":"v.","last":"Genabith"},{"first":"U.","last":"Reyle"}],"year":"2005","title":"Discourse representation theory","source":"H. Kamp, J. v. Genabith, and U. Reyle. 2005. Discourse representation theory. In Handbook of Philosophical Logic."},{"authors":[{"first":"R.","middle":"J.","last":"Kate"},{"first":"R.","middle":"J.","last":"Mooney"}],"year":"2007","title":"Learning language semantics from ambiguous supervision","source":"R. J. Kate and R. J. Mooney. 2007. Learning language semantics from ambiguous supervision. In Association for the Advancement of Artificial Intelligence (AAAI), pages 895–900, Cambridge, MA. MIT Press."},{"authors":[{"first":"R.","middle":"J.","last":"Kate"},{"first":"Y.","middle":"W.","last":"Wong"},{"first":"R.","middle":"J.","last":"Mooney"}],"year":"2005","title":"Learning to transform natural to formal languages","source":"R. J. Kate, Y. W. Wong, and R. J. Mooney. 2005. Learning to transform natural to formal languages. In Association for the Advancement of Artificial Intelligence (AAAI), pages 1062–1068, Cambridge, MA. MIT Press."},{"authors":[{"first":"T.","last":"Kwiatkowski"},{"first":"L.","last":"Zettlemoyer"},{"first":"S.","last":"Goldwater"},{"first":"M.","last":"Steedman"}],"year":"2010","title":"Inducing probabilistic CCG grammars from logical form with higher-order unification","source":"T. Kwiatkowski, L. Zettlemoyer, S. Goldwater, and M. Steedman. 2010. Inducing probabilistic CCG grammars from logical form with higher-order unification. In Empirical Methods in Natural Language Processing (EMNLP)."},{"authors":[{"first":"P.","last":"Liang"},{"first":"M.","middle":"I.","last":"Jordan"},{"first":"D.","last":"Klein"}],"year":"2009","title":"Learning semantic correspondences with less supervision","source":"P. Liang, M. I. Jordan, and D. Klein. 2009. Learning semantic correspondences with less supervision. In Association for Computational Linguistics and International Joint Conference on Natural Language Processing (ACL-IJCNLP), Singapore. Association for Computational Linguistics."},{"authors":[{"first":"P.","last":"Liang"},{"first":"M.","middle":"I.","last":"Jordan"},{"first":"D.","last":"Klein"}],"year":"2010","title":"Learning programs: A hierarchical Bayesian approach","source":"P. Liang, M. I. Jordan, and D. Klein. 2010. Learning programs: A hierarchical Bayesian approach. In International Conference on Machine Learning (ICML). Omnipress."},{"authors":[{"first":"S.","last":"Petrov"},{"first":"L.","last":"Barrett"},{"first":"R.","last":"Thibaux"},{"first":"D.","last":"Klein"}],"year":"2006","title":"Learning accurate, compact, and interpretable tree annotation","source":"S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In International Conference on Computational Linguistics and Association for Computational Linguistics (COLING/ACL), pages 433–440. Association for Computational Linguistics."},{"authors":[{"first":"S.","middle":"T.","last":"Piantadosi"},{"first":"N.","middle":"D.","last":"Goodman"},{"first":"B.","middle":"A.","last":"Ellis"},{"first":"J.","middle":"B.","last":"Tenenbaum"}],"year":"2008","title":"A Bayesian model of the acquisi-tion of compositional semantics","source":"S. T. Piantadosi, N. D. Goodman, B. A. Ellis, and J. B. Tenenbaum. 2008. A Bayesian model of the acquisi-tion of compositional semantics. In Proceedings of the Thirtieth Annual Conference of the Cognitive Science Society."},{"authors":[{"first":"H.","last":"Poon"},{"first":"P.","last":"Domingos"}],"year":"2009","title":"Unsupervised semantic parsing","source":"H. Poon and P. Domingos. 2009. Unsupervised semantic parsing. In Empirical Methods in Natural Language Processing (EMNLP), Singapore."},{"authors":[{"first":"A.","last":"Popescu"},{"first":"O.","last":"Etzioni"},{"first":"H.","last":"Kautz"}],"year":"2003","title":"Towards a theory of natural language interfaces to databases","source":"A. Popescu, O. Etzioni, and H. Kautz. 2003. Towards a theory of natural language interfaces to databases. In International Conference on Intelligent User Interfaces (IUI)."},{"authors":[{"first":"W.","last":"Schuler"}],"year":"2003","title":"Using model-theoretic semantic interpretation to guide statistical parsing and word recognition in a spoken language interface","source":"W. Schuler. 2003. Using model-theoretic semantic interpretation to guide statistical parsing and word recognition in a spoken language interface. In Association for Computational Linguistics (ACL). Association for Computational Linguistics."},{"authors":[{"first":"M.","last":"Steedman"}],"year":"2000","title":"The Syntactic Process","source":"M. Steedman. 2000. The Syntactic Process. MIT Press."},{"authors":[{"first":"L.","middle":"R.","last":"Tang"},{"first":"R.","middle":"J.","last":"Mooney"}],"year":"2001","title":"Using multiple clause constructors in inductive logic programming for semantic parsing","source":"L. R. Tang and R. J. Mooney. 2001. Using multiple clause constructors in inductive logic programming for semantic parsing. In European Conference on Machine Learning, pages 466–477."},{"authors":[{"first":"Y.","middle":"W.","last":"Wong"},{"first":"R.","middle":"J.","last":"Mooney"}],"year":"2007","title":"Learning synchronous grammars for semantic parsing with lambda calculus","source":"Y. W. Wong and R. J. Mooney. 2007. Learning synchronous grammars for semantic parsing with lambda calculus. In Association for Computational Linguistics (ACL), pages 960–967, Prague, Czech Republic. Association for Computational Linguistics."},{"authors":[{"first":"M.","last":"Zelle"},{"first":"R.","middle":"J.","last":"Mooney"}],"year":"1996","title":"Learning to parse database queries using inductive logic proramming","source":"M. Zelle and R. J. Mooney. 1996. Learning to parse database queries using inductive logic proramming. In Association for the Advancement of Artificial Intelligence (AAAI), Cambridge, MA. MIT Press."},{"authors":[{"first":"L.","middle":"S.","last":"Zettlemoyer"},{"first":"M.","last":"Collins"}],"year":"2005","title":"Learning to map sentences to logical form: Structured classifica-tion with probabilistic categorial grammars","source":"L. S. Zettlemoyer and M. Collins. 2005. Learning to map sentences to logical form: Structured classifica-tion with probabilistic categorial grammars. In Uncertainty in Artificial Intelligence (UAI), pages 658–666."},{"authors":[{"first":"L.","middle":"S.","last":"Zettlemoyer"},{"first":"M.","last":"Collins"}],"year":"2007","title":"Online learning of relaxed CCG grammars for parsing to logical form","source":"L. S. Zettlemoyer and M. Collins. 2007. Online learning of relaxed CCG grammars for parsing to logical form. In Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP/CoNLL), pages 678–687. 599"}],"cites":[{"style":0,"text":"Zelle and Mooney, 1996","origin":{"pointer":"/sections/2/paragraphs/0","offset":232,"length":22},"authors":[{"last":"Zelle"},{"last":"Mooney"}],"year":"1996","references":["/references/23"]},{"style":0,"text":"Tang and Mooney, 2001","origin":{"pointer":"/sections/2/paragraphs/0","offset":256,"length":21},"authors":[{"last":"Tang"},{"last":"Mooney"}],"year":"2001","references":["/references/21"]},{"style":0,"text":"Ge and Mooney, 2005","origin":{"pointer":"/sections/2/paragraphs/0","offset":279,"length":19},"authors":[{"last":"Ge"},{"last":"Mooney"}],"year":"2005","references":["/references/7"]},{"style":0,"text":"Zettlemoyer and Collins, 2005","origin":{"pointer":"/sections/2/paragraphs/0","offset":300,"length":29},"authors":[{"last":"Zettlemoyer"},{"last":"Collins"}],"year":"2005","references":["/references/24"]},{"style":0,"text":"Kate and Mooney, 2007","origin":{"pointer":"/sections/2/paragraphs/0","offset":331,"length":21},"authors":[{"last":"Kate"},{"last":"Mooney"}],"year":"2007","references":["/references/10"]},{"style":0,"text":"Zettlemoyer and Collins, 2007","origin":{"pointer":"/sections/2/paragraphs/0","offset":354,"length":29},"authors":[{"last":"Zettlemoyer"},{"last":"Collins"}],"year":"2007","references":["/references/25"]},{"style":0,"text":"Wong and Mooney, 2007","origin":{"pointer":"/sections/2/paragraphs/0","offset":385,"length":21},"authors":[{"last":"Wong"},{"last":"Mooney"}],"year":"2007","references":["/references/22"]},{"style":0,"text":"Kwiatkowski et al., 2010","origin":{"pointer":"/sections/2/paragraphs/0","offset":408,"length":24},"authors":[{"last":"Kwiatkowski"},{"last":"al."}],"year":"2010","references":["/references/12"]},{"style":0,"text":"Poon and Domingos, 2009","origin":{"pointer":"/sections/2/paragraphs/0","offset":557,"length":23},"authors":[{"last":"Poon"},{"last":"Domingos"}],"year":"2009","references":["/references/17"]},{"style":0,"text":"Clarke et al. (2010)","origin":{"pointer":"/sections/2/paragraphs/1","offset":6,"length":20},"authors":[{"last":"Clarke"},{"last":"al."}],"year":"2010","references":["/references/4"]},{"style":0,"text":"Steedman, 2000","origin":{"pointer":"/sections/2/paragraphs/4","offset":160,"length":14},"authors":[{"last":"Steedman"}],"year":"2000","references":["/references/20"]},{"style":0,"text":"Zettlemoyer and Collins (2005)","origin":{"pointer":"/sections/2/paragraphs/4","offset":223,"length":30},"authors":[{"last":"Zettlemoyer"},{"last":"Collins"}],"year":"2005","references":["/references/24"]},{"style":0,"text":"Kate et al., 2005","origin":{"pointer":"/sections/2/paragraphs/4","offset":449,"length":17},"authors":[{"last":"Kate"},{"last":"al."}],"year":"2005","references":["/references/11"]},{"style":0,"text":"Clarke et al. (2010)","origin":{"pointer":"/sections/2/paragraphs/4","offset":491,"length":20},"authors":[{"last":"Clarke"},{"last":"al."}],"year":"2010","references":["/references/4"]},{"style":0,"text":"Dechter, 2003","origin":{"pointer":"/sections/3/paragraphs/12","offset":408,"length":13},"authors":[{"last":"Dechter"}],"year":"2003","references":["/references/5"]},{"style":0,"text":"Carpenter, 1998","origin":{"pointer":"/sections/3/paragraphs/29","offset":123,"length":15},"authors":[{"last":"Carpenter"}],"year":"1998","references":["/references/3"]},{"style":0,"text":"Zettlemoyer and Collins (2005)","origin":{"pointer":"/sections/5/paragraphs/1","offset":41,"length":30},"authors":[{"last":"Zettlemoyer"},{"last":"Collins"}],"year":"2005","references":["/references/24"]},{"style":0,"text":"Clarke et al. (2010)","origin":{"pointer":"/sections/5/paragraphs/2","offset":171,"length":20},"authors":[{"last":"Clarke"},{"last":"al."}],"year":"2010","references":["/references/4"]},{"style":0,"text":"Clarke et al. (2010)","origin":{"pointer":"/sections/5/paragraphs/2","offset":207,"length":20},"authors":[{"last":"Clarke"},{"last":"al."}],"year":"2010","references":["/references/4"]},{"style":0,"text":"Clarke et al. (2010)","origin":{"pointer":"/sections/5/paragraphs/6","offset":331,"length":20},"authors":[{"last":"Clarke"},{"last":"al."}],"year":"2010","references":["/references/4"]},{"style":0,"text":"Petrov et al., 2006","origin":{"pointer":"/sections/5/paragraphs/9","offset":29,"length":19},"authors":[{"last":"Petrov"},{"last":"al."}],"year":"2006","references":["/references/15"]},{"style":0,"text":"Tang and Mooney (2001)","origin":{"pointer":"/sections/5/paragraphs/9","offset":168,"length":22},"authors":[{"last":"Tang"},{"last":"Mooney"}],"year":"2001","references":["/references/21"]},{"style":0,"text":"Wong and Mooney (2007)","origin":{"pointer":"/sections/5/paragraphs/9","offset":201,"length":22},"authors":[{"last":"Wong"},{"last":"Mooney"}],"year":"2007","references":["/references/22"]},{"style":0,"text":"Zettlemoyer and Collins (2005)","origin":{"pointer":"/sections/5/paragraphs/9","offset":231,"length":30},"authors":[{"last":"Zettlemoyer"},{"last":"Collins"}],"year":"2005","references":["/references/24"]},{"style":0,"text":"Zettlemoyer and Collins (2007)","origin":{"pointer":"/sections/5/paragraphs/9","offset":272,"length":30},"authors":[{"last":"Zettlemoyer"},{"last":"Collins"}],"year":"2007","references":["/references/25"]},{"style":0,"text":"Kwiatkowski et al. (2010)","origin":{"pointer":"/sections/5/paragraphs/9","offset":310,"length":25},"authors":[{"last":"Kwiatkowski"},{"last":"al."}],"year":"2010","references":["/references/12"]},{"style":0,"text":"Kwiatkowski et al. (2010)","origin":{"pointer":"/sections/5/paragraphs/9","offset":343,"length":25},"authors":[{"last":"Kwiatkowski"},{"last":"al."}],"year":"2010","references":["/references/12"]},{"style":0,"text":"Kwiatkowski et al. (2010)","origin":{"pointer":"/sections/5/paragraphs/10","offset":359,"length":25},"authors":[{"last":"Kwiatkowski"},{"last":"al."}],"year":"2010","references":["/references/12"]},{"style":0,"text":"Kwiatkowski et al., 2010","origin":{"pointer":"/sections/5/paragraphs/12","offset":286,"length":24},"authors":[{"last":"Kwiatkowski"},{"last":"al."}],"year":"2010","references":["/references/12"]},{"style":0,"text":"Steedman, 2000","origin":{"pointer":"/sections/6/paragraphs/0","offset":153,"length":14},"authors":[{"last":"Steedman"}],"year":"2000","references":["/references/20"]},{"style":0,"text":"Zettlemoyer and Collins, 2007","origin":{"pointer":"/sections/6/paragraphs/0","offset":604,"length":29},"authors":[{"last":"Zettlemoyer"},{"last":"Collins"}],"year":"2007","references":["/references/25"]},{"style":0,"text":"Kwiatkowski et al., 2010","origin":{"pointer":"/sections/6/paragraphs/0","offset":680,"length":24},"authors":[{"last":"Kwiatkowski"},{"last":"al."}],"year":"2010","references":["/references/12"]},{"style":0,"text":"Kamp and Reyle, 1993","origin":{"pointer":"/sections/6/paragraphs/3","offset":100,"length":20},"authors":[{"last":"Kamp"},{"last":"Reyle"}],"year":"1993","references":["/references/8"]},{"style":0,"text":"Kamp et al., 2005","origin":{"pointer":"/sections/6/paragraphs/3","offset":122,"length":17},"authors":[{"last":"Kamp"},{"last":"al."}],"year":"2005","references":["/references/9"]},{"style":0,"text":"Bos, 2009","origin":{"pointer":"/sections/6/paragraphs/3","offset":238,"length":9},"authors":[{"last":"Bos"}],"year":"2009","references":["/references/0"]},{"style":0,"text":"Liang et al. (2010)","origin":{"pointer":"/sections/6/paragraphs/4","offset":161,"length":19},"authors":[{"last":"Liang"},{"last":"al."}],"year":"2010","references":["/references/14"]},{"style":0,"text":"Eisenstein et al. (2009)","origin":{"pointer":"/sections/6/paragraphs/4","offset":245,"length":24},"authors":[{"last":"Eisenstein"},{"last":"al."}],"year":"2009","references":["/references/6"]},{"style":0,"text":"Piantadosi et al. (2008)","origin":{"pointer":"/sections/6/paragraphs/4","offset":354,"length":24},"authors":[{"last":"Piantadosi"},{"last":"al."}],"year":"2008","references":["/references/16"]},{"style":0,"text":"Clarke et al. (2010)","origin":{"pointer":"/sections/6/paragraphs/4","offset":501,"length":20},"authors":[{"last":"Clarke"},{"last":"al."}],"year":"2010","references":["/references/4"]},{"style":0,"text":"Schuler, 2003","origin":{"pointer":"/sections/6/paragraphs/5","offset":195,"length":13},"authors":[{"last":"Schuler"}],"year":"2003","references":["/references/19"]},{"style":0,"text":"Popescu et al., 2003","origin":{"pointer":"/sections/6/paragraphs/5","offset":232,"length":20},"authors":[{"last":"Popescu"},{"last":"al."}],"year":"2003","references":["/references/18"]},{"style":0,"text":"Clarke et al., 2010","origin":{"pointer":"/sections/6/paragraphs/5","offset":254,"length":19},"authors":[{"last":"Clarke"},{"last":"al."}],"year":"2010","references":["/references/4"]},{"style":0,"text":"Liang et al., 2009","origin":{"pointer":"/sections/6/paragraphs/5","offset":332,"length":18},"authors":[{"last":"Liang"},{"last":"al."}],"year":"2009","references":["/references/13"]},{"style":0,"text":"Branavan et al., 2009","origin":{"pointer":"/sections/6/paragraphs/5","offset":391,"length":21},"authors":[{"last":"Branavan"},{"last":"al."}],"year":"2009","references":["/references/1"]},{"style":0,"text":"Branavan et al., 2010","origin":{"pointer":"/sections/6/paragraphs/5","offset":414,"length":21},"authors":[{"last":"Branavan"},{"last":"al."}],"year":"2010","references":["/references/2"]}]}
