{"sections":[{"title":"","paragraphs":["Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 147–152, Portland, Oregon, June 19-24, 2011. c⃝2011 Association for Computational Linguistics"]},{"title":"From Bilingual Dictionaries to Interlingual Document Representations Jagadeesh Jagarlamudi University of Maryland College Park, USA jags@umiacs.umd.edu Hal Daumé III University of Maryland College Park, USA hal@umiacs.umd.edu Raghavendra Udupa Microsoft Research India Bangalore, India raghavu@microsoft.com Abstract","paragraphs":["Mapping documents into an interlingual representation can help bridge the language barrier of a cross-lingual corpus. Previous approaches use aligned documents as training data to learn an interlingual representation, making them sensitive to the domain of the training data. In this paper, we learn an interlingual representation in an unsupervised manner using only a bilingual dictionary. We first use the bilingual dictionary to find candidate document alignments and then use them to find an interlingual representation. Since the candidate alignments are noisy, we develop a robust learning algorithm to learn the interlingual representation. We show that bilingual dictionaries generalize to different domains better: our approach gives better performance than either a word by word translation method or Canonical Correlation Analysis (CCA) trained on a different domain."]},{"title":"1 Introduction","paragraphs":["The growth of text corpora in different languages poses an inherent problem of aligning documents across languages. Obtaining an explicit alignment, or a different way of bridging the language barrier, is an important step in many natural language processing (NLP) applications such as: document retrieval (Gale and Church, 1991; Rapp, 1999; Ballesteros and Croft, 1996; Munteanu and Marcu, 2005; Vu et al., 2009), Transliteration Mining (Klementiev and Roth, 2006; Hermjakob et al., 2008; Udupa et al., 2009; Ravi and Knight, 2009) and Multilingual Web Search (Gao et al., 2008; Gao et al., 2009). Aligning documents from different languages arises in all the above mentioned problems. In this paper, we address this problem by mapping documents into a common subspace (interlingual representation)1",". This common subspace generalizes the no-tion of vector space model for cross-lingual applications (Turney and Pantel, 2010).","There are two major approaches for solving the document alignment problem, depending on the available resources. The first approach, which is widely used in the Cross-lingual Information Retrieval (CLIR) literature, uses bilingual dictionaries to translate documents from one language (source) into another (target) language (Ballesteros and Croft, 1996; Pirkola et al., 2001). Then standard measures such as cosine similarity are used to identify target language documents that are close to the translated document. The second approach is to use training data of aligned document pairs to find a common subspace such that the aligned document pairs are maximally correlated (Susan T. Dumais, 1996; Vinokourov et al., 2003; Mimno et al., 2009; Platt et al., 2010; Haghighi et al., 2008) .","Both kinds of approaches have their own strengths and weaknesses. Dictionary based approaches treat source documents independently, i.e., each source language document is translated independently of other documents. Moreover, after translation, the relationship of a given source document with the rest of the source documents is ignored. On the other hand, supervised approaches use all the source and target language documents to infer an interlingual 1 We use the phrases “common subspace” and “interlingual","representation” interchangeably. 147 representation, but their strong dependency on the training data prevents them from generalizing well to test documents from a different domain.","In this paper, we propose a technique that combines the advantages of both these approaches. At a broad level, our approach uses bilingual dictionaries to identify initial noisy document alignments (Sec. 2.1) and then uses these noisy alignments as training data to learn a common subspace. Since the alignments are noisy, we need a learning algorithm that is robust to the errors in the training data. It is known that techniques like CCA overfit the training data (Rai and Daumé III, 2009). So, we start with an unsupervised approach such as Kernelized Sorting (Quadrianto et al., 2009) and develop a supervised variant of it (Sec. 2.2). Our supervised variant learns to modify the within language document similarities according to the given alignments. Since the original algorithm is unsupervised, we hope that its supervised variant is tolerant to errors in the candidate alignments. The primary advantage of our method is that, it does not use any training data and thus generalizes to test documents from different domains. And unlike the dictionary based approaches, we use all the documents in computing the common subspace and thus achieve better accuracies compared to the approaches which translate documents in isolation.","There are two main contributions of this work. First, we propose a discriminative technique to learn an interlingual representation using only a bilingual dictionary. Second, we develop a supervised variant of Kernelized Sorting algorithm (Quadrianto et al., 2009) which learns to modify within language document similarities according to a given alignment."]},{"title":"2 Approach","paragraphs":["Given a cross-lingual corpus, with an underlying unknown document alignment, we propose a technique to recover the hidden alignment. This is achieved by mapping documents into an interlingual representation. Our approach involves two stages. In the first stage, we use a bilingual dictionary to find initial candidate noisy document alignments. The second stage uses a robust learning algorithm to learn a common subspace from the noisy alignments identified in the first step. Subsequently, we project all the documents into the common subspace and use maximal matching to recover the hidden alignment. During this stage, we also learn mappings from the document spaces onto the common subspace. These mappings can be used to convert any new document into the interlingual representation. We describe each of these two steps in detail in the following two sub sections (Sec. 2.1 and Sec. 2.2). 2.1 Noisy Document Alignments Translating documents from one language into another language and finding the nearest neighbours gives potential alignments. Unfortunately, the resulting alignments may differ depending on the direction of the translation owing to the asymmetry of bilingual dictionaries and the nearest neighbour property. In order to overcome this asymmetry, we first turn the documents in both languages into bag of translation pairs representation.","We follow the feature representation used in Jagarlamudi and Daumé III (2010) and Boyd-Graber and Blei (2009). Each translation pair of the bilingual dictionary (also referred as a dictionary entry) is treated as a new feature. Given a document, every word is replaced with the set of bilingual dictionary entries that it participates in. If D represents the TFIDF weighted term × document matrix and T is a binary matrix matrix of size no of dictionary entries × vocab size, then converting documents into a bag of dictionary entries is given by the linear operation X(t)","← T D.2","After converting the documents into bag of dictionary entries representation, we form a bipartite graph with the documents of each language as a separate set of nodes. The edge weight Wij between a pair of documents x (t) i and y","(t)","j (in source and target language respectively) is computed as the Euclidean distance between those documents in the dictionary space. Let πij indicate the likeliness of a source document x (t) i is aligned to a target document y","(t)","j . We want each document to align to at least one document from other language. Moreover, we want to encourage similar documents to align to each other. We can formulate this objective and the constraints as the following minimum cost flow 2 Superscript (t) indicates that the data is in the form of bag","of dictionary entries 148 problem (Ravindra et al., 1993):","arg min π m,n ∑ i,j=1 Wijπij (1) ∀i ∑ j πij = 1 ; ∀j ∑ i πij = 1 ∀i, j 0 ≤ πij ≤ C where C is some user chosen constant, m and n are the number of documents in source and target languages respectively. Without the last constraint (πij ≤ C) this optimization problem always gives an integral solution and reduces to a maximum matching problem (Jonker and Volgenant, 1987). Since this solution may not be accurate, we allow many-to-many mapping by setting the constant C to a value less than one. In our experiments (Sec. 3), we found that setting C to a value less than 1 gave better performance analogous to the better performance of soft Expectation Maximization (EM) compared to hard-EM. The optimal solution of Eq. 1 can be found efficiently using linear programming (Ravindra et al., 1993). 2.2 Supervised Kernelized Sorting Kernelized Sorting is an unsupervised technique to align objects of different types, such as English and Spanish documents (Quadrianto et al., 2009; Jagaralmudi et al., 2010). The main advantage of this method is that it only uses the intra-language document similarities to identify the alignments across languages. In this section, we describe a supervised variant of Kernelized Sorting which takes a set of candidate alignments and learns to modify the intra-language document similarities to respect the given alignment. Since Kernelized Sorting does not rely on the inter-lingual document similarities at all, we hope that its supervised version is robust to noisy alignments.","Let X and Y be the TFIDF weighted term × document matrices in both the languages and let Kx and Ky be their linear dot product kernel matrices, i.e. , Kx = XT","X and Ky = Y T","Y . Let Π ∈ {0, 1}m×n","denote the permutation matrix which captures the alignment between documents of different languages, i.e. πij = 1 indicates documents xi and yj are aligned. Then Kernelized Sorting formulates Π as the solution of the following optimization problem (Gretton et al., 2005):","arg max Π","tr(KxΠKyΠT ) (2)","= arg max Π","tr(XT","X Π Y T","Y ΠT",") (3)","In our supervised version of Kernelized Sorting, we fix the permutation matrix (to say Π̂) and modify the kernel matrices Kx and Ky so that the objective function is maximized for the given permutation. Specifically, we find a mapping for each language, such that when the documents are projected into their common subspaces they are more likely to respect the alignment given by Π̂. Subsequently, the test documents are also projected into the common subspace and we return the nearest neighbors as the aligned pairs.","Let U and V be the mappings for the required subspace in both the languages, then we want to solve the following optimization problem:","arg max U,V","tr(XT","U U T","X Π̂ Y T","V V T","Y Π̂T",")","s.t. U T U = I & V T","V = I (4) where I is an identity matrix of appropriate size. For brevity, let Cxy denote the cross-covariance matrix (i.e. Cxy = X Π̂Y T",") then the above objective function becomes:","arg max U,V","tr(U U T C","xyV V T CT xy)","s.t. U T U = I & V T","V = I (5) We have used the cyclic property of the trace function while rewriting Eq. 4 to Eq. 5. We use alternative maximization to solve for the unknowns. Fixing V (to say V0), rewriting the objective function using the cyclic property of the trace function, forming the Lagrangian and setting its derivative to zero results in the following solution:","CxyV0V T 0 CT","xy U = λu U (6) For the initial iteration, we can substitute V0V T","0 as identity matrix which leaves the kernel matrix unchanged. Similarly, fixing U (to U0) and solving the optimization problem for V results: CT xyU0U T","0 Cxy V = λv V (7) 149 In the special case where both V0V T","0 and U0U T","0 are identity matrices, the above equations reduce to CxyCT","xy U = λu U and CT","xyCxy V = λv V . In this particular case, we can simultaneously solve for both U and V using Singular Value Decomposition (SVD) as:","U SV T = C xy (8) So for the first iteration, we do the SVD of the cross-covariance matrix and get the mappings. For the subsequent iterations, we use the mappings found by the previous iteration, as U0 and V0, and solve Eqs. 6 and 7 alternatively. 2.3 Summary In this section, we describe our procedure to recover document alignments. We first convert documents into bag of dictionary entries representation (Sec. 2.1). Then we solve the optimization problem in Eq. 1 to get the initial candidate alignments. We use the LEMON3","graph library to solve the min-cost flow problem. This step gives us the πij values for every cross-lingual document pair. We use them to form a relaxed permutation matrix ( Π̂) which is, subsequently, used to find the mappings (U and V ) for the documents of both the languages (i.e. solving Eq. 8). We use these mappings to project both source and target language documents into the common subspace and then solve the bipartite matching problem to recover the alignment."]},{"title":"3 Experiments","paragraphs":["For evaluation, we choose 2500 aligned document pairs from Wikipedia in English-Spanish and English-German language pairs. For both the data sets, we consider only words that occurred more than once in at least five documents. Of the words that meet the frequency criterion, we choose the most frequent 2000 words for English-Spanish data set. But, because of the compound word phenomenon of German, we retain all the frequent words for English-German data set. Subsequently we convert the documents into TFIDF weighted vectors. The bilingual dictionaries for both the language pairs are generated by running Giza++ (Och and Ney, 2003) on the Europarl data (Koehn, 2005). 3 https://lemon.cs.elte.hu/trac/lemon","En – Es En – De Word-by-Word 0.597 0.564 CCA (λ = 0.3) 0.627 0.485 CCA (λ = 0.5) 0.628 0.486 CCA (λ = 0.8) 0.637 0.487","OPCA 0.688 0.530 Ours (C = 0.6) 0.67 0.604 Ours (C = 1.0) 0.658 0.590 Table 1: Accuracy of different approaches on the Wikipedia documents in English-Spanish and English-German language pairs. For CCA, we regularize the within language covariance matrices as (1−λ)XXT","+λI and the regularization parameter λ value is also shown. We follow the process described in Sec. 2.3 to recover the document alignment for our method.","We compare our approach with a dictionary based approach, such as word-by-word translation, and supervised approaches, such as CCA (Vinokourov et al., 2003; Hotelling, 1936) and OPCA (Platt et al., 2010). Word-by-word translation and our approach use bilingual dictionary while CCA and OPCA use a training corpus of aligned documents. Since the bilingual dictionary is learnt from Europarl data set, for a fair comparison, we train supervised approaches on 3000 document pairs from Europarl data set. To prevent CCA from overfitting to the training domain, we regularize it heavily. For OPCA, we use a regularization parameter of 0.1 as suggested by Platt et al. (2010). For all the systems, we construct a bipartite graph between the documents of different languages, with edge weight be-ing the cross-lingual similarity given by the respective method and then find maximal matching (Jonker and Volgenant, 1987). We report the accuracy of the recovered alignment.","Table 1 shows accuracies of different methods on both Spanish and German data sets. For comparison purposes, we trained and tested CCA on documents from same domain (Wikipedia). It achieves 75% and 62% accuracies for the two data sets respectively but, as expected, it performed poorly when trained on Europarl articles. On the English-German data set, a simple word-by-word translation performed better than CCA and OPCA. For both the language pairs, our model performed better than word-by-word translation method and competitively with the 150 supervised approaches. Note that our method does not use any training data.","We also experimented with few values of the parameter C for the min-cost flow problem (Eq. 1). As noted previously, setting C = 1 will reduce the problem into a linear assignment problem. From the results, we see that solving a relaxed version of the problem gives better accuracies but the improve-ments are marginal (especially for English-German)."]},{"title":"4 Discussion","paragraphs":["For both language pairs, the accuracy of the first stage of our approach (Sec. 2.1) is almost same as that of word-by-word translation system. Thus, the improved performance of our system compared to word-by-word translation shows the effectiveness of the supervised Kernelized sorting.","The solution of our supervised Kernelized sorting (Eq. 8) resembles Latent Semantic Indexing (Deerwester, 1988). Except, we use a cross-covariance matrix instead of a term × document matrix. Efficient algorithms exist for solving SVD on arbitrarily large matrices, which makes our approach scalable to large data sets (Warmuth and Kuzmin, 2006). After solving Eq. 8, the mappings U and V can be improved by iteratively solving the Eqs. 6 and 7 respectively. But it leads the mappings to fit the noisy alignments exactly, so in this paper we stop after solving the SVD problem.","The extension of our approach to the situation with different number of documents on each side is straight forward. The only thing that changes is the way we compute alignment after finding the projec-tion directions. In this case, the input to the bipartite matching problem is modified by adding dummy documents to the language that has fewer documents and assigning a very high score to edges that connect to the dummy documents."]},{"title":"5 Conclusion","paragraphs":["In this paper we have presented an approach to recover document alignments from a comparable corpora using a bilingual dictionary. First, we use the bilingual dictionary to find a set of candidate noisy alignments. These noisy alignments are then fed into supervised Kernelized Sorting, which learns to modify within language document similarities to respect the given alignments.","Our approach exploits two complimentary information sources to recover a better alignment. The first step uses cross-lingual cues available in the form of a bilingual dictionary and the latter step exploits document structure captured in terms of within language document similarities. Experimental results show that our approach performs better than dictionary based approaches such as a word-by-word translation and is also competitive with supervised approaches like CCA and OPCA."]},{"title":"References","paragraphs":["Lisa Ballesteros and W. Bruce Croft. 1996. Dictionary methods for cross-lingual information retrieval. In Proceedings of the 7th International Conference on Database and Expert Systems Applications, DEXA ’96, pages 791–801, London, UK. Springer-Verlag.","Jordan Boyd-Graber and David M. Blei. 2009. Multilingual topic models for unaligned text. In Uncertainty in Artificial Intelligence.","Scott Deerwester. 1988. Improving Information Retrieval with Latent Semantic Indexing. In Christine L. Borgman and Edward Y. H. Pai, editors, Proceedings of the 51st ASIS Annual Meeting (ASIS ’88), volume 25, Atlanta, Georgia, October. American Society for Information Science.","William A. Gale and Kenneth W. Church. 1991. A program for aligning sentences in bilingual corpora. In Proceedings of the 29th annual meeting on Association for Computational Linguistics, pages 177–184, Morristown, NJ, USA. Association for Computational Linguistics.","Wei Gao, John Blitzer, and Ming Zhou. 2008. Using english information in non-english web search. In iN-EWS ’08: Proceeding of the 2nd ACM workshop on Improving non english web searching, pages 17–24, New York, NY, USA. ACM.","Wei Gao, John Blitzer, Ming Zhou, and Kam-Fai Wong. 2009. Exploiting bilingual information to improve web search. In Proceedings of Human Language Technologies: The 2009 Conference of the Association for Computational Linguistics, ACL-IJCNLP ’09, pages 1075–1083, Morristown, NJ, USA. ACL.","Arthur Gretton, Arthur Gretton, Olivier Bousquet, Olivier Bousquet, Er Smola, Bernhard Schlkopf, and Bernhard Schlkopf. 2005. Measuring statistical dependence with hilbert-schmidt norms. In Proceedings of Algorithmic Learning Theory, pages 63–77. Springer-Verlag. 151","Aria Haghighi, Percy Liang, Taylor B. Kirkpatrick, and Dan Klein. 2008. Learning bilingual lexicons from monolingual corpora. In Proceedings of ACL-08: HLT, pages 771–779, Columbus, Ohio, June. Association for Computational Linguistics.","Ulf Hermjakob, Kevin Knight, and Hal Daumé III. 2008. Name translation in statistical machine translation - learning when to transliterate. In Proceedings of ACL-08: HLT, pages 389–397, Columbus, Ohio, June. Association for Computational Linguistics.","H. Hotelling. 1936. Relation between two sets of variables. Biometrica, 28:322–377.","Jagadeesh Jagaralmudi, Seth Juarez, and Hal Daumé III. 2010. Kernelized sorting for natural language process-ing. In Proceedings of AAAI Conference on Artificial Intelligence.","Jagadeesh Jagarlamudi and Hal Daumé III. 2010. Extracting multilingual topics from unaligned comparable corpora. In Advances in Information Retrieval, 32nd European Conference on IR Research, ECIR, volume 5993, pages 444–456, Milton Keynes, UK. Springer.","R. Jonker and A. Volgenant. 1987. A shortest augment-ing path algorithm for dense and sparse linear assignment problems. Computing, 38(4):325–340.","Alexandre Klementiev and Dan Roth. 2006. Weakly supervised named entity transliteration and discovery from multilingual comparable corpora. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, ACL-44, pages 817–824, Stroudsburg, PA, USA. Association for Computational Linguistics.","Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In MT Summit.","David Mimno, Hanna M. Wallach, Jason Naradowsky, David A. Smith, and Andrew McCallum. 2009. Polylingual topic models. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2 - Volume 2, EMNLP ’09, pages 880–889, Stroudsburg, PA, USA. Association for Computational Linguistics.","Dragos Stefan Munteanu and Daniel Marcu. 2005. Improving machine translation performance by exploit-ing non-parallel corpora. Comput. Linguist., 31:477– 504, December.","Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.","Ari Pirkola, Turid Hedlund, Heikki Keskustalo, and Kalervo Jrvelin. 2001. Dictionary-based cross-language information retrieval: Problems, methods, and research findings. Information Retrieval, 4:209– 230.","John C. Platt, Kristina Toutanova, and Wen-tau Yih. 2010. Translingual document representations from discriminative projections. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10, pages 251–261, Stroudsburg, PA, USA.","Novi Quadrianto, Le Song, and Alex J. Smola. 2009. Kernelized sorting. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors, Advances in Neural Information Processing Systems 21, pages 1289–1296.","Piyush Rai and Hal Daumé III. 2009. Multi-label prediction via sparse infinite cca. In Advances in Neural Information Processing Systems, Vancouver, Canada.","Reinhard Rapp. 1999. Automatic identification of word translations from unrelated english and german corpora. In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics, ACL ’99, pages 519–526, Stroudsburg, PA, USA.","Sujith Ravi and Kevin Knight. 2009. Learning phoneme mappings for transliteration without parallel data. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chap-ter of the Association for Computational Linguistics, pages 37–45, Boulder, Colorado, June.","K. Ahuja Ravindra, L. Magnanti Thomas, and B. Orlin James. 1993. Network flows: Theory, algorithms, and applications.","Michael L. Littman Susan T. Dumais, Thomas K. Landauer. 1996. Automatic cross-linguistic information retrieval using latent semantic indexing. In Working Notes of the Workshop on Cross-Linguistic Information Retrieval, SIGIR, pages 16–23, Zurich, Switzerland. ACM.","Peter D. Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. J. Artif. Intell. Res. (JAIR), 37:141–188.","Raghavendra Udupa, K. Saravanan, A. Kumaran, and Jagadeesh Jagarlamudi. 2009. Mint: A method for effective and scalable mining of named entity transliterations from large comparable corpora. In EACL, pages 799–807. The Association for Computer Linguistics.","Alexei Vinokourov, John Shawe-taylor, and Nello Cristianini. 2003. Inferring a semantic representation of text via cross-language correlation analysis. In Advances in Neural Information Processing Systems, pages 1473–1480, Cambridge, MA. MIT Press.","Thuy Vu, AiTi Aw, and Min Zhang. 2009. Feature-based method for document alignment in comparable news corpora. In EACL, pages 843–851.","Manfred K. Warmuth and Dima Kuzmin. 2006. Randomized pca algorithms with regret bounds that are logarithmic in the dimension. In Neural Information Processing Systems, pages 1481–1488. 152"]}],"references":[{"authors":[{"first":"Lisa","last":"Ballesteros"},{"first":"W.","middle":"Bruce","last":"Croft"}],"year":"1996","title":"Dictionary methods for cross-lingual information retrieval","source":"Lisa Ballesteros and W. Bruce Croft. 1996. Dictionary methods for cross-lingual information retrieval. In Proceedings of the 7th International Conference on Database and Expert Systems Applications, DEXA ’96, pages 791–801, London, UK. Springer-Verlag."},{"authors":[{"first":"Jordan","last":"Boyd-Graber"},{"first":"David","middle":"M.","last":"Blei"}],"year":"2009","title":"Multilingual topic models for unaligned text","source":"Jordan Boyd-Graber and David M. Blei. 2009. Multilingual topic models for unaligned text. In Uncertainty in Artificial Intelligence."},{"authors":[{"first":"Scott","last":"Deerwester"}],"year":"1988","title":"Improving Information Retrieval with Latent Semantic Indexing","source":"Scott Deerwester. 1988. Improving Information Retrieval with Latent Semantic Indexing. In Christine L. Borgman and Edward Y. H. Pai, editors, Proceedings of the 51st ASIS Annual Meeting (ASIS ’88), volume 25, Atlanta, Georgia, October. American Society for Information Science."},{"authors":[{"first":"William","middle":"A.","last":"Gale"},{"first":"Kenneth","middle":"W.","last":"Church"}],"year":"1991","title":"A program for aligning sentences in bilingual corpora","source":"William A. Gale and Kenneth W. Church. 1991. A program for aligning sentences in bilingual corpora. In Proceedings of the 29th annual meeting on Association for Computational Linguistics, pages 177–184, Morristown, NJ, USA. Association for Computational Linguistics."},{"authors":[{"first":"Wei","last":"Gao"},{"first":"John","last":"Blitzer"},{"first":"Ming","last":"Zhou"}],"year":"2008","title":"Using english information in non-english web search","source":"Wei Gao, John Blitzer, and Ming Zhou. 2008. Using english information in non-english web search. In iN-EWS ’08: Proceeding of the 2nd ACM workshop on Improving non english web searching, pages 17–24, New York, NY, USA. ACM."},{"authors":[{"first":"Wei","last":"Gao"},{"first":"John","last":"Blitzer"},{"first":"Ming","last":"Zhou"},{"first":"Kam-Fai","last":"Wong"}],"year":"2009","title":"Exploiting bilingual information to improve web search","source":"Wei Gao, John Blitzer, Ming Zhou, and Kam-Fai Wong. 2009. Exploiting bilingual information to improve web search. In Proceedings of Human Language Technologies: The 2009 Conference of the Association for Computational Linguistics, ACL-IJCNLP ’09, pages 1075–1083, Morristown, NJ, USA. ACL."},{"authors":[{"first":"Arthur","last":"Gretton"},{"first":"Arthur","last":"Gretton"},{"first":"Olivier","last":"Bousquet"},{"first":"Olivier","last":"Bousquet"},{"first":"Er","last":"Smola"},{"first":"Bernhard","last":"Schlkopf"},{"first":"Bernhard","last":"Schlkopf"}],"year":"2005","title":"Measuring statistical dependence with hilbert-schmidt norms","source":"Arthur Gretton, Arthur Gretton, Olivier Bousquet, Olivier Bousquet, Er Smola, Bernhard Schlkopf, and Bernhard Schlkopf. 2005. Measuring statistical dependence with hilbert-schmidt norms. In Proceedings of Algorithmic Learning Theory, pages 63–77. Springer-Verlag. 151"},{"authors":[{"first":"Aria","last":"Haghighi"},{"first":"Percy","last":"Liang"},{"first":"Taylor","middle":"B.","last":"Kirkpatrick"},{"first":"Dan","last":"Klein"}],"year":"2008","title":"Learning bilingual lexicons from monolingual corpora","source":"Aria Haghighi, Percy Liang, Taylor B. Kirkpatrick, and Dan Klein. 2008. Learning bilingual lexicons from monolingual corpora. In Proceedings of ACL-08: HLT, pages 771–779, Columbus, Ohio, June. Association for Computational Linguistics."},{"authors":[{"first":"Ulf","last":"Hermjakob"},{"first":"Kevin","last":"Knight"},{"first":"Hal","last":"Daumé III"}],"year":"2008","title":"Name translation in statistical machine translation - learning when to transliterate","source":"Ulf Hermjakob, Kevin Knight, and Hal Daumé III. 2008. Name translation in statistical machine translation - learning when to transliterate. In Proceedings of ACL-08: HLT, pages 389–397, Columbus, Ohio, June. Association for Computational Linguistics."},{"authors":[{"first":"H.","last":"Hotelling"}],"year":"1936","title":"Relation between two sets of variables","source":"H. Hotelling. 1936. Relation between two sets of variables. Biometrica, 28:322–377."},{"authors":[{"first":"Jagadeesh","last":"Jagaralmudi"},{"first":"Seth","last":"Juarez"},{"first":"Hal","last":"Daumé III"}],"year":"2010","title":"Kernelized sorting for natural language process-ing","source":"Jagadeesh Jagaralmudi, Seth Juarez, and Hal Daumé III. 2010. Kernelized sorting for natural language process-ing. In Proceedings of AAAI Conference on Artificial Intelligence."},{"authors":[{"first":"Jagadeesh","last":"Jagarlamudi"},{"first":"Hal","last":"Daumé III"}],"year":"2010","title":"Extracting multilingual topics from unaligned comparable corpora","source":"Jagadeesh Jagarlamudi and Hal Daumé III. 2010. Extracting multilingual topics from unaligned comparable corpora. In Advances in Information Retrieval, 32nd European Conference on IR Research, ECIR, volume 5993, pages 444–456, Milton Keynes, UK. Springer."},{"authors":[{"first":"R.","last":"Jonker"},{"first":"A.","last":"Volgenant"}],"year":"1987","title":"A shortest augment-ing path algorithm for dense and sparse linear assignment problems","source":"R. Jonker and A. Volgenant. 1987. A shortest augment-ing path algorithm for dense and sparse linear assignment problems. Computing, 38(4):325–340."},{"authors":[{"first":"Alexandre","last":"Klementiev"},{"first":"Dan","last":"Roth"}],"year":"2006","title":"Weakly supervised named entity transliteration and discovery from multilingual comparable corpora","source":"Alexandre Klementiev and Dan Roth. 2006. Weakly supervised named entity transliteration and discovery from multilingual comparable corpora. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, ACL-44, pages 817–824, Stroudsburg, PA, USA. Association for Computational Linguistics."},{"authors":[{"first":"Philipp","last":"Koehn"}],"year":"2005","title":"Europarl: A parallel corpus for statistical machine translation","source":"Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In MT Summit."},{"authors":[{"first":"David","last":"Mimno"},{"first":"Hanna","middle":"M.","last":"Wallach"},{"first":"Jason","last":"Naradowsky"},{"first":"David","middle":"A.","last":"Smith"},{"first":"Andrew","last":"McCallum"}],"year":"2009","title":"Polylingual topic models","source":"David Mimno, Hanna M. Wallach, Jason Naradowsky, David A. Smith, and Andrew McCallum. 2009. Polylingual topic models. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2 - Volume 2, EMNLP ’09, pages 880–889, Stroudsburg, PA, USA. Association for Computational Linguistics."},{"authors":[{"first":"Dragos","middle":"Stefan","last":"Munteanu"},{"first":"Daniel","last":"Marcu"}],"year":"2005","title":"Improving machine translation performance by exploit-ing non-parallel corpora","source":"Dragos Stefan Munteanu and Daniel Marcu. 2005. Improving machine translation performance by exploit-ing non-parallel corpora. Comput. Linguist., 31:477– 504, December."},{"authors":[{"first":"Franz","middle":"Josef","last":"Och"},{"first":"Hermann","last":"Ney"}],"year":"2003","title":"A systematic comparison of various statistical alignment models","source":"Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51."},{"authors":[{"first":"Ari","last":"Pirkola"},{"first":"Turid","last":"Hedlund"},{"first":"Heikki","last":"Keskustalo"},{"first":"Kalervo","last":"Jrvelin"}],"year":"2001","title":"Dictionary-based cross-language information retrieval: Problems, methods, and research findings","source":"Ari Pirkola, Turid Hedlund, Heikki Keskustalo, and Kalervo Jrvelin. 2001. Dictionary-based cross-language information retrieval: Problems, methods, and research findings. Information Retrieval, 4:209– 230."},{"authors":[{"first":"John","middle":"C.","last":"Platt"},{"first":"Kristina","last":"Toutanova"},{"first":"Wen-tau","last":"Yih"}],"year":"2010","title":"Translingual document representations from discriminative projections","source":"John C. Platt, Kristina Toutanova, and Wen-tau Yih. 2010. Translingual document representations from discriminative projections. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10, pages 251–261, Stroudsburg, PA, USA."},{"authors":[{"first":"Novi","last":"Quadrianto"},{"first":"Le","last":"Song"},{"first":"Alex","middle":"J.","last":"Smola"}],"year":"2009","title":"Kernelized sorting","source":"Novi Quadrianto, Le Song, and Alex J. Smola. 2009. Kernelized sorting. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors, Advances in Neural Information Processing Systems 21, pages 1289–1296."},{"authors":[{"first":"Piyush","last":"Rai"},{"first":"Hal","last":"Daumé III"}],"year":"2009","title":"Multi-label prediction via sparse infinite cca","source":"Piyush Rai and Hal Daumé III. 2009. Multi-label prediction via sparse infinite cca. In Advances in Neural Information Processing Systems, Vancouver, Canada."},{"authors":[{"first":"Reinhard","last":"Rapp"}],"year":"1999","title":"Automatic identification of word translations from unrelated english and german corpora","source":"Reinhard Rapp. 1999. Automatic identification of word translations from unrelated english and german corpora. In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics, ACL ’99, pages 519–526, Stroudsburg, PA, USA."},{"authors":[{"first":"Sujith","last":"Ravi"},{"first":"Kevin","last":"Knight"}],"year":"2009","title":"Learning phoneme mappings for transliteration without parallel data","source":"Sujith Ravi and Kevin Knight. 2009. Learning phoneme mappings for transliteration without parallel data. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chap-ter of the Association for Computational Linguistics, pages 37–45, Boulder, Colorado, June."},{"authors":[{"first":"K.","middle":"Ahuja","last":"Ravindra"},{"first":"L.","middle":"Magnanti","last":"Thomas"},{"first":"B.","middle":"Orlin","last":"James"}],"year":"1993","title":"Network flows: Theory, algorithms, and applications","source":"K. Ahuja Ravindra, L. Magnanti Thomas, and B. Orlin James. 1993. Network flows: Theory, algorithms, and applications."},{"authors":[{"first":"Michael","middle":"L. Littman Susan T.","last":"Dumais"},{"first":"Thomas","middle":"K.","last":"Landauer"}],"year":"1996","title":"Automatic cross-linguistic information retrieval using latent semantic indexing","source":"Michael L. Littman Susan T. Dumais, Thomas K. Landauer. 1996. Automatic cross-linguistic information retrieval using latent semantic indexing. In Working Notes of the Workshop on Cross-Linguistic Information Retrieval, SIGIR, pages 16–23, Zurich, Switzerland. ACM."},{"authors":[{"first":"Peter","middle":"D.","last":"Turney"},{"first":"Patrick","last":"Pantel"}],"year":"2010","title":"From frequency to meaning: Vector space models of semantics","source":"Peter D. Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. J. Artif. Intell. Res. (JAIR), 37:141–188."},{"authors":[{"first":"Raghavendra","last":"Udupa"},{"first":"K.","last":"Saravanan"},{"first":"A.","last":"Kumaran"},{"first":"Jagadeesh","last":"Jagarlamudi"}],"year":"2009","title":"Mint: A method for effective and scalable mining of named entity transliterations from large comparable corpora","source":"Raghavendra Udupa, K. Saravanan, A. Kumaran, and Jagadeesh Jagarlamudi. 2009. Mint: A method for effective and scalable mining of named entity transliterations from large comparable corpora. In EACL, pages 799–807. The Association for Computer Linguistics."},{"authors":[{"first":"Alexei","last":"Vinokourov"},{"first":"John","last":"Shawe-taylor"},{"first":"Nello","last":"Cristianini"}],"year":"2003","title":"Inferring a semantic representation of text via cross-language correlation analysis","source":"Alexei Vinokourov, John Shawe-taylor, and Nello Cristianini. 2003. Inferring a semantic representation of text via cross-language correlation analysis. In Advances in Neural Information Processing Systems, pages 1473–1480, Cambridge, MA. MIT Press."},{"authors":[{"first":"Thuy","last":"Vu"},{"first":"AiTi","last":"Aw"},{"first":"Min","last":"Zhang"}],"year":"2009","title":"Feature-based method for document alignment in comparable news corpora","source":"Thuy Vu, AiTi Aw, and Min Zhang. 2009. Feature-based method for document alignment in comparable news corpora. In EACL, pages 843–851."},{"authors":[{"first":"Manfred","middle":"K.","last":"Warmuth"},{"first":"Dima","last":"Kuzmin"}],"year":"2006","title":"Randomized pca algorithms with regret bounds that are logarithmic in the dimension","source":"Manfred K. Warmuth and Dima Kuzmin. 2006. Randomized pca algorithms with regret bounds that are logarithmic in the dimension. In Neural Information Processing Systems, pages 1481–1488. 152"}],"cites":[{"style":0,"text":"Gale and Church, 1991","origin":{"pointer":"/sections/2/paragraphs/0","offset":307,"length":21},"authors":[{"last":"Gale"},{"last":"Church"}],"year":"1991","references":["/references/3"]},{"style":0,"text":"Rapp, 1999","origin":{"pointer":"/sections/2/paragraphs/0","offset":330,"length":10},"authors":[{"last":"Rapp"}],"year":"1999","references":["/references/22"]},{"style":0,"text":"Ballesteros and Croft, 1996","origin":{"pointer":"/sections/2/paragraphs/0","offset":342,"length":27},"authors":[{"last":"Ballesteros"},{"last":"Croft"}],"year":"1996","references":["/references/0"]},{"style":0,"text":"Munteanu and Marcu, 2005","origin":{"pointer":"/sections/2/paragraphs/0","offset":371,"length":24},"authors":[{"last":"Munteanu"},{"last":"Marcu"}],"year":"2005","references":["/references/16"]},{"style":0,"text":"Vu et al., 2009","origin":{"pointer":"/sections/2/paragraphs/0","offset":397,"length":15},"authors":[{"last":"Vu"},{"last":"al."}],"year":"2009","references":["/references/29"]},{"style":0,"text":"Klementiev and Roth, 2006","origin":{"pointer":"/sections/2/paragraphs/0","offset":439,"length":25},"authors":[{"last":"Klementiev"},{"last":"Roth"}],"year":"2006","references":["/references/13"]},{"style":0,"text":"Hermjakob et al., 2008","origin":{"pointer":"/sections/2/paragraphs/0","offset":466,"length":22},"authors":[{"last":"Hermjakob"},{"last":"al."}],"year":"2008","references":["/references/8"]},{"style":0,"text":"Udupa et al., 2009","origin":{"pointer":"/sections/2/paragraphs/0","offset":490,"length":18},"authors":[{"last":"Udupa"},{"last":"al."}],"year":"2009","references":["/references/27"]},{"style":0,"text":"Ravi and Knight, 2009","origin":{"pointer":"/sections/2/paragraphs/0","offset":510,"length":21},"authors":[{"last":"Ravi"},{"last":"Knight"}],"year":"2009","references":["/references/23"]},{"style":0,"text":"Gao et al., 2008","origin":{"pointer":"/sections/2/paragraphs/0","offset":562,"length":16},"authors":[{"last":"Gao"},{"last":"al."}],"year":"2008","references":["/references/4"]},{"style":0,"text":"Gao et al., 2009","origin":{"pointer":"/sections/2/paragraphs/0","offset":580,"length":16},"authors":[{"last":"Gao"},{"last":"al."}],"year":"2009","references":["/references/5"]},{"style":0,"text":"Turney and Pantel, 2010","origin":{"pointer":"/sections/2/paragraphs/1","offset":101,"length":23},"authors":[{"last":"Turney"},{"last":"Pantel"}],"year":"2010","references":["/references/26"]},{"style":0,"text":"Ballesteros and Croft, 1996","origin":{"pointer":"/sections/2/paragraphs/2","offset":326,"length":27},"authors":[{"last":"Ballesteros"},{"last":"Croft"}],"year":"1996","references":["/references/0"]},{"style":0,"text":"Pirkola et al., 2001","origin":{"pointer":"/sections/2/paragraphs/2","offset":355,"length":20},"authors":[{"last":"Pirkola"},{"last":"al."}],"year":"2001","references":["/references/18"]},{"style":0,"text":"Dumais, 1996","origin":{"pointer":"/sections/2/paragraphs/2","offset":685,"length":12},"authors":[{"last":"Dumais"}],"year":"1996","references":[]},{"style":0,"text":"Vinokourov et al., 2003","origin":{"pointer":"/sections/2/paragraphs/2","offset":699,"length":23},"authors":[{"last":"Vinokourov"},{"last":"al."}],"year":"2003","references":["/references/28"]},{"style":0,"text":"Mimno et al., 2009","origin":{"pointer":"/sections/2/paragraphs/2","offset":724,"length":18},"authors":[{"last":"Mimno"},{"last":"al."}],"year":"2009","references":["/references/15"]},{"style":0,"text":"Platt et al., 2010","origin":{"pointer":"/sections/2/paragraphs/2","offset":744,"length":18},"authors":[{"last":"Platt"},{"last":"al."}],"year":"2010","references":["/references/19"]},{"style":0,"text":"Haghighi et al., 2008","origin":{"pointer":"/sections/2/paragraphs/2","offset":764,"length":21},"authors":[{"last":"Haghighi"},{"last":"al."}],"year":"2008","references":["/references/7"]},{"style":0,"text":"Rai and Daumé III, 2009","origin":{"pointer":"/sections/2/paragraphs/5","offset":467,"length":23},"authors":[{"last":"Rai"},{"last":"Daumé III"}],"year":"2009","references":["/references/21"]},{"style":0,"text":"Quadrianto et al., 2009","origin":{"pointer":"/sections/2/paragraphs/5","offset":564,"length":23},"authors":[{"last":"Quadrianto"},{"last":"al."}],"year":"2009","references":["/references/20"]},{"style":0,"text":"Quadrianto et al., 2009","origin":{"pointer":"/sections/2/paragraphs/6","offset":240,"length":23},"authors":[{"last":"Quadrianto"},{"last":"al."}],"year":"2009","references":["/references/20"]},{"style":0,"text":"Jagarlamudi and Daumé III (2010)","origin":{"pointer":"/sections/3/paragraphs/1","offset":45,"length":32},"authors":[{"last":"Jagarlamudi"},{"last":"Daumé III"}],"year":"2010","references":["/references/11"]},{"style":0,"text":"Boyd-Graber and Blei (2009)","origin":{"pointer":"/sections/3/paragraphs/1","offset":82,"length":27},"authors":[{"last":"Boyd-Graber"},{"last":"Blei"}],"year":"2009","references":["/references/1"]},{"style":0,"text":"Ravindra et al., 1993","origin":{"pointer":"/sections/3/paragraphs/8","offset":35,"length":21},"authors":[{"last":"Ravindra"},{"last":"al."}],"year":"1993","references":["/references/24"]},{"style":0,"text":"Jonker and Volgenant, 1987","origin":{"pointer":"/sections/3/paragraphs/9","offset":343,"length":26},"authors":[{"last":"Jonker"},{"last":"Volgenant"}],"year":"1987","references":["/references/12"]},{"style":0,"text":"Ravindra et al., 1993","origin":{"pointer":"/sections/3/paragraphs/9","offset":771,"length":21},"authors":[{"last":"Ravindra"},{"last":"al."}],"year":"1993","references":["/references/24"]},{"style":0,"text":"Quadrianto et al., 2009","origin":{"pointer":"/sections/3/paragraphs/9","offset":953,"length":23},"authors":[{"last":"Quadrianto"},{"last":"al."}],"year":"2009","references":["/references/20"]},{"style":0,"text":"Jagaralmudi et al., 2010","origin":{"pointer":"/sections/3/paragraphs/9","offset":978,"length":24},"authors":[{"last":"Jagaralmudi"},{"last":"al."}],"year":"2010","references":["/references/10"]},{"style":0,"text":"Gretton et al., 2005","origin":{"pointer":"/sections/3/paragraphs/13","offset":249,"length":20},"authors":[{"last":"Gretton"},{"last":"al."}],"year":"2005","references":["/references/6"]},{"style":0,"text":"Och and Ney, 2003","origin":{"pointer":"/sections/4/paragraphs/0","offset":617,"length":17},"authors":[{"last":"Och"},{"last":"Ney"}],"year":"2003","references":["/references/17"]},{"style":0,"text":"Koehn, 2005","origin":{"pointer":"/sections/4/paragraphs/0","offset":658,"length":11},"authors":[{"last":"Koehn"}],"year":"2005","references":["/references/14"]},{"style":0,"text":"Vinokourov et al., 2003","origin":{"pointer":"/sections/4/paragraphs/4","offset":132,"length":23},"authors":[{"last":"Vinokourov"},{"last":"al."}],"year":"2003","references":["/references/28"]},{"style":0,"text":"Hotelling, 1936","origin":{"pointer":"/sections/4/paragraphs/4","offset":157,"length":15},"authors":[{"last":"Hotelling"}],"year":"1936","references":["/references/9"]},{"style":0,"text":"Platt et al., 2010","origin":{"pointer":"/sections/4/paragraphs/4","offset":184,"length":18},"authors":[{"last":"Platt"},{"last":"al."}],"year":"2010","references":["/references/19"]},{"style":0,"text":"Platt et al. (2010)","origin":{"pointer":"/sections/4/paragraphs/4","offset":650,"length":19},"authors":[{"last":"Platt"},{"last":"al."}],"year":"2010","references":["/references/19"]},{"style":0,"text":"Jonker and Volgenant, 1987","origin":{"pointer":"/sections/4/paragraphs/4","offset":885,"length":26},"authors":[{"last":"Jonker"},{"last":"Volgenant"}],"year":"1987","references":["/references/12"]},{"style":0,"text":"Deerwester, 1988","origin":{"pointer":"/sections/5/paragraphs/1","offset":94,"length":16},"authors":[{"last":"Deerwester"}],"year":"1988","references":["/references/2"]},{"style":0,"text":"Warmuth and Kuzmin, 2006","origin":{"pointer":"/sections/5/paragraphs/1","offset":319,"length":24},"authors":[{"last":"Warmuth"},{"last":"Kuzmin"}],"year":"2006","references":["/references/30"]}]}
