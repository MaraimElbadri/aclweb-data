{"sections":[{"title":"","paragraphs":["Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1036–1044, Portland, Oregon, June 19-24, 2011. c⃝2011 Association for Computational Linguistics"]},{"title":"Reordering with Source Language Collocations   Zhanyi Liu1,2 , Haifeng Wang 2 , Hua Wu2 , Ting Liu1 , Sheng Li 1","paragraphs":["1"]},{"title":"Harbin Institute of Technology, Harbin, China","paragraphs":["2"]},{"title":"Baidu Inc., Beijing, China {liuzhanyi, wanghaifeng, wu_hua}@baidu.com {tliu, lisheng}@hit.edu.cn    Abstract","paragraphs":["This paper proposes a novel reordering model for statistical machine translation (SMT) by means of modeling the translation orders of the source language collocations. The model is learned from a word-aligned bilingual corpus where the collocated words in source sentences are automatically detected. During decoding, the model is employed to softly constrain the translation orders of the source language collocations, so as to constrain the translation orders of those source phrases containing these collocated words. The experimental results show that the proposed method significantly improves the translation quality, achieving the absolute improvements of 1.1~1.4 BLEU score over the baseline methods."]},{"title":"1 Introduction","paragraphs":["Reordering for SMT is first proposed in IBM models (Brown et al., 1993), usually called IBM constraint model, where the movement of words during translation is modeled. Soon after, Wu (1997) proposed an ITG (Inversion Transduction Grammar) model for SMT, called ITG constraint model, where the reordering of words or phrases is constrained to two kinds: straight and inverted. In order to further improve the reordering performance, many structure-based methods are proposed, including the reordering model in hierarchical phrase-based SMT systems (Chiang, 2005) and syntax-based SMT systems (Zhang et al., 2007; Marton and Resnik, 2008; Ge, 2010; Visweswariah et al., 2010). Although the sentence structure has been taken into consideration, these methods don‟t explicitly make use of the strong correlations between words, such as collocations, which can effectively indicate reordering in the target language.","In this paper, we propose a novel method to improve the reordering for SMT by estimating the reordering score of the source-language collocations (source collocations for short in this paper). Given a bilingual corpus, the collocations in the source sentence are first detected automatically using a monolingual word alignment (MWA) method without employing additional resources (Liu et al., 2009), and then the reordering model based on the detected collocations is learned from the word-aligned bilingual corpus. The source collocation based reordering model is integrated into SMT systems as an additional feature to softly constrain the translation orders of the source collocations in the sentence to be translated, so as to constrain the translation orders of those source phrases contain-ing these collocated words.","This method has two advantages: (1) it can automatically detect and leverage collocated words in a sentence, including long-distance collocated words; (2) such a reordering model can be integrated into any SMT systems without resorting to any additional resources.","We implemented the proposed reordering model in a phrase-based SMT system, and the evaluation results show that our method significantly improves translation quality. As compared to the baseline systems, an absolute improvement of 1.1~1.4 BLEU score is achieved. 1036","The paper is organized as follows: In section 2, we describe the motivation to use source collocations for reordering, and briefly introduces the collocation extraction method. In section 3, we present our reordering model. And then we describe the experimental results in section 4 and 5. In section 6, we describe the related work. Lastly, we conclude in section 7."]},{"title":"2 Collocation","paragraphs":["A collocation is generally composed of a group of words that occur together more often than by chance. Collocations effectively reveal the strong association among words in a sentence and are widely employed in a variety of NLP tasks (Mckeown and Radey, 2000).","Given two words in a collocation, they can be translated in the same order as in the source language, or in the inverted order. We name the first case as straight, and the second inverted. Based on the observation that some collocations tend to have fixed translation orders such as “金融 jin-rong „financial‟ 危机 wei-ji „crisis‟” (financial crisis) whose English translation order is usually straight, and “ 法律 fa-lv „law‟ 范围 fan-wei „scope‟” (scope of law) whose English translation order is generally inverted, some methods have been proposed to improve the reordering model for SMT based on the collocated words crossing the neighboring components (Xiong et al., 2006). We further notice that some words are translated in different orders when they are collocated with different words. For instance, when “潮流 chao-liu „trend‟” is collocated with “时代 shi-dai „times‟”, they are often translated into the “trend of times”; when collocated with “历史 li-shi „history‟”, the translation usually becomes the “historical trend”. Thus, if we can automatically detect the collocations in the sentence to be translated and their orders in the target language, the reordering information of the collocations could be used to constrain the reordering of phrases during decoding. Therefore, in this paper, we propose to improve the reordering model for SMT by estimating the reordering score based on the translation orders of the source collocations.","In general, the collocations can be automatically identified based on syntactic information such as dependency trees (Lin, 1998). However these methods may suffer from parsing errors. Moreover, for many languages, no valid dependency parser exists. Liu et al. (2009) proposed to automatically detect the collocated words in a sentence with the MWA method. The advantage of this method lies in that it can identify the collocated words in a sentence without additional resources. In this paper, we employ MWA Model l~3 described in Liu et al. (2009) to detect collocations in sentences, which are shown in Eq. (1)~(3)."]},{"title":"","paragraphs":["  l j cj jwwtSAp 11 ModelMWA )|()|( (1)"]},{"title":"","paragraphs":["  l j jcj lcjdwwtSAp j 12 ModelMWA ),|()|()|( (2)"]},{"title":" ","paragraphs":["    l j jcj l i ii lcjdwwt wnSAp j 1 1 3 ModelMWA ),|()|( )|()|( (3) Where l","wS 1 is a monolingual sentence; i de-","notes the number of words collocating with iw ;","}&],1[|),{( icliciA ii  denotes the potentially","collocated words in S. The MWA models measure the collocated","words under different constraints. MWA Model 1","only models word collocation probabilities",")|( jcj wwt . MWA Model 2 additionally employs position collocation probabilities ),|( lcjd j . Be-sides the features in MWA Model 2, MWA Model 3 also considers fertility probabilities )|( ii wn  .","Given a sentence, the optimal collocated words can be obtained according to Eq. (4).",")|(maxarg* ModelMWA SApA i A (4)","Given a monolingual word aligned corpus, the collocation probabilities can be estimated as follows. 2 )|()|( ),( ijji ji wwpwwp wwr   (5) Where,"]},{"title":"","paragraphs":["   w j ji ji wwcount wwcount wwp ),(","),( )|( ; ),( ji ww ","denotes the collocated words in the corpus and ),( ji wwcount denotes the co-occurrence frequency. 1037"]},{"title":"3 Reordering Model with Source Language Collocations","paragraphs":["In this section, we first describe how to estimate the orientation probabilities for a given collocation, and then describe the estimation of the reordering score during translation. Finally, we describe the integration of the reordering model into the SMT system."]},{"title":"3.1 Reordering probability estimation","paragraphs":["Given a source collocation ),( ji ff and its corresponding translations ),( ji aa ee in a bilingual sentence pair, the reordering orientation of the collocation can be defined as in Eq. (6).     jiji jiji","aaji aajiaaji aajiaaji o ji","&or& ifinverted &or& ifstraight ,,, (6)","In our method, only those collocated words in source language that are aligned to different target words, are taken into consideration, and those be-ing aligned to the same target word are ignored.","Given a word-aligned bilingual corpus where the collocations in source sentences are detected, the probabilities of the translation orientation of collocations in the source language can be estimated, as follows:"]},{"title":"","paragraphs":["   o ji","ji","ji ffocount ffocount ffop","),,( ),,straight( ),|straight( (7)"]},{"title":"","paragraphs":["   o ji","ji","ji ffocount ffocount ffop ),,( ),,inverted( ),|inverted( (8)","Here, ),,( ji ffocount is collected according to the algorithm in Figure 1."]},{"title":"3.2 Reordering model","paragraphs":["Given a sentence l","fF 1 to be translated, the collocations are first detected using the algorithm described in Eq. (4). Then the reordering score is estimated according to the reordering probability weighted by the collocation probability of the collocated words. Formally, for a generated translation candidate T , the reordering score is calculated as follows.","),|(log),(),( ,,, ),(","i icii i i ciaaci ci ciO ffopffrTFP"]},{"title":"","paragraphs":[" (9) Input: A word-aligned bilingual corpus where the source collocations are detected Initialization: ),,( ji ffocount =0","for each sentence pair <F, E> in the corpus do for each collocated word pair ),( ici ff in F do if icii aaci  & or icii aaci  & then  ),,( ici ffstraightocount if icii aaci  & or icii aaci  & then  ),,( ici ffinvertedocount  Output: ),,( ji ffocount","Figure 1. Algorithm of estimating reordering frequency","Here, ),( ici ffr denotes the collocation probability of if and icf as shown in Eq. (5).","In addition to the detected collocated words in the sentence, we also consider other possible word pairs whose collocation probabilities are higher than a given threshold. Thus, the reordering score is further improved according to Eq. (10)."]},{"title":" ","paragraphs":["    ),(& )},{(),( ,,, ,,, ),( )},|(log),( ),|(log),(),( ji i ji","i icii i i ffr ciji jiaajiji ciaaci ci ciO ffopffr ffopffrTFP  (10)","Where  and  are two interpolation weights.  is the threshold of collocation probability. The weights and the threshold can be tuned using a development set."]},{"title":"3.3 Integrated into SMT system","paragraphs":["The SMT systems generally employ the log-linear model to integrate various features (Chiang, 2005; Koehn et al., 2007). Given an input sentence F, the final translation E* with the highest score is chosen from candidates, as in Eq. (11).","}),({maxarg* 1"]},{"title":"","paragraphs":["  M m mm E FEhE  (11)","Where hm(E, F) (m=1,...,M) denotes features. m"]},{"title":"","paragraphs":["is a feature weight. Our reordering model can be integrated into the","system as one feature as shown in (10). 1038  Figure 2. An example for reordering"]},{"title":"4 Evaluation of Our Method 4.1 Implementation","paragraphs":["We implemented our method in a phrase-based SMT system (Koehn et al., 2007). Based on the GIZA++ package (Och and Ney, 2003), we implemented a MWA tool for collocation detection. Thus, given a sentence to be translated, we first identify the collocations in the sentence, and then estimate the reordering score according to the translation hypothesis. For a translation option to be expanded, the reordering score inside this source phrase is calculated according to their translation orders of the collocations in the corresponding target phrase. The reordering score crossing the current translation option and the covered parts can be calculated according to the relative position of the collocated words. If the source phrase matched by the current translation option is behind the covered parts in the source sentence, then","...)|staight(log op is used, otherwise","...)|inverted(log op . For example, in Figure 2, the current translation option is ( 4332 eeff  ). The collocations related to this translation option are","),( 31 ff , ),( 32 ff , ),( 53 ff . The reordering scores can be estimated as follows:","),|straight(log),( 3131 ffopffr ","),|inverted(log),( 3232 ffopffr ","),|inverted(log),( 5353 ffopffr ","In order to improve the performance of the decoder, we design a heuristic function to estimate the future score, as shown in Figure 3. For any uncovered word and its collocates in the input sentence, if the collocate is uncovered, then the higher reordering probability is used. If the collocate has been covered, then the reordering orientation can Input: Input sentence L fF 1 Initialization: Score = 0 for each uncovered word if do","for each word jf ( icj   or )( , ji ffr ) do if jf is covered then","if i > j then Score+= ),|straight(log)( , jiji ffopffr ","else Score+= ),|inverted(log)( , jiji ffopffr  ","else Score += ),|(log)(maxarg , jijio ffopffr Output: Score","Figure 3. Heuristic function for estimating future score be determined according to the relative positions of the words and the corresponding reordering probability is employed."]},{"title":"4.2 Settings","paragraphs":["We use the FBIS corpus (LDC2003E14) to train a Chinese-to-English phrase-based translation model. And the SRI language modeling toolkit (Stolcke, 2002) is used to train a 5-gram language model on the English sentences of FBIS corpus.","We used the NIST evaluation set of 2002 as the development set to tune the feature weights of the SMT system and the interpolation parameters, based on the minimum error rate training method (Och, 2003), and the NIST evaluation sets of 2004 and 2008 (MT04 and MT08) as the test sets.","We use BLEU (Papineni et al., 2002) as evaluation metrics. We also calculate the statistical significance differences between our methods and the baseline method by using the paired bootstrap resample method (Koehn, 2004)."]},{"title":"4.3 Translation results","paragraphs":["We compare the proposed method with various","reordering methods in previous work.","Monotone model: no reordering model is used.","Distortion based reordering (DBR) model: a distortion based reordering method ( Al-Onaizan & Papineni, 2006). In this method, the distortion cost is defined in terms of words, rather than phrases. This method considers outbound, inbound, and pairwise distortions that f1 f2 f3 f4 f5 e4 e3 e2 e1 1039 Reorder models MT04 MT08 Monotone model 26.99 18.30","DBR model 26.64 17.83","MSDR model (Baseline) 28.77 18.42 MSDR+ DBR model 28.91 18.58 SCBR Model 1 29.21 19.28 SCBR Model 2 29.44 19.36 SCBR Model 3 29.50 19.44","SCBR models (1+2) 29.65 19.57","SCBR models (1+2+3) 29.75 19.61 Table 1. Translation results on various reordering models  T1: The two sides are also the basic stand of not relaxed. T2: The basic stance of the two sides have not relaxed. Reference: The basic stances of both sides did not move. Figure 4. Translation example. (*/*) denotes (pstraight / pinverted)","are directly estimated by simple counting over alignments in the word-aligned bilingual corpus. This method is similar to our proposed method. But our method considers the translation order of the collocated words.","msd-bidirectional-fe reordering (MSDR or Baseline) model: it is one of the reordering models in Moses. It considers three different orientation types (monotone, swap, and discontinuous) on both source phrases and target phrases. And the translation orders of both the next phrase and the previous phrase in respect to the current phrase are modeled.","Source collocation based reordering (SCBR) model: our proposed method. We investigate three reordering models based on the corresponding MWA models and their combinations. In SCBR Model i (i=1~3), we use MWA Model i as described in section 2 to obtain the collocated words and estimate the reordering probabilities according to section 3.","The experiential results are shown in Table 1. The DBR model suffers from serious data sparseness. For example, the reordering cases in the trained pairwise distortion model only covered 32~38% of those in the test sets. So its performance is worse than that of the monotone model. The MSDR model achieves higher BLEU scores than the monotone model and the DBR model. Our models further improve the translation quality, achieving better performance than the combination of MSDR model and DBR model. The results in Table 1 show that “MSDR + SCBR Model 3” performs the best among the SCBR models. This is because, as compared to MWA Model 1 and 2, MWA Model 3 takes more information into consideration, including not only the co-occurrence information of lexical tokens and the position of words, but also the fertility of words in a sentence. And when the three SCBR models are combined, the performance of the SMT system is further improved. As compared to other reordering models, our models achieve an absolute improvement of 0.98~1.19 BLEU score on the test sets, which are statistically significant (p < 0.05).","Figure 4 shows an example: T1 is generated by the baseline system and T2 is generated by the system where the SCBR models (1+2+3) 1 are used.  1 In the remainder of this paper, “SCBR models” means the combination of the SCBR models (1+2+3) unless it is explicitly explained.","Input: 双方 的 基本 立场 也 都 没有 松动 。 shuang-fang DE ji-ben li-chang ye dou mei-you song-dong . (0.99/0.01) both-side DE basic stance also both not loose . (0.21/0.79) (0.95/0.05) 1040","Reordering models MT04 MT08 MSDR model 28.77 18.42 MSDR+ DBR model 28.91 18.58 CBR model 28.96 18.77 WCBR model 29.15 19.10 WCBR+SCBR","models 29.87 19.83","Table 2. Translation results of co-occurrence based reordering models CBR model","SCBR","Model3","Consecutive words 77.9% 73.5%","Interrupted words 74.1% 87.8% Total 74.3% 84.9%","Table 3. Precisions of the reordering models on the development set The input sentence contains three collocations. The collocation (基本, 立场) is included in the same phrase and translated together as a whole. Thus its translation is correct in both translations. For the other two long-distance collocations (双方, 立场) and (立场, 松动), their translation orders are not correctly handled by the reordering model in the baseline system. For the collocation (双方, 立场), since the SCBR models indicate p(o=straight|双方, 立场) < p(o=inverted|双方, 立场), the system finally generates the translation T2 by constraining their translation order with the proposed model."]},{"title":"5 Collocations vs. Co-occurring Words","paragraphs":["We compared our method with the method that models the reordering orientations based on co-occurring words in the source sentences, rather than the collocations."]},{"title":"5.1 Co-occurrence based reordering model","paragraphs":["We use the similar algorithm described in section 3 to train the co-occurrence based reordering (CBR) model, except that the probability of the reordering orientation is estimated on the co-occurring words and the relative distance. Given an input sentence and a translation candidate, the reordering score is estimated as shown in Eq. (12)."]},{"title":"","paragraphs":[" ),( ,,, ),,|(log),( ji jijiaajiO ffopTFP ji (12) Here, ji is the relative distance of two words in the source sentence.","We also construct the weighted co-occurrence based reordering (WCBR) model. In this model, the probability of the reordering orientation is additionally weighted by the pointwise mutual information 2","score of the two words (Manning and Schütze, 1999), which is estimated as shown in Eq. (13)."]},{"title":"","paragraphs":[" ),( ,,,MI ),,|(log),( ),( ji jijiaajiji O ffopffs TFP ji (13)"]},{"title":"5.2 Translation results","paragraphs":["Table 2 shows the translation results. It can be seen that the performance of the SMT system is improved by integrating the CBR model. The performance of the CBR model is also better than that of the DBR model. It is because the former is trained based on all co-occurring aligned words, while the latter only considers the adjacent aligned words. When the WCBR model is used, the translation quality is further improved. However, its performance is still inferior to that of the SCBR models, indicating that our method (SCBR models) of modeling the translation orders of source collocations is more effective. Furthermore, we combine the weighted co-occurrence based model and our method, which outperform all the other models."]},{"title":"5.3 Result analysis Precision of prediction","paragraphs":["First of all, we investigate the performance of the reordering models by calculating precisions of the translation orders predicted by the reordering models. Based on the source sentences and reference translations of the development set, where the source words and target words are automatically aligned by the bilingual word alignment method, we construct the reference translation orders for two words. Against the references, we calculate three kinds of precisions as follows: |}1|||{| |}&1{| , ,,,, CW    jio ooj||i P ji aajiji ji (14)  2 For occurring words extraction, the window size is set to [-6, +6]. 1041 |}1|||{| |}&1{| , ,,,, IW    jio ooj||i P ji aajiji ji (15) |}{| |}{| , ,,,, total ji aajiji o","oo","P ji  (16) Here, jio , denotes the translation order of ( ji ff , )","predicted by the reordering models. If )|straight( , ji ffop  > ),inverted( ji f|fop  , then straight, jio , else if )|straight( , ji ffop  < ),inverted( ji f|fop  , then inverted, jio . ji aaji"]},{"title":"o","paragraphs":[",,, denotes the translation order derived from the word alignments. If ji aajiji oo ,,,,  , then the predicted translation order is correct, otherwise wrong. CWP and IWP denote the precisions calculated on the consecutive words and the interrupted words in the source sentences, respectively. totalP denotes the precision on both cases. Here, the CBR model and SCBR Model 3 are compared. The results are shown in Table 3.","From the results in Table 3, it can be seen that the CBR model has a higher precision on the consecutive words than the SCBR model, but lower precisions on the interrupted words. It is mainly because the CBR model introduces more noise when the relative distance of words is set to a large number, while the MWA method can effectively detect the long-distance collocations in sentences (Liu et al., 2009). This explains why the combination of the two models can obtain the highest BLEU score as shown in Table 2. On the whole, the SCBR Model 3 achieves higher precision than the CBR model. Effect of the reordering model","Then we evaluate the reordering results of the generated translations in the test sets. Using the above method, we construct the reference translation orders of collocations in the test sets. For a given word pair in a source sentence, if the translation order in the generated translation is the same as that in the reference translations, then it is correct, otherwise wrong.","We compare the translations of the baseline method, the co-occurrence based method, and our method (SCBR models). The precisions calculated on both kinds of words are shown in Table 4. From Test sets Baseline (MSDR) MSDR+ WCBR","MSDR+","SCBR","MT04 78.9% 80.8% 82.5%","MT08 80.7% 83.8% 85.0%","Table 4. Precisions (total) of the reordering models on the test sets the results, it can be seen that our method achieves higher precisions than both the baseline and the method modeling the translation orders of the co-occurring words. It indicates that the proposed method effectively constrains the reordering of source words during decoding and improves the translation quality."]},{"title":"6 Related Work","paragraphs":["Reordering was first proposed in the IBM models (Brown et al., 1993), later was named IBM constraint by Berger et al. (1996). This model treats the source word sequence as a coverage set that is processed sequentially and a source token is covered when it is translated into a new target token. In 1997, another model called ITG constraint was presented, in which the reordering order can be hierarchically modeled as straight or inverted for two nodes in a binary branching structure (Wu, 1997). Although the ITG constraint allows more flexible reordering during decoding, Zens and Ney (2003) showed that the IBM constraint results in higher BLEU scores. Our method models the reordering of collocated words in sentences instead of all words in IBM models or two neighboring blocks in ITG models.","For phrase-based SMT models, Koehn et al. (2003) linearly modeled the distance of phrase movements, which results in poor global reordering. More methods are proposed to explicitly model the movements of phrases (Tillmann, 2004; Koehn et al., 2005) or to directly predict the orientations of phrases (Tillmann and Zhang, 2005; Zens and Ney, 2006), conditioned on current source phrase or target phrase. Hierarchical phrase-based SMT methods employ SCFG bilingual translation model and allow flexible reordering (Chiang, 2005). However, these methods ignored the correlations among words in the source language or in the target language. In our method, we automatically detect the collocated words in sentences and 1042 their translation orders in the target languages, which are used to constrain the ordering models with the estimated reordering (straight or inverted) score. Moreover, our method allows flexible reordering by considering both consecutive words and interrupted words.","In order to further improve translation results, many researchers employed syntax-based reordering methods (Zhang et al., 2007; Marton and Resnik, 2008; Ge, 2010; Visweswariah et al., 2010). However these methods are subject to parsing errors to a large extent. Our method directly obtains collocation information without resorting to any linguistic knowledge or tools, therefore is suitable for any language pairs.","In addition, a few models employed the collocation information to improve the performance of the ITG constraints (Xiong et al., 2006). Xiong et al. used the consecutive co-occurring words as collocation information to constrain the reordering, which did not lead to higher translation quality in their experiments. In our method, we first detect both consecutive and interrupted collocated words in the source sentence, and then estimated the reordering score of these collocated words, which are used to softly constrain the reordering of source phrases."]},{"title":"7 Conclusions","paragraphs":["We presented a novel model to improve SMT by means of modeling the translation orders of source collocations. The model was learned from a word-aligned bilingual corpus where the potentially collocated words in source sentences were automatically detected by the MWA method. During decoding, the model is employed to softly constrain the translation orders of the source language collocations. Since we only model the reordering of collocated words, our methods can partially alleviate the data sparseness encountered by other methods directly modeling the reordering based on source phrases or target phrases. In addition, this kind of reordering information can be integrated into any SMT systems without resorting to any additional resources.","The experimental results show that the proposed method significantly improves the translation quality of a phrase based SMT system, achieving an absolute improvement of 1.1~1.4 BLEU score over the baseline methods."]},{"title":"References","paragraphs":["Yaser Al-Onaizan and Kishore Papineni. 2006. Distortion Models for Statistical Machine Translation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pp. 529-536.","Adam L. Berger, Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, Andrew S. Kehler, and Robert L. Mercer. 1996. Language Translation Apparatus and Method of Using Context-Based Translation Models. United States Patent, Patent Number 5510981, April.","Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert. L. Mercer. 1993. The Mathematics of Statistical Machine Translation: Parameter estimation. Computational Linguistics, 19(2): 263-311.","David Chiang. 2005. A Hierarchical Phrase-based Model for Statistical Machine Translation. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pp. 263-270.","Niyu Ge. 2010. A Direct Syntax-Driven Reordering Model for Phrase-Based Machine Translation. In Proceedings of Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pp. 849-857.","Philipp Koehn. 2004. Statistical Significance Tests for Machine Translation Evaluation. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pp. 388-395.","Philipp Koehn, Franz Joseph Och, and Daniel Marcu. 2003. Statistical Phrase-Based Translation. In Proceedings of the Joint Conference on Human Language Technologies and the Annual Meeting of the North American Chapter of the Association of Computational Linguistics, pp. 127-133.","Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In Proceedings of the 45th Annual Meeting of the ACL, Poster and Demonstration Sessions, pp. 177-180.","Philipp Koehn, Amittai Axelrod, Alexandra Birch Mayne, Chris Callison-Burch, Miles Osborne, and David Talbot. 2005. Edinburgh System Description for the 2005 IWSLT Speech Translation Evaluation. In Proceedings of International Workshop on Spoken Language Translation. 1043","Dekang Lin. 1998. Extracting Collocations from Text Corpora. In Proceedings of the 1st Workshop on Computational Terminology, pp. 57-63.","Zhanyi Liu, Haifeng Wang, Hua Wu, and Sheng Li. 2009. Collocation Extraction Using Monolingual Word Alignment Method. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pp. 487-495.","Christopher D. Manning and Hinrich Schütze. 1999. Foundations of Statistical Natural Language Processing, Cambridge, MA; London, U.K.: Bradford Book & MIT Press.","Yuval Marton and Philip Resnik. 2008. Soft Syntactic Constraints for Hierarchical Phrased-based Translation. In Proceedings of the 46st Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pp. 1003-1011.","Kathleen R. McKeown and Dragomir R. Radev. 2000. Collocations. In Robert Dale, Hermann Moisl, and Harold Somers (Ed.), A Handbook of Natural Language Processing, pp. 507-523.","Franz Josef Och. 2003. Minimum Error Rate Training in Statistical Machine Translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pp. 160-167.","Franz Josef Och and Hermann Ney. 2003. A Systematic Comparison of Various Statistical Alignment Models. Computational Linguistics, 29(1) : 19-51.","Kishore Papineni, Salim Roukos, Todd Ward, and Weij-ing Zhu. 2002. BLEU: A Method for Automatic Evaluation of Machine Translation. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pp. 311-318.","Andreas Stolcke. 2002. SRILM - An Extensible Language Modeling Toolkit. In Proceedings for the International Conference on Spoken Language Processing, pp. 901-904.","Christoph Tillmann. 2004. A Unigram Orientation Model for Statistical Machine Translation. In Proceedings of the Joint Conference on Human Language Technologies and the Annual Meeting of the North American Chapter of the Association of Computational Linguistics, pp. 101-104.","Christoph Tillmann and Tong Zhang. 2005. A Localized Prediction Model for Statistical Machine Translation. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pp. 557-564.","Karthik Visweswariah, Jiri Navratil, Jeffrey Sorensen, Vijil Chenthamarakshan, and Nanda Kambhatla. 2010. Syntax Based Reordering with Automatically Derived Rules for Improved Statistical Machine Translation. In Proceedings of the 23rd International Conference on Computational Linguistics, pp. 1119-1127.","Dekai Wu. 1997. Stochastic Inversion Transduction Grammars and Bilingual Parsing of Parallel Corpora. Computational Linguistics, 23(3):377-403.","Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maximum Entropy Based Phrase Reordering Model for Statistical Machine Translation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pp. 521-528.","Richard Zens and Herman Ney. 2003. A Comparative Study on Reordering Constraints in Statistical Machine Translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pp. 192-202.","Richard Zens and Herman Ney. 2006. Discriminative Reordering Models for Statistical Machine Translation. In Proceedings of the Workshop on Statistical Machine Translation, pp. 55-63.","Dongdong Zhang, Mu Li, Chi-Ho Li, and Ming Zhou. 2007. Phrase Reordering Model Integrating Syntactic Knowledge for SMT. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pp. 533-540.    1044"]}],"references":[{"authors":[{"first":"Yaser","last":"Al-Onaizan"},{"first":"Kishore","last":"Papineni"}],"year":"2006","title":"Distortion Models for Statistical Machine Translation","source":"Yaser Al-Onaizan and Kishore Papineni. 2006. Distortion Models for Statistical Machine Translation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pp. 529-536."},{"authors":[{"first":"Adam","middle":"L.","last":"Berger"},{"first":"Peter","middle":"F.","last":"Brown"},{"first":"Stephen","middle":"A. Della","last":"Pietra"},{"first":"Vincent","middle":"J. Della","last":"Pietra"},{"first":"Andrew","middle":"S.","last":"Kehler"},{"first":"Robert","middle":"L.","last":"Mercer"}],"year":"1996","title":"Language Translation Apparatus and Method of Using Context-Based Translation Models","source":"Adam L. Berger, Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, Andrew S. Kehler, and Robert L. Mercer. 1996. Language Translation Apparatus and Method of Using Context-Based Translation Models. United States Patent, Patent Number 5510981, April."},{"authors":[{"first":"Peter","middle":"F.","last":"Brown"},{"first":"Stephen","middle":"A. Della","last":"Pietra"},{"first":"Vincent","middle":"J. Della","last":"Pietra"},{"first":"Robert.","middle":"L.","last":"Mercer"}],"year":"1993","title":"The Mathematics of Statistical Machine Translation: Parameter estimation","source":"Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert. L. Mercer. 1993. The Mathematics of Statistical Machine Translation: Parameter estimation. Computational Linguistics, 19(2): 263-311."},{"authors":[{"first":"David","last":"Chiang"}],"year":"2005","title":"A Hierarchical Phrase-based Model for Statistical Machine Translation","source":"David Chiang. 2005. A Hierarchical Phrase-based Model for Statistical Machine Translation. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pp. 263-270."},{"authors":[{"first":"Niyu","last":"Ge"}],"year":"2010","title":"A Direct Syntax-Driven Reordering Model for Phrase-Based Machine Translation","source":"Niyu Ge. 2010. A Direct Syntax-Driven Reordering Model for Phrase-Based Machine Translation. In Proceedings of Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pp. 849-857."},{"authors":[{"first":"Philipp","last":"Koehn"}],"year":"2004","title":"Statistical Significance Tests for Machine Translation Evaluation","source":"Philipp Koehn. 2004. Statistical Significance Tests for Machine Translation Evaluation. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pp. 388-395."},{"authors":[{"first":"Philipp","last":"Koehn"},{"first":"Franz","middle":"Joseph","last":"Och"},{"first":"Daniel","last":"Marcu"}],"year":"2003","title":"Statistical Phrase-Based Translation","source":"Philipp Koehn, Franz Joseph Och, and Daniel Marcu. 2003. Statistical Phrase-Based Translation. In Proceedings of the Joint Conference on Human Language Technologies and the Annual Meeting of the North American Chapter of the Association of Computational Linguistics, pp. 127-133."},{"authors":[{"first":"Philipp","last":"Koehn"},{"first":"Hieu","last":"Hoang"},{"first":"Alexandra","last":"Birch"},{"first":"Chris","last":"Callison-Burch"},{"first":"Marcello","last":"Federico"},{"first":"Nicola","last":"Bertoldi"},{"first":"Brooke","last":"Cowan"},{"first":"Wade","last":"Shen"},{"first":"Christine","middle":"Moran Richard","last":"Zens"},{"first":"Chris","last":"Dyer"},{"first":"Ondrej","last":"Bojar"},{"first":"Alexandra","last":"Constantin"},{"first":"Evan","last":"Herbst"}],"year":"2007","title":"Moses: Open Source Toolkit for Statistical Machine Translation","source":"Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In Proceedings of the 45th Annual Meeting of the ACL, Poster and Demonstration Sessions, pp. 177-180."},{"authors":[{"first":"Philipp","last":"Koehn"},{"first":"Amittai","last":"Axelrod"},{"first":"Alexandra","middle":"Birch","last":"Mayne"},{"first":"Chris","last":"Callison-Burch"},{"first":"Miles","last":"Osborne"},{"first":"David","last":"Talbot"}],"year":"2005","title":"Edinburgh System Description for the 2005 IWSLT Speech Translation Evaluation","source":"Philipp Koehn, Amittai Axelrod, Alexandra Birch Mayne, Chris Callison-Burch, Miles Osborne, and David Talbot. 2005. Edinburgh System Description for the 2005 IWSLT Speech Translation Evaluation. In Proceedings of International Workshop on Spoken Language Translation. 1043"},{"authors":[{"first":"Dekang","last":"Lin"}],"year":"1998","title":"Extracting Collocations from Text Corpora","source":"Dekang Lin. 1998. Extracting Collocations from Text Corpora. In Proceedings of the 1st Workshop on Computational Terminology, pp. 57-63."},{"authors":[{"first":"Zhanyi","last":"Liu"},{"first":"Haifeng","last":"Wang"},{"first":"Hua","last":"Wu"},{"first":"Sheng","last":"Li"}],"year":"2009","title":"Collocation Extraction Using Monolingual Word Alignment Method","source":"Zhanyi Liu, Haifeng Wang, Hua Wu, and Sheng Li. 2009. Collocation Extraction Using Monolingual Word Alignment Method. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pp. 487-495."},{"authors":[{"first":"Christopher","middle":"D.","last":"Manning"},{"first":"Hinrich","last":"Schütze"}],"year":"1999","title":"Foundations of Statistical Natural Language Processing, Cambridge, MA; London, U","source":"Christopher D. Manning and Hinrich Schütze. 1999. Foundations of Statistical Natural Language Processing, Cambridge, MA; London, U.K.: Bradford Book & MIT Press."},{"authors":[{"first":"Yuval","last":"Marton"},{"first":"Philip","last":"Resnik"}],"year":"2008","title":"Soft Syntactic Constraints for Hierarchical Phrased-based Translation","source":"Yuval Marton and Philip Resnik. 2008. Soft Syntactic Constraints for Hierarchical Phrased-based Translation. In Proceedings of the 46st Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pp. 1003-1011."},{"authors":[{"first":"Kathleen","middle":"R.","last":"McKeown"},{"first":"Dragomir","middle":"R.","last":"Radev"}],"year":"2000","title":"Collocations","source":"Kathleen R. McKeown and Dragomir R. Radev. 2000. Collocations. In Robert Dale, Hermann Moisl, and Harold Somers (Ed.), A Handbook of Natural Language Processing, pp. 507-523."},{"authors":[{"first":"Franz","middle":"Josef","last":"Och"}],"year":"2003","title":"Minimum Error Rate Training in Statistical Machine Translation","source":"Franz Josef Och. 2003. Minimum Error Rate Training in Statistical Machine Translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pp. 160-167."},{"authors":[{"first":"Franz","middle":"Josef","last":"Och"},{"first":"Hermann","last":"Ney"}],"year":"2003","title":"A Systematic Comparison of Various Statistical Alignment Models","source":"Franz Josef Och and Hermann Ney. 2003. A Systematic Comparison of Various Statistical Alignment Models. Computational Linguistics, 29(1) : 19-51."},{"authors":[{"first":"Kishore","last":"Papineni"},{"first":"Salim","last":"Roukos"},{"first":"Todd","last":"Ward"},{"first":"Weij-ing","last":"Zhu"}],"year":"2002","title":"BLEU: A Method for Automatic Evaluation of Machine Translation","source":"Kishore Papineni, Salim Roukos, Todd Ward, and Weij-ing Zhu. 2002. BLEU: A Method for Automatic Evaluation of Machine Translation. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pp. 311-318."},{"authors":[{"first":"Andreas","last":"Stolcke"}],"year":"2002","title":"SRILM - An Extensible Language Modeling Toolkit","source":"Andreas Stolcke. 2002. SRILM - An Extensible Language Modeling Toolkit. In Proceedings for the International Conference on Spoken Language Processing, pp. 901-904."},{"authors":[{"first":"Christoph","last":"Tillmann"}],"year":"2004","title":"A Unigram Orientation Model for Statistical Machine Translation","source":"Christoph Tillmann. 2004. A Unigram Orientation Model for Statistical Machine Translation. In Proceedings of the Joint Conference on Human Language Technologies and the Annual Meeting of the North American Chapter of the Association of Computational Linguistics, pp. 101-104."},{"authors":[{"first":"Christoph","last":"Tillmann"},{"first":"Tong","last":"Zhang"}],"year":"2005","title":"A Localized Prediction Model for Statistical Machine Translation","source":"Christoph Tillmann and Tong Zhang. 2005. A Localized Prediction Model for Statistical Machine Translation. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pp. 557-564."},{"authors":[{"first":"Karthik","last":"Visweswariah"},{"first":"Jiri","last":"Navratil"},{"first":"Jeffrey","last":"Sorensen"},{"first":"Vijil","last":"Chenthamarakshan"},{"first":"Nanda","last":"Kambhatla"}],"year":"2010","title":"Syntax Based Reordering with Automatically Derived Rules for Improved Statistical Machine Translation","source":"Karthik Visweswariah, Jiri Navratil, Jeffrey Sorensen, Vijil Chenthamarakshan, and Nanda Kambhatla. 2010. Syntax Based Reordering with Automatically Derived Rules for Improved Statistical Machine Translation. In Proceedings of the 23rd International Conference on Computational Linguistics, pp. 1119-1127."},{"authors":[{"first":"Dekai","last":"Wu"}],"year":"1997","title":"Stochastic Inversion Transduction Grammars and Bilingual Parsing of Parallel Corpora","source":"Dekai Wu. 1997. Stochastic Inversion Transduction Grammars and Bilingual Parsing of Parallel Corpora. Computational Linguistics, 23(3):377-403."},{"authors":[{"first":"Deyi","last":"Xiong"},{"first":"Qun","last":"Liu"},{"first":"Shouxun","last":"Lin"}],"year":"2006","title":"Maximum Entropy Based Phrase Reordering Model for Statistical Machine Translation","source":"Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maximum Entropy Based Phrase Reordering Model for Statistical Machine Translation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pp. 521-528."},{"authors":[{"first":"Richard","last":"Zens"},{"first":"Herman","last":"Ney"}],"year":"2003","title":"A Comparative Study on Reordering Constraints in Statistical Machine Translation","source":"Richard Zens and Herman Ney. 2003. A Comparative Study on Reordering Constraints in Statistical Machine Translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pp. 192-202."},{"authors":[{"first":"Richard","last":"Zens"},{"first":"Herman","last":"Ney"}],"year":"2006","title":"Discriminative Reordering Models for Statistical Machine Translation","source":"Richard Zens and Herman Ney. 2006. Discriminative Reordering Models for Statistical Machine Translation. In Proceedings of the Workshop on Statistical Machine Translation, pp. 55-63."},{"authors":[{"first":"Dongdong","last":"Zhang"},{"first":"Mu","last":"Li"},{"first":"Chi-Ho","last":"Li"},{"first":"Ming","last":"Zhou"}],"year":"2007","title":"Phrase Reordering Model Integrating Syntactic Knowledge for SMT","source":"Dongdong Zhang, Mu Li, Chi-Ho Li, and Ming Zhou. 2007. Phrase Reordering Model Integrating Syntactic Knowledge for SMT. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pp. 533-540.    1044"}],"cites":[{"style":0,"text":"Brown et al., 1993","origin":{"pointer":"/sections/4/paragraphs/0","offset":52,"length":18},"authors":[{"last":"Brown"},{"last":"al."}],"year":"1993","references":["/references/2"]},{"style":0,"text":"Wu (1997)","origin":{"pointer":"/sections/4/paragraphs/0","offset":181,"length":9},"authors":[{"last":"Wu"}],"year":"1997","references":["/references/21"]},{"style":0,"text":"Chiang, 2005","origin":{"pointer":"/sections/4/paragraphs/0","offset":549,"length":12},"authors":[{"last":"Chiang"}],"year":"2005","references":["/references/3"]},{"style":0,"text":"Zhang et al., 2007","origin":{"pointer":"/sections/4/paragraphs/0","offset":593,"length":18},"authors":[{"last":"Zhang"},{"last":"al."}],"year":"2007","references":["/references/25"]},{"style":0,"text":"Marton and Resnik, 2008","origin":{"pointer":"/sections/4/paragraphs/0","offset":613,"length":23},"authors":[{"last":"Marton"},{"last":"Resnik"}],"year":"2008","references":["/references/12"]},{"style":0,"text":"Ge, 2010","origin":{"pointer":"/sections/4/paragraphs/0","offset":638,"length":8},"authors":[{"last":"Ge"}],"year":"2010","references":["/references/4"]},{"style":0,"text":"Visweswariah et al., 2010","origin":{"pointer":"/sections/4/paragraphs/0","offset":648,"length":25},"authors":[{"last":"Visweswariah"},{"last":"al."}],"year":"2010","references":["/references/20"]},{"style":0,"text":"Liu et al., 2009","origin":{"pointer":"/sections/4/paragraphs/1","offset":380,"length":16},"authors":[{"last":"Liu"},{"last":"al."}],"year":"2009","references":["/references/10"]},{"style":0,"text":"Mckeown and Radey, 2000","origin":{"pointer":"/sections/5/paragraphs/0","offset":235,"length":23},"authors":[{"last":"Mckeown"},{"last":"Radey"}],"year":"2000","references":[]},{"style":0,"text":"Xiong et al., 2006","origin":{"pointer":"/sections/5/paragraphs/1","offset":650,"length":18},"authors":[{"last":"Xiong"},{"last":"al."}],"year":"2006","references":["/references/22"]},{"style":0,"text":"Lin, 1998","origin":{"pointer":"/sections/5/paragraphs/2","offset":118,"length":9},"authors":[{"last":"Lin"}],"year":"1998","references":["/references/9"]},{"style":0,"text":"Liu et al. (2009)","origin":{"pointer":"/sections/5/paragraphs/2","offset":249,"length":17},"authors":[{"last":"Liu"},{"last":"al."}],"year":"2009","references":["/references/10"]},{"style":0,"text":"Liu et al. (2009)","origin":{"pointer":"/sections/5/paragraphs/2","offset":531,"length":17},"authors":[{"last":"Liu"},{"last":"al."}],"year":"2009","references":["/references/10"]},{"style":0,"text":"Chiang, 2005","origin":{"pointer":"/sections/17/paragraphs/0","offset":85,"length":12},"authors":[{"last":"Chiang"}],"year":"2005","references":["/references/3"]},{"style":0,"text":"Koehn et al., 2007","origin":{"pointer":"/sections/17/paragraphs/0","offset":99,"length":18},"authors":[{"last":"Koehn"},{"last":"al."}],"year":"2007","references":["/references/7"]},{"style":0,"text":"Koehn et al., 2007","origin":{"pointer":"/sections/20/paragraphs/0","offset":56,"length":18},"authors":[{"last":"Koehn"},{"last":"al."}],"year":"2007","references":["/references/7"]},{"style":0,"text":"Och and Ney, 2003","origin":{"pointer":"/sections/20/paragraphs/0","offset":106,"length":17},"authors":[{"last":"Och"},{"last":"Ney"}],"year":"2003","references":["/references/15"]},{"style":0,"text":"Stolcke, 2002","origin":{"pointer":"/sections/21/paragraphs/0","offset":137,"length":13},"authors":[{"last":"Stolcke"}],"year":"2002","references":["/references/17"]},{"style":0,"text":"Och, 2003","origin":{"pointer":"/sections/21/paragraphs/1","offset":192,"length":9},"authors":[{"last":"Och"}],"year":"2003","references":["/references/14"]},{"style":0,"text":"Papineni et al., 2002","origin":{"pointer":"/sections/21/paragraphs/2","offset":13,"length":21},"authors":[{"last":"Papineni"},{"last":"al."}],"year":"2002","references":["/references/16"]},{"style":0,"text":"Koehn, 2004","origin":{"pointer":"/sections/21/paragraphs/2","offset":209,"length":11},"authors":[{"last":"Koehn"}],"year":"2004","references":["/references/5"]},{"style":0,"text":"Al-Onaizan & Papineni, 2006","origin":{"pointer":"/sections/22/paragraphs/3","offset":80,"length":27},"authors":[{"last":"Al-Onaizan"},{"last":"Papineni"}],"year":"2006","references":["/references/0"]},{"style":0,"text":"Manning and Schütze, 1999","origin":{"pointer":"/sections/25/paragraphs/2","offset":24,"length":25},"authors":[{"last":"Manning"},{"last":"Schütze"}],"year":"1999","references":["/references/11"]},{"style":0,"text":"Liu et al., 2009","origin":{"pointer":"/sections/29/paragraphs/1","offset":387,"length":16},"authors":[{"last":"Liu"},{"last":"al."}],"year":"2009","references":["/references/10"]},{"style":0,"text":"Brown et al., 1993","origin":{"pointer":"/sections/30/paragraphs/0","offset":49,"length":18},"authors":[{"last":"Brown"},{"last":"al."}],"year":"1993","references":["/references/2"]},{"style":0,"text":"Berger et al. (1996)","origin":{"pointer":"/sections/30/paragraphs/0","offset":104,"length":20},"authors":[{"last":"Berger"},{"last":"al."}],"year":"1996","references":["/references/1"]},{"style":0,"text":"Wu, 1997","origin":{"pointer":"/sections/30/paragraphs/0","offset":486,"length":8},"authors":[{"last":"Wu"}],"year":"1997","references":["/references/21"]},{"style":0,"text":"Zens and Ney (2003)","origin":{"pointer":"/sections/30/paragraphs/0","offset":574,"length":19},"authors":[{"last":"Zens"},{"last":"Ney"}],"year":"2003","references":["/references/23"]},{"style":0,"text":"Koehn et al. (2003)","origin":{"pointer":"/sections/30/paragraphs/1","offset":29,"length":19},"authors":[{"last":"Koehn"},{"last":"al."}],"year":"2003","references":["/references/6"]},{"style":0,"text":"Tillmann, 2004","origin":{"pointer":"/sections/30/paragraphs/1","offset":213,"length":14},"authors":[{"last":"Tillmann"}],"year":"2004","references":["/references/18"]},{"style":0,"text":"Koehn et al., 2005","origin":{"pointer":"/sections/30/paragraphs/1","offset":229,"length":18},"authors":[{"last":"Koehn"},{"last":"al."}],"year":"2005","references":["/references/8"]},{"style":0,"text":"Tillmann and Zhang, 2005","origin":{"pointer":"/sections/30/paragraphs/1","offset":301,"length":24},"authors":[{"last":"Tillmann"},{"last":"Zhang"}],"year":"2005","references":["/references/19"]},{"style":0,"text":"Zens and Ney, 2006","origin":{"pointer":"/sections/30/paragraphs/1","offset":327,"length":18},"authors":[{"last":"Zens"},{"last":"Ney"}],"year":"2006","references":["/references/24"]},{"style":0,"text":"Chiang, 2005","origin":{"pointer":"/sections/30/paragraphs/1","offset":512,"length":12},"authors":[{"last":"Chiang"}],"year":"2005","references":["/references/3"]},{"style":0,"text":"Zhang et al., 2007","origin":{"pointer":"/sections/30/paragraphs/2","offset":108,"length":18},"authors":[{"last":"Zhang"},{"last":"al."}],"year":"2007","references":["/references/25"]},{"style":0,"text":"Marton and Resnik, 2008","origin":{"pointer":"/sections/30/paragraphs/2","offset":128,"length":23},"authors":[{"last":"Marton"},{"last":"Resnik"}],"year":"2008","references":["/references/12"]},{"style":0,"text":"Ge, 2010","origin":{"pointer":"/sections/30/paragraphs/2","offset":153,"length":8},"authors":[{"last":"Ge"}],"year":"2010","references":["/references/4"]},{"style":0,"text":"Visweswariah et al., 2010","origin":{"pointer":"/sections/30/paragraphs/2","offset":163,"length":25},"authors":[{"last":"Visweswariah"},{"last":"al."}],"year":"2010","references":["/references/20"]},{"style":0,"text":"Xiong et al., 2006","origin":{"pointer":"/sections/30/paragraphs/3","offset":114,"length":18},"authors":[{"last":"Xiong"},{"last":"al."}],"year":"2006","references":["/references/22"]}]}
