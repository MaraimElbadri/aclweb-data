{"sections":[{"title":"","paragraphs":["Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 102–111, Portland, Oregon, June 19-24, 2011. c⃝2011 Association for Computational Linguistics"]},{"title":"Joint Annotation of Search Queries Michael Bendersky Dept. of Computer Science University of Massachusetts Amherst, MA bemike@cs.umass.edu W. Bruce Croft Dept. of Computer Science University of Massachusetts Amherst, MA croft@cs.umass.edu David A. Smith Dept. of Computer Science University of Massachusetts Amherst, MA dasmith@cs.umass.edu Abstract","paragraphs":["Marking up search queries with linguistic annotations such as part-of-speech tags, capitalization, and segmentation, is an important part of query processing and understand-ing in information retrieval systems. Due to their brevity and idiosyncratic structure, search queries pose a challenge to existing NLP tools. To address this challenge, we propose a probabilistic approach for performing joint query annotation. First, we derive a robust set of unsupervised independent annotations, using queries and pseudo-relevance feedback. Then, we stack additional classifiers on the independent annotations, and exploit the dependencies between them to further improve the accuracy, even with a very limited amount of available training data. We evaluate our method using a range of queries extracted from a web search log. Experimental results verify the effectiveness of our approach for both short keyword queries, and verbose natural language queries."]},{"title":"1 Introduction","paragraphs":["Automatic mark-up of textual documents with linguistic annotations such as part-of-speech tags, sentence constituents, named entities, or semantic roles is a common practice in natural language processing (NLP). It is, however, much less common in information retrieval (IR) applications. Accordingly, in this paper, we focus on annotating search queries submitted by the users to a search engine.","There are several key differences between user queries and the documents used in NLP (e.g., news articles or web pages). As previous research shows, these differences severely limit the applicability of standard NLP techniques for annotating queries and require development of novel annotation approaches for query corpora (Bergsma and Wang, 2007; Barr et al., 2008; Lu et al., 2009; Bendersky et al., 2010; Li, 2010).","The most salient difference between queries and documents is their length. Most search queries are very short, and even longer queries are usually shorter than the average written sentence. Due to their brevity, queries often cannot be divided into sub-parts, and do not provide enough context for accurate annotations to be made using the standard NLP tools such as taggers, parsers or chunkers, which are trained on more syntactically coherent textual units.","A recent analysis of web query logs by Bendersky and Croft (2009) shows, however, that despite their brevity, queries are grammatically diverse. Some queries are keyword concatenations, some are semicomplete verbal phrases and some are wh-questions. It is essential for the search engine to correctly an-notate the query structure, and the quality of these query annotations has been shown to be a crucial first step towards the development of reliable and robust query processing, representation and understanding algorithms (Barr et al., 2008; Guo et al., 2008; Guo et al., 2009; Manshadi and Li, 2009; Li, 2010).","However, in current query annotation systems, even sentence-like queries are often hard to parse and annotate, as they are prone to contain misspellings and idiosyncratic grammatical structures. 102","(a) (b) (c) Term CAP TAG SEG who L X B won L V I the L X B 2004 L X B kentucky C N B derby C N I Term CAP TAG SEG kindred C N B where C X B would C X I i C X I be C V I Term CAP TAG SEG shih C N B tzu C N I health L N B problems L N I Figure 1: Examples of a mark-up scheme for annotating capitalization (L – lowercase, C – otherwise), POS tags (N – noun, V – verb, X – otherwise) and segmentation (B/I – beginning of/inside the chunk). They also tend to lack prepositions, proper punctuation, or capitalization, since users (often correctly) assume that these features are disregarded by the retrieval system.","In this paper, we propose a novel joint query annotation method to improve the effectiveness of existing query annotations, especially for longer, more complex search queries. Most existing research focuses on using a single type of annotation for information retrieval such as subject-verb-object dependencies (Balasubramanian and Allan, 2009), named-entity recognition (Guo et al., 2009), phrase chunk-ing (Guo et al., 2008), or semantic labeling (Li, 2010).","In contrast, the main focus of this work is on developing a unified approach for performing reliable annotations of different types. To this end, we propose a probabilistic method for performing a joint query annotation. This method allows us to exploit the dependency between different unsupervised annotations to further improve the accuracy of the entire set of annotations. For instance, our method can leverage the information about estimated parts-of-speech tags and capitalization of query terms to improve the accuracy of query segmentation.","We empirically evaluate the joint query annotation method on a range of query types. Instead of just focusing our attention on keyword queries, as is often done in previous work (Barr et al., 2008; Bergsma and Wang, 2007; Tan and Peng, 2008; Guo et al., 2008), we also explore the performance of our annotations with more complex natural language search queries such as verbal phrases and wh-questions, which often pose a challenge for IR applications (Bendersky et al., 2010; Kumaran and Allan, 2007; Kumaran and Carvalho, 2009; Lease, 2007). We show that even with a very limited amount of training data, our joint annotation method significantly outperforms annotations that were done in-dependently for these queries.","The rest of the paper is organized as follows. In Section 2 we demonstrate several examples of annotated search queries. Then, in Section 3, we in-troduce our joint query annotation method. In Section 4 we describe two types of independent query annotations that are used as input for the joint query annotation. Section 5 details the related work and Section 6 presents the experimental results. We draw the conclusions from our work in Section 7."]},{"title":"2 Query Annotation Example","paragraphs":["To demonstrate a possible implementation of linguistic annotation for search queries, Figure 1 presents a simple mark-up scheme, exemplified using three web search queries (as they appear in a search log): (a) who won the 2004 kentucky derby, (b) kindred where would i be, and (c) shih tzu health problems. In this scheme, each query is marked-up using three annotations: capitalization, POS tags, and segmentation indicators.","Note that all the query terms are non-capitalized, and no punctuation is provided by the user, which complicates the query annotation process. While the simple annotation described in Figure 1 can be done with a very high accuracy for standard document corpora, both previous work (Barr et al., 2008; Bergsma and Wang, 2007; Jones and Fain, 2003) and the experimental results in this paper indicate that it is challenging to perform well on queries.","The queries in Figure 1 illustrate this point. Query (a) in Figure 1 is a wh-question, and it contains 103 a capitalized concept (“Kentucky Derby”), a single verb, and four segments. Query (b) is a combination of an artist name and a song title and should be interpreted as Kindred — “Where Would I Be”. Query (c) is a concatenation of two short noun phrases: “Shih Tzu” and “health problems”."]},{"title":"3 Joint Query Annotation","paragraphs":["Given a search query Q, which consists of a sequence of terms (q1, . . . , qn), our goal is to annotate it with an appropriate set of linguistic structures ZQ. In this work, we assume that the set ZQ consists of shallow sequence annotations zQ, each of which takes the form zQ = (ζ1, . . . , ζn). In other words, each symbol ζi ∈ zQ annotates a single query term.","Many query annotations that are useful for IR can be represented using this simple form, including capitalization, POS tagging, phrase chunking, named entity recognition, and stopword indicators, to name just a few. For instance, Figure 1 demonstrates an example of a set of annotations ZQ. In this example, ZQ = {CAP, TAG, SEG}.","Most previous work on query annotation makes the independence assumption — every annotation zQ ∈ ZQ is done separately from the others. That is, it is assumed that the optimal linguistic annotation z∗(I) Q is the annotation that has the highest probability given the query Q, regardless of the other annotations in the set ZQ. Formally, z∗(I) Q = argmax","zQ p(zQ|Q) (1)","The main shortcoming of this approach is in the assumption that the linguistic annotations in the set ZQ are independent. In practice, there are dependencies between the different annotations, and they can be leveraged to derive a better estimate of the entire set of annotations.","For instance, imagine that we need to perform two annotations: capitalization and POS tagging. Know-ing that a query term is capitalized, we are more likely to decide that it is a proper noun. Vice versa, knowing that it is a preposition will reduce its probability of being capitalized. We would like to capture this intuition in the annotation process.","To address the problem of joint query annotation, we first assume that we have an initial set of annotations Z","∗(I)","Q , which were performed for query Q in-dependently of one another (we will show an example of how to derive such a set in Section 4). Given the initial set Z ∗(I) Q , we are interested in obtaining an annotation set Z ∗(J) Q , which jointly optimizes the","probability of all the annotations, i.e. Z ∗(J) Q = argmax","ZQ p(ZQ|Z ∗(I) Q ).","If the initial set of estimations is reasonably accurate, we can make the assumption that the annotations in the set Z ∗(J) Q are independent given the initial estimates Z ∗(I) Q , allowing us to separately op-","timize the probability of each annotation z∗(J) Q ∈ Z ∗(J) Q : z∗(J) Q = argmax","zQ p(zQ|Z ∗(I) Q ). (2)","From Eq. 2, it is evident that the joint annotation task becomes that of finding some optimal unobserved sequence (annotation z∗(J)","Q ), given the observed sequences (independent annotation set Z ∗(I) Q ). Accordingly, we can directly use a supervised se-","quential probabilistic model such as CRF (Lafferty","et al., 2001) to find the optimal z∗(J)","Q . In this CRF model, the optimal annotation z∗(J)","Q is the label we are trying to predict, and the set of independent annotations Z","∗(I)","Q is used as the basis for the features used for prediction. Figure 2 outlines the algorithm for performing the joint query annotation.","As input, the algorithm receives a training set of queries and their ground truth annotations. It then produces a set of independent annotation estimates, which are jointly used, together with the ground truth annotations, to learn a CRF model for each annotation type. Finally, these CRF models are used to predict annotations on a held-out set of queries, which are the output of the algorithm. 104","Input: Qt — training set of queries.","ZQt — ground truth annotations for the training set of queries.","Qh — held-out set of queries.","(1) Obtain a set of independent annotation estimates Z ∗(I) Qt (2) Initialize Z ∗(J) Qt ← ∅","(3) for each z∗(I) Qt ∈ Z ∗(I) Qt :","(4) Z′ Qt ← Z ∗(I) Qt \\ z∗(I)","Qt","(5) Train a CRF model CRF(zQt ) using zQt as a label and Z′","Qt as features.","(6) Predict annotation z∗(J) Qh , using CRF(zQt ). (7) Z ∗(J) Qh ← Z ∗(J) Qh ∪ z∗(J)","Qh . Output: Z ∗(J) Qh — predicted annotations for the held-out set of queries. Figure 2: Algorithm for performing joint query annotation.","Note that this formulation of joint query annotation can be viewed as a stacked classification, in which a second, more effective, classifier is trained using the labels inferred by the first classifier as features. Stacked classifiers were recently shown to be an efficient and effective strategy for structured classification in NLP (Nivre and McDonald, 2008; Martins et al., 2008)."]},{"title":"4 Independent Query Annotations","paragraphs":["While the joint annotation method proposed in Section 3 is general enough to be applied to any set of independent query annotations, in this work we focus on two previously proposed independent annotation methods based on either the query itself, or the top sentences retrieved in response to the query (Bendersky et al., 2010). The main benefits of these two annotation methods are that they can be easily implemented using standard software tools, do not require any labeled data, and provide reasonable annotation accuracy. Next, we briefly describe these two independent annotation methods. 4.1 Query-based estimation The most straightforward way to estimate the conditional probabilities in Eq. 1 is using the query it-self. To make the estimation feasible, Bendersky et al. (2010) take a bag-of-words approach, and assume independence between both the query terms and the corresponding annotation symbols. Thus, the indepentent annotations in Eq. 1 are given by z∗(QRY ) Q = argmax","(ζ1,...,ζn) ∏ i∈(1,...,n) p(ζi|qi). (3)","Following Bendersky et al. (2010) we use a large n-gram corpus (Brants and Franz, 2006) to estimate p(ζi|qi) for annotating the query with capitalization and segmentation mark-up, and a standard POS tagger1","for part-of-speech tagging of the query. 4.2 PRF-based estimation Given a short, often ungrammatical query, it is hard to accurately estimate the conditional probability in Eq. 1 using the query terms alone. For instance, a keyword query hawaiian falls, which refers to a location, is inaccurately interpreted by a standard POS tagger as a noun-verb pair. On the other hand, given a sentence from a corpus that is relevant to the query such as “Hawaiian Falls is a family-friendly waterpark”, the word “falls” is correctly identified by a standard POS tagger as a proper noun.","Accordingly, the document corpus can be bootstrapped in order to better estimate the query annotation. To this end, Bendersky et al. (2010) employ the pseudo-relevance feedback (PRF) — a method that has a long record of success in IR for tasks such as query expansion (Buckley, 1995; Lavrenko and Croft, 2001).","In the most general form, given the set of all retrievable sentences r in the corpus C one can derive p(zQ|Q) = ∑ r∈C p(zQ|r)p(r|Q). Since for most sentences the conditional proba-","bility of relevance to the query p(r|Q) is vanish-","ingly small, the above can be closely approximated 1 http://crftagger.sourceforge.net/ 105 by considering only a set of sentences R, retrieved at top-k positions in response to the query Q. This yields p(zQ|Q) ≈ ∑ r∈R p(zQ|r)p(r|Q). Intuitively, the equation above models the query as a mixture of top-k retrieved sentences, where each sentence is weighted by its relevance to the query. Furthermore, to make the estimation of the conditional probability p(zQ|r) feasible, it is assumed that the symbols ζi in the annotation sequence are independent, given a sentence r. Note that this assumption differs from the independence assumption in Eq. 3, since here the annotation symbols are not independent given the query Q.","Accordingly, the PRF-based estimate for independent annotations in Eq. 1 is z∗(P RF ) Q = argmax","(ζ1,...,ζn) ∑ r∈R ∏ i∈(1,...,n) p(ζi|r)p(r|Q).","(4)","Following Bendersky et al. (2010), an estimate of p(ζi|r) is a smoothed estimator that combines the information from the retrieved sentence r with the information about unigrams (for capitalization and POS tagging) and bigrams (for segmentation) from a large n-gram corpus (Brants and Franz, 2006)."]},{"title":"5 Related Work","paragraphs":["In recent years, linguistic annotation of search queries has been receiving increasing attention as an important step toward better query processing and understanding. The literature on query annotation includes query segmentation (Bergsma and Wang, 2007; Jones et al., 2006; Guo et al., 2008; Hagen et al., 2010; Hagen et al., 2011; Tan and Peng, 2008), part-of-speech and semantic tagging (Barr et al., 2008; Manshadi and Li, 2009; Li, 2010), named-entity recognition (Guo et al., 2009; Lu et al., 2009; Shen et al., 2008; Pasça, 2007), abbreviation disambiguation (Wei et al., 2008) and stopword detection (Lo et al., 2005; Jones and Fain, 2003).","Most of the previous work on query annotation focuses on performing a particular annotation task (e.g., segmentation or POS tagging) in isolation. However, these annotations are often related, and thus we take a joint annotation approach, which combines several independent annotations to improve the overall annotation accuracy. A similar approach was recently proposed by Guo et al. (2008). There are several key differences, however, between the work presented here and their work.","First, Guo et al. (2008) focus on query refinement (spelling corrections, word splitting, etc.) of short keyword queries. Instead, we are interested in annotation of queries of different types, including verbose natural language queries. While there is an overlap between query refinement and annotation, the focus of the latter is on providing linguistic information about existing queries (after initial refinement has been performed). Such information is especially important for more verbose and gramatically complex queries. In addition, while all the methods proposed by Guo et al. (2008) require large amounts of training data (thousands of training examples), our joint annotation method can be effectively trained with a minimal human labeling effort (several hundred training examples).","An additional research area which is relevant to this paper is the work on joint structure modeling (Finkel and Manning, 2009; Toutanova et al., 2008) and stacked classification (Nivre and Mc-Donald, 2008; Martins et al., 2008) in natural language processing. These approaches have been shown to be successful for tasks such as parsing and named entity recognition in newswire data (Finkel and Manning, 2009) or semantic role labeling in the Penn Treebank and Brown corpus (Toutanova et al., 2008). Similarly to this work in NLP, we demonstrate that a joint approach for modeling the linguistic query structure can also be beneficial for IR applications."]},{"title":"6 Experiments 6.1 Experimental Setup","paragraphs":["For evaluating the performance of our query annotation methods, we use a random sample of 250 queries2","from a search log. This sample is manually labeled with three annotations: capitalization, POS tags, and segmentation, according to the description of these annotations in Figure 1. In this set of 250 queries, there are 93 questions, 96 phrases contain-2 The annotations are available at","http://ciir.cs.umass.edu/∼","bemike/data.html 106 CAP","F1 (% impr) MQA (% impr)","i-QRY 0.641 (-/-) 0.779 (-/-)","i-PRF 0.711∗","(+10.9/-) 0.811∗","(+4.1/-)","j-QRY 0.620†(-3.3/-12.8) 0.805∗","(+3.3/-0.7)","j-PRF 0.718∗","(+12.0/+0.9) 0.840∗","†(+7.8/+3.6) TAG","Acc. (% impr) MQA (% impr)","i-QRY 0.893 (-/-) 0.878 (-/-)","i-PRF 0.916∗","(+2.6/-) 0.914∗","(+4.1/-)","j-QRY 0.913∗","(+2.2/-0.3) 0.912∗","(+3.9/-0.2)","j-PRF 0.924∗","(+3.5/+0.9) 0.922∗","(+5.0/+0.9)","SEG","F1 (% impr) MQA (% impr)","i-QRY 0.694 (-/-) 0.672 (-/-)","i-PRF 0.753∗","(+8.5/-) 0.710∗","(+5.7/-)","j-QRY 0.817∗","†(+17.7/+8.5) 0.803∗","†(+19.5/+13.1)","j-PRF 0.819∗ †(+18.0/+8.8) 0.803∗","†(+19.5/+13.1) Table 1: Summary of query annotation performance for capitalization (CAP), POS tagging (TAG) and segmentation. Numbers in parentheses indicate % of improvement over the i-QRY and i-PRF baselines, respectively. Best result per measure and annotation is boldfaced. ∗","and","† denote statistically significant differences with i-QRY and i-PRF, respectively. ing a verb, and 61 short keyword queries (Figure 1 contains a single example of each of these types).","In order to test the effectiveness of the joint query annotation, we compare four methods. In the first two methods, i-QRY and i-PRF the three annotations are done independently. Method i-QRY is based on z∗(QRY ) Q estimator (Eq. 3). Method i-PRF is based on the z∗(P RF )","Q estimator (Eq. 4).","The next two methods, j-QRY and j-PRF, are joint annotation methods, which perform a joint optimization over the entire set of annotations, as described in the algorithm in Figure 2. j-QRY and j-PRF differ in their choice of the initial independent annotation set Z","∗(I)","Q in line (1) of the algorithm (see Figure 2). j-QRY uses only the annotations performed by i-QRY (3 initial independent annotation estimates), while j-PRF combines the annotations performed by i-QRY with the annotations performed by i-PRF (6 initial annotation estimates). The CRF model training in line (6) of the algorithm is implemented using CRF++ toolkit3",". 3 http://crfpp.sourceforge.net/","The performance of the joint annotation methods is estimated using a 10-fold cross-validation. In order to test the statistical significance of improvements attained by the proposed methods we use a two-sided Fisher’s randomization test with 20,000 permutations. Results with p-value < 0.05 are considered statistically significant.","For reporting the performance of our methods we use two measures. The first measure is classification-oriented — treating the annotation decision for each query term as a classification. In case of capitalization and segmentation annotations these decisions are binary and we compute the precision and recall metrics, and report F1 — their harmonic mean. In case of POS tagging, the decisions are ternary, and hence we report the classification accuracy.","We also report an additional, IR-oriented performance measure. As is typical in IR, we propose measuring the performance of the annotation methods on a per-query basis, to verify that the methods have uniform impact across queries. Accordingly, we report the mean of classification accuracies per query (MQA). Formally, MQA is computed as","∑N i=1 accQi","N , where accQi is the classification accuracy for query Qi, and N is the number of queries.","The empirical evaluation is conducted as follows. In Section 6.2, we discuss the general performance of the four annotation techniques, and compare the effectiveness of independent and joint annotations. In Section 6.3, we analyze the performance of the independent and joint annotation methods by query type. In Section 6.4, we compare the difficulty of performing query annotations for different query types. Finally, in Section 6.5, we compare the effectiveness of the proposed joint annotation for query segmentation with the existing query segmentation methods. 6.2 General Evaluation Table 1 shows the summary of the performance of the two independent and two joint annotation methods for the entire set of 250 queries. For independent methods, we see that i-PRF outperforms i-QRY for 107","CAP Verbal Phrases Questions Keywords","F1 MQA F1 MQA F1 MQA","i-PRF 0.750 0.862 0.590 0.839 0.784 0.687","j-PRF 0.687∗ (-8.4%) 0.839∗","(-2.7%) 0.671∗","(+13.7%) 0.913∗","(+8.8%) 0.814 (+3.8%) 0.732∗","(+6.6%)","TAG Verbal Phrases Questions Keywords","Acc. MQA Acc. MQA Acc. MQA","i-PRF 0.908 0.908 0.932 0.935 0.880 0.890","j-PRF 0.904 (-0.4%) 0.906 (-0.2%) 0.951∗","(+2.1%) 0.953∗","(+1.9%) 0.893 (+1.5%) 0.900 (+1.1%)","SEG Verbal Phrases Questions Keywords","F1 MQA F1 MQA F1 MQA","i-PRF 0.751 0.700 0.740 0.700 0.816 0.747","j-PRF 0.772 (+2.8%) 0.742∗","(+6.0%) 0.858∗","(+15.9%) 0.838∗","(+19.7%) 0.844 (+3.4%) 0.853∗","(+14.2%) Table 2: Detailed analysis of the query annotation performance for capitalization (CAP), POS tagging (TAG) and segmentation by query type. Numbers in parentheses indicate % of improvement over the i-PRF baseline. Best result per measure and annotation is boldfaced. ∗","denotes statistically significant differences with i-PRF. all annotation types, using both performance measures.","In Table 1, we can also observe that the joint annotation methods are, in all cases, better than the corresponding independent ones. The highest improvements are attained by j-PRF, which always demonstrates the best performance both in terms of F1 and MQA. These results attest to both the importance of doing a joint optimization over the entire set of annotations and to the robustness of the initial annotations done by the i-PRF method. In all but one case, the j-PRF method, which uses these annotations as features, outperforms the j-QRY method that only uses the annotation done by i-QRY.","The most significant improvements as a result of joint annotation are observed for the segmentation task. In this task, joint annotation achieves close to 20% improvement in MQA over the i-QRY method, and more than 10% improvement in MQA over the i-PRF method. These improvements indicate that the segmentation decisions are strongly guided by capitalization and POS tagging. We also note that, in case of segmentation, the differences in performance between the two joint annotation methods, j-QRY and j-PRF, are not significant, indicating that the context of additional annotations in j-QRY makes up for the lack of more robust pseudo-relevance feedback based features.","We also note that the lowest performance improvement as a result of joint annotation is evidenced for POS tagging. The improvements of joint annotation method j-PRF over the i-PRF method are less than 1%, and are not statistically significant. This is not surprising, since the standard POS taggers often already use bigrams and capitalization at training time, and do not acquire much additional information from other annotations. 6.3 Evaluation by Query Type Table 2 presents a detailed analysis of the performance of the best independent (i-PRF) and joint (j-PRF) annotation methods by the three query types used for evaluation: verbal phrases, questions and keyword queries. From the analysis in Table 2, we note that the contribution of joint annotation varies significantly across query types. For instance, using j-PRF always leads to statistically significant improvements over the i-PRF baseline for questions. On the other hand, it is either statistically indistinguishable, or even significantly worse (in the case of capitalization) than the i-PRF baseline for the verbal phrases.","Table 2 also demonstrates that joint annotation has a different impact on various annotations for the same query type. For instance, j-PRF has a significant positive effect on capitalization and segmentation for keyword queries, but only marginally improves the POS tagging. Similarly, for the verbal phrases, j-PRF has a significant positive effect only for the segmentation annotation.","These variances in the performance of the j-PRF method point to the differences in the structure be-108 Annotation Performance by Query Type F1 Verbal Phrases Questions Keyword Queries 60 65 70 75 80 85 90 95 100 CAP SEG TAG Figure 3: Comparative performance (in terms of F1 for capitalization and segmentation and accuracy for POS tagging) of the j-PRF method on the three query types. tween the query types. While dependence between the annotations plays an important role for question and keyword queries, which often share a common grammatical structure, this dependence is less useful for verbal phrases, which have a more diverse linguistic structure. Accordingly, a more in-depth investigation of the linguistic structure of the verbal phrase queries is an interesting direction for future work. 6.4 Annotation Difficulty Recall that in our experiments, out of the overall 250 annotated queries, there are 96 verbal phrases, 93 questions and 61 keyword queries. Figure 3 shows a plot that contrasts the relative performance for these three query types of our best-performing joint annotation method, j-PRF, on capitalization, POS tagging and segmentation annotation tasks. Next, we analyze the performance profiles for the annotation tasks shown in Figure 3.","For the capitalization task, the performance of j-PRF on verbal phrases and questions is similar, with the difference below 3%. The performance for keyword queries is much higher — with improvement over 20% compared to either of the other two types. We attribute this increase to both a larger number of positive examples in the short keyword queries (a higher percentage of terms in keyword queries is capitalized) and their simpler syntactic structure (ad-SEG F1 MQA SEG-1 0.768 0.754 SEG-2 0.824∗","0.787∗ j-PRF 0.819∗","(+6.7%/-0.6%) 0.803∗","(+6.5%/+2.1%) Table 3: Comparison of the segmentation performance of the j-PRF method to two state-of-the-art segmentation methods. Numbers in parentheses indicate % of improvement over the SEG-1 and SEG-2 baselines respectively. Best result per measure and annotation is boldfaced. ∗ denotes statistically significant differences with SEG-1. jacent terms in these queries are likely to have the same case).","For the segmentation task, the performance is at its best for the question and keyword queries, and at its worst (with a drop of 11%) for the verbal phrases. We hypothesize that this is due to the fact that question queries and keyword queries tend to have repetitive structures, while the grammatical structure for verbose queries is much more diverse.","For the tagging task, the performance profile is reversed, compared to the other two tasks — the performance is at its worst for keyword queries, since their grammatical structure significantly differs from the grammatical structure of sentences in news articles, on which the POS tagger is trained. For question queries the performance is the best (6% increase over the keyword queries), since they resemble sentences encountered in traditional corpora.","It is important to note that the results reported in Figure 3 are based on training the joint annotation model on all available queries with 10-fold crossvalidation. We might get different profiles if a separate annotation model was trained for each query type. In our case, however, the number of queries from each type is not sufficient to train a reliable model. We leave the investigation of separate training of joint annotation models by query type to future work. 6.5 Additional Comparisons In order to further evaluate the proposed joint annotation method, j-PRF, in this section we compare its performance to other query annotation methods previously reported in the literature. Unfortunately, there is not much published work on query capitalization and query POS tagging that goes beyond the simple query-based methods described in Sec-109 tion 4.1. The published work on the more advanced methods usually requires access to large amounts of proprietary user data such as query logs and clicks (Barr et al., 2008; Guo et al., 2008; Guo et al., 2009).","Therefore, in this section we focus on recent work on query segmentation (Bergsma and Wang, 2007; Hagen et al., 2010). We compare the segmentation effectiveness of our best performing method, j-PRF, to that of these query segmentation methods.","The first method, SEG-1, was first proposed by Hagen et al. (2010). It is currently the most effective publicly disclosed unsupervised query segmentation method. SEG-1 method requires an access to a large web n-gram corpus (Brants and Franz, 2006). The optimal segmentation for query Q, S∗","Q, is then obtained using S∗ Q = argmax","S∈SQ ∑ s∈S,|s|>1","|s||s| count(s), where SQ is the set of all possible query segmentations, S is a possible segmentation, s is a segment in S, and count(s) is the frequency of s in the web n-gram corpus.","The second method, SEG-2, is based on a successful supervised segmentation method, which was first proposed by Bergsma and Wang (2007). SEG-2 employs a large set of features, and is pre-trained on the query collection described by Bergsma and Wang (2007). The features used by the SEG-2 method are described by Bendersky et al. (2009), and include, among others, n-gram frequencies in a sample of a query log, web corpus and Wikipedia titles.","Table 3 demonstrates the comparison between the j-PRF, SEG-1 and SEG-2 methods. When compared to the SEG-1 baseline, j-PRF is significantly more effective, even though it only employs bigram counts (see Eq. 4), instead of the high-order n-grams used by SEG-1, for computing the score of a segmentation. This results underscores the benefit of joint annotation, which leverages capitalization and POS tagging to improve the quality of the segmentation.","When compared to the SEG-2 baseline, j-PRF and SEG-2 are statistically indistinguishable. SEG-2 posits a slightly better F1, while j-PRF has a better MQA. This result demonstrates that the segmentation produced by the j-PRF method is as effective as the segmentation produced by the current supervised state-of-the-art segmentation methods, which employ external data sources and high-order n-grams. The benefit of the j-PRF method compared to the SEG-2 method, is that, simultaneously with the segmentation, it produces several additional query annotations (in this case, capitalization and POS tagging), eliminating the need to construct separate sequence classifiers for each annotation."]},{"title":"7 Conclusions","paragraphs":["In this paper, we have investigated a joint approach for annotating search queries with linguistic structures, including capitalization, POS tags and segmentation. To this end, we proposed a probabilistic approach for performing joint query annotation that takes into account the dependencies that exist between the different annotation types.","Our experimental findings over a range of queries from a web search log unequivocally point to the superiority of the joint annotation methods over both query-based and pseudo-relevance feedback based independent annotation methods. These findings indicate that the different annotations are mutuallydependent.","We are encouraged by the success of our joint query annotation technique, and intend to pursue the investigation of its utility for IR applications. In the future, we intend to research the use of joint query annotations for additional IR tasks, e.g., for constructing better query formulations for ranking algorithms."]},{"title":"8 Acknowledgment","paragraphs":["This work was supported in part by the Center for In-telligent Information Retrieval and in part by ARRA NSF IIS-9014442. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor. 110"]},{"title":"References","paragraphs":["Niranjan Balasubramanian and James Allan. 2009. Syntactic query models for restatement retrieval. In Proc. of SPIRE, pages 143–155.","Cory Barr, Rosie Jones, and Moira Regelson. 2008. The linguistic structure of english web-search queries. In Proc. of EMNLP, pages 1021–1030.","Michael Bendersky and W. Bruce Croft. 2009. Analysis of long queries in a large scale search log. In Proc. of Workshop on Web Search Click Data, pages 8–14.","Michael Bendersky, David Smith, and W. Bruce Croft. 2009. Two-stage query segmentation for information retrieval. In Proc. of SIGIR, pages 810–811.","Michael Bendersky, W. Bruce Croft, and David A. Smith. 2010. Structural annotation of search queries using pseudo-relevance feedback. In Proc. of CIKM, pages 1537–1540.","Shane Bergsma and Qin I. Wang. 2007. Learning noun phrase query segmentation. In Proc. of EMNLP, pages 819–826.","Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram Version 1.","Chris Buckley. 1995. Automatic query expansion using SMART. In Proc. of TREC-3, pages 69–80.","Jenny R. Finkel and Christopher D. Manning. 2009. Joint parsing and named entity recognition. In Proc. of NAACL, pages 326–334.","Jiafeng Guo, Gu Xu, Hang Li, and Xueqi Cheng. 2008. A unified and discriminative model for query refinement. In Proc. of SIGIR, pages 379–386.","Jiafeng Guo, Gu Xu, Xueqi Cheng, and Hang Li. 2009. Named entity recognition in query. In Proc. of SIGIR, pages 267–274.","Matthias Hagen, Martin Potthast, Benno Stein, and Christof Braeutigam. 2010. The power of naive query segmentation. In Proc. of SIGIR, pages 797–798.","Matthias Hagen, Martin Potthast, Benno Stein, and Christof Bräutigam. 2011. Query segmentation revisited. In Proc. of WWW, pages 97–106.","Rosie Jones and Daniel C. Fain. 2003. Query word dele-tion prediction. In Proc. of SIGIR, pages 435–436.","Rosie Jones, Benjamin Rey, Omid Madani, and Wiley Greiner. 2006. Generating query substitutions. In Proc. of WWW, pages 387–396.","Giridhar Kumaran and James Allan. 2007. A case for shorter queries, and helping user create them. In Proc. of NAACL, pages 220–227.","Giridhar Kumaran and Vitor R. Carvalho. 2009. Reducing long queries using query quality predictors. In Proc. of SIGIR, pages 564–571.","John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random fields: Probabilis-tic models for segmenting and labeling sequence data. In Proc. of ICML, pages 282–289.","Victor Lavrenko and W. Bruce Croft. 2001. Relevance based language models. In Proc. of SIGIR, pages 120– 127.","Matthew Lease. 2007. Natural language processing for information retrieval: the time is ripe (again). In Proceedings of PIKM.","Xiao Li. 2010. Understanding the semantic structure of noun phrase queries. In Proc. of ACL, pages 1337– 1345, Morristown, NJ, USA.","Rachel T. Lo, Ben He, and Iadh Ounis. 2005. Automatically building a stopword list for an information retrieval system. In Proc. of DIR.","Yumao Lu, Fuchun Peng, Gilad Mishne, Xing Wei, and Benoit Dumoulin. 2009. Improving Web search relevance with semantic features. In Proc. of EMNLP, pages 648–657.","Mehdi Manshadi and Xiao Li. 2009. Semantic Tagging of Web Search Queries. In Proc. of ACL, pages 861– 869.","André F. T. Martins, Dipanjan Das, Noah A. Smith, and Eric P. Xing. 2008. Stacking dependency parsers. In Proc. of EMNLP, pages 157–166.","Joakim Nivre and Ryan McDonald. 2008. Integrating graph-based and transition-based dependency parsers. In Proc. of ACL, pages 950–958.","Marius Pasça. 2007. Weakly-supervised discovery of named entities using web search queries. In Proc. of CIKM, pages 683–690.","Dou Shen, Toby Walkery, Zijian Zhengy, Qiang Yangz, and Ying Li. 2008. Personal name classification in web queries. In Proc. of WSDM, pages 149–158.","Bin Tan and Fuchun Peng. 2008. Unsupervised query segmentation using generative language models and Wikipedia. In Proc. of WWW, pages 347–356.","Kristina Toutanova, Aria Haghighi, and Christopher D. Manning. 2008. A global joint model for semantic role labeling. Computational Linguistics, 34:161–191, June.","Xing Wei, Fuchun Peng, and Benoit Dumoulin. 2008. Analyzing web text association to disambiguate abbreviation in queries. In Proc. of SIGIR, pages 751–752. 111"]}],"references":[{"authors":[{"first":"Niranjan","last":"Balasubramanian"},{"first":"James","last":"Allan"}],"year":"2009","title":"Syntactic query models for restatement retrieval","source":"Niranjan Balasubramanian and James Allan. 2009. Syntactic query models for restatement retrieval. In Proc. of SPIRE, pages 143–155."},{"authors":[{"first":"Cory","last":"Barr"},{"first":"Rosie","last":"Jones"},{"first":"Moira","last":"Regelson"}],"year":"2008","title":"The linguistic structure of english web-search queries","source":"Cory Barr, Rosie Jones, and Moira Regelson. 2008. The linguistic structure of english web-search queries. In Proc. of EMNLP, pages 1021–1030."},{"authors":[{"first":"Michael","last":"Bendersky"},{"first":"W.","middle":"Bruce","last":"Croft"}],"year":"2009","title":"Analysis of long queries in a large scale search log","source":"Michael Bendersky and W. Bruce Croft. 2009. Analysis of long queries in a large scale search log. In Proc. of Workshop on Web Search Click Data, pages 8–14."},{"authors":[{"first":"Michael","last":"Bendersky"},{"first":"David","last":"Smith"},{"first":"W.","middle":"Bruce","last":"Croft"}],"year":"2009","title":"Two-stage query segmentation for information retrieval","source":"Michael Bendersky, David Smith, and W. Bruce Croft. 2009. Two-stage query segmentation for information retrieval. In Proc. of SIGIR, pages 810–811."},{"authors":[{"first":"Michael","last":"Bendersky"},{"first":"W.","middle":"Bruce","last":"Croft"},{"first":"David","middle":"A.","last":"Smith"}],"year":"2010","title":"Structural annotation of search queries using pseudo-relevance feedback","source":"Michael Bendersky, W. Bruce Croft, and David A. Smith. 2010. Structural annotation of search queries using pseudo-relevance feedback. In Proc. of CIKM, pages 1537–1540."},{"authors":[{"first":"Shane","last":"Bergsma"},{"first":"Qin I","middle":".","last":"Wang"}],"year":"2007","title":"Learning noun phrase query segmentation","source":"Shane Bergsma and Qin I. Wang. 2007. Learning noun phrase query segmentation. In Proc. of EMNLP, pages 819–826."},{"authors":[{"first":"Thorsten","last":"Brants"},{"first":"Alex","last":"Franz"}],"year":"2006","title":"Web 1T 5-gram Version 1","source":"Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram Version 1."},{"authors":[{"first":"Chris","last":"Buckley"}],"year":"1995","title":"Automatic query expansion using SMART","source":"Chris Buckley. 1995. Automatic query expansion using SMART. In Proc. of TREC-3, pages 69–80."},{"authors":[{"first":"Jenny","middle":"R.","last":"Finkel"},{"first":"Christopher","middle":"D.","last":"Manning"}],"year":"2009","title":"Joint parsing and named entity recognition","source":"Jenny R. Finkel and Christopher D. Manning. 2009. Joint parsing and named entity recognition. In Proc. of NAACL, pages 326–334."},{"authors":[{"first":"Jiafeng","last":"Guo"},{"first":"Gu","last":"Xu"},{"first":"Hang","last":"Li"},{"first":"Xueqi","last":"Cheng"}],"year":"2008","title":"A unified and discriminative model for query refinement","source":"Jiafeng Guo, Gu Xu, Hang Li, and Xueqi Cheng. 2008. A unified and discriminative model for query refinement. In Proc. of SIGIR, pages 379–386."},{"authors":[{"first":"Jiafeng","last":"Guo"},{"first":"Gu","last":"Xu"},{"first":"Xueqi","last":"Cheng"},{"first":"Hang","last":"Li"}],"year":"2009","title":"Named entity recognition in query","source":"Jiafeng Guo, Gu Xu, Xueqi Cheng, and Hang Li. 2009. Named entity recognition in query. In Proc. of SIGIR, pages 267–274."},{"authors":[{"first":"Matthias","last":"Hagen"},{"first":"Martin","last":"Potthast"},{"first":"Benno","last":"Stein"},{"first":"Christof","last":"Braeutigam"}],"year":"2010","title":"The power of naive query segmentation","source":"Matthias Hagen, Martin Potthast, Benno Stein, and Christof Braeutigam. 2010. The power of naive query segmentation. In Proc. of SIGIR, pages 797–798."},{"authors":[{"first":"Matthias","last":"Hagen"},{"first":"Martin","last":"Potthast"},{"first":"Benno","last":"Stein"},{"first":"Christof","last":"Bräutigam"}],"year":"2011","title":"Query segmentation revisited","source":"Matthias Hagen, Martin Potthast, Benno Stein, and Christof Bräutigam. 2011. Query segmentation revisited. In Proc. of WWW, pages 97–106."},{"authors":[{"first":"Rosie","last":"Jones"},{"first":"Daniel","middle":"C.","last":"Fain"}],"year":"2003","title":"Query word dele-tion prediction","source":"Rosie Jones and Daniel C. Fain. 2003. Query word dele-tion prediction. In Proc. of SIGIR, pages 435–436."},{"authors":[{"first":"Rosie","last":"Jones"},{"first":"Benjamin","last":"Rey"},{"first":"Omid","last":"Madani"},{"first":"Wiley","last":"Greiner"}],"year":"2006","title":"Generating query substitutions","source":"Rosie Jones, Benjamin Rey, Omid Madani, and Wiley Greiner. 2006. Generating query substitutions. In Proc. of WWW, pages 387–396."},{"authors":[{"first":"Giridhar","last":"Kumaran"},{"first":"James","last":"Allan"}],"year":"2007","title":"A case for shorter queries, and helping user create them","source":"Giridhar Kumaran and James Allan. 2007. A case for shorter queries, and helping user create them. In Proc. of NAACL, pages 220–227."},{"authors":[{"first":"Giridhar","last":"Kumaran"},{"first":"Vitor","middle":"R.","last":"Carvalho"}],"year":"2009","title":"Reducing long queries using query quality predictors","source":"Giridhar Kumaran and Vitor R. Carvalho. 2009. Reducing long queries using query quality predictors. In Proc. of SIGIR, pages 564–571."},{"authors":[{"first":"John","middle":"D.","last":"Lafferty"},{"first":"Andrew","last":"McCallum"},{"first":"Fernando","middle":"C. N.","last":"Pereira"}],"year":"2001","title":"Conditional random fields: Probabilis-tic models for segmenting and labeling sequence data","source":"John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random fields: Probabilis-tic models for segmenting and labeling sequence data. In Proc. of ICML, pages 282–289."},{"authors":[{"first":"Victor","last":"Lavrenko"},{"first":"W.","middle":"Bruce","last":"Croft"}],"year":"2001","title":"Relevance based language models","source":"Victor Lavrenko and W. Bruce Croft. 2001. Relevance based language models. In Proc. of SIGIR, pages 120– 127."},{"authors":[{"first":"Matthew","last":"Lease"}],"year":"2007","title":"Natural language processing for information retrieval: the time is ripe (again)","source":"Matthew Lease. 2007. Natural language processing for information retrieval: the time is ripe (again). In Proceedings of PIKM."},{"authors":[{"first":"Xiao","last":"Li"}],"year":"2010","title":"Understanding the semantic structure of noun phrase queries","source":"Xiao Li. 2010. Understanding the semantic structure of noun phrase queries. In Proc. of ACL, pages 1337– 1345, Morristown, NJ, USA."},{"authors":[{"first":"Rachel","middle":"T.","last":"Lo"},{"first":"Ben","last":"He"},{"first":"Iadh","last":"Ounis"}],"year":"2005","title":"Automatically building a stopword list for an information retrieval system","source":"Rachel T. Lo, Ben He, and Iadh Ounis. 2005. Automatically building a stopword list for an information retrieval system. In Proc. of DIR."},{"authors":[{"first":"Yumao","last":"Lu"},{"first":"Fuchun","last":"Peng"},{"first":"Gilad","last":"Mishne"},{"first":"Xing","last":"Wei"},{"first":"Benoit","last":"Dumoulin"}],"year":"2009","title":"Improving Web search relevance with semantic features","source":"Yumao Lu, Fuchun Peng, Gilad Mishne, Xing Wei, and Benoit Dumoulin. 2009. Improving Web search relevance with semantic features. In Proc. of EMNLP, pages 648–657."},{"authors":[{"first":"Mehdi","last":"Manshadi"},{"first":"Xiao","last":"Li"}],"year":"2009","title":"Semantic Tagging of Web Search Queries","source":"Mehdi Manshadi and Xiao Li. 2009. Semantic Tagging of Web Search Queries. In Proc. of ACL, pages 861– 869."},{"authors":[{"first":"André","middle":"F. T.","last":"Martins"},{"first":"Dipanjan","last":"Das"},{"first":"Noah","middle":"A.","last":"Smith"},{"first":"Eric","middle":"P.","last":"Xing"}],"year":"2008","title":"Stacking dependency parsers","source":"André F. T. Martins, Dipanjan Das, Noah A. Smith, and Eric P. Xing. 2008. Stacking dependency parsers. In Proc. of EMNLP, pages 157–166."},{"authors":[{"first":"Joakim","last":"Nivre"},{"first":"Ryan","last":"McDonald"}],"year":"2008","title":"Integrating graph-based and transition-based dependency parsers","source":"Joakim Nivre and Ryan McDonald. 2008. Integrating graph-based and transition-based dependency parsers. In Proc. of ACL, pages 950–958."},{"authors":[{"first":"Marius","last":"Pasça"}],"year":"2007","title":"Weakly-supervised discovery of named entities using web search queries","source":"Marius Pasça. 2007. Weakly-supervised discovery of named entities using web search queries. In Proc. of CIKM, pages 683–690."},{"authors":[{"first":"Dou","last":"Shen"},{"first":"Toby","last":"Walkery"},{"first":"Zijian","last":"Zhengy"},{"first":"Qiang","last":"Yangz"},{"first":"Ying","last":"Li"}],"year":"2008","title":"Personal name classification in web queries","source":"Dou Shen, Toby Walkery, Zijian Zhengy, Qiang Yangz, and Ying Li. 2008. Personal name classification in web queries. In Proc. of WSDM, pages 149–158."},{"authors":[{"first":"Bin","last":"Tan"},{"first":"Fuchun","last":"Peng"}],"year":"2008","title":"Unsupervised query segmentation using generative language models and Wikipedia","source":"Bin Tan and Fuchun Peng. 2008. Unsupervised query segmentation using generative language models and Wikipedia. In Proc. of WWW, pages 347–356."},{"authors":[{"first":"Kristina","last":"Toutanova"},{"first":"Aria","last":"Haghighi"},{"first":"Christopher","middle":"D.","last":"Manning"}],"year":"2008","title":"A global joint model for semantic role labeling","source":"Kristina Toutanova, Aria Haghighi, and Christopher D. Manning. 2008. A global joint model for semantic role labeling. Computational Linguistics, 34:161–191, June."},{"authors":[{"first":"Xing","last":"Wei"},{"first":"Fuchun","last":"Peng"},{"first":"Benoit","last":"Dumoulin"}],"year":"2008","title":"Analyzing web text association to disambiguate abbreviation in queries","source":"Xing Wei, Fuchun Peng, and Benoit Dumoulin. 2008. Analyzing web text association to disambiguate abbreviation in queries. In Proc. of SIGIR, pages 751–752. 111"}],"cites":[{"style":0,"text":"Bergsma and Wang, 2007","origin":{"pointer":"/sections/2/paragraphs/1","offset":324,"length":22},"authors":[{"last":"Bergsma"},{"last":"Wang"}],"year":"2007","references":["/references/5"]},{"style":0,"text":"Barr et al., 2008","origin":{"pointer":"/sections/2/paragraphs/1","offset":348,"length":17},"authors":[{"last":"Barr"},{"last":"al."}],"year":"2008","references":["/references/1"]},{"style":0,"text":"Lu et al., 2009","origin":{"pointer":"/sections/2/paragraphs/1","offset":367,"length":15},"authors":[{"last":"Lu"},{"last":"al."}],"year":"2009","references":["/references/22"]},{"style":0,"text":"Bendersky et al., 2010","origin":{"pointer":"/sections/2/paragraphs/1","offset":384,"length":22},"authors":[{"last":"Bendersky"},{"last":"al."}],"year":"2010","references":["/references/4"]},{"style":0,"text":"Li, 2010","origin":{"pointer":"/sections/2/paragraphs/1","offset":408,"length":8},"authors":[{"last":"Li"}],"year":"2010","references":["/references/20"]},{"style":0,"text":"Bendersky and Croft (2009)","origin":{"pointer":"/sections/2/paragraphs/3","offset":39,"length":26},"authors":[{"last":"Bendersky"},{"last":"Croft"}],"year":"2009","references":["/references/2"]},{"style":0,"text":"Barr et al., 2008","origin":{"pointer":"/sections/2/paragraphs/3","offset":527,"length":17},"authors":[{"last":"Barr"},{"last":"al."}],"year":"2008","references":["/references/1"]},{"style":0,"text":"Guo et al., 2008","origin":{"pointer":"/sections/2/paragraphs/3","offset":546,"length":16},"authors":[{"last":"Guo"},{"last":"al."}],"year":"2008","references":["/references/9"]},{"style":0,"text":"Guo et al., 2009","origin":{"pointer":"/sections/2/paragraphs/3","offset":564,"length":16},"authors":[{"last":"Guo"},{"last":"al."}],"year":"2009","references":["/references/10"]},{"style":0,"text":"Manshadi and Li, 2009","origin":{"pointer":"/sections/2/paragraphs/3","offset":582,"length":21},"authors":[{"last":"Manshadi"},{"last":"Li"}],"year":"2009","references":["/references/23"]},{"style":0,"text":"Li, 2010","origin":{"pointer":"/sections/2/paragraphs/3","offset":605,"length":8},"authors":[{"last":"Li"}],"year":"2010","references":["/references/20"]},{"style":0,"text":"Balasubramanian and Allan, 2009","origin":{"pointer":"/sections/2/paragraphs/6","offset":312,"length":31},"authors":[{"last":"Balasubramanian"},{"last":"Allan"}],"year":"2009","references":["/references/0"]},{"style":0,"text":"Guo et al., 2009","origin":{"pointer":"/sections/2/paragraphs/6","offset":372,"length":16},"authors":[{"last":"Guo"},{"last":"al."}],"year":"2009","references":["/references/10"]},{"style":0,"text":"Guo et al., 2008","origin":{"pointer":"/sections/2/paragraphs/6","offset":409,"length":16},"authors":[{"last":"Guo"},{"last":"al."}],"year":"2008","references":["/references/9"]},{"style":0,"text":"Li, 2010","origin":{"pointer":"/sections/2/paragraphs/6","offset":450,"length":8},"authors":[{"last":"Li"}],"year":"2010","references":["/references/20"]},{"style":0,"text":"Barr et al., 2008","origin":{"pointer":"/sections/2/paragraphs/8","offset":179,"length":17},"authors":[{"last":"Barr"},{"last":"al."}],"year":"2008","references":["/references/1"]},{"style":0,"text":"Bergsma and Wang, 2007","origin":{"pointer":"/sections/2/paragraphs/8","offset":198,"length":22},"authors":[{"last":"Bergsma"},{"last":"Wang"}],"year":"2007","references":["/references/5"]},{"style":0,"text":"Tan and Peng, 2008","origin":{"pointer":"/sections/2/paragraphs/8","offset":222,"length":18},"authors":[{"last":"Tan"},{"last":"Peng"}],"year":"2008","references":["/references/28"]},{"style":0,"text":"Guo et al., 2008","origin":{"pointer":"/sections/2/paragraphs/8","offset":242,"length":16},"authors":[{"last":"Guo"},{"last":"al."}],"year":"2008","references":["/references/9"]},{"style":0,"text":"Bendersky et al., 2010","origin":{"pointer":"/sections/2/paragraphs/8","offset":453,"length":22},"authors":[{"last":"Bendersky"},{"last":"al."}],"year":"2010","references":["/references/4"]},{"style":0,"text":"Kumaran and Allan, 2007","origin":{"pointer":"/sections/2/paragraphs/8","offset":477,"length":23},"authors":[{"last":"Kumaran"},{"last":"Allan"}],"year":"2007","references":["/references/15"]},{"style":0,"text":"Kumaran and Carvalho, 2009","origin":{"pointer":"/sections/2/paragraphs/8","offset":502,"length":26},"authors":[{"last":"Kumaran"},{"last":"Carvalho"}],"year":"2009","references":["/references/16"]},{"style":0,"text":"Lease, 2007","origin":{"pointer":"/sections/2/paragraphs/8","offset":530,"length":11},"authors":[{"last":"Lease"}],"year":"2007","references":["/references/19"]},{"style":0,"text":"Barr et al., 2008","origin":{"pointer":"/sections/3/paragraphs/1","offset":282,"length":17},"authors":[{"last":"Barr"},{"last":"al."}],"year":"2008","references":["/references/1"]},{"style":0,"text":"Bergsma and Wang, 2007","origin":{"pointer":"/sections/3/paragraphs/1","offset":301,"length":22},"authors":[{"last":"Bergsma"},{"last":"Wang"}],"year":"2007","references":["/references/5"]},{"style":0,"text":"Jones and Fain, 2003","origin":{"pointer":"/sections/3/paragraphs/1","offset":325,"length":20},"authors":[{"last":"Jones"},{"last":"Fain"}],"year":"2003","references":["/references/13"]},{"style":0,"text":"Nivre and McDonald, 2008","origin":{"pointer":"/sections/4/paragraphs/34","offset":336,"length":24},"authors":[{"last":"Nivre"},{"last":"McDonald"}],"year":"2008","references":["/references/25"]},{"style":0,"text":"Martins et al., 2008","origin":{"pointer":"/sections/4/paragraphs/34","offset":362,"length":20},"authors":[{"last":"Martins"},{"last":"al."}],"year":"2008","references":["/references/24"]},{"style":0,"text":"Bendersky et al., 2010","origin":{"pointer":"/sections/5/paragraphs/0","offset":304,"length":22},"authors":[{"last":"Bendersky"},{"last":"al."}],"year":"2010","references":["/references/4"]},{"style":0,"text":"Bendersky et al. (2010)","origin":{"pointer":"/sections/5/paragraphs/0","offset":763,"length":23},"authors":[{"last":"Bendersky"},{"last":"al."}],"year":"2010","references":["/references/4"]},{"style":0,"text":"Bendersky et al. (2010)","origin":{"pointer":"/sections/5/paragraphs/2","offset":10,"length":23},"authors":[{"last":"Bendersky"},{"last":"al."}],"year":"2010","references":["/references/4"]},{"style":0,"text":"Brants and Franz, 2006","origin":{"pointer":"/sections/5/paragraphs/2","offset":64,"length":22},"authors":[{"last":"Brants"},{"last":"Franz"}],"year":"2006","references":["/references/6"]},{"style":0,"text":"Bendersky et al. (2010)","origin":{"pointer":"/sections/5/paragraphs/4","offset":116,"length":23},"authors":[{"last":"Bendersky"},{"last":"al."}],"year":"2010","references":["/references/4"]},{"style":0,"text":"Buckley, 1995","origin":{"pointer":"/sections/5/paragraphs/4","offset":269,"length":13},"authors":[{"last":"Buckley"}],"year":"1995","references":["/references/7"]},{"style":0,"text":"Lavrenko and Croft, 2001","origin":{"pointer":"/sections/5/paragraphs/4","offset":284,"length":24},"authors":[{"last":"Lavrenko"},{"last":"Croft"}],"year":"2001","references":["/references/18"]},{"style":0,"text":"Bendersky et al. (2010)","origin":{"pointer":"/sections/5/paragraphs/11","offset":10,"length":23},"authors":[{"last":"Bendersky"},{"last":"al."}],"year":"2010","references":["/references/4"]},{"style":0,"text":"Brants and Franz, 2006","origin":{"pointer":"/sections/5/paragraphs/11","offset":274,"length":22},"authors":[{"last":"Brants"},{"last":"Franz"}],"year":"2006","references":["/references/6"]},{"style":0,"text":"Bergsma and Wang, 2007","origin":{"pointer":"/sections/6/paragraphs/0","offset":232,"length":22},"authors":[{"last":"Bergsma"},{"last":"Wang"}],"year":"2007","references":["/references/5"]},{"style":0,"text":"Jones et al., 2006","origin":{"pointer":"/sections/6/paragraphs/0","offset":256,"length":18},"authors":[{"last":"Jones"},{"last":"al."}],"year":"2006","references":["/references/14"]},{"style":0,"text":"Guo et al., 2008","origin":{"pointer":"/sections/6/paragraphs/0","offset":276,"length":16},"authors":[{"last":"Guo"},{"last":"al."}],"year":"2008","references":["/references/9"]},{"style":0,"text":"Hagen et al., 2010","origin":{"pointer":"/sections/6/paragraphs/0","offset":294,"length":18},"authors":[{"last":"Hagen"},{"last":"al."}],"year":"2010","references":["/references/11"]},{"style":0,"text":"Hagen et al., 2011","origin":{"pointer":"/sections/6/paragraphs/0","offset":314,"length":18},"authors":[{"last":"Hagen"},{"last":"al."}],"year":"2011","references":["/references/12"]},{"style":0,"text":"Tan and Peng, 2008","origin":{"pointer":"/sections/6/paragraphs/0","offset":334,"length":18},"authors":[{"last":"Tan"},{"last":"Peng"}],"year":"2008","references":["/references/28"]},{"style":0,"text":"Barr et al., 2008","origin":{"pointer":"/sections/6/paragraphs/0","offset":392,"length":17},"authors":[{"last":"Barr"},{"last":"al."}],"year":"2008","references":["/references/1"]},{"style":0,"text":"Manshadi and Li, 2009","origin":{"pointer":"/sections/6/paragraphs/0","offset":411,"length":21},"authors":[{"last":"Manshadi"},{"last":"Li"}],"year":"2009","references":["/references/23"]},{"style":0,"text":"Li, 2010","origin":{"pointer":"/sections/6/paragraphs/0","offset":434,"length":8},"authors":[{"last":"Li"}],"year":"2010","references":["/references/20"]},{"style":0,"text":"Guo et al., 2009","origin":{"pointer":"/sections/6/paragraphs/0","offset":471,"length":16},"authors":[{"last":"Guo"},{"last":"al."}],"year":"2009","references":["/references/10"]},{"style":0,"text":"Lu et al., 2009","origin":{"pointer":"/sections/6/paragraphs/0","offset":489,"length":15},"authors":[{"last":"Lu"},{"last":"al."}],"year":"2009","references":["/references/22"]},{"style":0,"text":"Shen et al., 2008","origin":{"pointer":"/sections/6/paragraphs/0","offset":506,"length":17},"authors":[{"last":"Shen"},{"last":"al."}],"year":"2008","references":["/references/27"]},{"style":0,"text":"Pasça, 2007","origin":{"pointer":"/sections/6/paragraphs/0","offset":525,"length":11},"authors":[{"last":"Pasça"}],"year":"2007","references":["/references/26"]},{"style":0,"text":"Wei et al., 2008","origin":{"pointer":"/sections/6/paragraphs/0","offset":568,"length":16},"authors":[{"last":"Wei"},{"last":"al."}],"year":"2008","references":["/references/30"]},{"style":0,"text":"Lo et al., 2005","origin":{"pointer":"/sections/6/paragraphs/0","offset":610,"length":15},"authors":[{"last":"Lo"},{"last":"al."}],"year":"2005","references":["/references/21"]},{"style":0,"text":"Jones and Fain, 2003","origin":{"pointer":"/sections/6/paragraphs/0","offset":627,"length":20},"authors":[{"last":"Jones"},{"last":"Fain"}],"year":"2003","references":["/references/13"]},{"style":0,"text":"Guo et al. (2008)","origin":{"pointer":"/sections/6/paragraphs/1","offset":374,"length":17},"authors":[{"last":"Guo"},{"last":"al."}],"year":"2008","references":["/references/9"]},{"style":0,"text":"Guo et al. (2008)","origin":{"pointer":"/sections/6/paragraphs/2","offset":7,"length":17},"authors":[{"last":"Guo"},{"last":"al."}],"year":"2008","references":["/references/9"]},{"style":0,"text":"Guo et al. (2008)","origin":{"pointer":"/sections/6/paragraphs/2","offset":577,"length":17},"authors":[{"last":"Guo"},{"last":"al."}],"year":"2008","references":["/references/9"]},{"style":0,"text":"Finkel and Manning, 2009","origin":{"pointer":"/sections/6/paragraphs/3","offset":101,"length":24},"authors":[{"last":"Finkel"},{"last":"Manning"}],"year":"2009","references":["/references/8"]},{"style":0,"text":"Toutanova et al., 2008","origin":{"pointer":"/sections/6/paragraphs/3","offset":127,"length":22},"authors":[{"last":"Toutanova"},{"last":"al."}],"year":"2008","references":["/references/29"]},{"style":0,"text":"Nivre and Mc-Donald, 2008","origin":{"pointer":"/sections/6/paragraphs/3","offset":179,"length":25},"authors":[{"last":"Nivre"},{"last":"Mc-Donald"}],"year":"2008","references":[]},{"style":0,"text":"Martins et al., 2008","origin":{"pointer":"/sections/6/paragraphs/3","offset":206,"length":20},"authors":[{"last":"Martins"},{"last":"al."}],"year":"2008","references":["/references/24"]},{"style":0,"text":"Finkel and Manning, 2009","origin":{"pointer":"/sections/6/paragraphs/3","offset":383,"length":24},"authors":[{"last":"Finkel"},{"last":"Manning"}],"year":"2009","references":["/references/8"]},{"style":0,"text":"Toutanova et al., 2008","origin":{"pointer":"/sections/6/paragraphs/3","offset":474,"length":22},"authors":[{"last":"Toutanova"},{"last":"al."}],"year":"2008","references":["/references/29"]},{"style":0,"text":"Barr et al., 2008","origin":{"pointer":"/sections/7/paragraphs/84","offset":1006,"length":17},"authors":[{"last":"Barr"},{"last":"al."}],"year":"2008","references":["/references/1"]},{"style":0,"text":"Guo et al., 2008","origin":{"pointer":"/sections/7/paragraphs/84","offset":1025,"length":16},"authors":[{"last":"Guo"},{"last":"al."}],"year":"2008","references":["/references/9"]},{"style":0,"text":"Guo et al., 2009","origin":{"pointer":"/sections/7/paragraphs/84","offset":1043,"length":16},"authors":[{"last":"Guo"},{"last":"al."}],"year":"2009","references":["/references/10"]},{"style":0,"text":"Bergsma and Wang, 2007","origin":{"pointer":"/sections/7/paragraphs/85","offset":74,"length":22},"authors":[{"last":"Bergsma"},{"last":"Wang"}],"year":"2007","references":["/references/5"]},{"style":0,"text":"Hagen et al., 2010","origin":{"pointer":"/sections/7/paragraphs/85","offset":98,"length":18},"authors":[{"last":"Hagen"},{"last":"al."}],"year":"2010","references":["/references/11"]},{"style":0,"text":"Hagen et al. (2010)","origin":{"pointer":"/sections/7/paragraphs/86","offset":47,"length":19},"authors":[{"last":"Hagen"},{"last":"al."}],"year":"2010","references":["/references/11"]},{"style":0,"text":"Brants and Franz, 2006","origin":{"pointer":"/sections/7/paragraphs/86","offset":224,"length":22},"authors":[{"last":"Brants"},{"last":"Franz"}],"year":"2006","references":["/references/6"]},{"style":0,"text":"Bergsma and Wang (2007)","origin":{"pointer":"/sections/7/paragraphs/90","offset":111,"length":23},"authors":[{"last":"Bergsma"},{"last":"Wang"}],"year":"2007","references":["/references/5"]},{"style":0,"text":"Bergsma and Wang (2007)","origin":{"pointer":"/sections/7/paragraphs/90","offset":231,"length":23},"authors":[{"last":"Bergsma"},{"last":"Wang"}],"year":"2007","references":["/references/5"]},{"style":0,"text":"Bendersky et al. (2009)","origin":{"pointer":"/sections/7/paragraphs/90","offset":311,"length":23},"authors":[{"last":"Bendersky"},{"last":"al."}],"year":"2009","references":["/references/3"]}]}
