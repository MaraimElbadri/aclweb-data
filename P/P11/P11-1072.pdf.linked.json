{"sections":[{"title":"","paragraphs":["Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 712–721, Portland, Oregon, June 19-24, 2011. c⃝2011 Association for Computational Linguistics"]},{"title":"Learning Sub-Word Units for Open Vocabulary Speech Recognition Carolina Parada","paragraphs":["1"]},{"title":", Mark Dredze","paragraphs":["1"]},{"title":", Abhinav Sethy","paragraphs":["2"]},{"title":", and Ariya Rastrow","paragraphs":["1 1"]},{"title":"Human Language Technology Center of Excellence, Johns Hopkins University 3400 N Charles Street, Baltimore, MD, USA carolinap@jhu.edu, mdredze@cs.jhu.edu, ariya@jhu.edu","paragraphs":["2"]},{"title":"IBM T.J. Watson Research Center, Yorktown Heights, NY, USA asethy@us.ibm.com Abstract","paragraphs":["Large vocabulary speech recognition systems fail to recognize words beyond their vocabulary, many of which are information rich terms, like named entities or foreign words. Hybrid word/sub-word systems solve this problem by adding sub-word units to large vocabulary word based systems; new words can then be represented by combinations of sub-word units. Previous work heuristically created the sub-word lexicon from phonetic representations of text using simple statistics to select common phone sequences. We propose a probabilistic model to learn the sub-word lexicon optimized for a given task. We consider the task of out of vocabulary (OOV) word detection, which relies on output from a hybrid model. A hybrid model with our learned sub-word lexicon reduces error by 6.3% and 7.6% (absolute) at a 5% false alarm rate on an English Broadcast News and MIT Lectures task respectively."]},{"title":"1 Introduction","paragraphs":["Most automatic speech recognition systems operate with a large but limited vocabulary, finding the most likely words in the vocabulary for the given acoustic signal. While large vocabulary continuous speech recognition (LVCSR) systems produce high quality transcripts, they fail to recognize out of vocabulary (OOV) words. Unfortunately, OOVs are often information rich nouns, such as named entities and foreign words, and mis-recognizing them can have a disproportionate impact on transcript coherence.","Hybrid word/sub-word recognizers can produce a sequence of sub-word units in place of OOV words. Ideally, the recognizer outputs a complete word for in-vocabulary (IV) utterances, and sub-word units for OOVs. Consider the word “Slobodan”, the given name of the former president of Serbia. As an un-common English word, it is unlikely to be in the vocabulary of an English recognizer. While a LVCSR system would output the closest known words (e.x. “slow it dawn”), a hybrid system could output a sequence of multi-phoneme units: s l ow, b ax, d ae n. The latter is more useful for automatically recovering the word’s orthographic form, identify-ing that an OOV was spoken, or improving performance of a spoken term detection system with OOV queries. In fact, hybrid systems have improved OOV spoken term detection (Mamou et al., 2007; Parada et al., 2009), achieved better phone error rates, especially in OOV regions (Rastrow et al., 2009b), and obtained state-of-the-art performance for OOV detection (Parada et al., 2010).","Hybrid recognizers vary in a number of ways: sub-word unit type: variable-length phoneme units (Rastrow et al., 2009a; Bazzi and Glass, 2001) or joint letter sound sub-words (Bisani and Ney, 2005); unit creation: data-driven or linguistically motivated (Choueiter, 2009); and how they are in-corporated in LVCSR systems: hierarchical (Bazzi, 2002) or flat models (Bisani and Ney, 2005).","In this work, we consider how to optimally create sub-word units for a hybrid system. These units are variable-length phoneme sequences, although in principle our work can be use for other unit types. Previous methods for creating the sub-word lexi-712 con have relied on simple statistics computed from the phonetic representation of text (Rastrow et al., 2009a). These units typically represent the most frequent phoneme sequences in English words. However, it isn’t clear why these units would produce the best hybrid output. Instead, we introduce a probabilistic model for learning the optimal units for a given task. Our model learns a segmentation of a text corpus given some side information: a mapping between the vocabulary and a label set; learned units are predictive of class labels.","In this paper, we learn sub-word units optimized for OOV detection. OOV detection aims to identify regions in the LVCSR output where OOVs were uttered. Towards this goal, we are interested in selecting units such that the recognizer outputs them only for OOV regions while prefering to output a complete word for in-vocabulary regions. Our approach yields improvements over state-of-the-art results.","We begin by presenting our log-linear model for learning sub-word units with a simple but effective inference procedure. After reviewing existing OOV detection approaches, we detail how the learned units are integrated into a hybrid speech recognition system. We show improvements in OOV detection, and evaluate impact on phone error rates."]},{"title":"2 Learning Sub-Word Units","paragraphs":["Given raw text, our objective is to produce a lexicon of sub-word units that can be used by a hybrid system for open vocabulary speech recognition. Rather than relying on the text alone, we also utilize side information: a mapping of words to classes so we can optimize learning for a specific task.","The provided mapping assigns labels Y to the corpus. We maximize the probability of the observed labeling sequence Y given the text W : P (Y |W ). We assume there is a latent segmentation S of this corpus which impacts Y . The complete data likelihood becomes: P (Y |W ) =","∑","S P (Y, S|W ) during training. Since we are maximizing the observed Y , segmentation S must discriminate between different possible labels.","We learn variable-length multi-phone units by segmenting the phonetic representation of each word in the corpus. Resulting segments form the sub-word lexicon.1","Learning input includes a list of words to segment taken from raw text, a mapping between words and classes (side information indicating whether token is IV or OOV), a pronunciation dictionary D, and a letter to sound model (L2S), such as the one described in Chen (2003). The corpus W is the list of types (unique words) in the raw text input. This forces each word to have a unique segmentation, shared by all common tokens. Words are converted into phonetic representations according to their most likely dictionary pronunciation; non-dictionary words use the L2S model.2 2.1 Model Inspired by the morphological segmentation model of Poon et al. (2009), we assume P (Y, S|W ) is a log-linear model parameterized by Λ: PΛ(Y, S|W ) =","1 Z(W ) uΛ(Y, S, W ) (1) where uΛ(Y, S, W ) defines the score of the proposed segmentation S for words W and labels Y according to model parameters Λ. Sub-word units σ compose S, where each σ is a phone sequence, including the full pronunciation for vocabulary words; the collection of σs form the lexicon. Each unit σ is present in a segmentation with some context c = (φl, φr) of the form φlσφr. Features based on the context and the unit itself parameterize uΛ.","In addition to scoring a segmentation based on features, we include two priors inspired by the Minimum Description Length (MDL) principle suggested by Poon et al. (2009). The lexicon prior favors smaller lexicons by placing an exponential prior with negative weight on the length of the lexicon","∑","σ |σ|, where |σ| is the length of the unit σ in number of phones. Minimizing the lexicon prior favors a trivial lexicon of only the phones. The corpus prior counters this effect, an exponential prior with negative weight on the number of units in each word’s segmentation, where |si| is the segmentation length and |wi| is the length of the word in phones. Learning strikes a balance between the two priors. Using these definitions, the segmentation score uΛ(Y, S, W ) is given as: 1 Since sub-word units can expand full-words, we refer to","both words and sub-words simply as units. 2 The model can also take multiple pronunciations (§3.1). 713 s l ow b ax d ae n","s l ow (#,#, , b, ax)","b ax (l,ow, , d, ae)","d ae n (b,ax, , #, #) Figure 1: Units and bigram phone context (in parenthesis) for an example segmentation of the word “slobodan”. uΛ(Y, S, W ) = exp ( ∑ σ,y λσ,yfσ,y(S, Y ) + ∑ c,y λc,yfc,y(S, Y ) + α · ∑ σ∈S |σ| + β · ∑ i∈W |si|/|wi| ) (2) fσ,y(S, Y ) are the co-occurrence counts of the pair (σ, y) where σ is a unit under segmentation S and y is the label. fc,y(S, Y ) are the co-occurrence counts for the context c and label y under S. The model parameters are Λ = {λσ,y, λc,y : ∀σ, c, y}. The negative weights for the lexicon (α) and corpus priors (β) are tuned on development data. The normalizer Z sums over all possible segmentations and labels: Z(W ) = ∑ S′ ∑ Y ′","uΛ(Y ′ , S′",", W ) (3)","Consider the example segmentation for the word “slobodan” with pronunciation s,l,ow,b,ax,d,ae,n (Figure 1). The bigram phone context as a four-tuple appears below each unit; the first two entries correspond to the left context, and last two the right context. The example corpus (Figure 2) demonstrates how unit features fσ,y and context features fc,y are computed."]},{"title":"3 Model Training","paragraphs":["Learning maximizes the log likelihood of the observed labels Y ∗","given the words W :","l(Y ∗ |W ) = log ∑ S","1 Z(W )","uΛ(Y ∗ , S, W ) (4) We use the Expectation-Maximization algorithm, where the expectation step predicts segmentations S Labeled corpus: president/y = 0 milosevic/y = 1 Segmented corpus: p r eh z ih d ih n t/0 m ih/1 l aa/1 s ax/1 v ih ch/1 Unit-feature:Value p r eh z ih d ih n t/0:1 m ih/1:1 l aa/1:1 s ax/1:1 v ih ch/1:1 Context-feature:Value (#/0,#/0, ,l/1,aa/1):1, (m/1,ih/1, ,s/1,ax/1):1, (l/1,aa/1, ,v/1,ih/1):1, (s/1,ax/1, ,#/0,#/0):1, (#/0,#/0, ,#/0,#/0):1 Figure 2: A small example corpus with segmentations and corresponding features. The notation m ih/1:1 represents unit/label:feature-value. Overlapping context features capture rich segmentation regularities associated with each class. given the model’s current parameters Λ (§3.1), and the maximization step updates these parameters using gradient ascent. The partial derivatives of the objective (4) with respect to each parameter λi are:","∂l(Y ∗","|W ) ∂λi","= ES|Y ∗ ,W [fi] − ES,Y |W [fi] (5) The gradient takes the usual form, where we encourage the expected segmentation from the current model given the correct labels to equal the expected segmentation and expected labels. The next section discusses computing these expectations. 3.1 Inference Inference is challenging since the lexicon prior renders all word segmentations interdependent. Consider a simple two word corpus: cesar (s,iy,z,er), and cesium (s,iy,z,iy,ax,m). Numerous segmentations are possible; each word has 2N−1","possible segmentations, where N is the number of phones in its pronunciation (i.e., 23","× 25","= 256). However, if we decide to segment the first word as: {s iy, z er}, then the segmentation for “cesium”:{s iy, z iy ax m} will incur a lexicon prior penalty for including the new segment z iy ax m. If instead we segment “cesar” as {s iy z, er}, the segmentation {s iy, z iy ax m} incurs double penalty for the lexicon prior (since we are including two new units in the lexicon: s iy and z iy ax m). This dependency requires joint segmentation of the entire corpus, which is intractable. Hence, we resort to approximations of the expectations in Eq. (5).","One approach is to use Gibbs Sampling: it-erating through each word, sampling a new seg-714 mentation conditioned on the segmentation of all other words. The sampling distribution requires enumerating all possible segmentations for each word (2N−1",") and computing the conditional probabilities for each segmentation: P (S|Y ∗",", W ) = P (Y ∗",", S|W )/P (Y ∗","|W ) (the features are extracted from the remaining words in the corpus). Using M sampled segmentations S1, S2, . . . Sm we compute ES|Y ∗",",W [fi] as follows:","ES|Y ∗ ,W [fi] ≈ 1 M ∑ j fi[Sj] Similarly, to compute ES,Y |W we sample a segmentation and a label for each word. We compute the joint probability of P (Y, S|W ) for each segmentation-label pair using Eq. (1). A sampled segmentation can introduce new units, which may have higher probability than existing ones.","Using these approximations in Eq. (5), we update the parameters using gradient ascent:","λ̄new = λ̄old + γ∇lλ̄(Y ∗ |W ) where γ > 0 is the learning rate.","To obtain the best segmentation, we use deterministic annealing. Sampling operates as usual, except that the parameters are divided by a value, which starts large and gradually drops to zero. To make burn in faster for sampling, the sampler is initialized with the most likely segmentation from the previous iteration. To initialize the sampler the first time, we set all the parameters to zero (only the priors have non-zero values) and run deterministic annealing to obtain the first segmentation of the corpus. 3.2 Efficient Sampling Sampling a segmentation for the corpus requires computing the normalization constant (3), which contains a summation over all possible corpus segmentations. Instead, we approximate this constant by sampling words independently, keeping fixed all other segmentations. Still, even sampling a single word’s segmentation requires enumerating probabilities for all possible segmentations.","We sample a segmentation efficiently using dynamic programming. We can represent all possible segmentations for a word as a finite state machine (FSM) (Figure 3), where arcs weights arise from scoring the segmentation’s features. This weight is the negative log probability of the resulting model after adding the corresponding features and priors.","However, the lexicon prior poses a problem for this construction since the penalty incurred by a new unit in the segmentation depends on whether that unit is present elsewhere in that segmentation. For example, consider the segmentation for the word ANJANI: AA N, JH, AA N, IY. If none of these units are in the lexicon, this segmentation yields the lowest prior penalty since it repeats the unit AA N. 3","This global dependency means paths must encode the full unit history, making computing forward-backward probabilities inefficient.","Our solution is to use the Metropolis-Hastings algorithm, which samples from the true distribution P (Y, S|W ) by first sampling a new label and segmentation (y′",", s′",") from a simpler proposal distribution Q(Y, S|W ). The new assignment (y′",", s′",") is accepted with probability:","α(Y ′ , S′","|Y, S, W )=min „ 1,","P (Y ′",", S′","|W )Q(Y, S|Y ′",", S′",", W )","P (Y, S|W )Q(Y ′",", S′","|Y, S, W ) «","We choose the proposal distribution Q(Y, S|W ) as Eq. (1) omitting the lexicon prior, removing the challenge for efficient computation. The probability of accepting a sample becomes:","α(Y ′ , S′","|Y, S, W )=min „ 1, P","σ∈S′ |σ| P σ∈S |σ| « (6) We sample a path from the FSM by running the forward-backward algorithm, where the backward computations are carried out explicitly, and the forward pass is done through sampling, i.e. we traverse the machine only computing forward probabilities for arcs leaving the sampled state.4","Once we sample a segmentation (and label) we accept it according to Eq. (6) or keep the previous segmentation if rejected.","Alg. 1 shows our full sub-word learning procedure, where sampleSL (Alg. 2) samples a segmentation and label sequence for the entire corpus from P (Y, S|W ), and sampleS samples a segmentation from P (S|Y ∗",", W ). 3 Splitting at phone boundaries yields the same lexicon prior","but a higher corpus prior. 4 We use OpenFst’s RandGen operation with a costumed arc-","selector (http://www.openfst.org/). 715 0 1AA 5 AA_N_JH_AA_N 4 AA_N_JH_AA 3 AA_N_JH 2 AA_N N_JH_AA_N N_JH_AA N_JH N 6 N_JH_AA_N_IY IY N AA_NAA AA_N_IY JH_AA_N JH_AA JH JH_AA_N_IY Figure 3: FSM representing all segmentations for the word ANJANI with pronunciation: AA,N,JH,AA,N,IY","Algorithm 1 Training","Input: Lexicon L from training text W , Dictionary D, Mapping M , L2S pronunciations, Annealing temp T .","Initialization: Assign label y∗","m = M [wm]. λ̄0 = 0̄ S0 = random segmentation for each word in L.","for i = 1 to K do","/* E-Step */","Si = bestSegmentation(T, λi−1, Si−1).","for k = 1 to NumSamples do (S′","k, Y ′","k) = sampleSL(P (Y, Si|W ),Q(Y, Si|W )) S̃k = sampleS(P (Si|Y ∗",", W ),Q(Si|Y ∗",", W )) end for /* M-Step */ ES,Y |W [fi] = 1","NumSamples","∑","k fσ,l[S′","k, Y ′","k]","ES|Y ∗ ,W [fσ,l] = 1","NumSamples","∑ k fσ,l[ S̃k, Y ∗","]","λ̄i = λ̄i−1 + γ∇Lλ̄(Y ∗ |W ) end for S = bestSegmentation(T, λK, S0) Output: Lexicon Lo from S"]},{"title":"4 OOV Detection Using Hybrid Models","paragraphs":["To evaluate our model for learning sub-word units, we consider the task of out-of-vocabulary (OOV) word detection. OOV detection for ASR output can be categorized into two broad groups: 1) hybrid (filler) models: which explicitly model OOVs using either filler, sub-words, or generic word models (Bazzi, 2002; Schaaf, 2001; Bisani and Ney, 2005; Klakow et al., 1999; Wang, 2009); and 2) confidence-based approaches: which label unreliable regions as OOVs based on different confidence scores, such as acoustic scores, language models, and lattice scores (Lin et al., 2007; Burget et al., 2008; Sun et al., 2001; Wessel et al., 2001).","In the next section we detail the OOV detection approach we employ, which combines hybrid and","Algorithm 2 sampleSL(P (S, Y |W ), Q(S, Y |W ))","for m = 1 to M (NumWords) do (s′","m, y′","m) = Sample segmentation/label pair for word wm according to Q(S, Y |W ) Y ′","= {y","1 . . . ym−1y′ mym+1 . . . yM }","S′ = {s","1 . . . sm−1s′ msm+1 . . . sM } α=min ( 1, P σ∈S′ |σ| P σ∈S |σ| )","with prob α : ym,k = y′","m, sm,k = s′","m","with prob (1 − α) : ym,k = ym, sm,k = sm","end for","return (S′","k, Y ′","k) = [(s1,k, y1,k) . . . (sM,k, yM,k)] confidence-based models, achieving state-of-the art performance for this task. 4.1 OOV Detection Approach We use the state-of-the-art OOV detection model of Parada et al. (2010), a second order CRF with features based on the output of a hybrid recognizer. This detector processes hybrid recognizer output, so we can evaluate different sub-word unit lexicons for the hybrid recognizer and measure the change in OOV detection accuracy.","Our model (§2.1) can be applied to this task by using a dictionary D to label words as IV (yi = 0 if wi ∈ D) and OOV (yi = 1 if wi /∈ D). This results in a labeled corpus, where the labeling sequence Y indicates the presence of out-of-vocabulary words (OOVs). For comparison we evaluate a baseline method (Rastrow et al., 2009b) for selecting units.","Given a sub-word lexicon, the word and sub-words are combined to form a hybrid language model (LM) to be used by the LVCSR system. This hybrid LM captures dependencies between word and sub-words. In the LM training data, all OOVs are represented by the smallest number of sub-words which corresponds to their pronunciation. Pronunciations for all OOVs are obtained using grapheme 716 to phone models (Chen, 2003).","Since sub-words represent OOVs while building the hybrid LM, the existence of sub-words in ASR output indicate an OOV region. A simple solution to the OOV detection problem would then be reduced to a search for the sub-words in the output of the ASR system. The search can be on the one-best transcripts, lattices or confusion networks. While lattices contain more information, they are harder to process; confusion networks offer a trade-off between richness (posterior probabilities are already computed) and compactness (Mangu et al., 1999).","Two effective indications of OOVs are the existence of sub-words (Eq. 7) and high entropy in a network region (Eq. 8), both of which are used as features in the model of Parada et al. (2010). Sub-word Posterior = ∑ σ∈tj p(σ|tj) (7) Word-Entropy = − ∑ w∈tj p(w|tj) log p(w|tj) (8) tj is the current bin in the confusion network and σ is a sub-word in the hybrid dictionary. Improving the sub-word unit lexicon, improves the quality of the confusion networks for OOV detection."]},{"title":"5 Experimental Setup","paragraphs":["We used the data set constructed by Can et al. (2009) (OOVCORP) for the evaluation of Spoken Term Detection of OOVs since it focuses on the OOV problem. The corpus contains 100 hours of transcribed Broadcast News English speech. There are 1290 unique OOVs in the corpus, which were selected with a minimum of 5 acoustic instances per word and short OOVs inappropriate for STD (less than 4 phones) were explicitly excluded. Example OOVs include: NATALIE, PUTIN, QAEDA, HOLLOWAY, COROLLARIES, HYPERLINKED, etc. This resulted in roughly 24K (2%) OOV tokens.","For LVCSR, we used the IBM Speech Recogni-tion Toolkit (Soltau et al., 2005)5","to obtain a transcript of the audio. Acoustic models were trained on 300 hours of HUB4 data (Fiscus et al., 1998) and utterances containing OOV words as marked in OOVCORP were excluded. The language model was trained on 400M words from various text sources","5","The IBM system used speaker adaptive training based on maximum likelihood with no discriminative training. with a 83K word vocabulary. The LVCSR system’s WER on the standard RT04 BN test set was 19.4%. Excluded utterances amount to 100hrs. These were divided into 5 hours of training for the OOV detector and 95 hours of test. Note that the OOV detector training set is different from the LVCSR training set.","We also use a hybrid LVCSR system, combin-ing word and sub-word units obtained from either our approach or a state-of-the-art baseline approach (Rastrow et al., 2009a) (§5.2). Our hybrid system’s lexicon has 83K words and 5K or 10K sub-words. Note that the word vocabulary is common to both systems and only the sub-words are selected using either approach. The word vocabulary used is close to most modern LVCSR system vocabularies for English Broadcast News; the resulting OOVs are more challenging but more realistic (i.e. mostly named entities and technical terms). The 1290 words are OOVs to both the word and hybrid systems.","In addition we report OOV detection results on a MIT lectures data set (Glass et al., 2010) consisting of 3 Hrs from two speakers with a 1.5% OOV rate. These were divided into 1 Hr for training the OOV detector and 2 Hrs for testing. Note that the LVCSR system is trained on Broadcast News data. This out-of-domain test-set help us evaluate the cross-domain performance of the proposed and baseline hybrid systems. OOVs in this data set correspond mainly to technical terms in computer science and math. e.g. ALGORITHM, DEBUG, COMPILER, LISP. 5.1 Learning parameters For learning the sub-words we randomly selected from training 5,000 words which belong to the 83K vocabulary and 5,000 OOVs6",". For development we selected an additional 1,000 IV and 1,000 OOVs. This was used to tune our model hyper parameters (set to α = −1, β = −20). There is no overlap of OOVs in training, development and test sets. All feature weights were initialized to zero and had a Gaussian prior with variance σ = 100. Each of the words in training and development was converted to their most-likely pronunciation using the dictionary","6","This was used to obtain the 5K hybrid system. To learn sub-words for the 10K hybrid system we used 10K in-vocabulary words and 10K OOVs. All words were randomly selected from the LM training text. 717 for IV words or the L2S model for OOVs.7","The learning rate was γk = γ","(k+1+A)τ , where k is the iteration, A is the stability constant (set to 0.1K), γ = 0.4, and τ = 0.6. We used K = 40 iterations for learning and 200 samples to compute the expectations in Eq. 5. The sampler was initialized by sampling for 500 iterations with deterministic annealing for a temperature varying from 10 to 0 at 0.1 intervals. Final segmentations were obtained using 10, 000 samples and the same temperature schedule. We limit segmentations to those including units of at most 5 phones to speed sampling with no significant degradation in performance. We observed improved performance by dis-allowing whole word units. 5.2 Baseline Unit Selection We used Rastrow et al. (2009a) as our baseline unit selection method, a data driven approach where the language model training text is converted into phones using the dictionary (or a letter-to-sound model for OOVs), and a N-gram phone LM is estimated on this data and pruned using a relative entropy based method. The hybrid lexicon includes resulting sub-words – ranging from unigrams to 5-gram phones, and the 83K word lexicon. 5.3 Evaluation We obtain confusion networks from both the word and hybrid LVCSR systems. We align the LVCSR transcripts with the reference transcripts and tag each confusion region as either IV or OOV. The OOV detector classifies each region in the confusion network as IV/OOV. We report OOV detection accuracy using standard detection error tradeoff (DET) curves (Martin et al., 1997). DET curves measure tradeoffs between false alarms (x-axis) and misses (y-axis), and are useful for determining the optimal operating point for an application; lower curves are better. Following Parada et al. (2010) we separately evaluate unobserved OOVs.8","7","In this work we ignore pronunciation variability and simply consider the most likely pronunciation for each word. It is straightforward to extend to multiple pronunciations by first sampling a pronunciation for each word and then sampling a segmentation for that pronunciation.","8","Once an OOV word has been observed in the OOV detector training data, even if it was not in the LVCSR training data, it is no longer truly OOV."]},{"title":"6 Results","paragraphs":["We compare the performance of a hybrid system with baseline units9","(§5.2) and one with units learned by our model on OOV detection and phone error rate. We present results using a hybrid system with 5k and 10k sub-words.","We evaluate the CRF OOV detector with two different feature sets. The first uses only Word Entropy and Sub-word Posterior (Eqs. 7 and 8) (Figure 4)10",". The second (context) uses the extended context features of Parada et al. (2010) (Figure 5). Specifically, we include all trigrams obtained from the best hypothesis of the recognizer (a window of 5 words around current confusion bin). Predictions at different FA rates are obtained by varying a probability threshold.","At a 5% FA rate, our system (This Paper 5k) reduces the miss OOV rate by 6.3% absolute over the baseline (Baseline 5k) when evaluating all OOVs. For unobserved OOVs, it achieves 3.6% absolute improvement. A larger lexicon (Baseline 10k and This Paper 10k ) shows similar relative improvements. Note that the features used so far do not necessarily provide an advantage for unobserved versus observed OOVs, since they ignore the decoded word/sub-word sequence. In fact, the performance on un-observed OOVs is better.","OOV detection improvements can be attributed to increased coverage of OOV regions by the learned sub-words compared to the baseline. Table 1 shows the percent of Hits: sub-word units predicted in OOV regions, and False Alarms: sub-word units predicted for in-vocabulary words. We can see that the proposed system increases the Hits by over 8% absolute, while increasing the False Alarms by 0.3%. Interestingly, the average sub-word length for the proposed units exceeded that of the baseline units by 0.3 phones (Baseline 5K average length was 2.92, while that of This Paper 5K was 3.2).","9","Our baseline results differ from Parada et al. (2010). When implementing the lexicon baseline, we discovered that their hybrid units were mistakenly derived from text containing test OOVs. Once excluded, the relative improvements of previous work remain, but the absolute error rates are higher.","10","All real-valued features were normalized and quantized using the uniform-occupancy partitioning described in White et al. (2007). We used 50 partitions with a minimum of 100 training values per partition. 718","0","5 10","15","20 %FA 30 35 40 45 50 55 60 65 70 % M i s s e s Baseline (5k) This Paper (5k) Baseline (10k) This Paper (10k) (a)","0","5 10","15","20 %FA 30 35 40 45 50 55 60 65 70 % M i s s e s Baseline (5k) This Paper (5k) Baseline (10k) This Paper (10k)","(b) Figure 4: DET curves for OOV detection using baseline hybrid systems for different lexicon size and proposed discriminative hybrid system on OOVCORP data set. Evaluation on un-observed OOVs (a) and all OOVs (b).","0","5 10","15","20 %FA 30 35 40 45 50 55 60 65 70 % M i s s e s Baseline (10k) Baseline (10k) + context-features This Paper (10k) This Paper (10k) + context-features (a)","0","5 10","15","20 %FA 10 20 30 40 50 60 70 80 % M i s s e s Baseline (10k) Baseline (10k) + context-features This Paper (10k) This Paper (10k) + context-features","(b) Figure 5: Effect of adding context features to baseline and discriminative hybrid systems on OOVCORP data set. Evaluation on un-observed OOVs (a) and all OOVs (b).","Consistent with previously published results, including context achieves large improvement in performance. The proposed hybrid system (This Paper 10k + context-features) still improves over the baseline (Baseline 10k + context-features), however the relative gain is reduced. In this case, we obtain larger gains for un-observed OOVs which benefit less from the context clues learned in training.","Lastly, we report OOV detection performance on MIT Lectures. Both the sub-word lexicon and the LVCSR models were trained on Broadcast News data, helping us evaluate the robustness of learned sub-words across domains. Note that the OOVs in these domains are quite different: MIT Lectures’ OOVs correspond to technical computer sci-Hybrid System Hits FAs Baseline (5k) 18.25 1.49 This Paper (5k) 26.78 1.78 Baseline (10k) 24.26 1.82 This Paper (10k) 28.96 1.92 Table 1: Coverage of OOV regions by baseline and proposed sub-words in OOVCORP. ence and math terms, while in Broadcast News they are mainly named-entities.","Figure 6 and 7 show the OOV detection results in the MIT Lectures data set. For un-observed OOVs, the proposed system (This Paper 10k) reduces the miss OOV rate by 7.6% with respect to the baseline (Baseline 10k) at a 5% FA rate. Similar to Broadcast News results, we found that the learned sub-words provide larger coverage of OOV regions in MIT Lectures domain. These results suggest that the proposed sub-words are not simply modeling the training OOVs (named-entities) better than the baseline sub-words, but also describe better novel unexpected words. Furthermore, including context features does not seem as helpful. We conjecture that this is due to the higher WER11","and the less structured nature of the domain: i.e. ungrammatical sentences, disfluencies, incomplete sentences, making it more difficult to predict OOVs based on context. 11 W ER = 32.7% since the LVCSR system was trained on","Broadcast News data as described in Section 5. 719","0","5 10","15","20 %FA 30 40 50 60 70 80 90 % M i s s e s Baseline (5k) This Paper (5k) Baseline (10k) This Paper (10k) (a)","0","5 10","15","20 %FA 30 40 50 60 70 80 90 % M i s s e s Baseline (5k) This Paper (5k) Baseline (10k) This Paper (10k)","(b) Figure 6: DET curves for OOV detection using baseline hybrid systems for different lexicon size and proposed discriminative hybrid system on MIT Lectures data set. Evaluation on un-observed OOVs (a) and all OOVs (b).","0","5 10","15","20 %FA 30 40 50 60 70 80 90 % M i s s e s Baseline (10k) Baseline (10k) + context-features This Paper (10k) This Paper (10k) + context-features (a)","0","5 10","15","20 %FA 30 40 50 60 70 80 90 % M i s s e s Baseline (10k) Baseline (10k) + context-features This Paper (10k) This Paper (10k) + context-features","(b) Figure 7: Effect of adding context features to baseline and discriminative hybrid systems on MIT Lectures data set. Evaluation on un-observed OOVs (a) and all OOVs (b). 6.1 Improved Phonetic Transcription We consider the hybrid lexicon’s impact on Phone Error Rate (PER) with respect to the reference transcription. The reference phone sequence is obtained by doing forced alignment of the audio stream to the reference transcripts using acoustic models. This provides an alignment of the pronunciation variant of each word in the reference and the recognizer’s one-best output. The aligned words are converted to the phonetic representation using the dictionary.","Table 2 presents PERs for the word and different hybrid systems. As previously reported (Rastrow et al., 2009b), the hybrid systems achieve better PER, specially in OOV regions since they predict sub-word units for OOVs. Our method achieves modest improvements in PER compared to the hybrid baseline. No statistically significant improvements in PER were observed on MIT Lectures."]},{"title":"7 Conclusions","paragraphs":["Our probabilistic model learns sub-word units for hybrid speech recognizers by segmenting a text corpus while exploiting side information. Applying our","System OOV IV All","Word 1.62 6.42 8.04 Hybrid: Baseline (5k) 1.56 6.44 8.01 Hybrid: Baseline (10k) 1.51 6.41 7.92 Hybrid: This Paper (5k) 1.52 6.42 7.94 Hybrid: This Paper (10k) 1.45 6.39 7.85 Table 2: Phone Error Rate for OOVCORP. method to the task of OOV detection, we obtain an absolute error reduction of 6.3% and 7.6% at a 5% false alarm rate on an English Broadcast News and MIT Lectures task respectively, when compared to a baseline system. Furthermore, we have confirmed previous work that hybrid systems achieve better phone accuracy, and our model makes modest improvements over a baseline with a similarly sized sub-word lexicon. We plan to further explore our new lexicon’s performance for other languages and tasks, such as OOV spoken term detection."]},{"title":"Acknowledgments","paragraphs":["We gratefully acknowledge Bhuvaha Ramabhadran for many insightful discussions and the anonymous reviewers for their helpful comments. This work was funded by a Google PhD Fellowship. 720"]},{"title":"References","paragraphs":["Issam Bazzi and James Glass. 2001. Learning units for domain-independent out-of-vocabulary word modeling. In EuroSpeech.","Issam Bazzi. 2002. Modelling out-of-vocabulary words for robust speech recognition. Ph.D. thesis, Massachusetts Institute of Technology.","M. Bisani and H. Ney. 2005. Open vocabulary speech recognition with flat hybrid models. In INTER-SPEECH.","L. Burget, P. Schwarz, P. Matejka, M. Hannemann, A. Rastrow, C. White, S. Khudanpur, H. Hermansky, and J. Cernocky. 2008. Combination of strongly and weakly constrained recognizers for reliable detection of OOVS. In ICASSP.","D. Can, E. Cooper, A. Sethy, M. Saraclar, and C. White. 2009. Effect of pronounciations on OOV queries in spoken term detection. Proceedings of ICASSP.","Stanley F. Chen. 2003. Conditional and joint models for grapheme-to-phoneme conversion. In Eurospeech, pages 2033–2036.","G. Choueiter. 2009. Linguistically-motivated sub-word modeling with applications to speech recognition. Ph.D. thesis, Massachusetts Institute of Technology.","Jonathan Fiscus, John Garofolo, Mark Przybocki, William Fisher, and David Pallett, 1998. 1997 English Broadcast News Speech (HUB4). Linguistic Data Consortium, Philadelphia.","James Glass, Timothy Hazen, Lee Hetherington, and Chao Wang. 2010. Analysis and processing of lecture audio data: Preliminary investigations. In North American Chapter of the Association for Computational Linguistics (NAACL).","Dietrich Klakow, Georg Rose, and Xavier Aubert. 1999. OOV-detection in large vocabulary system using automatically defined word-fragments as fillers. In Eurospeech.","Hui Lin, J. Bilmes, D. Vergyri, and K. Kirchhoff. 2007. OOV detection by joint word/phone lattice alignment. In ASRU, pages 478–483, Dec.","Jonathan Mamou, Bhuvana Ramabhadran, and Olivier Siohan. 2007. Vocabulary independent spoken term detection. In Proceedings of SIGIR.","L. Mangu, E. Brill, and A. Stolcke. 1999. Finding consensus among words. In Eurospeech.","A. Martin, G. Doddington, T. Kamm, M. Ordowski, and M. Przybocky. 1997. The det curve in assessment of detection task performance. In Eurospeech.","Carolina Parada, Abhinav Sethy, and Bhuvana Ramabhadran. 2009. Query-by-example spoken term detection for oov terms. In ASRU.","Carolina Parada, Mark Dredze, Denis Filimonov, and Fred Jelinek. 2010. Contextual information improves oov detection in speech. In North American Chapter of the Association for Computational Linguistics (NAACL).","H. Poon, C. Cherry, and K. Toutanova. 2009. Unsupervised morphological segmentation with log-linear models. In ACL.","Ariya Rastrow, Abhinav Sethy, and Bhuvana Ramabhadran. 2009a. A new method for OOV detection using hybrid word/fragment system. Proceedings of ICASSP.","Ariya Rastrow, Abhinav Sethy, Bhuvana Ramabhadran, and Fred Jelinek. 2009b. Towards using hybrid, word, and fragment units for vocabulary independent LVCSR systems. INTERSPEECH.","T. Schaaf. 2001. Detection of OOV words using generalized word models and a semantic class language model. In Eurospeech.","H. Soltau, B. Kingsbury, L. Mangu, D. Povey, G. Saon, and G. Zweig. 2005. The ibm 2004 conversational telephony system for rich transcription. In ICASSP.","H. Sun, G. Zhang, f. Zheng, and M. Xu. 2001. Using word confidence measure for OOV words detection in a spontaneous spoken dialog system. In Eurospeech.","Stanley Wang. 2009. Using graphone models in automatic speech recognition. Master’s thesis, Massachusetts Institute of Technology.","F. Wessel, R. Schluter, K. Macherey, and H. Ney. 2001. Confidence measures for large vocabulary continuous speech recognition. IEEE Transactions on Speech and Audio Processing, 9(3).","Christopher White, Jasha Droppo, Alex Acero, and Julian Odell. 2007. Maximum entropy confidence estimation for speech recognition. In ICASSP. 721"]}],"references":[{"authors":[{"first":"Issam","last":"Bazzi"},{"first":"James","last":"Glass"}],"year":"2001","title":"Learning units for domain-independent out-of-vocabulary word modeling","source":"Issam Bazzi and James Glass. 2001. Learning units for domain-independent out-of-vocabulary word modeling. In EuroSpeech."},{"authors":[{"first":"Issam","last":"Bazzi"}],"year":"2002","title":"Modelling out-of-vocabulary words for robust speech recognition","source":"Issam Bazzi. 2002. Modelling out-of-vocabulary words for robust speech recognition. Ph.D. thesis, Massachusetts Institute of Technology."},{"authors":[{"first":"M.","last":"Bisani"},{"first":"H.","last":"Ney"}],"year":"2005","title":"Open vocabulary speech recognition with flat hybrid models","source":"M. Bisani and H. Ney. 2005. Open vocabulary speech recognition with flat hybrid models. In INTER-SPEECH."},{"authors":[{"first":"L.","last":"Burget"},{"first":"P.","last":"Schwarz"},{"first":"P.","last":"Matejka"},{"first":"M.","last":"Hannemann"},{"first":"A.","last":"Rastrow"},{"first":"C.","last":"White"},{"first":"S.","last":"Khudanpur"},{"first":"H.","last":"Hermansky"},{"first":"J.","last":"Cernocky"}],"year":"2008","title":"Combination of strongly and weakly constrained recognizers for reliable detection of OOVS","source":"L. Burget, P. Schwarz, P. Matejka, M. Hannemann, A. Rastrow, C. White, S. Khudanpur, H. Hermansky, and J. Cernocky. 2008. Combination of strongly and weakly constrained recognizers for reliable detection of OOVS. In ICASSP."},{"authors":[{"first":"D.","last":"Can"},{"first":"E.","last":"Cooper"},{"first":"A.","last":"Sethy"},{"first":"M.","last":"Saraclar"},{"first":"C.","last":"White"}],"year":"2009","title":"Effect of pronounciations on OOV queries in spoken term detection","source":"D. Can, E. Cooper, A. Sethy, M. Saraclar, and C. White. 2009. Effect of pronounciations on OOV queries in spoken term detection. Proceedings of ICASSP."},{"authors":[{"first":"Stanley","middle":"F.","last":"Chen"}],"year":"2003","title":"Conditional and joint models for grapheme-to-phoneme conversion","source":"Stanley F. Chen. 2003. Conditional and joint models for grapheme-to-phoneme conversion. In Eurospeech, pages 2033–2036."},{"authors":[{"first":"G.","last":"Choueiter"}],"year":"2009","title":"Linguistically-motivated sub-word modeling with applications to speech recognition","source":"G. Choueiter. 2009. Linguistically-motivated sub-word modeling with applications to speech recognition. Ph.D. thesis, Massachusetts Institute of Technology."},{"authors":[{"first":"Jonathan","last":"Fiscus"},{"first":"John","last":"Garofolo"},{"first":"Mark","last":"Przybocki"},{"first":"William","last":"Fisher"},{"first":"David","last":"Pallett"}],"year":"1998","title":"1997 English Broadcast News Speech (HUB4)","source":"Jonathan Fiscus, John Garofolo, Mark Przybocki, William Fisher, and David Pallett, 1998. 1997 English Broadcast News Speech (HUB4). Linguistic Data Consortium, Philadelphia."},{"authors":[{"first":"James","last":"Glass"},{"first":"Timothy","last":"Hazen"},{"first":"Lee","last":"Hetherington"},{"first":"Chao","last":"Wang"}],"year":"2010","title":"Analysis and processing of lecture audio data: Preliminary investigations","source":"James Glass, Timothy Hazen, Lee Hetherington, and Chao Wang. 2010. Analysis and processing of lecture audio data: Preliminary investigations. In North American Chapter of the Association for Computational Linguistics (NAACL)."},{"authors":[{"first":"Dietrich","last":"Klakow"},{"first":"Georg","last":"Rose"},{"first":"Xavier","last":"Aubert"}],"year":"1999","title":"OOV-detection in large vocabulary system using automatically defined word-fragments as fillers","source":"Dietrich Klakow, Georg Rose, and Xavier Aubert. 1999. OOV-detection in large vocabulary system using automatically defined word-fragments as fillers. In Eurospeech."},{"authors":[{"first":"Hui","last":"Lin"},{"first":"J.","last":"Bilmes"},{"first":"D.","last":"Vergyri"},{"first":"K.","last":"Kirchhoff"}],"year":"2007","title":"OOV detection by joint word/phone lattice alignment","source":"Hui Lin, J. Bilmes, D. Vergyri, and K. Kirchhoff. 2007. OOV detection by joint word/phone lattice alignment. In ASRU, pages 478–483, Dec."},{"authors":[{"first":"Jonathan","last":"Mamou"},{"first":"Bhuvana","last":"Ramabhadran"},{"first":"Olivier","last":"Siohan"}],"year":"2007","title":"Vocabulary independent spoken term detection","source":"Jonathan Mamou, Bhuvana Ramabhadran, and Olivier Siohan. 2007. Vocabulary independent spoken term detection. In Proceedings of SIGIR."},{"authors":[{"first":"L.","last":"Mangu"},{"first":"E.","last":"Brill"},{"first":"A.","last":"Stolcke"}],"year":"1999","title":"Finding consensus among words","source":"L. Mangu, E. Brill, and A. Stolcke. 1999. Finding consensus among words. In Eurospeech."},{"authors":[{"first":"A.","last":"Martin"},{"first":"G.","last":"Doddington"},{"first":"T.","last":"Kamm"},{"first":"M.","last":"Ordowski"},{"first":"M.","last":"Przybocky"}],"year":"1997","title":"The det curve in assessment of detection task performance","source":"A. Martin, G. Doddington, T. Kamm, M. Ordowski, and M. Przybocky. 1997. The det curve in assessment of detection task performance. In Eurospeech."},{"authors":[{"first":"Carolina","last":"Parada"},{"first":"Abhinav","last":"Sethy"},{"first":"Bhuvana","last":"Ramabhadran"}],"year":"2009","title":"Query-by-example spoken term detection for oov terms","source":"Carolina Parada, Abhinav Sethy, and Bhuvana Ramabhadran. 2009. Query-by-example spoken term detection for oov terms. In ASRU."},{"authors":[{"first":"Carolina","last":"Parada"},{"first":"Mark","last":"Dredze"},{"first":"Denis","last":"Filimonov"},{"first":"Fred","last":"Jelinek"}],"year":"2010","title":"Contextual information improves oov detection in speech","source":"Carolina Parada, Mark Dredze, Denis Filimonov, and Fred Jelinek. 2010. Contextual information improves oov detection in speech. In North American Chapter of the Association for Computational Linguistics (NAACL)."},{"authors":[{"first":"H.","last":"Poon"},{"first":"C.","last":"Cherry"},{"first":"K.","last":"Toutanova"}],"year":"2009","title":"Unsupervised morphological segmentation with log-linear models","source":"H. Poon, C. Cherry, and K. Toutanova. 2009. Unsupervised morphological segmentation with log-linear models. In ACL."},{"authors":[{"first":"Ariya","last":"Rastrow"},{"first":"Abhinav","last":"Sethy"},{"first":"Bhuvana","last":"Ramabhadran"}],"year":"2009a","title":"A new method for OOV detection using hybrid word/fragment system","source":"Ariya Rastrow, Abhinav Sethy, and Bhuvana Ramabhadran. 2009a. A new method for OOV detection using hybrid word/fragment system. Proceedings of ICASSP."},{"authors":[{"first":"Ariya","last":"Rastrow"},{"first":"Abhinav","last":"Sethy"},{"first":"Bhuvana","last":"Ramabhadran"},{"first":"Fred","last":"Jelinek"}],"year":"2009b","title":"Towards using hybrid, word, and fragment units for vocabulary independent LVCSR systems","source":"Ariya Rastrow, Abhinav Sethy, Bhuvana Ramabhadran, and Fred Jelinek. 2009b. Towards using hybrid, word, and fragment units for vocabulary independent LVCSR systems. INTERSPEECH."},{"authors":[{"first":"T.","last":"Schaaf"}],"year":"2001","title":"Detection of OOV words using generalized word models and a semantic class language model","source":"T. Schaaf. 2001. Detection of OOV words using generalized word models and a semantic class language model. In Eurospeech."},{"authors":[{"first":"H.","last":"Soltau"},{"first":"B.","last":"Kingsbury"},{"first":"L.","last":"Mangu"},{"first":"D.","last":"Povey"},{"first":"G.","last":"Saon"},{"first":"G.","last":"Zweig"}],"year":"2005","title":"The ibm 2004 conversational telephony system for rich transcription","source":"H. Soltau, B. Kingsbury, L. Mangu, D. Povey, G. Saon, and G. Zweig. 2005. The ibm 2004 conversational telephony system for rich transcription. In ICASSP."},{"authors":[{"first":"H.","last":"Sun"},{"first":"G.","last":"Zhang"},{"first":"f.","last":"Zheng"},{"first":"M.","last":"Xu"}],"year":"2001","title":"Using word confidence measure for OOV words detection in a spontaneous spoken dialog system","source":"H. Sun, G. Zhang, f. Zheng, and M. Xu. 2001. Using word confidence measure for OOV words detection in a spontaneous spoken dialog system. In Eurospeech."},{"authors":[{"first":"Stanley","last":"Wang"}],"year":"2009","title":"Using graphone models in automatic speech recognition","source":"Stanley Wang. 2009. Using graphone models in automatic speech recognition. Master’s thesis, Massachusetts Institute of Technology."},{"authors":[{"first":"F.","last":"Wessel"},{"first":"R.","last":"Schluter"},{"first":"K.","last":"Macherey"},{"first":"H.","last":"Ney"}],"year":"2001","title":"Confidence measures for large vocabulary continuous speech recognition","source":"F. Wessel, R. Schluter, K. Macherey, and H. Ney. 2001. Confidence measures for large vocabulary continuous speech recognition. IEEE Transactions on Speech and Audio Processing, 9(3)."},{"authors":[{"first":"Christopher","last":"White"},{"first":"Jasha","last":"Droppo"},{"first":"Alex","last":"Acero"},{"first":"Julian","last":"Odell"}],"year":"2007","title":"Maximum entropy confidence estimation for speech recognition","source":"Christopher White, Jasha Droppo, Alex Acero, and Julian Odell. 2007. Maximum entropy confidence estimation for speech recognition. In ICASSP. 721"}],"cites":[{"style":0,"text":"Mamou et al., 2007","origin":{"pointer":"/sections/7/paragraphs/1","offset":815,"length":18},"authors":[{"last":"Mamou"},{"last":"al."}],"year":"2007","references":["/references/11"]},{"style":0,"text":"Parada et al., 2009","origin":{"pointer":"/sections/7/paragraphs/1","offset":835,"length":19},"authors":[{"last":"Parada"},{"last":"al."}],"year":"2009","references":["/references/14"]},{"style":0,"text":"Rastrow et al., 2009b","origin":{"pointer":"/sections/7/paragraphs/1","offset":919,"length":21},"authors":[{"last":"Rastrow"},{"last":"al."}],"year":"2009b","references":["/references/18"]},{"style":0,"text":"Parada et al., 2010","origin":{"pointer":"/sections/7/paragraphs/1","offset":1004,"length":19},"authors":[{"last":"Parada"},{"last":"al."}],"year":"2010","references":["/references/15"]},{"style":0,"text":"Rastrow et al., 2009a","origin":{"pointer":"/sections/7/paragraphs/2","offset":96,"length":21},"authors":[{"last":"Rastrow"},{"last":"al."}],"year":"2009a","references":["/references/17"]},{"style":0,"text":"Bazzi and Glass, 2001","origin":{"pointer":"/sections/7/paragraphs/2","offset":119,"length":21},"authors":[{"last":"Bazzi"},{"last":"Glass"}],"year":"2001","references":["/references/0"]},{"style":0,"text":"Bisani and Ney, 2005","origin":{"pointer":"/sections/7/paragraphs/2","offset":175,"length":20},"authors":[{"last":"Bisani"},{"last":"Ney"}],"year":"2005","references":["/references/2"]},{"style":0,"text":"Choueiter, 2009","origin":{"pointer":"/sections/7/paragraphs/2","offset":254,"length":15},"authors":[{"last":"Choueiter"}],"year":"2009","references":["/references/6"]},{"style":0,"text":"Bazzi, 2002","origin":{"pointer":"/sections/7/paragraphs/2","offset":335,"length":11},"authors":[{"last":"Bazzi"}],"year":"2002","references":["/references/1"]},{"style":0,"text":"Bisani and Ney, 2005","origin":{"pointer":"/sections/7/paragraphs/2","offset":364,"length":20},"authors":[{"last":"Bisani"},{"last":"Ney"}],"year":"2005","references":["/references/2"]},{"style":0,"text":"Rastrow et al., 2009a","origin":{"pointer":"/sections/7/paragraphs/3","offset":341,"length":21},"authors":[{"last":"Rastrow"},{"last":"al."}],"year":"2009a","references":["/references/17"]},{"style":0,"text":"Chen (2003)","origin":{"pointer":"/sections/8/paragraphs/5","offset":260,"length":11},"authors":[{"last":"Chen"}],"year":"2003","references":["/references/5"]},{"style":0,"text":"Poon et al. (2009)","origin":{"pointer":"/sections/8/paragraphs/5","offset":637,"length":18},"authors":[{"last":"Poon"},{"last":"al."}],"year":"2009","references":["/references/16"]},{"style":0,"text":"Poon et al. (2009)","origin":{"pointer":"/sections/8/paragraphs/7","offset":151,"length":18},"authors":[{"last":"Poon"},{"last":"al."}],"year":"2009","references":["/references/16"]},{"style":0,"text":"Bazzi, 2002","origin":{"pointer":"/sections/10/paragraphs/0","offset":297,"length":11},"authors":[{"last":"Bazzi"}],"year":"2002","references":["/references/1"]},{"style":0,"text":"Schaaf, 2001","origin":{"pointer":"/sections/10/paragraphs/0","offset":310,"length":12},"authors":[{"last":"Schaaf"}],"year":"2001","references":["/references/19"]},{"style":0,"text":"Bisani and Ney, 2005","origin":{"pointer":"/sections/10/paragraphs/0","offset":324,"length":20},"authors":[{"last":"Bisani"},{"last":"Ney"}],"year":"2005","references":["/references/2"]},{"style":0,"text":"Klakow et al., 1999","origin":{"pointer":"/sections/10/paragraphs/0","offset":346,"length":19},"authors":[{"last":"Klakow"},{"last":"al."}],"year":"1999","references":["/references/9"]},{"style":0,"text":"Wang, 2009","origin":{"pointer":"/sections/10/paragraphs/0","offset":367,"length":10},"authors":[{"last":"Wang"}],"year":"2009","references":["/references/22"]},{"style":0,"text":"Lin et al., 2007","origin":{"pointer":"/sections/10/paragraphs/0","offset":555,"length":16},"authors":[{"last":"Lin"},{"last":"al."}],"year":"2007","references":["/references/10"]},{"style":0,"text":"Burget et al., 2008","origin":{"pointer":"/sections/10/paragraphs/0","offset":573,"length":19},"authors":[{"last":"Burget"},{"last":"al."}],"year":"2008","references":["/references/3"]},{"style":0,"text":"Sun et al., 2001","origin":{"pointer":"/sections/10/paragraphs/0","offset":594,"length":16},"authors":[{"last":"Sun"},{"last":"al."}],"year":"2001","references":["/references/21"]},{"style":0,"text":"Wessel et al., 2001","origin":{"pointer":"/sections/10/paragraphs/0","offset":612,"length":19},"authors":[{"last":"Wessel"},{"last":"al."}],"year":"2001","references":["/references/23"]},{"style":0,"text":"Parada et al. (2010)","origin":{"pointer":"/sections/10/paragraphs/17","offset":196,"length":20},"authors":[{"last":"Parada"},{"last":"al."}],"year":"2010","references":["/references/15"]},{"style":0,"text":"Rastrow et al., 2009b","origin":{"pointer":"/sections/10/paragraphs/18","offset":306,"length":21},"authors":[{"last":"Rastrow"},{"last":"al."}],"year":"2009b","references":["/references/18"]},{"style":0,"text":"Chen, 2003","origin":{"pointer":"/sections/10/paragraphs/19","offset":401,"length":10},"authors":[{"last":"Chen"}],"year":"2003","references":["/references/5"]},{"style":0,"text":"Mangu et al., 1999","origin":{"pointer":"/sections/10/paragraphs/20","offset":524,"length":18},"authors":[{"last":"Mangu"},{"last":"al."}],"year":"1999","references":["/references/12"]},{"style":0,"text":"Parada et al. (2010)","origin":{"pointer":"/sections/10/paragraphs/21","offset":170,"length":20},"authors":[{"last":"Parada"},{"last":"al."}],"year":"2010","references":["/references/15"]},{"style":0,"text":"Can et al. (2009)","origin":{"pointer":"/sections/11/paragraphs/0","offset":36,"length":17},"authors":[{"last":"Can"},{"last":"al."}],"year":"2009","references":["/references/4"]},{"style":0,"text":"Soltau et al., 2005","origin":{"pointer":"/sections/11/paragraphs/1","offset":56,"length":19},"authors":[{"last":"Soltau"},{"last":"al."}],"year":"2005","references":["/references/20"]},{"style":0,"text":"Fiscus et al., 1998","origin":{"pointer":"/sections/11/paragraphs/2","offset":93,"length":19},"authors":[{"last":"Fiscus"},{"last":"al."}],"year":"1998","references":["/references/7"]},{"style":0,"text":"Rastrow et al., 2009a","origin":{"pointer":"/sections/11/paragraphs/5","offset":145,"length":21},"authors":[{"last":"Rastrow"},{"last":"al."}],"year":"2009a","references":["/references/17"]},{"style":0,"text":"Glass et al., 2010","origin":{"pointer":"/sections/11/paragraphs/6","offset":72,"length":18},"authors":[{"last":"Glass"},{"last":"al."}],"year":"2010","references":["/references/8"]},{"style":0,"text":"Rastrow et al. (2009a)","origin":{"pointer":"/sections/11/paragraphs/11","offset":668,"length":22},"authors":[{"last":"Rastrow"},{"last":"al."}],"year":"2009a","references":["/references/17"]},{"style":0,"text":"Martin et al., 1997","origin":{"pointer":"/sections/11/paragraphs/11","offset":1456,"length":19},"authors":[{"last":"Martin"},{"last":"al."}],"year":"1997","references":["/references/13"]},{"style":0,"text":"Parada et al. (2010)","origin":{"pointer":"/sections/11/paragraphs/11","offset":1672,"length":20},"authors":[{"last":"Parada"},{"last":"al."}],"year":"2010","references":["/references/15"]},{"style":0,"text":"Parada et al. (2010)","origin":{"pointer":"/sections/12/paragraphs/3","offset":61,"length":20},"authors":[{"last":"Parada"},{"last":"al."}],"year":"2010","references":["/references/15"]},{"style":0,"text":"Parada et al. (2010)","origin":{"pointer":"/sections/12/paragraphs/7","offset":33,"length":20},"authors":[{"last":"Parada"},{"last":"al."}],"year":"2010","references":["/references/15"]},{"style":0,"text":"White et al. (2007)","origin":{"pointer":"/sections/12/paragraphs/9","offset":109,"length":19},"authors":[{"last":"White"},{"last":"al."}],"year":"2007","references":["/references/24"]},{"style":0,"text":"Rastrow et al., 2009b","origin":{"pointer":"/sections/12/paragraphs/51","offset":89,"length":21},"authors":[{"last":"Rastrow"},{"last":"al."}],"year":"2009b","references":["/references/18"]}]}
