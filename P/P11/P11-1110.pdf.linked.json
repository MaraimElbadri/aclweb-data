{"sections":[{"title":"","paragraphs":["Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1098–1108, Portland, Oregon, June 19-24, 2011. c⃝2011 Association for Computational Linguistics"]},{"title":"Learning From Collective Human Behavior to Introduce Diversity in Lexical Choice Vahed Qazvinian Department of EECS University of Michigan Ann Arbor, MI vahed@umich.edu Dragomir R. Radev School of Information Department of EECS University of Michigan Ann Arbor, MI radev@umich.edu Abstract","paragraphs":["We analyze collective discourse, a collective human behavior in content generation, and show that it exhibits diversity, a property of general collective systems. Using extensive analysis, we propose a novel paradigm for designing summary generation systems that reflect the diversity of perspectives seen in reallife collective summarization. We analyze 50 sets of summaries written by human about the same story or artifact and investigate the diversity of perspectives across these summaries. We show how different summaries use various phrasal information units (i.e., nuggets) to express the same atomic semantic units, called factoids. Finally, we present a ranker that employs distributional similarities to build a network of words, and captures the diversity of perspectives by detecting communities in this network. Our experiments show how our system outperforms a wide range of other document ranking systems that leverage diversity."]},{"title":"1 Introduction","paragraphs":["In sociology, the term collective behavior is used to denote mass activities that are not centrally coordinated (Blumer, 1951). Collective behavior is different from group behavior in the following ways: (a) it involves limited social interaction, (b) membership is fluid, and (c) it generates weak and unconventional norms (Smelser, 1963). In this paper, we focus on the computational analysis of collective discourse, a collective behavior seen in interactive content contribution and text summarization in online social media. In collective discourse each individual’s behavior is largely independent of that of other individuals.","In social media, discourse (Grosz and Sidner, 1986) is often a collective reaction to an event. One scenario leading to collective reaction to a well-defined subject is when an event occurs (a movie is released, a story occurs, a paper is published) and people independently write about it (movie reviews, news headlines, citation sentences). This process of content generation happens over time, and each person chooses the aspects to cover. Each event has an onset and a time of death after which nothing is written about it. Tracing the generation of content over many instances will reveal temporal patterns that will allow us to make sense of the text generated around a particular event.","To understand collective discourse, we are interested in behavior that happens over a short period of time. We focus on topics that are relatively well-defined in scope such as a particular event or a single news event that does not evolve over time. This can eventually be extended to events and issues that are evolving either in time or scope such as elections, wars, or the economy.","In social sciences and the study of complex systems a lot of work has been done to study such collective systems, and their properties such as selforganization (Page, 2007) and diversity (Hong and Page, 2009; Fisher, 2009). However, there is little work that studies a collective system in which members individually write summaries.","In most of this paper, we will be concerned with developing a complex systems view of the set of collectively written summaries, and give evidence of 1098 the diversity of perspectives and its cause. We believe that out experiments will give insight into new models of text generation, which is aimed at model-ing the process of producing natural language texts, and is best characterized as the process of mak-ing choices between alternate linguistic realizations, also known as lexical choice (Elhadad, 1995; Barzilay and Lee, 2002; Stede, 1995)."]},{"title":"2 Prior Work","paragraphs":["In summarization, a number of previous methods have focused on diversity. (Mei et al., 2010) introduce a diversity-focused ranking methodology based on reinforced random walks in information networks. Their random walk model introduces the rich-gets-richer mechanism to PageRank with reinforcements on transition probabilities between vertices. A similar ranking model is the Grasshopper ranking model (Zhu et al., 2007), which leverages an absorbing random walk. This model starts with a regular time-homogeneous random walk, and in each step the node with the highest weight is set as an absorbing state. The multi-view point summarization of opinionated text is discussed in (Paul et al., 2010). Paul et al. introduce Comparative LexRank, based on the LexRank ranking model (Erkan and Radev, 2004). Their random walk formulation is to score sentences and pairs of sentences from opposite viewpoints (clusters) based on both their representativeness of the collection as well as their contrastiveness with each other. Once a lexical similarity graph is built, they modify the graph based on cluster information and perform LexRank on the modified cosine similarity graph.","The most well-known paper that address diversity in summarization is (Carbonell and Goldstein, 1998), which introduces Maximal Marginal Relevance (MMR). This method is based on a greedy algorithm that picks sentences in each step that are the least similar to the summary so far. There are a few other diversity-focused summarization systems like C-LexRank (Qazvinian and Radev, 2008), which employs document clustering. These papers try to increase diversity in summarizing documents, but do not explain the type of the diversity in their in-puts. In this paper, we give an insightful discussion on the nature of the diversity seen in collective discourse, and will explain why some of the mentioned methods may not work under such environments.","In prior work on evaluating independent contributions in content generation, Voorhees (Voorhees, 1998) studied IR systems and showed that relevance judgments differ significantly between humans but relative rankings show high degrees of stability across annotators. However, perhaps the closest work to this paper is (van Halteren and Teufel, 2004) in which 40 Dutch students and 10 NLP researchers were asked to summarize a BBC news report, resulting in 50 different summaries. Teufel and van Halteren also used 6 DUC1","-provided summaries, and annotations from 10 student participants and 4 additional researchers, to create 20 summaries for another news article in the DUC datasets. They calculated the Kappa statistic (Carletta, 1996; Krippendorff, 1980) and observed high agreement, indicating that the task of atomic semantic unit (factoid) extraction can be robustly performed in naturally occurring text, without any copy-editing.","The diversity of perspectives and the unprecedented growth of the factoid inventory also affects evaluation in text summarization. Evaluation methods are either extrinsic, in which the summaries are evaluated based on their quality in performing a specific task (Spärck-Jones, 1999) or intrinsic where the quality of the summary itself is evaluated, regardless of any applied task (van Halteren and Teufel, 2003; Nenkova and Passonneau, 2004). These evaluation methods assess the information content in the summaries that are generated automatically.","Finally, recent research on analyzing online social media shown a growing interest in mining news stories and headlines because of its broad applications ranging from “meme” tracking and spike detection (Leskovec et al., 2009) to text summarization (Barzilay and McKeown, 2005). In similar work on blogs, it is shown that detecting topics (Kumar et al., 2003; Adar et al., 2007) and sentiment (Pang and Lee, 2004) in the blogosphere can help identify influential bloggers (Adar et al., 2004; Java et al., 2006) and mine opinions about products (Mishne and Glance, 2006). 1 Document Understanding Conference 1099"]},{"title":"3 Data Annotation","paragraphs":["The datasets used in our experiments represent two completely different categories: news headlines, and scientific citation sentences. Theheadlines datasets consist of 25 clusters of news headlines collected from Google News2",", and the citations datasets have 25 clusters of citations to specific scientific papers from the ACL Anthology Network (AAN)3",". Each cluster consists of a number of unique summaries (headlines or citations) about the same artifact (nonevolving news story or scientific paper) written by different people. Table 1 lists some of the clusters with the number of summaries in them. ID type Name Story/Title # 1 hdl miss Miss Venezuela wins miss universe’09 125 2 hdl typhoon Second typhoon hit philippines 100 3 hdl russian Accident at Russian hydro-plant 101 4 hdl redsox Boston Red Sox win world series 99 5 hdl gervais “Invention of Lying” movie reviewed 97","· · · · · · · · · 25 hdl yale Yale lab tech in court 10 26 cit N03-1017 Statistical Phrase-Based Translation 172 27 cit P02-1006 Learning Surface Text Patterns ... 72 28 cit P05-1012 On-line Large-Margin Training ... 71 29 cit C96-1058 Three New Probabilistic Models ... 66 30 cit P05-1033 A Hierarchical Phrase-Based Model ... 65","· · · · · · · · · 50 cit H05-1047 A Semantic Approach to Recognizing ... 7 Table 1: Some of the annotated datasets and the number of summaries in each of them (hdl = headlines; cit = citations) 3.1 Nuggets vs. Factoids We define an annotation task that requires explicit definitions that distinguish between phrases that represent the same or different information units. Unfortunately, there is little consensus in the literature on such definitions. Therefore, we follow (van Halteren and Teufel, 2003) and make the following distinction. We define a nugget to be a phrasal information unit. Different nuggets may all represent the same atomic semantic unit, which we call as a factoid. In the following headlines, which are randomly extracted from the redsox dataset, nuggets are manually underlined. red sox win 2007 world series boston red sox blank rockies to clinch world series 2 news.google.com 3 http://clair.si.umich.edu/clair/anthology/","boston fans celebrate world series win; 37 arrests reported","These 3 headlines contain 9 nuggets, which represent 5 factoids or classes of equivalent nuggets. f1 : {red sox, boston, boston red sox} f2 : {2007 world series, world series win, world series} f3 : {rockies} f4 : {37 arrests} f5 : {fans celebrate}","This example suggests that different headlines on the same story written independently of one another use different phrases (nuggets) to refer to the same semantic unit (e.g., “red sox” vs. “boston” vs. “boston red sox”) or to semantic units corresponding to different aspects of the story (e.g., “37 arrests” vs. “rockies”). In the former case different nuggets are used to represent the same factoid, while in the latter case different nuggets are used to express different factoids. This analogy is similar to the definition of factoids in (van Halteren and Teufel, 2004).","The following citation sentences to Koehn’s work suggest that a similar phenomenon also happens in citations.","We also compared our model with pharaoh (Koehn et al, 2003).","Koehn et al (2003) find that phrases longer than three words improve performance little.","Koehn et al (2003) suggest limiting phrase length to three words or less.","For further information on these parameter settings, confer (koehn et al, 2003).","where the first author mentions “pharaoh” as a contribution of Koehn et al, but the second and third use different nuggets to represent the same contribution: use of trigrams. However, as the last citation shows, a citation sentence, unlike news headlines, may cover no information about the target paper.","The use of phrasal information as nuggets is an essential element to our experiments, since some headline writers often try to use uncommon terms to refer to a factoid. For instance, two headlines from the redsox cluster are: Short wait for bossox this time Soxcess started upstairs 1100","Following these examples, we asked two annotators to annotate all 1, 390 headlines, and 926 citations. The annotators were asked to follow precise guidelines in nugget extraction. Our guidelines instructed annotators to extract non-overlapping phrases from each headline as nuggets. Therefore, each nugget should be a substring of the headline that represents a semantic unit4",".","Previously (Lin and Hovy, 2002) had shown that information overlap judgment is a difficult task for human annotators. To avoid such a difficulty, we enforced our annotators to extract non-overlapping nuggets from a summary to make sure that they are mutually independent and that information overlap between them is minimized.","Finding agreement between annotated well-defined nuggets is straightforward and can be calculated in terms of Kappa. However, when nuggets themselves are to be extracted by annotators, the task becomes less obvious. To calculate the agreement, we annotated 10 randomly selected headline clusters twice and designed a simple evaluation scheme based on Kappa5",". For each n-gram, w, in a given headline, we look if w is part of any nugget in either human annotations. If w occurs in both or neither, then the two annotators agree on it, and otherwise they do not. Based on this agreement setup, we can formalize the κ statistic as κ =","Pr(a)−Pr(e)","1−Pr(e) where P r(a) is the relative observed agreement among annotators, and P r(e) is the probability that annotators agree by chance if each annotator is randomly assigning categories.","Table 2 shows the unigram, bigram, and trigram-based average κ between the two human annotators (Human1, Human2). These results suggest that human annotators can reach substantial agreement when bigram and trigram nuggets are examined, and has reasonable agreement for unigram nuggets."]},{"title":"4 Diversity","paragraphs":["We study the diversity of ways with which human summarizers talk about the same story or event and explain why such a diversity exists.","4","Before the annotations, we lower-cased all summaries and removed duplicates","5","Previously (Qazvinian and Radev, 2010) have shown high agreement in human judgments in a similar task on citation annotation","Average κ unigram bigram trigram","Human1 vs. Human2 0.76 ± 0.4 0.80 ± 0.4 0.89 ± 0.3 Table 2: Agreement between different annotators in terms of average Kappa in 25 headline clusters.","100 101","102 10−2 10−1 100 Pr(X ≥ c) c headlines  ","Pr(X ≥ c)","100 101","10210−2 10−1 100 Pr(X ≥ c) c citations  ","Pr(X ≥ c) Figure 1: The cumulative probability distribution for the frequency of factoids (i.e., the probability that a factoid will be mentioned in c different summaries) across in each category. 4.1 Skewed Distributions Our first experiment is to analyze the popularity of different factoids. For each factoid in the annotated clusters, we extract its count, X, which is equal to the number of summaries it has been mentioned in, and then we look at the distribution of X. Figure 1 shows the cumulative probability distribution for these counts (i.e., the probability that a factoid will be mentioned in at least c different summaries) in both categories.","These highly skewed distributions indicate that a large number of factoids (more than 28%) are only mentioned once across different clusters (e.g., “poor pitching of colorado” in the redsox cluster), and that a few factoids are mentioned in a large number of headlines (likely using different nuggets). The large number of factoids that are only mentioned in one headline indicates that different summarizers increase diversity by focusing on different aspects of a story or a paper. The set of nuggets also exhibit similar skewed distributions. If we look at individual nuggets, the redsox set shows that about 63 (or 80%) of the nuggets get mentioned in only one headline, resulting in a right-skewed distribution.","The factoid analysis of the datasets reveals two main causes for the content diversity seen in headlines: (1) writers focus on different aspects of the story and therefore write about different factoids 1101 (e.g., “celebrations” vs. “poor pitching of colorado”). (2) writer use different nuggets to represent the same factoid (e.g., “redsox” vs. “bosox”). In the following sections we analyze the extent at which each scenario happens.","100","101","102","1030 200 400 600 800 1000 number of summaries Inventory size headlines   Nuggets Factoids","100","101","102","1030 50 100 150 200 250 300 350 number of summaries Inventory size citations   Nuggets Factoids Figure 2: The number of unique factoids and nuggets observed by reading n random summaries in all the clusters of each category 4.2 Factoid Inventory The emergence of diversity in covering different factoids suggests that looking at more summaries will capture a larger number of factoids. In order to analyze the growth of the factoid inventory, we perform a simple experiment. We shuffle the set of summaries from all 25 clusters in each category, and then look at the number of unique factoids and nuggets seen after reading nth","summary. This number shows the amount of information that a randomly selected subset of n writers represent. This is important to study in order to find out whether we need a large number of summaries to capture all aspects of a story and build a complete factoid inventory. The plot in Figure 4.1 shows, at each n, the number of unique factoids and nuggets observed by reading n random summaries from the 25 clusters in each category. These curves are plotted on a semi-log scale to emphasize the difference between the growth patterns of the nugget inventories and the factoid inventories6",".","This finding numerically confirms a similar observation on human summary annotations discussed in (van Halteren and Teufel, 2003; van Halteren and Teufel, 2004). In their work, van Halteren and Teufel indicated that more than 10-20 human summaries are needed for a full factoid inventory. However, our experiments with nuggets of nearly 2, 400 independent human summaries suggest that neither the nugget inventory nor the number of factoids will be likely to show asymptotic behavior. However, these plots show that the nugget inventory grows at a much faster rate than factoids. This means that a lot of the diversity seen in human summarization is a result of the so called different lexical choices that represent the same semantic units or factoids. 4.3 Summary Quality In previous sections we gave evidence for the diversity seen in human summaries. However, a more important question to answer is whether these summaries all cover important aspects of the story. Here, we examine the quality of these summaries, study the distribution of information coverage in them, and investigate the number of summaries required to build a complete factoid inventory.","The information covered in each summary can be determined by the set of factoids (and not nuggets) and their frequencies across the datasets. For example, in the redsox dataset, “red sox”, “boston”, and “boston red sox” are nuggets that all represent the same piece of information: the red sox team. Therefore, different summaries that use these nuggets to refer to the red sox team should not be seen as very different.","We use the Pyramid model (Nenkova and Passonneau, 2004) to value different summary factoids. Intuitively, factoids that are mentioned more frequently are more salient aspects of the story. Therefore, our pyramid model uses the normalized frequency at which a factoid is mentioned across a dataset as its weight. In the pyramid model, the individual factoids fall in tiers. If a factoid appears in more summaries, it falls in a higher tier. In principle, if the term wi appears |wi| times in the set of 6 Similar experiment using individual clusters exhibit similar","behavior 1102 headlines it is assigned to the tier T|wi|. The pyramid score that we use is computed as follows. Suppose the pyramid has n tiers, Ti, where tier Tn is the top tier and T1 is the bottom. The weight of the factoids in tier Ti will be i (i.e. they appeared in i summaries). If |Ti| denotes the number of factoids in tier Ti, and Di is the number of factoids in the summary that appear in Ti, then the total factoid weight for the summary is D =","∑n","i=1 i × Di. Ad-","ditionally, the optimal pyramid score for a summary","is M ax = ∑n","i=1 i × |Ti|. Finally, the pyramid score","for a summary can be calculated as P = D M ax","Based on this scoring scheme, we can use the annotated datasets to determine the quality of individual headlines. First, for each set we look at the variation in pyramid scores that individual summaries obtain in their set. Figure 3 shows, for each cluster, the variation in the pyramid scores (25th to 75th percentile range) of individual summaries evaluated against the factoids of that cluster. This figure indicates that the pyramid score of almost all summaries obtain values with high variations in most of the clusters For instance, individual headlines from redsox obtain pyramid scores as low as 0.00 and as high as 0.93. This high variation confirms the previous observations on diversity of information coverage in different summaries.","Additionally, this figure shows that headlines generally obtain higher values than citations when considered as summaries. One reason, as explained be-fore, is that a citation may not cover any important contribution of the paper it is citing, when headlines generally tend to cover some aspects of the story.","High variation in quality means that in order to capture a larger information content we need to read a greater number of summaries. But how many headlines should one read to capture a desired level of information content? To answer this question, we perform an experiment based on drawing random summaries from the pool of all the clusters in each category. We perform a Monte Carlo simulation, in which for each n, we draw n random summaries, and look at the pyramid score achieved by reading these headlines. The pyramid score is calculated using the factoids from all 25 clusters in each category7",". Each experiment is repeated 1, 000 times to find the statistical significance of the experiment and the variation from the average pyramid scores.","Figure 4.3 shows the average pyramid scores over different n values in each category on a log-log scale. This figure shows how pyramid score grows and approaches 1.00 rapidly as more randomly selected summaries are seen.","100","101","102","10310−2 10−1 100 number of summaries Pyramid Score   headlines citations Figure 4: Average pyramid score obtained by reading n random summaries shows rapid asymptotic behavior."]},{"title":"5 Diversity-based Ranking","paragraphs":["In previous sections we showed that the diversity seen in human summaries could be according to different nuggets or phrases that represent the same factoid. Ideally, a summarizer that seeks to increase diversity should capture this phenomenon and avoid covering redundant nuggets. In this section, we use different state of the art summarization systems to rank the set of summaries in each cluster with respect to information content and diversity. To evaluate each system, we cut the ranked list at a constant length (in terms of the number of words) and calculate the pyramid score of the remaining text. 5.1 Distributional Similarity We have designed a summary ranker that will produce a ranked list of documents with respect to the diversity of their contents. Our model works based on ranking individual words and using the ranked list of words to rank documents that contain them.","In order to capture the nuggets of equivalent semantic classes, we use a distributional similarity of 7 Similar experiment using individual clusters exhibit similar","results 1103 0 0.2 0.4 0.6 0.8 1","abortion","amazon","babies","burger","colombia","england","gervais","google","ireland","maine","mercury","miss","monkey","mozart","nobel","priest","ps3slim","radiation","redsox","russian","scientist","soupy","sweden","typhoon yale A00_1023","A00_1043","A00_2024","C00_1072","C96_1058","D03_1017","D04_9907","H05_1047","H05_1079","J04_4002","N03_1017","N04_1033","P02_1006","P03_1001","P05_1012","P05_1013","P05_1014","P05_1033","P97_1003","P99_1065","W00_0403","W00_0603","W03_0301","W03_0510","W05_1203 Pyramid Score   headlines citations Figure 3: The 25th to 75th percentile pyramid score range in individual clusters words that is inspired by (Lee, 1999). We represent each word by its context in the cluster and find the similarity of such contexts. Particularly, each word wi is represented by a bag of words, li, that have a surface distance of 3 or smaller to wi anywhere in the cluster. In other words, li contains any word that co-occurs with wi in a 4-gram in the cluster. This bag of words representation of words enables us to find the word-pair similarities. sim(wi, wj) = ⃗li · ⃗lj √ |⃗li||⃗lj| (1)","We use the pair-wise similarities of words in each cluster, and build a network of words and their similarities. Intuitively, words that appear in similar contexts are more similar to each other and will have a stronger edge between them in the network. Therefore, similar words, or words that appear in similar contexts, will form communities in this graph. Ideally, each community in the word similarity network would represent a factoid. To find the communities in the word network we use (Clauset et al., 2004), a hierarchical agglomeration algorithm which works by greedily optimizing the modularity in a linear running time for sparse graphs.","The community detection algorithm will assign to each word wi, a community label Ci. For each community, we use LexRank to rank the words using the similarities in Equation 1, and assign a score to each word wi as S(wi) = Ri","|Ci| , where Ri is the rank of wi in its community, and |Ci| is the number of words that belong to Ci. Figure 5.1 shows part police second sox celebrations red jump baseball unhappy sweeps pitching hittingarrest victorytitle dynasty fan poorer 2nd poor glory Pajek Figure 5: Part of the word similarity graph in the redsox cluster of the word similarity graph in the redsox cluster, in which each node is color-coded with its community. This figure illustrates how words that are semantically related to the same aspects of the story fall in the same communities (e.g., “police” and “arrest”). Finally, to rank sentences, we define the score of each document Dj as the sum of the scores of its words. pds(Dj) = ∑ wi∈Dj S(wi)","Intuitively, sentences that contain higher ranked words in highly populated communities will have a smaller score. To rank the sentences, we sort them in an ascending order, and cut the list when its size is greater than the length limit. 5.2 Other Methods 5.2.1 Random","For each cluster in each category (citations and headlines), this method simply gets a random per-1104 mutations of the summaries. In the headlines datasets, where most of the headlines cover some factoids about the story, we expect this method to perform reasonably well since randomization will increase the chances of covering headlines that focus on different factoids. However, in the citations dataset, where a citing sentence may cover no information about the cited paper, randomization has the drawback of selecting citations that have no valuable information in them. 5.2.2 LexRank","LexRank (Erkan and Radev, 2004) works by first building a graph of all the documents (Di) in a cluster. The edges between corresponding nodes (di) represent the cosine similarity between them is above a threshold (0.10 following (Erkan and Radev, 2004)). Once the network is built, the system finds the most central sentences by performing a random walk on the graph. p(dj) = (1 − λ) 1 |D| + λ ∑ di p(di)P (di → dj) (2) 5.2.3 MMR","Maximal Marginal Relevance (MMR) (Carbonell and Goldstein, 1998) uses the pairwise cosine similarity matrix and greedily chooses sentences that are the least similar to those already in the summary. In particular,","M M R = arg minDi∈D−A [ maxDj∈A Sim(Di, Dj) ] where A is the set of documents in the summary, initialized to A = ∅. 5.2.4 DivRank","Unlike other time-homogeneous random walks (e.g., PageRank), DivRank does not assume that the transition probabilities remain constant over time. DivRank uses a vertex-reinforced random walk model to rank graph nodes based on a diversity based centrality. The basic assumption in DivRank is that the transition probability from a node to other is reinforced by the number of previous visits to the target node (Mei et al., 2010). Particularly, let’s assume pT (u, v) is the transition probability from any node u to node v at time T . Then,","pT (di, dj) = (1 − λ).p∗ (d j) + λ.","p0(di, dj).NT (dj) DT (di) (3)","where NT (dj) is the number of times the walk has visited dj up to time T and DT (di) = ∑ dj∈V p0(di, dj)NT (dj) (4) Here, p∗","(dj) is the prior distribution that deter-","mines the preference of visiting vertex dj. We try","two variants of this algorithm: DivRank, in which","p∗ (dj) is uniform, and DivRank with priors in","which p∗","(dj) ∝ l(Dj)−β",", where l(D","j) is the number of the words in the document Dj and β is a parameter (β = 0.8). 5.2.5 C-LexRank","C-LexRank is a clustering-based model in which the cosine similarities of document pairs are used to build a network of documents. Then the the network is split into communities, and the most salient documents in each community are selected (Qazvinian and Radev, 2008). C-LexRank focuses on finding communities of documents using their cosine similarity. The intuition is that documents that are more similar to each other contain similar factoids. We expect C-LexRank to be a strong ranker, but incapable of capturing the diversity caused by using different phrases to express the same meaning. The reason is that different nuggets that represent the same factoid often have no words in common (e.g., “victory” and “glory”) and won’t be captured by a lexical measure like cosine similarity. 5.3 Experiments We use each of the systems explained above to rank the summaries in each cluster. Each ranked list is then cut at a certain length (50 words for headlines, and 150 for citations) and the information content in the remaining text is examined using the pyramid score.","Table 3 shows the average pyramid score achieved by different methods in each category. The method based on the distributional similarities of words outperforms other methods in the citations category. All methods show similar results in the headlines category, where most headlines cover at least 1 factoid about the story and a random ranker performs reasonably well. Table 4 shows top 3 headlines from 3 rankers: word distributional similarity (WDS), C-LexRank, and MMR. In this example, the first 3 1105 Method","headlines citations Mean","pyramid 95% C.I. pyramid 95% C.I. R 0.928 [0.896, 0.959] 0.716 [0.625, 0.807] 0.822 MMR 0.930 [0.902, 0.960] 0.766 [0.684, 0.847] 0.848 LR 0.918 [0.891, 0.945] 0.728 [0.635, 0.822] 0.823 DR 0.927 [0.900, 0.955] 0.736 [0.667, 0.804] 0.832 DR(p) 0.916 [0.884, 0.949] 0.764 [0.697, 0.831] 0.840 C-LR 0.942 [0.919, 0.965] 0.781 [0.710, 0.852] 0.862 WDS 0.931 [0.905, 0.958] 0.813 [0.738, 0.887] 0.872 R=Random; LR=LexRank; DR=DivRank; DR(p)=DivRank with Priors; C-LR=C-LexRank; WDS=Word Distributional Similarity; C.I.=Confidence In-terval Table 3: Comparison of different ranking systems Method Top 3 headlines WDS 1: how sweep it is 2: fans celebrate red sox win 3: red sox take title C-LR 1: world series: red sox sweep rockies 2: red sox take world series 3: red sox win world series MMR 1:red sox scale the rockies 2: boston sweep colorado to win world series 3: rookies respond in first crack at the big time","C-LR=C-LexRank; WDS=Word Distributional Similarity Table 4: Top 3 ranked summaries of the redsox cluster using different methods headlines produced by WDS cover two important factoids: “red sox winning the title” and “fans celebrating”. However, the second factoid is absent in the other two."]},{"title":"6 Conclusion and Future Work","paragraphs":["Our experiments on two different categories of human-written summaries (headlines and citations) showed that a lot of the diversity seen in human summarization comes from different nuggets that may actually represent the same semantic information (i.e., factoids). We showed that the factoids exhibit a skewed distribution model, and that the size of the nugget inventory asymptotic behavior even with a large number of summaries. We also showed high variation in summary quality across different summaries in terms of pyramid score, and that the information covered by reading n summaries has a rapidly growing asymptotic behavior as n increases. Finally, we proposed a ranking system that employs word distributional similarities to identify semantically equivalent words, and compared it with a wide range of summarization systems that leverage diversity.","In the future, we plan to move to content from other collective systems on Web. In order to generalize our findings, we plan to examine blog comments, online reviews, and tweets (that discuss the same URL). We also plan to build a generation system that employs the Yule model (Yule, 1925) to determine the importance of each aspect (e.g. who, when, where, etc.) in order to produce summaries that include diverse aspects of a story.","Our work has resulted in a publicly available dataset 8","of 25 annotated news clusters with nearly 1, 400 headlines, and 25 clusters of citation sentences with more than 900 citations. We believe that this dataset can open new dimensions in studying diversity and other aspects of automatic text generation."]},{"title":"7 Acknowledgments","paragraphs":["This work is supported by the National Science Foundation grant number IIS-0705832 and grant number IIS-0968489. Any opinions, findings, and conclusions or recommendations expressed in this paper are those of the authors and do not necessarily reflect the views of the supporters."]},{"title":"References","paragraphs":["Eytan Adar, Li Zhang, Lada A. Adamic, and Rajan M. Lukose. 2004. Implicit structure and the dynamics of 8 http://www-personal.umich.edu/ṽahed/ data.html 1106 Blogspace. In WWW’04, Workshop on the Weblogging Ecosystem.","Eytan Adar, Daniel S. Weld, Brian N. Bershad, and Steven S. Gribble. 2007. Why we search: visualiz-ing and predicting user behavior. In WWW’07, pages 161–170, New York, NY, USA.","Regina Barzilay and Lillian Lee. 2002. Bootstrapping lexical choice via multiple-sequence alignment. In Proceedings of the ACL-02 conference on Empirical methods in natural language processing - Volume 10, EMNLP ’02, pages 164–171.","Regina Barzilay and Kathleen R. McKeown. 2005. Sentence fusion for multidocument news summarization. Comput. Linguist., 31(3):297–328.","Herbert Blumer. 1951. Collective behavior. In Lee, Alfred McClung, Ed., Principles of Sociology.","Jaime G. Carbonell and Jade Goldstein. 1998. The use of MMR, diversity-based reranking for reordering documents and producing summaries. In SIGIR’98, pages 335–336.","Jean Carletta. 1996. Assessing agreement on classification tasks: the kappa statistic. Comput. Linguist., 22(2):249–254.","Aaron Clauset, Mark E. J. Newman, and Cristopher Moore. 2004. Finding community structure in very large networks. Phys. Rev. E, 70(6).","Michael Elhadad. 1995. Using argumentation in text generation. Journal of Pragmatics, 24:189–220.","GüneşErkan and Dragomir R. Radev. 2004. Lexrank: Graph-based centrality as salience in text summarization. Journal of Artificial Intelligence Research (JAIR).","Len Fisher. 2009. The Perfect Swarm: The Science of Complexity in Everyday Life. Basic Books.","Barbara J. Grosz and Candace L. Sidner. 1986. Atten-tion, intentions, and the structure of discourse. Comput. Linguist., 12:175–204, July.","Lu Hong and Scott Page. 2009. Interpreted and generated signals. Journal of Economic Theory, 144(5):2174–2196.","Akshay Java, Pranam Kolari, Tim Finin, and Tim Oates. 2006. Modeling the spread of influence on the blogosphere. In WWW’06.","Klaus Krippendorff. 1980. Content Analysis: An Introduction to its Methodology. Beverly Hills: Sage Publications.","Ravi Kumar, Jasmine Novak, Prabhakar Raghavan, and Andrew Tomkins. 2003. On the bursty evolution of blogspace. In WWW’03, pages 568–576, New York, NY, USA.","Lillian Lee. 1999. Measures of distributional similarity. In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics, pages 25–32.","Jure Leskovec, Lars Backstrom, and Jon Kleinberg. 2009. Meme-tracking and the dynamics of the news cycle. In KDD ’09: Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 497–506.","Chin-Yew Lin and Eduard Hovy. 2002. Manual and automatic evaluation of summaries. In ACL-Workshop on Automatic Summarization.","Qiaozhu Mei, Jian Guo, and Dragomir Radev. 2010. Divrank: the interplay of prestige and diversity in information networks. In Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 1009–1018.","Gilad Mishne and Natalie Glance. 2006. Predicting movie sales from blogger sentiment. In AAAI 2006 Spring Symposium on Computational Approaches to Analysing Weblogs (AAAI-CAAW 2006).","Ani Nenkova and Rebecca Passonneau. 2004. Evaluating content selection in summarization: The pyramid method. Proceedings of the HLT-NAACL conference.","Scott E. Page. 2007. The Difference: How the Power of Diversity Creates Better Groups, Firms, Schools, and Societies. Princeton University Press.","Bo Pang and Lillian Lee. 2004. A sentimental educa-tion: sentiment analysis using subjectivity summarization based on minimum cuts. In ACL’04, Morristown, NJ, USA.","Michael Paul, ChengXiang Zhai, and Roxana Girju. 2010. Summarizing contrastive viewpoints in opinionated text. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Process-ing, pages 66–76.","Vahed Qazvinian and Dragomir R. Radev. 2008. Scientific paper summarization using citation summary net-works. In COLING 2008, Manchester, UK.","Vahed Qazvinian and Dragomir R. Radev. 2010. Identifying non-explicit citing sentences for citation-based summarization. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 555–564, Uppsala, Sweden, July. Association for Computational Linguistics.","Neil J. Smelser. 1963. Theory of Collective Behavior. Free Press.","Karen Spärck-Jones. 1999. Automatic summarizing: factors and directions. In Inderjeet Mani and Mark T. Maybury, editors, Advances in automatic text summarization, chapter 1, pages 1 – 12. The MIT Press.","Manfred Stede. 1995. Lexicalization in natural language generation: a survey. Artificial Intelligence Review, (8):309–336.","Hans van Halteren and Simone Teufel. 2003. Examin-ing the consensus between human summaries: initial experiments with factoid analysis. In Proceedings of 1107 the HLT-NAACL 03 on Text summarization workshop, pages 57–64, Morristown, NJ, USA. Association for Computational Linguistics.","Hans van Halteren and Simone Teufel. 2004. Evaluating information content by factoid analysis: human annotation and stability. In EMNLP’04, Barcelona.","Ellen M. Voorhees. 1998. Variations in relevance judgments and the measurement of retrieval effectiveness. In SIGIR ’98: Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 315–323.","G. Udny Yule. 1925. A mathematical theory of evolution, based on the conclusions of dr. j. c. willis, f.r.s. Philosophical Transactions of the Royal Society of London. Series B, Containing Papers of a Biological Character, 213:21–87.","Xiaojin Zhu, Andrew Goldberg, Jurgen Van Gael, and David Andrzejewski. 2007. Improving diversity in ranking using absorbing random walks. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 97–104. 1108"]}],"references":[{"authors":[{"first":"Eytan","last":"Adar"},{"first":"Li","last":"Zhang"},{"first":"Lada","middle":"A.","last":"Adamic"},{"first":"Rajan","middle":"M.","last":"Lukose"}],"year":"2004","title":"Implicit structure and the dynamics of 8 http://www-personal","source":"Eytan Adar, Li Zhang, Lada A. Adamic, and Rajan M. Lukose. 2004. Implicit structure and the dynamics of 8 http://www-personal.umich.edu/ṽahed/ data.html 1106 Blogspace. In WWW’04, Workshop on the Weblogging Ecosystem."},{"authors":[{"first":"Eytan","last":"Adar"},{"first":"Daniel","middle":"S.","last":"Weld"},{"first":"Brian","middle":"N.","last":"Bershad"},{"first":"Steven","middle":"S.","last":"Gribble"}],"year":"2007","title":"Why we search: visualiz-ing and predicting user behavior","source":"Eytan Adar, Daniel S. Weld, Brian N. Bershad, and Steven S. Gribble. 2007. Why we search: visualiz-ing and predicting user behavior. In WWW’07, pages 161–170, New York, NY, USA."},{"authors":[{"first":"Regina","last":"Barzilay"},{"first":"Lillian","last":"Lee"}],"year":"2002","title":"Bootstrapping lexical choice via multiple-sequence alignment","source":"Regina Barzilay and Lillian Lee. 2002. Bootstrapping lexical choice via multiple-sequence alignment. In Proceedings of the ACL-02 conference on Empirical methods in natural language processing - Volume 10, EMNLP ’02, pages 164–171."},{"authors":[{"first":"Regina","last":"Barzilay"},{"first":"Kathleen","middle":"R.","last":"McKeown"}],"year":"2005","title":"Sentence fusion for multidocument news summarization","source":"Regina Barzilay and Kathleen R. McKeown. 2005. Sentence fusion for multidocument news summarization. Comput. Linguist., 31(3):297–328."},{"authors":[{"first":"Herbert","last":"Blumer"}],"year":"1951","title":"Collective behavior","source":"Herbert Blumer. 1951. Collective behavior. In Lee, Alfred McClung, Ed., Principles of Sociology."},{"authors":[{"first":"Jaime","middle":"G.","last":"Carbonell"},{"first":"Jade","last":"Goldstein"}],"year":"1998","title":"The use of MMR, diversity-based reranking for reordering documents and producing summaries","source":"Jaime G. Carbonell and Jade Goldstein. 1998. The use of MMR, diversity-based reranking for reordering documents and producing summaries. In SIGIR’98, pages 335–336."},{"authors":[{"first":"Jean","last":"Carletta"}],"year":"1996","title":"Assessing agreement on classification tasks: the kappa statistic","source":"Jean Carletta. 1996. Assessing agreement on classification tasks: the kappa statistic. Comput. Linguist., 22(2):249–254."},{"authors":[{"first":"Aaron","last":"Clauset"},{"first":"Mark","middle":"E. J.","last":"Newman"},{"first":"Cristopher","last":"Moore"}],"year":"2004","title":"Finding community structure in very large networks","source":"Aaron Clauset, Mark E. J. Newman, and Cristopher Moore. 2004. Finding community structure in very large networks. Phys. Rev. E, 70(6)."},{"authors":[{"first":"Michael","last":"Elhadad"}],"year":"1995","title":"Using argumentation in text generation","source":"Michael Elhadad. 1995. Using argumentation in text generation. Journal of Pragmatics, 24:189–220."},{"authors":[{"last":"GüneşErkan"},{"first":"Dragomir","middle":"R.","last":"Radev"}],"year":"2004","title":"Lexrank: Graph-based centrality as salience in text summarization","source":"GüneşErkan and Dragomir R. Radev. 2004. Lexrank: Graph-based centrality as salience in text summarization. Journal of Artificial Intelligence Research (JAIR)."},{"authors":[{"first":"Len","last":"Fisher"}],"year":"2009","title":"The Perfect Swarm: The Science of Complexity in Everyday Life","source":"Len Fisher. 2009. The Perfect Swarm: The Science of Complexity in Everyday Life. Basic Books."},{"authors":[{"first":"Barbara","middle":"J.","last":"Grosz"},{"first":"Candace","middle":"L.","last":"Sidner"}],"year":"1986","title":"Atten-tion, intentions, and the structure of discourse","source":"Barbara J. Grosz and Candace L. Sidner. 1986. Atten-tion, intentions, and the structure of discourse. Comput. Linguist., 12:175–204, July."},{"authors":[{"first":"Lu","last":"Hong"},{"first":"Scott","last":"Page"}],"year":"2009","title":"Interpreted and generated signals","source":"Lu Hong and Scott Page. 2009. Interpreted and generated signals. Journal of Economic Theory, 144(5):2174–2196."},{"authors":[{"first":"Akshay","last":"Java"},{"first":"Pranam","last":"Kolari"},{"first":"Tim","last":"Finin"},{"first":"Tim","last":"Oates"}],"year":"2006","title":"Modeling the spread of influence on the blogosphere","source":"Akshay Java, Pranam Kolari, Tim Finin, and Tim Oates. 2006. Modeling the spread of influence on the blogosphere. In WWW’06."},{"authors":[{"first":"Klaus","last":"Krippendorff"}],"year":"1980","title":"Content Analysis: An Introduction to its Methodology","source":"Klaus Krippendorff. 1980. Content Analysis: An Introduction to its Methodology. Beverly Hills: Sage Publications."},{"authors":[{"first":"Ravi","last":"Kumar"},{"first":"Jasmine","last":"Novak"},{"first":"Prabhakar","last":"Raghavan"},{"first":"Andrew","last":"Tomkins"}],"year":"2003","title":"On the bursty evolution of blogspace","source":"Ravi Kumar, Jasmine Novak, Prabhakar Raghavan, and Andrew Tomkins. 2003. On the bursty evolution of blogspace. In WWW’03, pages 568–576, New York, NY, USA."},{"authors":[{"first":"Lillian","last":"Lee"}],"year":"1999","title":"Measures of distributional similarity","source":"Lillian Lee. 1999. Measures of distributional similarity. In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics, pages 25–32."},{"authors":[{"first":"Jure","last":"Leskovec"},{"first":"Lars","last":"Backstrom"},{"first":"Jon","last":"Kleinberg"}],"year":"2009","title":"Meme-tracking and the dynamics of the news cycle","source":"Jure Leskovec, Lars Backstrom, and Jon Kleinberg. 2009. Meme-tracking and the dynamics of the news cycle. In KDD ’09: Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 497–506."},{"authors":[{"first":"Chin-Yew","last":"Lin"},{"first":"Eduard","last":"Hovy"}],"year":"2002","title":"Manual and automatic evaluation of summaries","source":"Chin-Yew Lin and Eduard Hovy. 2002. Manual and automatic evaluation of summaries. In ACL-Workshop on Automatic Summarization."},{"authors":[{"first":"Qiaozhu","last":"Mei"},{"first":"Jian","last":"Guo"},{"first":"Dragomir","last":"Radev"}],"year":"2010","title":"Divrank: the interplay of prestige and diversity in information networks","source":"Qiaozhu Mei, Jian Guo, and Dragomir Radev. 2010. Divrank: the interplay of prestige and diversity in information networks. In Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 1009–1018."},{"authors":[{"first":"Gilad","last":"Mishne"},{"first":"Natalie","last":"Glance"}],"year":"2006","title":"Predicting movie sales from blogger sentiment","source":"Gilad Mishne and Natalie Glance. 2006. Predicting movie sales from blogger sentiment. In AAAI 2006 Spring Symposium on Computational Approaches to Analysing Weblogs (AAAI-CAAW 2006)."},{"authors":[{"first":"Ani","last":"Nenkova"},{"first":"Rebecca","last":"Passonneau"}],"year":"2004","title":"Evaluating content selection in summarization: The pyramid method","source":"Ani Nenkova and Rebecca Passonneau. 2004. Evaluating content selection in summarization: The pyramid method. Proceedings of the HLT-NAACL conference."},{"authors":[{"first":"Scott","middle":"E.","last":"Page"}],"year":"2007","title":"The Difference: How the Power of Diversity Creates Better Groups, Firms, Schools, and Societies","source":"Scott E. Page. 2007. The Difference: How the Power of Diversity Creates Better Groups, Firms, Schools, and Societies. Princeton University Press."},{"authors":[{"first":"Bo","last":"Pang"},{"first":"Lillian","last":"Lee"}],"year":"2004","title":"A sentimental educa-tion: sentiment analysis using subjectivity summarization based on minimum cuts","source":"Bo Pang and Lillian Lee. 2004. A sentimental educa-tion: sentiment analysis using subjectivity summarization based on minimum cuts. In ACL’04, Morristown, NJ, USA."},{"authors":[{"first":"Michael","last":"Paul"},{"first":"ChengXiang","last":"Zhai"},{"first":"Roxana","last":"Girju"}],"year":"2010","title":"Summarizing contrastive viewpoints in opinionated text","source":"Michael Paul, ChengXiang Zhai, and Roxana Girju. 2010. Summarizing contrastive viewpoints in opinionated text. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Process-ing, pages 66–76."},{"authors":[{"first":"Vahed","last":"Qazvinian"},{"first":"Dragomir","middle":"R.","last":"Radev"}],"year":"2008","title":"Scientific paper summarization using citation summary net-works","source":"Vahed Qazvinian and Dragomir R. Radev. 2008. Scientific paper summarization using citation summary net-works. In COLING 2008, Manchester, UK."},{"authors":[{"first":"Vahed","last":"Qazvinian"},{"first":"Dragomir","middle":"R.","last":"Radev"}],"year":"2010","title":"Identifying non-explicit citing sentences for citation-based summarization","source":"Vahed Qazvinian and Dragomir R. Radev. 2010. Identifying non-explicit citing sentences for citation-based summarization. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 555–564, Uppsala, Sweden, July. Association for Computational Linguistics."},{"authors":[{"first":"Neil","middle":"J.","last":"Smelser"}],"year":"1963","title":"Theory of Collective Behavior","source":"Neil J. Smelser. 1963. Theory of Collective Behavior. Free Press."},{"authors":[{"first":"Karen","last":"Spärck-Jones"}],"year":"1999","title":"Automatic summarizing: factors and directions","source":"Karen Spärck-Jones. 1999. Automatic summarizing: factors and directions. In Inderjeet Mani and Mark T. Maybury, editors, Advances in automatic text summarization, chapter 1, pages 1 – 12. The MIT Press."},{"authors":[{"first":"Manfred","last":"Stede"}],"year":"1995","title":"Lexicalization in natural language generation: a survey","source":"Manfred Stede. 1995. Lexicalization in natural language generation: a survey. Artificial Intelligence Review, (8):309–336."},{"authors":[{"first":"Hans","last":"van Halteren"},{"first":"Simone","last":"Teufel"}],"year":"2003","title":"Examin-ing the consensus between human summaries: initial experiments with factoid analysis","source":"Hans van Halteren and Simone Teufel. 2003. Examin-ing the consensus between human summaries: initial experiments with factoid analysis. In Proceedings of 1107 the HLT-NAACL 03 on Text summarization workshop, pages 57–64, Morristown, NJ, USA. Association for Computational Linguistics."},{"authors":[{"first":"Hans","last":"van Halteren"},{"first":"Simone","last":"Teufel"}],"year":"2004","title":"Evaluating information content by factoid analysis: human annotation and stability","source":"Hans van Halteren and Simone Teufel. 2004. Evaluating information content by factoid analysis: human annotation and stability. In EMNLP’04, Barcelona."},{"authors":[{"first":"Ellen","middle":"M.","last":"Voorhees"}],"year":"1998","title":"Variations in relevance judgments and the measurement of retrieval effectiveness","source":"Ellen M. Voorhees. 1998. Variations in relevance judgments and the measurement of retrieval effectiveness. In SIGIR ’98: Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 315–323."},{"authors":[{"first":"G.","middle":"Udny","last":"Yule"}],"year":"1925","title":"A mathematical theory of evolution, based on the conclusions of dr","source":"G. Udny Yule. 1925. A mathematical theory of evolution, based on the conclusions of dr. j. c. willis, f.r.s. Philosophical Transactions of the Royal Society of London. Series B, Containing Papers of a Biological Character, 213:21–87."},{"authors":[{"first":"Xiaojin","last":"Zhu"},{"first":"Andrew","last":"Goldberg"},{"first":"Jurgen","last":"Van Gael"},{"first":"David","last":"Andrzejewski"}],"year":"2007","title":"Improving diversity in ranking using absorbing random walks","source":"Xiaojin Zhu, Andrew Goldberg, Jurgen Van Gael, and David Andrzejewski. 2007. Improving diversity in ranking using absorbing random walks. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 97–104. 1108"}],"cites":[{"style":0,"text":"Blumer, 1951","origin":{"pointer":"/sections/2/paragraphs/0","offset":113,"length":12},"authors":[{"last":"Blumer"}],"year":"1951","references":["/references/4"]},{"style":0,"text":"Smelser, 1963","origin":{"pointer":"/sections/2/paragraphs/0","offset":325,"length":13},"authors":[{"last":"Smelser"}],"year":"1963","references":["/references/27"]},{"style":0,"text":"Grosz and Sidner, 1986","origin":{"pointer":"/sections/2/paragraphs/1","offset":28,"length":22},"authors":[{"last":"Grosz"},{"last":"Sidner"}],"year":"1986","references":["/references/11"]},{"style":0,"text":"Page, 2007","origin":{"pointer":"/sections/2/paragraphs/3","offset":161,"length":10},"authors":[{"last":"Page"}],"year":"2007","references":["/references/22"]},{"style":0,"text":"Hong and Page, 2009","origin":{"pointer":"/sections/2/paragraphs/3","offset":188,"length":19},"authors":[{"last":"Hong"},{"last":"Page"}],"year":"2009","references":["/references/12"]},{"style":0,"text":"Fisher, 2009","origin":{"pointer":"/sections/2/paragraphs/3","offset":209,"length":12},"authors":[{"last":"Fisher"}],"year":"2009","references":["/references/10"]},{"style":0,"text":"Elhadad, 1995","origin":{"pointer":"/sections/2/paragraphs/4","offset":496,"length":13},"authors":[{"last":"Elhadad"}],"year":"1995","references":["/references/8"]},{"style":0,"text":"Barzilay and Lee, 2002","origin":{"pointer":"/sections/2/paragraphs/4","offset":511,"length":22},"authors":[{"last":"Barzilay"},{"last":"Lee"}],"year":"2002","references":["/references/2"]},{"style":0,"text":"Stede, 1995","origin":{"pointer":"/sections/2/paragraphs/4","offset":535,"length":11},"authors":[{"last":"Stede"}],"year":"1995","references":["/references/29"]},{"style":0,"text":"Mei et al., 2010","origin":{"pointer":"/sections/3/paragraphs/0","offset":75,"length":16},"authors":[{"last":"Mei"},{"last":"al."}],"year":"2010","references":["/references/19"]},{"style":0,"text":"Zhu et al., 2007","origin":{"pointer":"/sections/3/paragraphs/0","offset":403,"length":16},"authors":[{"last":"Zhu"},{"last":"al."}],"year":"2007","references":["/references/34"]},{"style":0,"text":"Paul et al., 2010","origin":{"pointer":"/sections/3/paragraphs/0","offset":679,"length":17},"authors":[{"last":"Paul"},{"last":"al."}],"year":"2010","references":["/references/24"]},{"style":0,"text":"Erkan and Radev, 2004","origin":{"pointer":"/sections/3/paragraphs/0","offset":778,"length":21},"authors":[{"last":"Erkan"},{"last":"Radev"}],"year":"2004","references":[]},{"style":0,"text":"Carbonell and Goldstein, 1998","origin":{"pointer":"/sections/3/paragraphs/1","offset":70,"length":29},"authors":[{"last":"Carbonell"},{"last":"Goldstein"}],"year":"1998","references":["/references/5"]},{"style":0,"text":"Qazvinian and Radev, 2008","origin":{"pointer":"/sections/3/paragraphs/1","offset":358,"length":25},"authors":[{"last":"Qazvinian"},{"last":"Radev"}],"year":"2008","references":["/references/25"]},{"style":0,"text":"Voorhees, 1998","origin":{"pointer":"/sections/3/paragraphs/2","offset":87,"length":14},"authors":[{"last":"Voorhees"}],"year":"1998","references":["/references/32"]},{"style":0,"text":"Halteren and Teufel, 2004","origin":{"pointer":"/sections/3/paragraphs/2","offset":322,"length":25},"authors":[{"last":"Halteren"},{"last":"Teufel"}],"year":"2004","references":[]},{"style":0,"text":"Carletta, 1996","origin":{"pointer":"/sections/3/paragraphs/3","offset":202,"length":14},"authors":[{"last":"Carletta"}],"year":"1996","references":["/references/6"]},{"style":0,"text":"Krippendorff, 1980","origin":{"pointer":"/sections/3/paragraphs/3","offset":218,"length":18},"authors":[{"last":"Krippendorff"}],"year":"1980","references":["/references/14"]},{"style":0,"text":"Spärck-Jones, 1999","origin":{"pointer":"/sections/3/paragraphs/4","offset":263,"length":18},"authors":[{"last":"Spärck-Jones"}],"year":"1999","references":["/references/28"]},{"style":0,"text":"Halteren and Teufel, 2003","origin":{"pointer":"/sections/3/paragraphs/4","offset":386,"length":25},"authors":[{"last":"Halteren"},{"last":"Teufel"}],"year":"2003","references":[]},{"style":0,"text":"Nenkova and Passonneau, 2004","origin":{"pointer":"/sections/3/paragraphs/4","offset":413,"length":28},"authors":[{"last":"Nenkova"},{"last":"Passonneau"}],"year":"2004","references":["/references/21"]},{"style":0,"text":"Leskovec et al., 2009","origin":{"pointer":"/sections/3/paragraphs/5","offset":204,"length":21},"authors":[{"last":"Leskovec"},{"last":"al."}],"year":"2009","references":["/references/17"]},{"style":0,"text":"Barzilay and McKeown, 2005","origin":{"pointer":"/sections/3/paragraphs/5","offset":250,"length":26},"authors":[{"last":"Barzilay"},{"last":"McKeown"}],"year":"2005","references":["/references/3"]},{"style":0,"text":"Kumar et al., 2003","origin":{"pointer":"/sections/3/paragraphs/5","offset":340,"length":18},"authors":[{"last":"Kumar"},{"last":"al."}],"year":"2003","references":["/references/15"]},{"style":0,"text":"Adar et al., 2007","origin":{"pointer":"/sections/3/paragraphs/5","offset":360,"length":17},"authors":[{"last":"Adar"},{"last":"al."}],"year":"2007","references":["/references/1"]},{"style":0,"text":"Pang and Lee, 2004","origin":{"pointer":"/sections/3/paragraphs/5","offset":394,"length":18},"authors":[{"last":"Pang"},{"last":"Lee"}],"year":"2004","references":["/references/23"]},{"style":0,"text":"Adar et al., 2004","origin":{"pointer":"/sections/3/paragraphs/5","offset":473,"length":17},"authors":[{"last":"Adar"},{"last":"al."}],"year":"2004","references":["/references/0"]},{"style":0,"text":"Java et al., 2006","origin":{"pointer":"/sections/3/paragraphs/5","offset":492,"length":17},"authors":[{"last":"Java"},{"last":"al."}],"year":"2006","references":["/references/13"]},{"style":0,"text":"Mishne and Glance, 2006","origin":{"pointer":"/sections/3/paragraphs/5","offset":545,"length":23},"authors":[{"last":"Mishne"},{"last":"Glance"}],"year":"2006","references":["/references/20"]},{"style":0,"text":"Halteren and Teufel, 2003","origin":{"pointer":"/sections/4/paragraphs/4","offset":478,"length":25},"authors":[{"last":"Halteren"},{"last":"Teufel"}],"year":"2003","references":[]},{"style":0,"text":"Halteren and Teufel, 2004","origin":{"pointer":"/sections/4/paragraphs/7","offset":548,"length":25},"authors":[{"last":"Halteren"},{"last":"Teufel"}],"year":"2004","references":[]},{"style":0,"text":"Lin and Hovy, 2002","origin":{"pointer":"/sections/4/paragraphs/17","offset":12,"length":18},"authors":[{"last":"Lin"},{"last":"Hovy"}],"year":"2002","references":["/references/18"]},{"style":0,"text":"Qazvinian and Radev, 2010","origin":{"pointer":"/sections/5/paragraphs/4","offset":12,"length":25},"authors":[{"last":"Qazvinian"},{"last":"Radev"}],"year":"2010","references":["/references/26"]},{"style":0,"text":"Halteren and Teufel, 2003","origin":{"pointer":"/sections/5/paragraphs/25","offset":103,"length":25},"authors":[{"last":"Halteren"},{"last":"Teufel"}],"year":"2003","references":[]},{"style":0,"text":"Halteren and Teufel, 2004","origin":{"pointer":"/sections/5/paragraphs/25","offset":134,"length":25},"authors":[{"last":"Halteren"},{"last":"Teufel"}],"year":"2004","references":[]},{"style":0,"text":"Nenkova and Passonneau, 2004","origin":{"pointer":"/sections/5/paragraphs/27","offset":26,"length":28},"authors":[{"last":"Nenkova"},{"last":"Passonneau"}],"year":"2004","references":["/references/21"]},{"style":0,"text":"Lee, 1999","origin":{"pointer":"/sections/6/paragraphs/50","offset":153,"length":9},"authors":[{"last":"Lee"}],"year":"1999","references":["/references/16"]},{"style":0,"text":"Clauset et al., 2004","origin":{"pointer":"/sections/6/paragraphs/51","offset":493,"length":20},"authors":[{"last":"Clauset"},{"last":"al."}],"year":"2004","references":["/references/7"]},{"style":0,"text":"Erkan and Radev, 2004","origin":{"pointer":"/sections/6/paragraphs/56","offset":9,"length":21},"authors":[{"last":"Erkan"},{"last":"Radev"}],"year":"2004","references":[]},{"style":0,"text":"Erkan and Radev, 2004","origin":{"pointer":"/sections/6/paragraphs/56","offset":230,"length":21},"authors":[{"last":"Erkan"},{"last":"Radev"}],"year":"2004","references":[]},{"style":0,"text":"Carbonell and Goldstein, 1998","origin":{"pointer":"/sections/6/paragraphs/57","offset":34,"length":29},"authors":[{"last":"Carbonell"},{"last":"Goldstein"}],"year":"1998","references":["/references/5"]},{"style":0,"text":"Mei et al., 2010","origin":{"pointer":"/sections/6/paragraphs/59","offset":411,"length":16},"authors":[{"last":"Mei"},{"last":"al."}],"year":"2010","references":["/references/19"]},{"style":0,"text":"Qazvinian and Radev, 2008","origin":{"pointer":"/sections/6/paragraphs/71","offset":242,"length":25},"authors":[{"last":"Qazvinian"},{"last":"Radev"}],"year":"2008","references":["/references/25"]},{"style":0,"text":"Yule, 1925","origin":{"pointer":"/sections/7/paragraphs/1","offset":278,"length":10},"authors":[{"last":"Yule"}],"year":"1925","references":["/references/33"]}]}
