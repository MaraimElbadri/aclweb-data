{"sections":[{"title":"","paragraphs":["Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 479–484, Portland, Oregon, June 19-24, 2011. c⃝2011 Association for Computational Linguistics"]},{"title":"Identifying Word Translations from Comparable Corpora Using Latent Topic Models Ivan Vulić, Wim De Smet and Marie-Francine Moens Department of Computer Science K.U. Leuven Celestijnenlaan 200A Leuven, Belgium {ivan.vulic,wim.desmet,sien.moens}@cs.kuleuven.be Abstract","paragraphs":["A topic model outputs a set of multinomial distributions over words for each topic. In this paper, we investigate the value of bilingual topic models, i.e., a bilingual Latent Dirichlet Allocation model for finding translations of terms in comparable corpora without using any linguistic resources. Experiments on a document-aligned English-Italian Wikipedia corpus confirm that the developed methods which only use knowledge from word-topic distributions outperform methods based on similarity measures in the original word-document space. The best results, obtained by combining knowledge from word-topic distributions with similarity measures in the original space, are also reported."]},{"title":"1 Introduction","paragraphs":["Generative models for documents such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003) are based upon the idea that latent variables exist which determine how words in documents might be generated. Fitting a generative model means finding the best set of those latent variables in order to explain the observed data. Within that setting, documents are observed as mixtures of latent topics, where topics are probability distributions over words.","Our goal is to model and test the capability of probabilistic topic models to identify potential translations from document-aligned text collections. A representative example of such a comparable text collection is Wikipedia, where one may observe articles discussing the same topic, but strongly varying in style, length and even vocabulary, while still shar-ing a certain amount of main concepts (or topics). We try to establish a connection between such latent topics and an idea known as the distributional hypothesis (Harris, 1954) - words with a similar mean-ing are often used in similar contexts.","Besides the obvious context of direct cooccurrence, we believe that topic models are an additional source of knowledge which might be used to improve results in the quest for translation candidates extracted without the availability of a translation dictionary and linguistic knowledge. We designed several methods, all derived from the core idea of using word distributions over topics as an extra source of contextual knowledge. Two words are potential translation candidates if they are often present in the same cross-lingual topics and not observed in other cross-lingual topics. In other words, a word w2 from a target language is a potential translation candidate for a word w1 from a source language, if the distribution of w2 over the target language topics is similar to the distribution of w1 over the source language topics.","The remainder of this paper is structured as follows. Section 2 describes related work, focusing on previous attempts to use topic models to recognize potential translations. Section 3 provides a short summary of the BiLDA model used in the experiments, presents all main ideas behind our work and gives an overview and a theoretical background of the methods. Section 4 evaluates and discusses initial results. Finally, section 5 proposes several extensions and gives a summary of the current work. 479"]},{"title":"2 Related Work","paragraphs":["The idea to acquire translation candidates based on comparable and unrelated corpora comes from (Rapp, 1995). Similar approaches are described in (Diab and Finch, 2000), (Koehn and Knight, 2002) and (Gaussier et al., 2004). These methods need an initial lexicon of translations, cognates or similar words which are then used to acquire additional translations of the context words. In contrast, our method does not bootstrap on language pairs that share morphology, cognates or similar words.","Some attempts of obtaining translations using cross-lingual topic models have been made in the last few years, but they are model-dependent and do not provide a general environment to adapt and apply other topic models for the task of finding translation correspondences. (Ni et al., 2009) have designed a probabilistic topic model that fits Wikipedia data, but they did not use their models to obtain potential translations. (Mimno et al., 2009) retrieve a list of potential translations simply by selecting a small number N of the most probable words in both languages and then add the Cartesian product of these sets for every topic to a set of candidate translations. This approach is straightforward, but it does not catch the structure of the latent topic space completely.","Another model proposed in (Boyd-Graber and Blei, 2009) builds topics as distributions over bilingual matchings where matching priors may come from different initial evidences such as a machine readable dictionary, edit distance, or the Pointwise Mutual Information (PMI) statistic scores from available parallel corpora. The main shortcoming is that it introduces external knowledge for matching priors, suffers from overfitting and uses a restricted vocabulary."]},{"title":"3 Methodology","paragraphs":["In this section we present the topic model we used in our experiments and outline the formal framework within which three different approaches for acquir-ing potential word translations were built. 3.1 Bilingual LDA The topic model we use is a bilingual extension of a standard LDA model, called bilingual LDA (BiLDA), which has been presented in (Ni et al., 2009; Mimno et al., 2009; De Smet and Moens, 2009). As the name suggests, it is an extension of the basic LDA model, taking into account bilinguality and designed for parallel document pairs. We test its performance on a collection of comparable texts which are document-aligned and therefore share their topics. BiLDA takes advantage of the document alignment by using a single variable that contains the topic distribution θ, that is language-independent by assumption and shared by the paired bilingual comparable documents. Topics for each document are sampled from θ, from which the words are sampled in conjugation with the vocabulary distribution φ (for language S) and ψ (for language T). Algorithm 3.1 summarizes the generative story, while figure 1 shows the plate model. Algorithm 3.1: GENERATIVE STORY FOR BILDA() for each document pair dj do    for each word position i ∈ djS do {","sample zS ji ∼ M ult(θ)","sample wS ji ∼ M ult(φ, zS","ji) for each word position i ∈ djT do {","sample zT ji ∼ M ult(θ)","sample wT ji ∼ M ult(ψ, zT","ji) DN M φ β ψ α θ zS ji zT","ji wS ji wT","ji Figure 1: The standard bilingual LDA model","Having one common θ for both of the related documents implies parallelism between the texts. This observation does not completely hold for comparable corpora with topically aligned texts. To train the 480 model we use Gibbs sampling, similar to the sampling method for monolingual LDA, with parameters α and β set to 50/K and 0.01 respectively, where K denotes the number of topics. After the training we end up with a set of φ and ψ word-topic probability distributions that are used for the calcu-lations of the word associations.","If we are given a source vocabulary W S",", then the distribution φ of sampling a new token as word wi ∈ W S","from a topic z k can be obtained as follows: P (wi|zk) = φk,i = n (wi) k + β","∑|W S","| j=1 n (wj) k + W S","β (1) where, for a word wi and a topic zk, n","(wi)","k denotes","the total number of times that the topic zk is assigned","to the word wi from the vocabulary W S",", β is a sym-","metric Dirichlet prior,","∑|WS| j=1 n","(wj)","k is the total number of words assigned to the topic zk, and |W S","| is the total number of distinct words in the vocabulary. The formula for a set of ψ word-topic probability distributions for the target side of a corpus is computed in an analogical manner. 3.2 Main Framework Once we derive a shared set of topics along with language-specific distributions of words over topics, it is possible to use them for the computation of the similarity between words in different languages. 3.2.1 KL Method The similarity between a source word w1 and a target word w2 is measured by the extent to which they share the same topics, i.e., by the extent that their conditional topic distributions are similar. One way of expressing similarity is the Kullback-Leibler (KL) divergence, already used in a monolingual setting in (Steyvers and Griffiths, 2007). The similarity between two words is based on the similarity between χ(1)","and χ(2)",", the similarity of conditional topic distributions for words w1 and w2, where χ(1)","= P (Z|w","1)1 and χ(2)","= P (Z|w","2). We have to calculate the probabilities P (zj|wi), which describe a probability that a given word is assigned to a particular topic. If we apply Bayes’ rule, we get P (Z|w) =","P (w|Z)P (Z) P (w) , where P (Z) and P (w) 1 P (Z|w1) refers to a set of all conditional topic distributions","P (zj|w1) are prior distributions for topics and words respectively. P (Z) is a uniform distribution for the BiLDA model, whereas this assumption clearly does not hold for topic models with a non-uniform topic prior. P (w) is given by P (w) = P (w|Z)P (Z). If the assumption of uniformity for P (Z) holds, we can write: P (zj|wi) ∝ P (wi|zj) N ormφ =","φj,i N ormφ (2) for an English word wi, and: P (zj|wi) ∝ P (wi|zj) N ormψ =","ψj,i N ormψ (3)","for a French word wi, where N ormφ denotes the normalization factor","∑K","j=1 P (wi|zj), i.e., the sum of all probabilities φ (or probabilities ψ for N ormψ) for the currently observed word wi. We can then calculate the KL divergence as follows:","KL(χ(1) , χ(2)",") ∝ K ∑ j=1","φj,1 N ormφ log φj,1/N ormφ ψj,2/N ormψ (4) 3.2.2 Cue Method An alternative, more straightforward approach (called the Cue method) tries to express similarity between two words emphasizing the associative relation between two words in a more natural way. It models the probability P (w2|w1), i.e., the probability that a target word w2 will be generated as a response to a cue source word w1. For the BiLDA model we can write: P (w2|w1) = K ∑ j=1 P (w2|zj)P (zj|w1) = K ∑ j=1 ψj,2","φj,1 N ormφ (5)","This conditioning automatically compromises between word frequency and semantic relatedness (Griffiths et al., 2007), since higher frequency words tend to have higher probabilities across all topics, but the distribution over topics P (zj|w1) ensures that semantically related topics dominate the sum. 481 3.2.3 TI Method The last approach borrows an idea from information retrieval and constructs word vectors over a shared latent topic space. Values within vectors are the TF-ITF (term frequency - inverse topic frequency) scores which are calculated in a completely analogical manner as the TF-IDF scores for the original word-document space (Manning and Schütze, 1999). If we are given a source word wi, n","(wi)","k,S denotes the number of times the word wi is associated with a source topic zk. Term frequency (TF) of the source word wi for the source topic zk is given as: T Fi,k = n (wi) k,S","∑ wj∈W S n (wj) k,S (6)","Inverse topical frequency (ITF) measures the general importance of the source word wi across all source topics. Rare words are given a higher importance and thus they tend to be more descriptive for a specific topic. The inverse topical frequency for the source word wi is calculated as2",": IT Fi = log K 1 + |k : n (wi) k,S > 0| (7)","The finalTF-ITF score for the source word wi and the topic zk is given by T F −IT Fi,k = T Fi,k ·IT Fi. We calculate the TF-ITF scores for target words associated with target topics in an analogical manner. Source and target words share the same K-dimensional topical space, where K-dimensional vectors consisting of the TF-ITF scores are built for all words. The standard cosine similarity metric is then used to find the most similar word vectors from the target vocabulary for a source word vector. We name this method the TI method. For in-stance, given a source word w1 represented by a K-dimensional vector S1","and a target word w","2 repre-","sented by a K-dimensional vector T 2",", the similarity","between the two words is calculated as follows: 2 Stronger association with a topic is modeled by setting a","higher threshold value in n(w","i)","k,S > threshold, where we have chosen 0. cos(w1, w2) =","∑K k=1 S1","k · T 2","k","√∑K","k=1 (S1 k)2 ·","√∑K k=1 (T 2","k )2 (8)"]},{"title":"4 Results and Discussion","paragraphs":["As our training corpus, we use the English-Italian Wikipedia corpus of 18, 898 document pairs, where each aligned pair discusses the same subject. In order to reduce data sparsity, we keep only lemmatized noun forms for further analysis. Our Italian vocabulary consists of 7, 160 nouns, while our English vocabulary contains 9, 166 nouns. The subset of the 650 most frequent terms was used for testing. We have used the Google Translate tool for evaluations. As our baseline system, we use the cosine similarity between Italian word vectors and English word vectors with TF-IDF scores in the original word-document space (Cos), with aligned documents.","Table 1 shows the Precision@1 scores (the percentage of words where the first word from the list of translations is the correct one) for all three approaches (KL, Cue and TI), for different number of topics K. Although KL is designed specifically to measure the similarity of two distributions, its results are significantly below those of theCue and TI, whose performances are comparable. Whereas the latter two methods yield the highest results around the 2, 000 topics mark, the performance of KL in-creases linearly with the number of topics. This is an undesirable result as good results are computationally hard to get.","We have also detected that we are able to boost overall scores if we combine two methods. We have opted for the two best methods (TI+Cue), where overall score is calculated by Score = λ·ScoreCue+ ScoreT I .3","We also provide the results obtained by linearly combining (with equal weights) the cosine similarity between TF-ITF vectors with that between TF-IDF vector (TI+Cos).","In a more lenient evaluation setting we employ the mean reciprocal rank (MRR) (Voorhees, 1999). For a source word w, rankw denotes the rank of its correct translation within the retrieved list of potential translations. MRR is then defined as follows:","3","The value of λ is empirically set to 10 482 K KL Cue TI TI+Cue TI+Cos 200 0.3015 0.1800 0.3169 0.2862 0.5369 500 0.2846 0.3338 0.3754 0.4000 0.5308 800 0.2969 0.4215 0.4523 0.4877 0.5631 1200 0.3246 0.5138 0.4969 0.5708 0.5985 1500 0.3323 0.5123 0.4938 0.5723 0.5908 1800 0.3569 0.5246 0.5154 0.5985 0.6123 2000 0.3954 0.5246 0.5385 0.6077 0.6046 2200 0.4185 0.5323 0.5169 0.5908 0.6015 2600 0.4292 0.4938 0.5185 0.5662 0.5907 3000 0.4354 0.4554 0.4923 0.5631 0.5953 3500 0.4585 0.4492 0.4785 0.5738 0.5785 Table 1: Precision@1 scores for the test subset of the IT-EN Wikipedia corpus (baseline precision score: 0.5031) M RR = 1 |V | ∑ w∈V","1 rankw (9)","where V denotes the set of words used for evaluation. We kept only the top 20 candidates from the ranked list. Table 2 shows the MRR scores for the same set of experiments. K KL Cue TI TI+Cue TI+Cos 200 0.3569 0.2990 0.3868 0.4189 0.5899 500 0.3349 0.4331 0.4431 0.4965 0.5808 800 0.3490 0.5093 0.5215 0.5733 0.6173 1200 0.3773 0.5751 0.5618 0.6372 0.6514 1500 0.3865 0.5756 0.5562 0.6320 0.6435 1800 0.4169 0.5858 0.5802 0.6581 0.6583 2000 0.4561 0.5841 0.5914 0.6616 0.6548 2200 0.4686 0.5898 0.5753 0.6471 0.6523 2600 0.4763 0.5550 0.5710 0.6268 0.6416 3000 0.4848 0.5272 0.5572 0.6257 0.6465 3500 0.5022 0.5199 0.5450 0.6238 0.6310 Table 2: MRR scores for the test subset of the IT-EN Wikipedia corpus (baseline MRR score: 0.5890)","Topic models have the ability to build clusters of words which might not always co-occur together in the same textual units and therefore add extra information of potential relatedness. Although we have presented results for a document-aligned corpus, the framework is completely generic and applicable to other topically related corpora.","Again, the KL method has the weakest performance among the three methods based on the word-topic distributions, while the other two methods seem very useful when combined together or when combined with the similarity measure used in the original word-document space. We believe that the results are in reality even higher than presented in the paper, due to errors in the evaluation tool (e.g., the Italian word raggio is correctly translated as ray, but Google Translate returns radius as the first translation candidate).","All proposed methods retrieve lists of semantically related words, where synonymy is not the only semantic relation observed. Such lists provide comprehensible and useful contextual information in the target language for the source word, even when the correct translation candidate is missing, as might be seen in table 3.","(1) romanzo (2) paesaggio (3) cavallo (novel) (landscape) (horse) writer tourist horse novella painting stud novellette landscape horseback humorist local hoof novelist visitor breed essayist hut stamina penchant draftsman luggage formative tourism mare foreword attraction riding author vegetation pony Table 3: Lists of the top 10 translation candidates, where the correct translation is not found (column 1), lies hidden lower in the list (2), and is retrieved as the first candidate (3); K=2000; TI+Cue."]},{"title":"5 Conclusion","paragraphs":["We have presented a generic, language-independent framework for mining translations of words from latent topic models. We have proven that topical knowledge is useful and improves the quality of word translations. The quality of translations depends only on the quality of a topic model and its ability to find latent relations between words. Our next steps involve experiments with other topic models and other corpora, and combining this unsupervised approach with other tools for lexicon extraction and synonymy detection from unrelated and comparable corpora."]},{"title":"Acknowledgements","paragraphs":["The research has been carried out in the framework of the TermWise Knowledge Platform (IOF-KP/09/001) funded by the Industrial Research Fund K.U. Leuven, Belgium, and the Flemish SBO-IWT project AMASS++ (SBO-IWT 0060051). 483"]},{"title":"References","paragraphs":["David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet Allocation. Journal of Machine Learning Research, 3:993–1022.","Jordan Boyd-Graber and David M. Blei. 2009. Multilin-gual topic models for unaligned text. In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, UAI ’09, pages 75–82.","Wim De Smet and Marie-Francine Moens. 2009. Cross-language linking of news stories on the web using interlingual topic modelling. In Proceedings of the CIKM 2009 Workshop on Social Web Search and Mining, pages 57–64.","Mona T. Diab and Steve Finch. 2000. A statistical translation model using comparable corpora. In Proceedings of the 2000 Conference on Content-Based Multimedia Information Access (RIAO), pages 1500–1508.","Éric Gaussier, Jean-Michel Renders, Irina Matveeva, Cyril Goutte, and Hervé Déjean. 2004. A geometric view on bilingual lexicon extraction from comparable corpora. In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, pages 526–533.","Thomas L. Griffiths, Mark Steyvers, and Joshua B. Tenenbaum. 2007. Topics in semantic representation. Psychological Review, 114(2):211–244.","Zellig S. Harris. 1954. Distributional structure. In Word 10 (23), pages 146–162.","Philipp Koehn and Kevin Knight. 2002. Learning a translation lexicon from monolingual corpora. In Proceedings of the ACL-02 Workshop on Unsupervised Lexical Acquisition - Volume 9, ULA ’02, pages 9–16.","Christopher D. Manning and Hinrich Schütze. 1999. Foundations of Statistical Natural Language Process-ing. MIT Press, Cambridge, MA, USA.","David Mimno, Hanna M. Wallach, Jason Naradowsky, David A. Smith, and Andrew McCallum. 2009. Polylingual topic models. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 880–889.","Xiaochuan Ni, Jian-Tao Sun, Jian Hu, and Zheng Chen. 2009. Mining multilingual topics from Wikipedia. In Proceedings of the 18th International World Wide Web Conference, pages 1155–1156.","Reinhard Rapp. 1995. Identifying word translations in non-parallel texts. In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, ACL ’95, pages 320–322.","Mark Steyvers and Tom Griffiths. 2007. Probabilistic topic models. Handbook of Latent Semantic Analysis, 427(7):424–440.","Ellen M. Voorhees. 1999. The TREC-8 question answer-ing track report. In Proceedings of the Eighth TExt Retrieval Conference (TREC-8). 484"]}],"references":[{"authors":[{"first":"David","middle":"M.","last":"Blei"},{"first":"Andrew","middle":"Y.","last":"Ng"},{"first":"Michael I","middle":".","last":"Jordan"}],"year":"2003","title":"Latent Dirichlet Allocation","source":"David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet Allocation. Journal of Machine Learning Research, 3:993–1022."},{"authors":[{"first":"Jordan","last":"Boyd-Graber"},{"first":"David","middle":"M.","last":"Blei"}],"year":"2009","title":"Multilin-gual topic models for unaligned text","source":"Jordan Boyd-Graber and David M. Blei. 2009. Multilin-gual topic models for unaligned text. In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, UAI ’09, pages 75–82."},{"authors":[{"first":"Wim","last":"De Smet"},{"first":"Marie-Francine","last":"Moens"}],"year":"2009","title":"Cross-language linking of news stories on the web using interlingual topic modelling","source":"Wim De Smet and Marie-Francine Moens. 2009. Cross-language linking of news stories on the web using interlingual topic modelling. In Proceedings of the CIKM 2009 Workshop on Social Web Search and Mining, pages 57–64."},{"authors":[{"first":"Mona","middle":"T.","last":"Diab"},{"first":"Steve","last":"Finch"}],"year":"2000","title":"A statistical translation model using comparable corpora","source":"Mona T. Diab and Steve Finch. 2000. A statistical translation model using comparable corpora. In Proceedings of the 2000 Conference on Content-Based Multimedia Information Access (RIAO), pages 1500–1508."},{"authors":[{"first":"Éric","last":"Gaussier"},{"first":"Jean-Michel","last":"Renders"},{"first":"Irina","last":"Matveeva"},{"first":"Cyril","last":"Goutte"},{"first":"Hervé","last":"Déjean"}],"year":"2004","title":"A geometric view on bilingual lexicon extraction from comparable corpora","source":"Éric Gaussier, Jean-Michel Renders, Irina Matveeva, Cyril Goutte, and Hervé Déjean. 2004. A geometric view on bilingual lexicon extraction from comparable corpora. In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, pages 526–533."},{"authors":[{"first":"Thomas","middle":"L.","last":"Griffiths"},{"first":"Mark","last":"Steyvers"},{"first":"Joshua","middle":"B.","last":"Tenenbaum"}],"year":"2007","title":"Topics in semantic representation","source":"Thomas L. Griffiths, Mark Steyvers, and Joshua B. Tenenbaum. 2007. Topics in semantic representation. Psychological Review, 114(2):211–244."},{"authors":[{"first":"Zellig","middle":"S.","last":"Harris"}],"year":"1954","title":"Distributional structure","source":"Zellig S. Harris. 1954. Distributional structure. In Word 10 (23), pages 146–162."},{"authors":[{"first":"Philipp","last":"Koehn"},{"first":"Kevin","last":"Knight"}],"year":"2002","title":"Learning a translation lexicon from monolingual corpora","source":"Philipp Koehn and Kevin Knight. 2002. Learning a translation lexicon from monolingual corpora. In Proceedings of the ACL-02 Workshop on Unsupervised Lexical Acquisition - Volume 9, ULA ’02, pages 9–16."},{"authors":[{"first":"Christopher","middle":"D.","last":"Manning"},{"first":"Hinrich","last":"Schütze"}],"year":"1999","title":"Foundations of Statistical Natural Language Process-ing","source":"Christopher D. Manning and Hinrich Schütze. 1999. Foundations of Statistical Natural Language Process-ing. MIT Press, Cambridge, MA, USA."},{"authors":[{"first":"David","last":"Mimno"},{"first":"Hanna","middle":"M.","last":"Wallach"},{"first":"Jason","last":"Naradowsky"},{"first":"David","middle":"A.","last":"Smith"},{"first":"Andrew","last":"McCallum"}],"year":"2009","title":"Polylingual topic models","source":"David Mimno, Hanna M. Wallach, Jason Naradowsky, David A. Smith, and Andrew McCallum. 2009. Polylingual topic models. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 880–889."},{"authors":[{"first":"Xiaochuan","last":"Ni"},{"first":"Jian-Tao","last":"Sun"},{"first":"Jian","last":"Hu"},{"first":"Zheng","last":"Chen"}],"year":"2009","title":"Mining multilingual topics from Wikipedia","source":"Xiaochuan Ni, Jian-Tao Sun, Jian Hu, and Zheng Chen. 2009. Mining multilingual topics from Wikipedia. In Proceedings of the 18th International World Wide Web Conference, pages 1155–1156."},{"authors":[{"first":"Reinhard","last":"Rapp"}],"year":"1995","title":"Identifying word translations in non-parallel texts","source":"Reinhard Rapp. 1995. Identifying word translations in non-parallel texts. In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, ACL ’95, pages 320–322."},{"authors":[{"first":"Mark","last":"Steyvers"},{"first":"Tom","last":"Griffiths"}],"year":"2007","title":"Probabilistic topic models","source":"Mark Steyvers and Tom Griffiths. 2007. Probabilistic topic models. Handbook of Latent Semantic Analysis, 427(7):424–440."},{"authors":[{"first":"Ellen","middle":"M.","last":"Voorhees"}],"year":"1999","title":"The TREC-8 question answer-ing track report","source":"Ellen M. Voorhees. 1999. The TREC-8 question answer-ing track report. In Proceedings of the Eighth TExt Retrieval Conference (TREC-8). 484"}],"cites":[{"style":0,"text":"Blei et al., 2003","origin":{"pointer":"/sections/2/paragraphs/0","offset":75,"length":17},"authors":[{"last":"Blei"},{"last":"al."}],"year":"2003","references":["/references/0"]},{"style":0,"text":"Harris, 1954","origin":{"pointer":"/sections/2/paragraphs/1","offset":523,"length":12},"authors":[{"last":"Harris"}],"year":"1954","references":["/references/6"]},{"style":0,"text":"Rapp, 1995","origin":{"pointer":"/sections/3/paragraphs/0","offset":97,"length":10},"authors":[{"last":"Rapp"}],"year":"1995","references":["/references/11"]},{"style":0,"text":"Diab and Finch, 2000","origin":{"pointer":"/sections/3/paragraphs/0","offset":147,"length":20},"authors":[{"last":"Diab"},{"last":"Finch"}],"year":"2000","references":["/references/3"]},{"style":0,"text":"Koehn and Knight, 2002","origin":{"pointer":"/sections/3/paragraphs/0","offset":171,"length":22},"authors":[{"last":"Koehn"},{"last":"Knight"}],"year":"2002","references":["/references/7"]},{"style":0,"text":"Gaussier et al., 2004","origin":{"pointer":"/sections/3/paragraphs/0","offset":200,"length":21},"authors":[{"last":"Gaussier"},{"last":"al."}],"year":"2004","references":["/references/4"]},{"style":0,"text":"Ni et al., 2009","origin":{"pointer":"/sections/3/paragraphs/1","offset":273,"length":15},"authors":[{"last":"Ni"},{"last":"al."}],"year":"2009","references":["/references/10"]},{"style":0,"text":"Mimno et al., 2009","origin":{"pointer":"/sections/3/paragraphs/1","offset":427,"length":18},"authors":[{"last":"Mimno"},{"last":"al."}],"year":"2009","references":["/references/9"]},{"style":0,"text":"Boyd-Graber and Blei, 2009","origin":{"pointer":"/sections/3/paragraphs/2","offset":27,"length":26},"authors":[{"last":"Boyd-Graber"},{"last":"Blei"}],"year":"2009","references":["/references/1"]},{"style":0,"text":"Ni et al., 2009","origin":{"pointer":"/sections/4/paragraphs/0","offset":348,"length":15},"authors":[{"last":"Ni"},{"last":"al."}],"year":"2009","references":["/references/10"]},{"style":0,"text":"Mimno et al., 2009","origin":{"pointer":"/sections/4/paragraphs/0","offset":365,"length":18},"authors":[{"last":"Mimno"},{"last":"al."}],"year":"2009","references":["/references/9"]},{"style":0,"text":"Smet and Moens, 2009","origin":{"pointer":"/sections/4/paragraphs/0","offset":388,"length":20},"authors":[{"last":"Smet"},{"last":"Moens"}],"year":"2009","references":[]},{"style":0,"text":"Steyvers and Griffiths, 2007","origin":{"pointer":"/sections/4/paragraphs/25","offset":749,"length":28},"authors":[{"last":"Steyvers"},{"last":"Griffiths"}],"year":"2007","references":["/references/12"]},{"style":0,"text":"Griffiths et al., 2007","origin":{"pointer":"/sections/4/paragraphs/43","offset":93,"length":22},"authors":[{"last":"Griffiths"},{"last":"al."}],"year":"2007","references":["/references/5"]},{"style":0,"text":"Manning and Schütze, 1999","origin":{"pointer":"/sections/4/paragraphs/43","offset":646,"length":25},"authors":[{"last":"Manning"},{"last":"Schütze"}],"year":"1999","references":["/references/8"]},{"style":0,"text":"Voorhees, 1999","origin":{"pointer":"/sections/5/paragraphs/4","offset":79,"length":14},"authors":[{"last":"Voorhees"}],"year":"1999","references":["/references/13"]}]}
