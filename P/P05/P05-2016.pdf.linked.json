{"sections":[{"title":"","paragraphs":["Proceedings of the ACL Student Research Workshop, pages 91–96, Ann Arbor, Michigan, June 2005. c⃝2005 Association for Computational Linguistics"]},{"title":"Dependency-Based Statistical Machine Translation Heidi J. Fox Brown Laboratory for Linguistic Information Processing Brown University, Box 1910, Providence, RI 02912 hjf@cs.brown.edu Abstract","paragraphs":["We present a Czech-English statistical machine translation system which performs tree-to-tree translation of dependency structures. The only bilingual resource required is a sentence-aligned parallel corpus. All other resources are monolingual. We also refer to an evaluation method and plan to compare our system’s output with a benchmark system."]},{"title":"1 Introduction","paragraphs":["The goal of statistical machine translation (SMT) is to develop mathematical models of the translation process whose parameters can be automatically estimated from a parallel corpus. Given a string of foreign words F, we seek to find the English string E which is a “correct” translation of the foreign string. The first work on SMT done at IBM (Brown et al., 1990; Brown et al., 1992; Brown et al., 1993; Berger et al., 1994), used a noisy-channel model, resulting in what Brown et al. (1993) call “the Fundamental Equation of Machine Translation”:","Ê =argmax E P (E)P (F | E) (1)","In this equation we see that the translation problem is factored into two subproblems. P (E) is the language model and P (F | E) is the translation model. The work described here focuses on developing improvements to the translation model.","While the IBM work was groundbreaking, it was also deficient in several ways. Their model translates words in isolation, and the component which accounts for word order differences between languages is based on linear position in the sentence. Conspicuously absent is all but the most elementary use of syntactic information. Several researchers have subsequently formulated models which incorporate the intuition that syntactically close constituents tend to stay close across languages. Below are descriptions of some of these different methods of integrating syntax.","• Stochastic Inversion Transduction Grammars (Wu and Wong, 1998): This formalism uses a grammar for English and from it derives a possible grammar for the foreign language. This derivation includes adding productions where the order of the RHS is reversed from the or-dering of the English.","• Syntax-based Statistical Translation (Yamada and Knight, 2001): This model extends the above by allowing all possible permutations of the RHS of the English rules.","• Statistical Phrase-based Translation (Koehn et al., 2003): Here “phrase-based” means “subsequence-based”, as there is no guarantee that the phrases learned by the model will have any relation to what we would think of as syntactic phrases.","• Dependency-based Translation ( Čmejrek et al., 2003): This model assumes a dependency parser for the foreign language. The syntactic structure and labels are preserved during translation. Transfer is purely lexical. A generator builds an English sentence out of the structure, labels, and translated words. 91"]},{"title":"2 System Overview","paragraphs":["The basic framework of our system is quite similar to that of Čmejrek et al. (2003) (we reuse many of their ancillary modules). The difference is in how we use the dependency structures. Čmejrek et al. only translate the lexical items. The dependency structure and any features on the nodes are preserved and all other processing is left to the generator. In addition to lexical translation, our system models structural changes and changes to feature values, for although dependency structures are fairly well preserved across languages (Fox, 2002), there are certainly many instances where the structure must be modified.","While the entire translation system is too large to discuss in detail here, I will provide brief descriptions of ancillary components. References are provided, where available, for those who want more in-formation. 2.1 Corpus Preparation Our parallel Czech-English corpus is comprised of Wall Street Journal articles from 1989. The English data is from the University of Pennsylvania Treebank (Marcus et al., 1993; Marcus et al., 1994). The Czech translations of these articles are provided as part of the Prague Dependency Treebank (PDT) (Böhmová et al., 2001). In order to learn the parameters for our model, we must first create aligned dependency structures for the sentence pairs in our corpus. This process begins with the building of dependency structures.","Since Czech is a highly inflected language, morphological tagging is extremely helpful for downstream processing. We generate the tags using the system described in (Hajič and Hladká, 1998). The tagged sentences are parsed by the Charniak parser, this time trained on Czech data from the PDT. The resulting phrase structures are converted to tectogrammatical dependency structures via the procedure documented in (Böhmová, 2001). Under this formalism, function words are deleted and any in-formation contained in them is preserved in features attached to the remaining nodes. Finally, functors (such as agent or patient) are automatically assigned to nodes in the tree ( Žabokrtský et al., 2002).","On the English side, the process is simpler. We japan automobile dealers association... ...NNP NNP NNPS NN japan automobile dealers association... ...NNP NNP NNPS NN SPLIT N N A N CZ3 CZ2 CZ1 ... obchodnı́k japonský ...automobilasociace EN2 EN1 EN2 EN1 EN3 Figure 1: Left SPLIT Example parse with the Charniak parser (Charniak, 2000) and convert the resulting phrase-structure trees to a function-argument formalism, which, like the tectogrammatic formalism, removes function words. This conversion is accomplished via deterministic application of approximately 20 rules. 2.2 Aligning the Dependency Structures We now generate the alignments between the pairs of dependency structures we have created. We be-gin by producing word alignments with a model very similar to that of IBM Model 4 (Brown et al., 1993). We keep fifty possible alignments and require that each word has at least two possible alignments. We then align phrases based on the alignments of the words in each phrase span. If there is no satisfactory alignment, then we allow for structural mutations. The probabilities for these mutations are refined via another round of alignment. The structural mutations allowed are described below. Examples are shown in phrase-structure format rather than dependency format for ease of explanation. 92 BUD CZ2 CZ1","bear stearnsN NN společnost EN1 EN2 ... stearns ... NNP NNP bear ... ... Figure 2: BUD Example • KEEP: No change. This is the default.","• SPLIT: One English phrase aligns with two Czech phrases and splitting the English phrase results in a better alignment. There are three types of split (left, right, middle) whose probabilities are also estimated. In the original structure of Figure 1, English node EN1 would align with Czech nodes CZ1 and CZ2. Splitting the English by adding child node EN3 results in a better alignment.","• BUD: This adds a unary level in the English tree in the case when one English node aligns to two Czech nodes, but one of the Czech nodes is the parent of the other. In Figure 2, the Czech has one extra word “společnost” (“company”) compared with the English. English node EN1 would normally align to both CZ1 and CZ2. Adding a unary node EN2 to the English results in a better alignment.","• ERASE: There is no corresponding Czech node for the English one. In Figure 3, the English has two nodes, EN1 and EN2, which have no corresponding Czech nodes. Erasing them brings the Czech and English structures into alignment.","• PHRASE-TO-WORD: An entire English phrase aligns with one Czech word. This operates similarly to ERASE. NNJJ WDT VBD NNP NNJJ WDT VBD NNP ERASE ERASE A N P V N CZ2 CZ1 který... rok srpen ...fiskálnı́ zařı́ EN4 EN3 EN2EN1 year began august ...which... fiscal EN4 EN3 year began august ...which... fiscal Figure 3: ERASE Example"]},{"title":"3 Translation Model","paragraphs":["Given E, the parse of the English string, our translation model can be formalized as P (F | E). Let E1 . . . En be the nodes in the English parse, F be a parse of the Czech string, and F1 . . . Fm be the nodes in the Czech parse. Then, P (F | E) = ∑ FforF P (F1 . . . Fm | E1 . . . En) (2) We initially make several strong independence assumptions which we hope to eventually weaken. The first is that each Czech parse node is generated independently of every other one. Further, we specify that each English parse node generates exactly one (possibly NULL) Czech parse node. P (F | E) = ∏ Fi∈F P (Fi | E1 . . . En) = n ∏ i=1 P (Fi | Ei)","(3) An English parse node Ei contains the following","information: • An English word: ei","• A part of speech: te i","• A vector of n features (e.g. negation or tense):","< φe","i [1], . . . , φe","i [n] > 93 • A list of dependent nodes","In order to produce a Czech parse node Fi, we must generate the following:","Lemma fi: We generate the Czech lemma fi dependent only on the English word ei. Part of Speech t f i : We generate Czech part of speech t f i dependent on the part of speech of","the Czech parent t f par(i) and the corresponding","English part of speech te","i .","Features < φf i [1], . . . , φ","f","i [n] >: There are several features (see Table 1) associated with each parse node. Of these, all except IND are typical morphological and analytical features. IND (indicator) is a loosely-specified feature comprised of functors, where assigned, and other words or small phrases (often prepositions) which are attached to the node and indicate something about the node’s function in the sentence. (e.g. an IND of “at” could indicate a locative function). We generate each Czech feature φf","i [j] dependent only on its corresponding English feature φe","i [j].","Head Position hi: When an English word is aligned to the head of a Czech phrase, the English word is typically also the head of its respective phrase. But, this is not always the case, so we model the probability that the English head will be aligned to either the Czech head or to one of its children. To simplify, we set the probability for each particular child being the head to be uniform in the number of children. The head position is generated independent of the rest of the sentence.","Structural Mutation mi: Dependency structures are fairly well preserved across languages, but there are cases when the structures need to be modified. Section 2.2 contains descriptions of the different structural changes which we model. The mutation type is generated independent of the rest of the sentence. Feature Description NEG Negation STY Style (e.g. statement, question) QUO Is node part of a quoted expression? MD Modal verb associated with node TEN Tense (past, present, future) MOOD Mood (infinitive, perfect, progressive) CONJ Is node part of a conjoined expression? IND Indicator Table 1: Features 3.1 Model with Independence Assumptions With all of the independence assumptions described above, the translation model becomes: P (Fi | Ei) = P (fi | ei)P (t f i | te","i , t f par(i)) ×P (hi)P (mi) n ∏ j=1","P (φf i [j] | φe","i [j]) (4)"]},{"title":"4 Training","paragraphs":["The Czech and English data are preprocessed (see Section 2.1) and the resulting dependency structures are aligned (see Section 2.2). We estimate the model parameters from this aligned data by maximum like-lihood estimation. In addition, we gather the inverse probabilities P (E | F ) for use in the figure of merit which guides the decoder’s search."]},{"title":"5 Decoding","paragraphs":["Given a Czech sentence to translate, we first process it as described in Section 2.1. The resulting dependency structure is the input to the decoder. The decoder itself is a best-first decoder whose priority queue holds partially-constructed English nodes.","For our figure of merit to guide the search, we use the probability P (E | F ). We normalize this using the perplexity (2H",") to compensate for the different number of possible values for the features φ[j]. Given two different features whose values have the same probability, the figure of merit for the feature with the greater uncertainty will be boosted. This prevents features with few possible values from monopolizing the search at the expense of the other features. Thus, for feature φe","i [j], the figure of merit is","F OM = P (φe","i [j] | φf","i [j]) × 2H(Φe","i [j]|φf","i [j])","(5) 94","Since our goal is to build a forest of partial translations, we translate each Czech dependency node independently of the others. (As more conditioning factors are added in the future, we will instead translate small subtrees rather than single nodes.) Each translated node Ei is constructed incrementally in the following order: 1. Choose the head position hi","2. Generate the part of speech te i","3. For j = 1..n, generate φe i [j] 4. Choose a structural mutation mi","English nodes continue to be generated until either the queue or some other stopping condition is reached (e.g. having a certain number of possible translations for each Czech node). After stopping, we are left with a forest of English dependency nodes or subtrees."]},{"title":"6 Language Model","paragraphs":["We use a syntax-based language model which was originally developed for use in speech recognition (Charniak, 2001) and later adapted to work with a syntax-based machine translation system (Charniak et al., 2001). This language model requires a forest of partial phrase structures as input. Therefore, the format of the output of the decoder must be changed. This is the inverse transformation of the one performed during corpus preparation. We accomplish this with a statistical tree transformation model whose parameters are estimated during the corpus preparation phase."]},{"title":"7 Evaluation","paragraphs":["We propose to evaluate system performance with version 0.9 of the NIST automated scorer (NIST, 2002), which is a modification of the BLEU system (Papineni et al., 2001). BLEU calculates a score based on a weighted sum of the counts of matching n-grams, along with a penalty for a significant difference in length between the system output and the reference translation closest in length. Experiments have shown a high degree of correlation between BLEU score and the translation quality judgments of humans. The most interesting difference in the NIST scorer is that it weights n-grams based on a notion of informativeness. Details of the scorer can be found in their paper.","For our experiments, we propose to use the data from the PDT, which has already been segmented into training, held out (or development test), and evaluation sets. As a baseline, we will run the GIZA++ implementation of IBM’s Model 4 translation algorithm under the same training conditions as our own system (Al-Onaizan et al., 1999; Och and Ney, 2000; Germann et al., 2001)."]},{"title":"8 Future Work","paragraphs":["Our first priority is to complete the final pieces so that we have an end-to-end system to experiment with. Once we are able to evaluate our system output, our first priority will be to analyze the system errors and adjust the model accordingly. We recognize that our independence assumptions are generally too strong, and improving them is a hight priority. Adding more conditioning factors should improve the quality of the decoder output as well as reducing the amount of probability mass lost on structures which are not well formed. With this will come sparse data issues, so it will also be important for us to incorporate smoothing into the model.","There are many interesting subproblems which deserve attention and we hope to examine at least a couple of these in the near future. Among these are discontinuous constituents, head switching, phrasal translation, English word stemming, and improved modeling of structural changes."]},{"title":"Acknowledgments","paragraphs":["This work was supported in part by NSF grant IGERT-9870676. We would like to thank Jan Hajič, Martin Čmejrek, Jan Cuřı́n for all of their assistance."]},{"title":"References","paragraphs":["Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin Knight, John Lafferty, Dan Melamed, Franz-Josef Och, David Purdy, Noah A. Smith, and David Yarowsky. 1999. Statistical machine translation: Final report, JHU workshop 1999. www.clsp.jhu.edu/ws99/projects/mt/final report/mtfinal-report.ps. 95","Adam L. Berger, Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, John R. Gillett, John D. Lafferty, Robert L. Mercer, Harry Printz, and Luboš Ureš. 1994. The Candide system for machine translation. In Proceedings of the ARPA Human Language Technology Workshop.","Alena Böhmová, Jan Hajič, Eva Hajičová, and Barbora Hladká. 2001. The Prague Dependency Treebank: Three-level annotation scenario. In Anne Abeillé, editor, Treebanks: Building and Using Syntactically An-notated Corpora. Kluwer Academic Publishers.","Alena Böhmová. 2001. Automatic procedures in tectogrammatical tagging. The Prague Bulletin of Mathematical Linguistics, 76.","Peter F. Brown, John Cocke, Stephen A. Della Pietra, Vincent J. Della Pietra, Fredrick Jelinek, John D. Lafferty, Robert L. Mercer, and Paul S. Roossin. 1990. A statistical approach to machine translation. Computational Linguistics, 16(2):79–85.","Peter F. Brown, Stephen A. Della Petra, Vincent J. Della Pietra, John D. Lafferty, and Robert L. Mercer. 1992. Analysis, statistical transfer, and synthesis in machine translation. In Proceedings of the Fourth International Conference on Theoretical and Methodological Issues in Machine Translation, pages 83–100.","Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311, June.","Eugene Charniak, Kevin Knight, and Kenji Yamada. 2001. Syntax-based language models for statistical machine translation. In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics, Toulouse, France, July.","Eugene Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings of the 1st Meeting of the North American Chapter of the Association for Computational Linguistics.","Eugene Charniak. 2001. Immediate-head parsing for language models. In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics, pages 116–123, Toulouse, France, July.","Martin Čmejrek, Jan Cuřı́n, and Jiřı́ Havelka. 2003. Czech-English Dependency-based Machine Translation. In EACL 2003 Proceedings of the Conference, pages 83–90, April 12–17, 2003.","Heidi Fox. 2002. Phrasal cohesion and statistical machine translation. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP 2002), July.","Ulrich Germann, Michael Jahr, Kevin Knight, Daniel Marcu, and Kenji Yamada. 2001. Fast decoding and optimal decoding for machine translation. In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics.","Jan Hajič and Barbora Hladká. 1998. Tagging Inflective Languages: Prediction of Morphological Categories for a Rich, Structured Tagset. In Proceedings of COLING-ACL Conference, pages 483–490, Montreal, Canada.","Philip Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the Human Language Technology and North American Association for Computational Linguistics Conference, Edmonton, Canada, May.","Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 13(2):313–330, June.","Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann Bies, Mark Ferguson, Karen Katz, and Britta Schasberger. 1994. The Penn Treebank: Annotating predicate argument structure. In Proceedings of the ARPA Human Language Technology Workshop, pages 114–119.","NIST. 2002. Automatic evaluation of machine translation quality using n-gram co-occurrence statistics. www.nist.gov/speech/tests/mt/doc/ngram-study.pdf.","Franz Josef Och and Hermann Ney. 2000. Improved statistical alignment models. In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics, pages 440–447.","Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2001. Bleu: A method for automatic evaluation of machine translation. Technical report, IBM.","Dekai Wu and Hongsing Wong. 1998. Machine translation with a stochastic grammatical channel. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics, pages 1408–1414.","Kenji Yamada and Kevin Knight. 2001. A syntax-based statistical translation model. In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics.","Zdeněk Žabokrtský, Petr Sgall, and Sašo Džeroski. 2002. Machine learning approach to automatic functor as-signment in the Prague Dependency Treebank. In Proceedings of LREC 2002 (Third International Conference on Language Resources and Evaluation), volume V, pages 1513–1520, Las Palmas de Gran Canaria, Spain. 96"]}],"references":[{"authors":[{"first":"Yaser","last":"Al-Onaizan"},{"first":"Jan","last":"Curin"},{"first":"Michael","last":"Jahr"},{"first":"Kevin","last":"Knight"},{"first":"John","last":"Lafferty"},{"first":"Dan","last":"Melamed"},{"first":"Franz-Josef","last":"Och"},{"first":"David","last":"Purdy"},{"first":"Noah","middle":"A.","last":"Smith"},{"first":"David","last":"Yarowsky"}],"year":"1999","title":"Statistical machine translation: Final report, JHU workshop 1999","source":"Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin Knight, John Lafferty, Dan Melamed, Franz-Josef Och, David Purdy, Noah A. Smith, and David Yarowsky. 1999. Statistical machine translation: Final report, JHU workshop 1999. www.clsp.jhu.edu/ws99/projects/mt/final report/mtfinal-report.ps. 95"},{"authors":[{"first":"Adam","middle":"L.","last":"Berger"},{"first":"Peter","middle":"F.","last":"Brown"},{"first":"Stephen","middle":"A. Della","last":"Pietra"},{"first":"Vincent","middle":"J. Della","last":"Pietra"},{"first":"John","middle":"R.","last":"Gillett"},{"first":"John","middle":"D.","last":"Lafferty"},{"first":"Robert","middle":"L.","last":"Mercer"},{"first":"Harry","last":"Printz"},{"first":"Luboš","last":"Ureš"}],"year":"1994","title":"The Candide system for machine translation","source":"Adam L. Berger, Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, John R. Gillett, John D. Lafferty, Robert L. Mercer, Harry Printz, and Luboš Ureš. 1994. The Candide system for machine translation. In Proceedings of the ARPA Human Language Technology Workshop."},{"authors":[{"first":"Alena","last":"Böhmová"},{"first":"Jan","last":"Hajič"},{"first":"Eva","last":"Hajičová"},{"first":"Barbora","last":"Hladká"}],"year":"2001","title":"The Prague Dependency Treebank: Three-level annotation scenario","source":"Alena Böhmová, Jan Hajič, Eva Hajičová, and Barbora Hladká. 2001. The Prague Dependency Treebank: Three-level annotation scenario. In Anne Abeillé, editor, Treebanks: Building and Using Syntactically An-notated Corpora. Kluwer Academic Publishers."},{"authors":[{"first":"Alena","last":"Böhmová"}],"year":"2001","title":"Automatic procedures in tectogrammatical tagging","source":"Alena Böhmová. 2001. Automatic procedures in tectogrammatical tagging. The Prague Bulletin of Mathematical Linguistics, 76."},{"authors":[{"first":"Peter","middle":"F.","last":"Brown"},{"first":"John","last":"Cocke"},{"first":"Stephen","middle":"A. Della","last":"Pietra"},{"first":"Vincent","middle":"J. Della","last":"Pietra"},{"first":"Fredrick","last":"Jelinek"},{"first":"John","middle":"D.","last":"Lafferty"},{"first":"Robert","middle":"L.","last":"Mercer"},{"first":"Paul","middle":"S.","last":"Roossin"}],"year":"1990","title":"A statistical approach to machine translation","source":"Peter F. Brown, John Cocke, Stephen A. Della Pietra, Vincent J. Della Pietra, Fredrick Jelinek, John D. Lafferty, Robert L. Mercer, and Paul S. Roossin. 1990. A statistical approach to machine translation. Computational Linguistics, 16(2):79–85."},{"authors":[{"first":"Peter","middle":"F.","last":"Brown"},{"first":"Stephen","middle":"A. Della","last":"Petra"},{"first":"Vincent","middle":"J. Della","last":"Pietra"},{"first":"John","middle":"D.","last":"Lafferty"},{"first":"Robert","middle":"L.","last":"Mercer"}],"year":"1992","title":"Analysis, statistical transfer, and synthesis in machine translation","source":"Peter F. Brown, Stephen A. Della Petra, Vincent J. Della Pietra, John D. Lafferty, and Robert L. Mercer. 1992. Analysis, statistical transfer, and synthesis in machine translation. In Proceedings of the Fourth International Conference on Theoretical and Methodological Issues in Machine Translation, pages 83–100."},{"authors":[{"first":"Peter","middle":"F.","last":"Brown"},{"first":"Stephen","middle":"A. Della","last":"Pietra"},{"first":"Vincent","middle":"J. Della","last":"Pietra"},{"first":"Robert","middle":"L.","last":"Mercer"}],"year":"1993","title":"The mathematics of machine translation: Parameter estimation","source":"Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311, June."},{"authors":[{"first":"Eugene","last":"Charniak"},{"first":"Kevin","last":"Knight"},{"first":"Kenji","last":"Yamada"}],"year":"2001","title":"Syntax-based language models for statistical machine translation","source":"Eugene Charniak, Kevin Knight, and Kenji Yamada. 2001. Syntax-based language models for statistical machine translation. In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics, Toulouse, France, July."},{"authors":[{"first":"Eugene","last":"Charniak"}],"year":"2000","title":"A maximum-entropy-inspired parser","source":"Eugene Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings of the 1st Meeting of the North American Chapter of the Association for Computational Linguistics."},{"authors":[{"first":"Eugene","last":"Charniak"}],"year":"2001","title":"Immediate-head parsing for language models","source":"Eugene Charniak. 2001. Immediate-head parsing for language models. In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics, pages 116–123, Toulouse, France, July."},{"authors":[{"first":"Martin","last":"Čmejrek"},{"first":"Jan","last":"Cuřı́n"},{"first":"Jiřı́","last":"Havelka"}],"year":"2003","title":"Czech-English Dependency-based Machine Translation","source":"Martin Čmejrek, Jan Cuřı́n, and Jiřı́ Havelka. 2003. Czech-English Dependency-based Machine Translation. In EACL 2003 Proceedings of the Conference, pages 83–90, April 12–17, 2003."},{"authors":[{"first":"Heidi","last":"Fox"}],"year":"2002","title":"Phrasal cohesion and statistical machine translation","source":"Heidi Fox. 2002. Phrasal cohesion and statistical machine translation. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP 2002), July."},{"authors":[{"first":"Ulrich","last":"Germann"},{"first":"Michael","last":"Jahr"},{"first":"Kevin","last":"Knight"},{"first":"Daniel","last":"Marcu"},{"first":"Kenji","last":"Yamada"}],"year":"2001","title":"Fast decoding and optimal decoding for machine translation","source":"Ulrich Germann, Michael Jahr, Kevin Knight, Daniel Marcu, and Kenji Yamada. 2001. Fast decoding and optimal decoding for machine translation. In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics."},{"authors":[{"first":"Jan","last":"Hajič"},{"first":"Barbora","last":"Hladká"}],"year":"1998","title":"Tagging Inflective Languages: Prediction of Morphological Categories for a Rich, Structured Tagset","source":"Jan Hajič and Barbora Hladká. 1998. Tagging Inflective Languages: Prediction of Morphological Categories for a Rich, Structured Tagset. In Proceedings of COLING-ACL Conference, pages 483–490, Montreal, Canada."},{"authors":[{"first":"Philip","last":"Koehn"},{"first":"Franz","middle":"Josef","last":"Och"},{"first":"Daniel","last":"Marcu"}],"year":"2003","title":"Statistical phrase-based translation","source":"Philip Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the Human Language Technology and North American Association for Computational Linguistics Conference, Edmonton, Canada, May."},{"authors":[{"first":"Mitchell","middle":"P.","last":"Marcus"},{"first":"Beatrice","last":"Santorini"},{"first":"Mary","middle":"Ann","last":"Marcinkiewicz"}],"year":"1993","title":"Building a large annotated corpus of English: The Penn Treebank","source":"Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 13(2):313–330, June."},{"authors":[{"first":"Mitchell","last":"Marcus"},{"first":"Grace","last":"Kim"},{"first":"Mary","middle":"Ann","last":"Marcinkiewicz"},{"first":"Robert","last":"MacIntyre"},{"first":"Ann","last":"Bies"},{"first":"Mark","last":"Ferguson"},{"first":"Karen","last":"Katz"},{"first":"Britta","last":"Schasberger"}],"year":"1994","title":"The Penn Treebank: Annotating predicate argument structure","source":"Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann Bies, Mark Ferguson, Karen Katz, and Britta Schasberger. 1994. The Penn Treebank: Annotating predicate argument structure. In Proceedings of the ARPA Human Language Technology Workshop, pages 114–119."},{"authors":[{"last":"NIST"}],"year":"2002","title":"Automatic evaluation of machine translation quality using n-gram co-occurrence statistics","source":"NIST. 2002. Automatic evaluation of machine translation quality using n-gram co-occurrence statistics. www.nist.gov/speech/tests/mt/doc/ngram-study.pdf."},{"authors":[{"first":"Franz","middle":"Josef","last":"Och"},{"first":"Hermann","last":"Ney"}],"year":"2000","title":"Improved statistical alignment models","source":"Franz Josef Och and Hermann Ney. 2000. Improved statistical alignment models. In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics, pages 440–447."},{"authors":[{"first":"Kishore","last":"Papineni"},{"first":"Salim","last":"Roukos"},{"first":"Todd","last":"Ward"},{"first":"Wei-Jing","last":"Zhu"}],"year":"2001","title":"Bleu: A method for automatic evaluation of machine translation","source":"Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2001. Bleu: A method for automatic evaluation of machine translation. Technical report, IBM."},{"authors":[{"first":"Dekai","last":"Wu"},{"first":"Hongsing","last":"Wong"}],"year":"1998","title":"Machine translation with a stochastic grammatical channel","source":"Dekai Wu and Hongsing Wong. 1998. Machine translation with a stochastic grammatical channel. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics, pages 1408–1414."},{"authors":[{"first":"Kenji","last":"Yamada"},{"first":"Kevin","last":"Knight"}],"year":"2001","title":"A syntax-based statistical translation model","source":"Kenji Yamada and Kevin Knight. 2001. A syntax-based statistical translation model. In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics."},{"authors":[{"first":"Zdeněk","last":"Žabokrtský"},{"first":"Petr","last":"Sgall"},{"first":"Sašo","last":"Džeroski"}],"year":"2002","title":"Machine learning approach to automatic functor as-signment in the Prague Dependency Treebank","source":"Zdeněk Žabokrtský, Petr Sgall, and Sašo Džeroski. 2002. Machine learning approach to automatic functor as-signment in the Prague Dependency Treebank. In Proceedings of LREC 2002 (Third International Conference on Language Resources and Evaluation), volume V, pages 1513–1520, Las Palmas de Gran Canaria, Spain. 96"}],"cites":[{"style":0,"text":"Brown et al., 1990","origin":{"pointer":"/sections/2/paragraphs/0","offset":346,"length":18},"authors":[{"last":"Brown"},{"last":"al."}],"year":"1990","references":["/references/4"]},{"style":0,"text":"Brown et al., 1992","origin":{"pointer":"/sections/2/paragraphs/0","offset":366,"length":18},"authors":[{"last":"Brown"},{"last":"al."}],"year":"1992","references":["/references/5"]},{"style":0,"text":"Brown et al., 1993","origin":{"pointer":"/sections/2/paragraphs/0","offset":386,"length":18},"authors":[{"last":"Brown"},{"last":"al."}],"year":"1993","references":["/references/6"]},{"style":0,"text":"Berger et al., 1994","origin":{"pointer":"/sections/2/paragraphs/0","offset":406,"length":19},"authors":[{"last":"Berger"},{"last":"al."}],"year":"1994","references":["/references/1"]},{"style":0,"text":"Brown et al. (1993)","origin":{"pointer":"/sections/2/paragraphs/0","offset":474,"length":19},"authors":[{"last":"Brown"},{"last":"al."}],"year":"1993","references":["/references/6"]},{"style":0,"text":"Wu and Wong, 1998","origin":{"pointer":"/sections/2/paragraphs/4","offset":46,"length":17},"authors":[{"last":"Wu"},{"last":"Wong"}],"year":"1998","references":["/references/20"]},{"style":0,"text":"Yamada and Knight, 2001","origin":{"pointer":"/sections/2/paragraphs/5","offset":40,"length":23},"authors":[{"last":"Yamada"},{"last":"Knight"}],"year":"2001","references":["/references/21"]},{"style":0,"text":"Koehn et al., 2003","origin":{"pointer":"/sections/2/paragraphs/6","offset":40,"length":18},"authors":[{"last":"Koehn"},{"last":"al."}],"year":"2003","references":["/references/14"]},{"style":0,"text":"Fox, 2002","origin":{"pointer":"/sections/3/paragraphs/0","offset":539,"length":9},"authors":[{"last":"Fox"}],"year":"2002","references":["/references/11"]},{"style":0,"text":"Marcus et al., 1993","origin":{"pointer":"/sections/3/paragraphs/1","offset":394,"length":19},"authors":[{"last":"Marcus"},{"last":"al."}],"year":"1993","references":["/references/15"]},{"style":0,"text":"Marcus et al., 1994","origin":{"pointer":"/sections/3/paragraphs/1","offset":415,"length":19},"authors":[{"last":"Marcus"},{"last":"al."}],"year":"1994","references":["/references/16"]},{"style":0,"text":"Böhmová et al., 2001","origin":{"pointer":"/sections/3/paragraphs/1","offset":540,"length":20},"authors":[{"last":"Böhmová"},{"last":"al."}],"year":"2001","references":["/references/2"]},{"style":0,"text":"Hajič and Hladká, 1998","origin":{"pointer":"/sections/3/paragraphs/2","offset":166,"length":22},"authors":[{"last":"Hajič"},{"last":"Hladká"}],"year":"1998","references":["/references/13"]},{"style":0,"text":"Böhmová, 2001","origin":{"pointer":"/sections/3/paragraphs/2","offset":414,"length":13},"authors":[{"last":"Böhmová"}],"year":"2001","references":["/references/3"]},{"style":0,"text":"Charniak, 2000","origin":{"pointer":"/sections/3/paragraphs/3","offset":318,"length":14},"authors":[{"last":"Charniak"}],"year":"2000","references":["/references/8"]},{"style":0,"text":"Brown et al., 1993","origin":{"pointer":"/sections/3/paragraphs/3","offset":791,"length":18},"authors":[{"last":"Brown"},{"last":"al."}],"year":"1993","references":["/references/6"]},{"style":0,"text":"Charniak, 2001","origin":{"pointer":"/sections/7/paragraphs/0","offset":99,"length":14},"authors":[{"last":"Charniak"}],"year":"2001","references":["/references/9"]},{"style":0,"text":"Charniak et al., 2001","origin":{"pointer":"/sections/7/paragraphs/0","offset":189,"length":21},"authors":[{"last":"Charniak"},{"last":"al."}],"year":"2001","references":["/references/7"]},{"style":0,"text":"NIST, 2002","origin":{"pointer":"/sections/8/paragraphs/0","offset":89,"length":10},"authors":[{"last":"NIST"}],"year":"2002","references":["/references/17"]},{"style":0,"text":"Papineni et al., 2001","origin":{"pointer":"/sections/8/paragraphs/0","offset":146,"length":21},"authors":[{"last":"Papineni"},{"last":"al."}],"year":"2001","references":["/references/19"]},{"style":0,"text":"Al-Onaizan et al., 1999","origin":{"pointer":"/sections/8/paragraphs/1","offset":309,"length":23},"authors":[{"last":"Al-Onaizan"},{"last":"al."}],"year":"1999","references":["/references/0"]},{"style":0,"text":"Och and Ney, 2000","origin":{"pointer":"/sections/8/paragraphs/1","offset":334,"length":17},"authors":[{"last":"Och"},{"last":"Ney"}],"year":"2000","references":["/references/18"]},{"style":0,"text":"Germann et al., 2001","origin":{"pointer":"/sections/8/paragraphs/1","offset":353,"length":20},"authors":[{"last":"Germann"},{"last":"al."}],"year":"2001","references":["/references/12"]}]}
