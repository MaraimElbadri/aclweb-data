{"sections":[{"title":"","paragraphs":["Proceedings of the 43rd Annual Meeting of the ACL, pages 403–410, Ann Arbor, June 2005. c⃝2005 Association for Computational Linguistics"]},{"title":"Domain Kernels for Word Sense Disambiguation Alo Gliozzo and Claudio Giuliano and Carlo Strapparava ITC-irst, Istituto per la Ricerca Scientica e Tecnologica I-38050, Trento, ITALY { gliozzo,giuliano,strappa} @itc.it Abstract","paragraphs":["In this paper we present a supervised Word Sense Disambiguation methodology, that exploits kernel methods to model sense distinctions. In particular a combination of kernel functions is adopted to estimate independently both syntagmatic and domain similarity. We dened a kernel function, namely the Domain Kernel, that allowed us to plug external knowledge into the supervised learning process. External knowledge is acquired from unlabeled data in a totally unsupervised way, and it is represented by means of Domain Models. We evaluated our methodology on several lexical sample tasks in different languages, outperforming signicantly the state-of-the-art for each of them, while reducing the amount of labeled training data required for learning."]},{"title":"1 Introduction","paragraphs":["The main limitation of many supervised approaches for Natural Language Processing (NLP) is the lack of available annotated training data. This problem is known as the Knowledge Acquisition Bottleneck.","To reach high accuracy, state-of-the-art systems for Word Sense Disambiguation (WSD) are designed according to a supervised learning framework, in which the disambiguation of each word in the lexicon is performed by constructing a different classier. A large set of sense tagged examples is then required to train each classier. This methodology is called word expert approach (Small, 1980; Yarowsky and Florian, 2002). However this is clearly unfeasible for all-words WSD tasks, in which all the words of an open text should be disambiguated.","On the other hand, the word expert approach works very well for lexical sample WSD tasks (i.e. tasks in which it is required to disambiguate only those words for which enough training data is provided). As the original rationale of the lexical sample tasks was to dene a clear experimental settings to enhance the comprehension of WSD, they should be considered as preceding exercises to all-words tasks. However this is not the actual case. Algorithms designed for lexical sample WSD are often based on pure supervision and hence data hungry.","We think that lexical sample WSD should regain its original explorative role and possibly use a minimal amount of training data, exploiting instead external knowledge acquired in an unsupervised way to reach the actual state-of-the-art performance.","By the way, minimal supervision is the basis of state-of-the-art systems for all-words tasks (e.g. (Mihalcea and Faruque, 2004; Decadt et al., 2004)), that are trained on small sense tagged corpora (e.g. SemCor), in which few examples for a subset of the ambiguous words in the lexicon can be found. Thus improving the performance of WSD systems with few learning examples is a fundamental step towards the direction of designing a WSD system that works well on real texts.","In addition, it is a common opinion that the performance of state-of-the-art WSD systems is not satisfactory from an applicative point of view yet. 403","To achieve these goals we identied two promising research directions:","1. Modeling independently domain and syntagmatic aspects of sense distinction, to improve the feature representation of sense tagged examples (Gliozzo et al., 2004).","2. Leveraging external knowledge acquired from unlabeled corpora.","The rst direction is motivated by the linguistic assumption that syntagmatic and domain (associative) relations are both crucial to represent sense distictions, while they are basically originated by very different phenomena. Syntagmatic relations hold among words that are typically located close to each other in the same sentence in a given temporal order, while domain relations hold among words that are typically used in the same semantic domain (i.e. in texts having similar topics (Gliozzo et al., 2004)). Their different nature suggests to adopt different learning strategies to detect them.","Regarding the second direction, external knowledge would be required to help WSD algorithms to better generalize over the data available for training. On the other hand, most of the state-of-the-art supervised approaches to WSD are still completely based on internal information only (i.e. the only information available to the training algorithm is the set of manually annotated examples). For example, in the Senseval-3 evaluation exercise (Mihalcea and Edmonds, 2004) many lexical sample tasks were provided, beyond the usual labeled training data, with a large set of unlabeled data. However, at our knowledge, none of the participants exploited this unlabeled material. Exploring this direction is the main focus of this paper. In particular we acquire a Domain Model (DM) for the lexicon (i.e. a lexical resource representing domain associations among terms), and we exploit this information inside our supervised WSD algorithm. DMs can be automatically induced from unlabeled corpora, allowing the portability of the methodology among languages.","We identied kernel methods as a viable framework in which to implement the assumptions above (Strapparava et al., 2004).","Exploiting the properties of kernels, we have dened independently a set of domain and syntagmatic kernels and we combined them in order to dene a complete kernel for WSD. The domain kernels estimate the (domain) similarity (Magnini et al., 2002) among contexts, while the syntagmatic kernels evaluate the similarity among collocations.","We will demonstrate that using DMs induced from unlabeled corpora is a feasible strategy to in-crease the generalization capability of the WSD algorithm. Our system far outperforms the state-of-the-art systems in all the tasks in which it has been tested. Moreover, a comparative analysis of the learning curves shows that the use of DMs allows us to remarkably reduce the amount of sense-tagged examples, opening new scenarios to develop systems for all-words tasks with minimal supervision.","The paper is structured as follows. Section 2 in-troduces the notion of Domain Model. In particular an automatic acquisition technique based on Latent Semantic Analysis (LSA) is described. In Section 3 we present a WSD system based on a combination of kernels. In particular we dene a Domain Kernel (see Section 3.1) and a Syntagmatic Kernel (see Section 3.2), to model separately syntagmatic and domain aspects. In Section 4 our WSD system is evaluated in the Senseval-3 English, Italian, Spanish and Catalan lexical sample tasks."]},{"title":"2 Domain Models","paragraphs":["The simplest methodology to estimate the similarity among the topics of two texts is to represent them by means of vectors in the Vector Space Model (VSM), and to exploit the cosine similarity. More formally, let C = ft1, t2, . . . , tng be a corpus, let V = fw1, w2, . . . , wkg be its vocabulary, let T be the k n term-by-document matrix representing C, such that ti,j is the frequency of word wi into the text tj. The VSM is a k-dimensional space Rk",", in which the text tj 2 C is represented by means of the vector ⃗tj such that the ith","component of ⃗tj is ti,j. The similarity among two texts in the VSM is estimated by computing the cosine among them.","However this approach does not deal well with lexical variability and ambiguity. For example the two sentences he is affected by AIDS and HIV is a virus do not have any words in common. In the 404 VSM their similarity is zero because they have orthogonal vectors, even if the concepts they express are very closely related. On the other hand, the similarity between the two sentences the laptop has been infected by a virus and HIV is a virus would turn out very high, due to the ambiguity of the word virus.","To overcome this problem we introduce the notion of Domain Model (DM), and we show how to use it in order to dene a domain VSM in which texts and terms are represented in a uniform way.","A DM is composed by soft clusters of terms. Each cluster represents a semantic domain, i.e. a set of terms that often co-occur in texts having similar topics. A DM is represented by a k k′","rectangular matrix D, containing the degree of association among terms and domains, as illustrated in Table 1.","MEDICINE COMPUTER SCIENCE HIV 1 0 AIDS 1 0 virus 0.5 0.5 laptop 0 1 Table 1: Example of Domain Matrix","DMs can be used to describe lexical ambiguity and variability. Lexical ambiguity is represented by associating one term to more than one domain, while variability is represented by associating different terms to the same domain. For example the term virus is associated to both the domain COMPUTER SCIENCE and the domain MEDICINE (ambiguity) while the domain MEDICINE is associated to both the terms AIDS and HIV (variability).","More formally, let D = fD1, D2, ..., Dk′ g be a set of domains, such that k′","k. A DM is fully dened by a k k′","domain matrix D representing in each cell di,z the domain relevance of term wi with respect to the domain Dz. The domain matrix D is used to dene a function D : Rk","! Rk′",", that maps","the vectors ⃗tj expressed into the classical VSM, into","the vectors ⃗t′","j in the domain VSM. D is dened by1","D(⃗tj) = ⃗tj(IIDF D) = ⃗t′","j (1) 1","In (Wong et al., 1985) the formula 1 is used to dene a Generalized Vector Space Model, of which the Domain VSM is a particular instance.","where IIDF","is a k k diagonal matrix such that iIDF i,i = IDF (wi), ⃗tj is represented as a row vector, and IDF (wi) is the Inverse Document Frequency of wi.","Vectors in the domain VSM are called Domain Vectors (DVs). DVs for texts are estimated by exploiting the formula 1, while the DV ⃗w′","i, corresponding to the word wi 2 V is the ith","row of the domain matrix D. To be a valid domain matrix such vectors should be normalized (i,e. h ⃗w′","i, ⃗w′","ii = 1).","In the Domain VSM the similarity among DVs is estimated by taking into account second order relations among terms. For example the similarity of the two sentences He is affected by AIDS and HIV is a virus is very high, because the terms AIDS, HIV and virus are highly associated to the domain MEDICINE.","A DM can be estimated from hand made lexical resources such as WORDNET DOMAINS (Magnini and Cavaglia, 2000), or by performing a term clustering process on a large corpus. We think that the second methodology is more attractive, because it allows us to automatically acquire DMs for different languages.","In this work we propose the use of Latent Semantic Analysis (LSA) to induce DMs from corpora. LSA is an unsupervised technique for estimating the similarity among texts and terms in a corpus. LSA is performed by means of a Singular Value Decomposition (SVD) of the term-by-document matrix T describing the corpus. The SVD algorithm can be exploited to acquire a domain matrix D from a large corpus C in a totally unsupervised way. SVD decomposes the term-by-document matrix T into three matrixes T ’ VΣk′ UT","where Σ","k′ is the diagonal k k matrix containing the highest k′","k eigenvalues of T, and all the remaining elements set to 0. The parameter k′","is the dimensionality of the Domain VSM and can be xed in advance2",". Under this setting we dene the domain matrix DLSA as","DLSA = IN V √","Σk′ (2)","where IN","is a diagonal matrix such that iN","i,i = 1q","⟨ ⃗w′ i, ⃗w′","i⟩",", ⃗w′","i is the ith","row of the matrix Vp","Σk′.3 2 It is not clear how to choose the right dimensionality. In","our experiments we used 50 dimensions. 3 When DLSA is substituted in Equation 1 the Domain VSM 405"]},{"title":"3 Kernel Methods for WSD","paragraphs":["In the introduction we discussed two promising directions for improving the performance of a supervised disambiguation system. In this section we show how these requirements can be efciently implemented in a natural and elegant way by using kernel methods.","The basic idea behind kernel methods is to embed the data into a suitable feature space F via a mapping function φ : X ! F , and then use a linear algorithm for discovering nonlinear patterns. Instead of using the explicit mapping φ, we can use a kernel function K : X X ! R, that corresponds to the inner product in a feature space which is, in general, different from the input space.","Kernel methods allow us to build a modular system, as the kernel function acts as an interface between the data and the learning algorithm. Thus the kernel function becomes the only domain specic module of the system, while the learning algorithm is a general purpose component. Potentially any kernel function can work with any kernel-based algorithm. In our system we use Support Vector Machines (Cristianini and Shawe-Taylor, 2000).","Exploiting the properties of the kernel functions, it is possible to dene the kernel combination schema as KC (xi, xj) = n ∑ l=1 Kl(xi, xj)","√ Kl(xj, xj)Kl(xi, xi) (3)","Our WSD system is then dened as combination of n basic kernels. Each kernel adds some additional dimensions to the feature space. In particular, we have dened two families of kernels: Domain and Syntagmatic kernels. The former is composed by both the Domain Kernel (KD) and the Bag-of-Words kernel (KBoW ), that captures domain aspects (see Section 3.1). The latter captures the syntagmatic aspects of sense distinction and it is composed by two kernels: the collocation kernel (KColl) and is equivalent to a Latent Semantic Space (Deerwester et al., 1990). The only difference in our formulation is that the vectors representing the terms in the Domain VSM are normalized by the matrix IN",", and then rescaled, according to their IDF value, by matrix IIDF",". Note the analogy with the tf idf term weighting schema (Salton and McGill, 1983), widely adopted in Information Retrieval. the Part of Speech kernel (KP oS) (see Section 3.2). The WSD kernels (K ′","W SD and KW SD) are then dened by combining them (see Section 3.3). 3.1 Domain Kernels In (Magnini et al., 2002), it has been claimed that knowing the domain of the text in which the word is located is a crucial information for WSD. For example the (domain) polysemy among the COMPUTER SCIENCE and the MEDICINE senses of the word virus can be solved by simply considering the domain of the context in which it is located.","This assumption can be modeled by dening a kernel that estimates the domain similarity among the contexts of the words to be disambiguated, namely the Domain Kernel. The Domain Kernel estimates the similarity among the topics (domains) of two texts, so to capture domain aspects of sense distinction. It is a variation of the Latent Semantic Kernel (Shawe-Taylor and Cristianini, 2004), in which a DM (see Section 2) is exploited to dene an explicit mapping D : Rk","! Rk′","from the classical VSM into the Domain VSM. The Domain Kernel is dened by KD(ti, tj) = hD(ti), D(tj)i","√ hD(ti), D(tj)ihD(ti), D(tj)i (4)","where D is the Domain Mapping dened in equation 1. Thus the Domain Kernel requires a Domain Matrix D. For our experiments we acquire the matrix DLSA, described in equation 2, from a generic collection of unlabeled documents, as explained in Section 2.","A more traditional approach to detect topic (domain) similarity is to extract Bag-of-Words (BoW) features from a large window of text around the word to be disambiguated. The BoW kernel, denoted by KBoW , is a particular case of the Domain Kernel, in which D = I, and I is the identity matrix. The BoW kernel does not require a DM, then it can be applied to the strictly supervised settings, in which an external knowledge source is not provided. 3.2 Syntagmatic kernels Kernel functions are not restricted to operate on vectorial objects ⃗x 2 Rk",". In principle kernels can be dened for any kind of object representation, as for 406 example sequences and trees. As stated in Section 1, syntagmatic relations hold among words collocated in a particular temporal order, thus they can be modeled by analyzing sequences of words.","We identied the string kernel (or word sequence kernel) (Shawe-Taylor and Cristianini, 2004) as a valid instrument to model our assumptions. The string kernel counts how many times a (non-contiguous) subsequence of symbols u of length n occurs in the input string s, and penalizes non-contiguous occurrences according to the number of gaps they contain (gap-weighted subsequence kernel).","Formally, let V be the vocabulary, the feature space associated with the gap-weighted subsequence kernel of length n is indexed by a set I of subsequences over V of length n. The (explicit) mapping function is dened by φn u(s) = ∑ i:u=s(i)","λl(i) , u 2 V n","(5)","where u = s(i) is a subsequence of s in the positions given by the tuple i, l(i) is the length spanned by u, and λ 2]0, 1] is the decay factor used to penalize non-contiguous subsequences.","The associate gap-weighted subsequence kernel is dened by","kn (s","i, sj) = ⟨φn (s","i), φn (s j)⟩ = X u∈V n","φn (s","i)φn (s j) (6)","We modied the generic denition of the string kernel in order to make it able to recognize collocations in a local window of the word to be disambiguated. In particular we dened two Syntagmatic kernels: the n-gram Collocation Kernel and the n-gram PoS Kernel. The n-gram Collocation kernel Kn","Coll is dened as a gap-weighted subsequence kernel applied to sequences of lemmata around the word l0 to be disambiguated (i.e. l−3, l−2, l−1, l0, l+1, l+2, l+3). This formulation allows us to estimate the number of common (sparse) subsequences of lemmata (i.e. collocations) between two examples, in order to capture syntagmatic similarity. In analogy we dened the PoS kernel K n","P oS, by setting s to the sequence of PoSs p−3, p−2, p−1, p0, p+1, p+2, p+3, where p0 is the PoS of the word to be disambiguated.","The denition of the gap-weighted subsequence kernel, provided by equation 6, depends on the parameter n, that represents the length of the subsequences analyzed when estimating the similarity among sequences. For example, K 2","Coll allows us to represent the bigrams around the word to be disambiguated in a more exible way (i.e. bigrams can be sparse). In WSD, typical features are bigrams and trigrams of lemmata and PoSs around the word to be disambiguated, then we dened the Collocation Kernel and the PoS Kernel respectively by equations 7 and 84",". KColl(si, sj) = p ∑ l=1","Kl Coll(si, sj) (7) KP oS(si, sj) = p ∑ l=1","Kl P oS(si, sj) (8) 3.3 WSD kernels In order to show the impact of using Domain Models in the supervised learning process, we dened two WSD kernels, by applying the kernel combination schema described by equation 3. Thus the following WSD kernels are fully specied by the list of the kernels that compose them. Kwsd composed by KColl, KP oS and KBoW","K′ wsd composed by KColl, KP oS, KBoW and KD The only difference between the two systems is that K′","wsd uses Domain Kernel KD. K′","wsd exploits external knowledge, in contrast to Kwsd, whose only available information is the labeled training data."]},{"title":"4 Evaluation and Discussion","paragraphs":["In this section we present the performance of our kernel-based algorithms for WSD. The objectives of these experiments are: to study the combination of different kernels,","to understand the benets of plugging external information using domain models, to verify the portability of our methodology","among different languages. 4 The parameters p and λ are optimized by cross-validation.","The best results are obtained setting p = 2, λ = 0.5 for KColl","and λ → 0 for KPoS. 407 4.1 WSD tasks We conducted the experiments on four lexical sample tasks (English, Catalan, Italian and Spanish) of the Senseval-3 competition (Mihalcea and Edmonds, 2004). Table 2 describes the tasks by reporting the number of words to be disambiguated, the mean polysemy, and the dimension of training, test and unlabeled corpora. Note that the organizers of the English task did not provide any unlabeled material. So for English we used a domain model built from a portion of BNC corpus, while for Spanish, Italian and Catalan we acquired DMs from the unlabeled corpora made available by the organizers.","#w pol # train # test # unlab Catalan 27 3.11 4469 2253 23935 English 57 6.47 7860 3944 - Italian 45 6.30 5145 2439 74788 Spanish 46 3.30 8430 4195 61252 Table 2: Dataset descriptions 4.2 Kernel Combination In this section we present an experiment to empirically study the kernel combination. The basic kernels (i.e. KBoW , KD, KColl and KP oS) have been compared to the combined ones (i.e. Kwsd and K′","wsd) on the English lexical sample task.","The results are reported in Table 3. The results show that combining kernels signicantly improves the performance of the system. KD KBoW KPoS KColl Kwsd K′","wsd","F1 65.5 63.7 62.9 66.7 69.7 73.3 Table 3: The performance (F1) of each basic kernel and their combination for English lexical sample task. 4.3 Portability and Performance We evaluated the performance of K ′","wsd and Kwsd on the lexical sample tasks described above. The results are showed in Table 4 and indicate that using DMs allowed K′","wsd to signicantly outperform Kwsd.","In addition, K′","wsd turns out the best systems for all the tested Senseval-3 tasks. Finally, the performance of K ′","wsd are higher than the human agreement for the English and Spanish tasks5",". Note that, in order to guarantee an uniform application to any language, we do not use any syntactic information provided by a parser. 4.4 Learning Curves The Figures 1, 2, 3 and 4 show the learning curves evaluated on K′","wsd and Kwsd for all the lexical sample tasks.","The learning curves indicate that K ′","wsd is far superior to Kwsd for all the tasks, even with few examples. The result is extremely promising, for it demonstrates that DMs allow to drastically reduce the amount of sense tagged data required for learning. It is worth noting, as reported in Table 5, that K′","wsd achieves the same performance of Kwsd using about half of the training data.","% of training English 54 Catalan 46 Italian 51 Spanish 50 Table 5: Percentage of sense tagged examples required by K′","wsd to achieve the same performance of Kwsd with full training."]},{"title":"5 Conclusion and Future Works","paragraphs":["In this paper we presented a supervised algorithm for WSD, based on a combination of kernel functions. In particular we modeled domain and syntagmatic aspects of sense distinctions by dening respectively domain and syntagmatic kernels. The Domain kernel exploits Domain Models, acquired from external untagged corpora, to estimate the similarity among the contexts of the words to be disambiguated. The syntagmatic kernels evaluate the similarity between collocations.","We evaluated our algorithm on several Senseval-3 lexical sample tasks (i.e. English, Spanish, Italian and Catalan) signicantly improving the state-ot-the-art for all of them. In addition, the performance","5","It is not clear if the inter-annotator-agreement can be considerated the upper bound for a WSD system. 408","MF Agreement BEST Kwsd K′","wsd DM+ English 55.2 67.3 72.9 69.7 73.3 3.6 Catalan 66.3 93.1 85.2 85.2 89.0 3.8 Italian 18.0 89.0 53.1 53.1 61.3 8.2 Spanish 67.7 85.3 84.2 84.2 88.2 4.0 Table 4: Comparative evaluation on the lexical sample tasks. Columns report: the Most Frequent baseline, the inter annotator agreement, the F1 of the best system at Senseval-3, the F1 of Kwsd, the F1 of K′","wsd,","DM+ (the improvement due to DM, i.e. K ′ wsd Kwsd). 0.5 0.55 0.6 0.65 0.7 0.75 0 0.2 0.4 0.6 0.8 1 F1 Percentage of training set K'wsd K wsd Figure 1: Learning curves for English lexical sample task. 0.65 0.7 0.75 0.8 0.85 0.9 0 0.2 0.4 0.6 0.8 1 F1 Percentage of training set K'wsd K wsd Figure 2: Learning curves for Catalan lexical sample task. of our system outperforms the inter annotator agreement in both English and Spanish, achieving the upper bound performance.","We demonstrated that using external knowledge 0.25 0.3 0.35 0.4 0.45 0.5 0.55 0.6 0.65 0 0.2 0.4 0.6 0.8 1 F1 Percentage of training set K'wsd K wsd Figure 3: Learning curves for Italian lexical sample task. 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0 0.2 0.4 0.6 0.8 1 F1 Percentage of training set K'wsd K wsd Figure 4: Learning curves for Spanish lexical sample task. inside a supervised framework is a viable methodology to reduce the amount of training data required for learning. In our approach the external knowledge is represented by means of Domain Models automat-409 ically acquired from corpora in a totally unsupervised way. Experimental results show that the use of Domain Models allows us to reduce the amount of training data, opening an interesting research direction for all those NLP tasks for which the Knowledge Acquisition Bottleneck is a crucial problem. In particular we plan to apply the same methodology to Text Categorization, by exploiting the Domain Kernel to estimate the similarity among texts. In this implementation, our WSD system does not exploit syntactic information produced by a parser. For the future we plan to integrate such information by adding a tree kernel (i.e. a kernel function that evaluates the similarity among parse trees) to the kernel combination schema presented in this paper. Last but not least, we are going to apply our approach to develop supervised systems for all-words tasks, where the quantity of data available to train each word expert classier is very low."]},{"title":"Acknowledgments","paragraphs":["Alo Gliozzo and Carlo Strapparava were partially supported by the EU project Meaning (IST-2001-34460). Claudio Giuliano was supported by the EU project Dot.Kom (IST-2001-34038). We would like to thank Oier Lopez de Lacalle for useful comments."]},{"title":"References","paragraphs":["N. Cristianini and J. Shawe-Taylor. 2000. An introduction to Support Vector Machines. Cambridge University Press.","B. Decadt, V. Hoste, W. Daelemens, and A. van den Bosh. 2004. Gambl, genetic algorithm optimiza-tion of memory-based wsd. In Proc. of Senseval-3, Barcelona, July.","S. Deerwester, S. Dumais, G. Furnas, T. Landauer, and R. Harshman. 1990. Indexing by latent semantic analysis. Journal of the American Society of Information Science.","A. Gliozzo, C. Strapparava, and I. Dagan. 2004. Unsupervised and supervised exploitation of semantic do-mains in lexical disambiguation. Computer Speech and Language, 18(3):275299.","B. Magnini and G. Cavaglia. 2000. Integrating subject eld codes into WordNet. In Proceedings of LREC-2000, pages 14131418, Athens, Greece, June.","B. Magnini, C. Strapparava, G. Pezzulo, and A. Gliozzo. 2002. The role of domain information in word sense disambiguation. Natural Language Engineering, 8(4):359373.","R. Mihalcea and P. Edmonds, editors. 2004. Proceedings of SENSEVAL-3, Barcelona, Spain, July.","R. Mihalcea and E. Faruque. 2004. Senselearner: Minimally supervised WSD for all words in open text. In Proceedings of SENSEVAL-3, Barcelona, Spain, July.","G. Salton and M.H. McGill. 1983. Introduction to modern information retrieval. McGraw-Hill, New York.","J. Shawe-Taylor and N. Cristianini. 2004. Kernel Methods for Pattern Analysis. Cambridge University Press.","S. Small. 1980. Word Expert Parsing: A Theory of Distributed Word-based Natural Language Understanding. Ph.D. Thesis, Department of Computer Science, University of Maryland.","C. Strapparava, A. Gliozzo, and C. Giuliano. 2004. Pattern abstraction and term similarity for word sense disambiguation: Irst at senseval-3. In Proc. of SENSEVAL-3 Third International Workshop on Evaluation of Systems for the Semantic Analysis of Text, pages 229234, Barcelona, Spain, July.","S.K.M. Wong, W. Ziarko, and P.C.N. Wong. 1985. Generalized vector space model in information retrieval. In Proceedings of the 8th","ACM SIGIR Conference.","D. Yarowsky and R. Florian. 2002. Evaluating sense disambiguation across diverse parameter space. Natural Language Engineering, 8(4):293310. 410"]}],"references":[{"authors":[{"first":"N.","last":"Cristianini"},{"first":"J.","last":"Shawe-Taylor"}],"year":"2000","title":"An introduction to Support Vector Machines","source":"N. Cristianini and J. Shawe-Taylor. 2000. An introduction to Support Vector Machines. Cambridge University Press."},{"authors":[{"first":"B.","last":"Decadt"},{"first":"V.","last":"Hoste"},{"first":"W.","last":"Daelemens"},{"first":"A.","middle":"van den","last":"Bosh"}],"year":"2004","title":"Gambl, genetic algorithm optimiza-tion of memory-based wsd","source":"B. Decadt, V. Hoste, W. Daelemens, and A. van den Bosh. 2004. Gambl, genetic algorithm optimiza-tion of memory-based wsd. In Proc. of Senseval-3, Barcelona, July."},{"authors":[{"first":"S.","last":"Deerwester"},{"first":"S.","last":"Dumais"},{"first":"G.","last":"Furnas"},{"first":"T.","last":"Landauer"},{"first":"R.","last":"Harshman"}],"year":"1990","title":"Indexing by latent semantic analysis","source":"S. Deerwester, S. Dumais, G. Furnas, T. Landauer, and R. Harshman. 1990. Indexing by latent semantic analysis. Journal of the American Society of Information Science."},{"authors":[{"first":"A.","last":"Gliozzo"},{"first":"C.","last":"Strapparava"},{"first":"I.","last":"Dagan"}],"year":"2004","title":"Unsupervised and supervised exploitation of semantic do-mains in lexical disambiguation","source":"A. Gliozzo, C. Strapparava, and I. Dagan. 2004. Unsupervised and supervised exploitation of semantic do-mains in lexical disambiguation. Computer Speech and Language, 18(3):275299."},{"authors":[{"first":"B.","last":"Magnini"},{"first":"G.","last":"Cavaglia"}],"year":"2000","title":"Integrating subject eld codes into WordNet","source":"B. Magnini and G. Cavaglia. 2000. Integrating subject eld codes into WordNet. In Proceedings of LREC-2000, pages 14131418, Athens, Greece, June."},{"authors":[{"first":"B.","last":"Magnini"},{"first":"C.","last":"Strapparava"},{"first":"G.","last":"Pezzulo"},{"first":"A.","last":"Gliozzo"}],"year":"2002","title":"The role of domain information in word sense disambiguation","source":"B. Magnini, C. Strapparava, G. Pezzulo, and A. Gliozzo. 2002. The role of domain information in word sense disambiguation. Natural Language Engineering, 8(4):359373."},{"authors":[{"first":"R.","last":"Mihalcea"},{"first":"P.","last":"Edmonds"},{"last":"editors"}],"year":"2004","title":"Proceedings of SENSEVAL-3, Barcelona, Spain, July","source":"R. Mihalcea and P. Edmonds, editors. 2004. Proceedings of SENSEVAL-3, Barcelona, Spain, July."},{"authors":[{"first":"R.","last":"Mihalcea"},{"first":"E.","last":"Faruque"}],"year":"2004","title":"Senselearner: Minimally supervised WSD for all words in open text","source":"R. Mihalcea and E. Faruque. 2004. Senselearner: Minimally supervised WSD for all words in open text. In Proceedings of SENSEVAL-3, Barcelona, Spain, July."},{"authors":[{"first":"G.","last":"Salton"},{"first":"M.","middle":"H.","last":"McGill"}],"year":"1983","title":"Introduction to modern information retrieval","source":"G. Salton and M.H. McGill. 1983. Introduction to modern information retrieval. McGraw-Hill, New York."},{"authors":[{"first":"J.","last":"Shawe-Taylor"},{"first":"N.","last":"Cristianini"}],"year":"2004","title":"Kernel Methods for Pattern Analysis","source":"J. Shawe-Taylor and N. Cristianini. 2004. Kernel Methods for Pattern Analysis. Cambridge University Press."},{"authors":[{"first":"S.","last":"Small"}],"year":"1980","title":"Word Expert Parsing: A Theory of Distributed Word-based Natural Language Understanding","source":"S. Small. 1980. Word Expert Parsing: A Theory of Distributed Word-based Natural Language Understanding. Ph.D. Thesis, Department of Computer Science, University of Maryland."},{"authors":[{"first":"C.","last":"Strapparava"},{"first":"A.","last":"Gliozzo"},{"first":"C.","last":"Giuliano"}],"year":"2004","title":"Pattern abstraction and term similarity for word sense disambiguation: Irst at senseval-3","source":"C. Strapparava, A. Gliozzo, and C. Giuliano. 2004. Pattern abstraction and term similarity for word sense disambiguation: Irst at senseval-3. In Proc. of SENSEVAL-3 Third International Workshop on Evaluation of Systems for the Semantic Analysis of Text, pages 229234, Barcelona, Spain, July."},{"authors":[{"first":"S.","middle":"K. M.","last":"Wong"},{"first":"W.","last":"Ziarko"},{"first":"P.","middle":"C. N.","last":"Wong"}],"year":"1985","title":"Generalized vector space model in information retrieval","source":"S.K.M. Wong, W. Ziarko, and P.C.N. Wong. 1985. Generalized vector space model in information retrieval. In Proceedings of the 8th"},{"authors":[],"source":"ACM SIGIR Conference."},{"authors":[{"first":"D.","last":"Yarowsky"},{"first":"R.","last":"Florian"}],"year":"2002","title":"Evaluating sense disambiguation across diverse parameter space","source":"D. Yarowsky and R. Florian. 2002. Evaluating sense disambiguation across diverse parameter space. Natural Language Engineering, 8(4):293310. 410"}],"cites":[{"style":0,"text":"Small, 1980","origin":{"pointer":"/sections/2/paragraphs/1","offset":378,"length":11},"authors":[{"last":"Small"}],"year":"1980","references":["/references/10"]},{"style":0,"text":"Yarowsky and Florian, 2002","origin":{"pointer":"/sections/2/paragraphs/1","offset":391,"length":26},"authors":[{"last":"Yarowsky"},{"last":"Florian"}],"year":"2002","references":["/references/14"]},{"style":0,"text":"Mihalcea and Faruque, 2004","origin":{"pointer":"/sections/2/paragraphs/4","offset":100,"length":26},"authors":[{"last":"Mihalcea"},{"last":"Faruque"}],"year":"2004","references":["/references/7"]},{"style":0,"text":"Decadt et al., 2004","origin":{"pointer":"/sections/2/paragraphs/4","offset":128,"length":19},"authors":[{"last":"Decadt"},{"last":"al."}],"year":"2004","references":["/references/1"]},{"style":0,"text":"Gliozzo et al., 2004","origin":{"pointer":"/sections/2/paragraphs/7","offset":143,"length":20},"authors":[{"last":"Gliozzo"},{"last":"al."}],"year":"2004","references":["/references/3"]},{"style":0,"text":"Gliozzo et al., 2004","origin":{"pointer":"/sections/2/paragraphs/9","offset":490,"length":20},"authors":[{"last":"Gliozzo"},{"last":"al."}],"year":"2004","references":["/references/3"]},{"style":0,"text":"Mihalcea and Edmonds, 2004","origin":{"pointer":"/sections/2/paragraphs/10","offset":443,"length":26},"authors":[{"last":"Mihalcea"},{"last":"Edmonds"}],"year":"2004","references":[]},{"style":0,"text":"Strapparava et al., 2004","origin":{"pointer":"/sections/2/paragraphs/11","offset":94,"length":24},"authors":[{"last":"Strapparava"},{"last":"al."}],"year":"2004","references":["/references/11"]},{"style":0,"text":"Magnini et al., 2002","origin":{"pointer":"/sections/2/paragraphs/12","offset":224,"length":20},"authors":[{"last":"Magnini"},{"last":"al."}],"year":"2002","references":["/references/5"]},{"style":0,"text":"Wong et al., 1985","origin":{"pointer":"/sections/3/paragraphs/19","offset":4,"length":17},"authors":[{"last":"Wong"},{"last":"al."}],"year":"1985","references":["/references/12"]},{"style":0,"text":"Magnini and Cavaglia, 2000","origin":{"pointer":"/sections/3/paragraphs/28","offset":80,"length":26},"authors":[{"last":"Magnini"},{"last":"Cavaglia"}],"year":"2000","references":["/references/4"]},{"style":0,"text":"Cristianini and Shawe-Taylor, 2000","origin":{"pointer":"/sections/4/paragraphs/2","offset":399,"length":34},"authors":[{"last":"Cristianini"},{"last":"Shawe-Taylor"}],"year":"2000","references":["/references/0"]},{"style":0,"text":"Deerwester et al., 1990","origin":{"pointer":"/sections/4/paragraphs/5","offset":532,"length":23},"authors":[{"last":"Deerwester"},{"last":"al."}],"year":"1990","references":["/references/2"]},{"style":0,"text":"Salton and McGill, 1983","origin":{"pointer":"/sections/4/paragraphs/7","offset":58,"length":23},"authors":[{"last":"Salton"},{"last":"McGill"}],"year":"1983","references":["/references/8"]},{"style":0,"text":"Magnini et al., 2002","origin":{"pointer":"/sections/4/paragraphs/8","offset":91,"length":20},"authors":[{"last":"Magnini"},{"last":"al."}],"year":"2002","references":["/references/5"]},{"style":0,"text":"Shawe-Taylor and Cristianini, 2004","origin":{"pointer":"/sections/4/paragraphs/9","offset":350,"length":34},"authors":[{"last":"Shawe-Taylor"},{"last":"Cristianini"}],"year":"2004","references":["/references/9"]},{"style":0,"text":"Shawe-Taylor and Cristianini, 2004","origin":{"pointer":"/sections/4/paragraphs/16","offset":57,"length":34},"authors":[{"last":"Shawe-Taylor"},{"last":"Cristianini"}],"year":"2004","references":["/references/9"]},{"style":0,"text":"Mihalcea and Edmonds, 2004","origin":{"pointer":"/sections/5/paragraphs/4","offset":167,"length":26},"authors":[{"last":"Mihalcea"},{"last":"Edmonds"}],"year":"2004","references":[]}]}
