{"sections":[{"title":"Ranking Algorithms for Named–Entity Extraction: Boosting and the Voted Perceptron Michael Collins AT&T Labs-Research, Florham Park, New Jersey. mcollins@research.att.com Abstract","paragraphs":["This paper describes algorithms which rerank the top N hypotheses from a maximum-entropy tagger, the applica-tion being the recovery of named-entity boundaries in a corpus of web data. The first approach uses a boosting algorithm for ranking problems. The second approach uses the voted perceptron algorithm. Both algorithms give comparable, significant improvements over the maximum-entropy baseline. The voted perceptron algorithm can be considerably more efficient to train, at some cost in computation on test examples."]},{"title":"1 Introduction","paragraphs":["Recent work in statistical approaches to parsing and tagging has begun to consider methods which incorporate global features of candidate structures. Examples of such techniques are Markov Random Fields (Abney 1997; Della Pietra et al. 1997; John-son et al. 1999), and boosting algorithms (Freund et al. 1998; Collins 2000; Walker et al. 2001). One appeal of these methods is their flexibility in incorporating features into a model: essentially any features which might be useful in discriminating good from bad structures can be included. A second appeal of these methods is that their training criterion is often discriminative, attempting to explicitly push the score or probability of the correct structure for each training sentence above the score of competing structures. This discriminative property is shared by the methods of (Johnson et al. 1999; Collins 2000), and also the Conditional Random Field methods of (Lafferty et al. 2001).","In a previous paper (Collins 2000), a boosting algorithm was used to rerank the output from an existing statistical parser, giving significant improvements in parsing accuracy on Wall Street Journal data. Similar boosting algorithms have been applied to natural language generation, with good results, in (Walker et al. 2001). In this paper we apply reranking methods to named-entity extraction. A state-of-the-art (maximum-entropy) tagger is used to generate 20 possible segmentations for each input sentence, along with their probabilities. We describe a number of additional global features of these candidate segmentations. These additional features are used as evidence in reranking the hypotheses from the max-ent tagger. We describe two learning algorithms: the boosting method of (Collins 2000), and a variant of the voted perceptron algorithm, which was initially described in (Freund & Schapire 1999). We applied the methods to a corpus of over one million words of tagged web data. The methods give significant improvements over the maximum-entropy tagger (a 17.7% relative reduction in error-rate for the voted perceptron, and a 15.6% relative improvement for the boosting method).","One contribution of this paper is to show that existing reranking methods are useful for a new domain, named-entity tagging, and to suggest global features which give improvements on this task. We should stress that another contribution is to show that a new algorithm, the voted perceptron, gives very credible results on a natural language task. It is an extremely simple algorithm to implement, and is very fast to train (the testing phase is slower, but by no means sluggish). It should be a viable alternative to methods such as the boosting or Markov Random Field algorithms described in previous work."]},{"title":"2 Background 2.1 The data","paragraphs":["Over a period of a year or so we have had over one million words of named-entity data annotated. The Computational Linguistics (ACL), Philadelphia, July 2002, pp. 489-496. Proceedings of the 40th Annual Meeting of the Association for data is drawn from web pages, the aim being to support a question-answering system over web data. A number of categories are annotated: the usual people, organization and location categories, as well as less frequent categories such as brand-names, scientific terms, event titles (such as concerts) and so on. From this data we created a training set of 53,609 sentences (1,047,491 words), and a test set of 14,717 sentences (291,898 words).","The task we consider is to recover named-entity boundaries. We leave the recovery of the categories of entities to a separate stage of processing.1","We evaluate different methods on the task through precision and recall. If a method proposes","entities on the test set, and","of these are correct (i.e., an entity is marked by the annotator with exactly the same span as that proposed) then the precision of a method is   ",". Similarly, if","is the total number of en-","tities in the human annotated version of the test set,","then the recall is "," . 2.2 The baseline tagger The problem can be framed as a tagging task – to tag each word as being either the start of an entity, a continuation of an entity, or not to be part of an entity at all (we will use the tags S, C and N respectively for these three cases). As a baseline model we used a maximum entropy tagger, very similar to the ones described in (Ratnaparkhi 1996; Borthwick et. al 1998; McCallum et al. 2000). Max-ent taggers have been shown to be highly competitive on a number of tagging tasks, such as part-of-speech tagging (Ratnaparkhi 1996), named-entity recognition (Borthwick et. al 1998), and information extraction tasks (McCallum et al. 2000). Thus the maximum-entropy tagger we used represents a serious baseline for the task. We used the following features (several of the features were inspired by the approach of (Bikel et. al 1999), an HMM model which gives excellent results on named entity extraction):","","The word being tagged, the previous word, and the next word.","","The previous tag, and the previous two tags (bigram and trigram features).","1","In initial experiments, we found that forcing the tagger to recover categories as well as the segmentation, by exploding the number of tags, reduced performance on the segmentation task, presumably due to sparse data problems. ","A compound feature of three fields: (a) Is the","word at the start of a sentence?; (b) does the word","occur in a list of words which occur more frequently","as lower case rather than upper case words in a large","corpus of text? (c) the type of the first letter","of","the word, where","","","is defined as ‘A’ if","is a","capitalized letter, ‘a’ if","is a lower-case letter, ‘0’","if","is a digit, and","otherwise. For example, if the","word Animal is seen at the start of a sentence, and","it occurs in the list of frequent lower-cased words,","then it would be mapped to the feature 1-1-A. ","The word with each character mapped to its"," ",". For example, G.M. would be mapped to","A.A., and Animal would be mapped to Aaaaaa. ","The word with each character mapped to its","type, but repeated consecutive character types are","not repeated in the mapped string. For example, An-","imal would be mapped to Aa, G.M. would again be","mapped to A.A.. The tagger was applied and trained in the same","way as described in (Ratnaparkhi 1996). The feature","templates described above are used to create a set of"," binary features","","",", where","is the tag, and","is the “history”, or context. An example is","         if t = S and the word being tagged = “Mr.”  otherwise","The parameters of the model are for"," ",",","defining a conditional distribution over the tags","given a history","as","   "," ","","","","  ","",""," The parameters are trained using Generalized Iterative Scaling. Following (Ratnaparkhi 1996), we only include features which occur 5 times or more in training data. In decoding, we use a beam search to recover 20 candidate tag sequences for each sentence (the sentence is decoded from left to right, with the top 20 most probable hypotheses being stored at each point). 2.3 Applying the baseline tagger As a baseline we trained a model on the full 53,609 sentences of training data, and decoded the 14,717 sentences of test data. This gave 20 candidates per test sentence, along with their probabilities. The baseline method is to take the most probable candidate for each test data sentence, and then to calculate precision and recall figures. Our aim is to come up with strategies for reranking the test data candidates, in such a way that precision and recall is improved.","In developing a reranking strategy, the 53,609 sentences of training data were split into a 41,992 sentence training portion, and a 11,617 sentence development set. The training portion was split into 5 sections, and in each case the maximum-entropy tagger was trained on 4/5 of the data, then used to decode the remaining 1/5. The top 20 hypotheses under a beam search, together with their log probabilities, were recovered for each training sentence. In a similar way, a model trained on the 41,992 sentence set was used to produce 20 hypotheses for each sentence in the development set."]},{"title":"3 Global features 3.1 The global-feature generator","paragraphs":["The module we describe in this section generates global features for each candidate tagged sequence. As input it takes a sentence, along with a proposed segmentation (i.e., an assignment of a tag for each word in the sentence). As output, it produces a set of feature strings. We will use the following tagged sentence as a running example in this section:","Whether/N you/N ’/N re/N an/N aging/N flower/N child/N or/N a/N clueless/N Gen/S Xer/C ,/N “/N The/S Day/C They/C Shot/C John/C Lennon/C ,/N ”/N playing/N at/N the/N Dougherty/S Arts/C Center/C ,/N entertains/N the/N imagination/N ./N","An example feature type is simply to list the full strings of entities that appear in the tagged input. In this example, this would give the three features WE=Gen Xer WE=The Day They Shot John Lennon WE=Dougherty Arts Center","Here WE stands for “whole entity”. Throughout this section, we will write the features in this format. The start of the feature string indicates the feature type (in this case WE), followed by =. Following the type, there are generally 1 or more words or other symbols, which we will separate with the symbol .","A seperate module in our implementation takes the strings produced by the global-feature generator, and hashes them to integers. For example, suppose the three strings WE=Gen Xer, WE=The Day They Shot John Lennon, WE=Dougherty Arts Center were hashed to 100, 250, and 500 respectively. Conceptually, the candidate","is represented by a large number of features","","","for"," where","is the","number of distinct feature strings in training data.","In this example, only","","",",  ","and","","","take the value  , all other features being zero. 3.2 Feature templates We now introduce some notation with which to describe the full set of global features. First, we assume the following primitives of an input candidate:"," ","for","is the ’th tag in the tagged sequence.","  for","is the ’th word.","  for is ","if  begins with a lower-case letter,  otherwise."," ","for","is a transformation of","",", where the transformation is applied in the same way as the final feature type in the maximum entropy tagger. Each character in the word is mapped to its","",", but repeated consecutive character types are not repeated in the mapped string. For example, Animal would be mapped to Aa in this feature, G.M. would again be mapped to A.A.."," ","for","is the same as",", but has an additional flag appended. The flag indicates whether or not the word appears in a dictionary of words which appeared more often lower-cased than capitalized in a large corpus of text. In our example, Animal appears in the lexicon, but G.M. does not, so the two values for","would be Aa1 and A.A.0 respectively.","In addition,  ","and","are all defined to be","NULL if  or ",".","Most of the features we describe are anchored on entity boundaries in the candidate segmentation. We will use “feature templates” to describe the features that we used. As an example, suppose that an entity","Description Feature Template","The whole entity string WE= ","","","","The","","features within the entity FF=","   ","","","","The","","features within the entity GF=    ","","","","The last word in the entity LW=","Indicates whether the last word is lower-cased LWLC= ","Bigram boundary features of the words before/after the start","of the entity","BO00=","","","","","BO01=","","","","","BO10=","","","","","BO11=",""," Bigram boundary features of the words before/after the end of the entity","BE00=","",""," BE01=","","","","BE10=","","","","BE11=",""," Trigram boundary features of the words before/after the start of the entity (16 features total, only 4 shown)","TO000=","","","",""," ","TO111=","","","","","","TO2000=","","",""," TO2111=","","",""," Trigram boundary features of the words before/after the end of the entity (16 features total, only 4 shown)","TE000=","","","","","","TE111=","","","","","TE2000=","","","","","  ","","TE2111=","","","","","","","","Prefix features PF=","PF2=","PF="," ","PF2=","","","","PF=","","","","","   PF2=","","","","","","","","Suffix features SF=","SF2=","SF="," ","SF2=","","","","SF=","","","","","   SF2=","","","","","",""," Figure 1: The full set of entity-anchored feature templates. One of these features is generated for each entity seen in a candidate. We take the entity to span words","  inclusive in the candidate.","is seen from words","to","inclusive in a segmenta-","tion. Then the WE feature described in the previous","section can be generated by the template","WE=  ",""," Applying this template to the three entities in the running example generates the three feature strings described in the previous section. As another example, consider the template FF=","   ",". This","will generate a feature string for each of the entities","in a candidate, this time using the values   ","rather than","","","",". For the full set of feature templates that are anchored around entities, see figure 1.","A second set of feature templates is anchored around quotation marks. In our corpus, entities (typically with long names) are often seen surrounded by quotes. For example, “The Day They Shot John Lennon”, the name of a band, appears in the running example. Define","to be the index of any double quotation marks in the candidate,","to be the index of the next (matching) double quotation marks if they appear in the candidate. Additionally, define","to be the index of the last word beginning with a lower case letter, upper case letter, or digit within the quotation marks. The first set of feature templates tracks the values of","","for the words within quotes:2","Q=","","","  ","","",""," ","  ","","Q2=","","","   ","",""," ","","","","  ","","",""," 2 We only included these features if",", to prevent","an explosion in the length of feature strings. The next set of feature templates are sensitive","to whether the entire sequence between quotes is","tagged as a named entity. Define","","to be if","","S, and","=C for","","","","(i.e.,","","","if the sequence of words within the quotes is tagged","as a single entity). Also define to be the number","of upper cased words within the quotes,","to be the","number of lower case words, and to be ","if , ","otherwise. Then two other templates are:","QF="," ","","","","   ","","QF2="," ","","","   ","","In the “The Day They Shot John Lennon” example","we would have","","","provided that the entire se-","quence within quotes was tagged as an entity. Ad-","ditionally,",",  , and  . The val-","ues for","","  and","","","would be and ","(these features are derived from The and Lennon, which respectively do and don’t appear in the capitalization lexicon). This would give QF=        and QF2=","     ",".","At this point, we have fully described the repre-","sentation used as input to the reranking algorithms.","The maximum-entropy tagger gives 20 proposed","segmentations for each input sentence. Each can-","didate is represented by the log probability","","","from the tagger, as well as the values of the global","features","","","for","",". In the next section we describe algorithms which blend these two sources of information, the aim being to improve upon a strategy which just takes the candidate from","the tagger with the highest score for  ","."]},{"title":"4 Ranking Algorithms 4.1 Notation","paragraphs":["This section introduces notation for the reranking task. The framework is derived by the transformation from ranking problems to a margin-based classification problem in (Freund et al. 1998). It is also related to the Markov Random Field methods for parsing suggested in (Johnson et al. 1999), and the boosting methods for parsing in (Collins 2000). We consider the following set-up:","","Training data is a set of example input/output pairs. In tagging we would have training examples","","","","","where each","","is a sentence and each","","is the","correct sequence of tags for that sentence.","","We assume some way of enumerating a set of","candidates for a particular sentence. We use","to","denote the","’th candidate for the","’th sentence in","training data, and","","","","","  "," ","","to denote","the set of candidates for",". In this paper, the top","outputs from a maximum entropy tagger are used as","the set of candidates.","","Without loss of generality we take","to be the","candidate for ","which has the most correct tags, i.e.,","is closest to being correct.3","","","","","  is the probability that the base model","assigns to   . We define","","","","","","","","",".","","We assume a set of","additional features,","","","for"," ",". The features could be arbitrary functions of the candidates; our hope is to include features which help in discriminating good candidates from bad ones. ","Finally, the parameters of the model are a vector of"," parameters,  ","  ","  . The ranking function is defined as","","","  ","","   ","  "," This function assigns a real-valued number to a candidate",". It will be taken to be a measure of the plausibility of a candidate, higher scores meaning higher plausibility. As such, it assigns a ranking to different candidate structures for the same sentence,","3","In the event that multiple candidates get the same, highest score, the candidate with the highest value of log-likelihood under the baseline model is taken as","","",".","and in particular the output on a training or test ex-","ample","is   ","","","",". In this paper we","take the features","to be fixed, the learning problem","being to choose a good setting for the parameters",".","In some parts of this paper we will use vec-","tor notation. Define","","","to be the vector","  ","","","","","  . Then the ranking score","can also be written as  ","","","where","","","is the dot product between vectors","and",". 4.2 The boosting algorithm The first algorithm we consider is the boosting algorithm for ranking described in (Collins 2000). The algorithm is a modification of the method in (Freund et al. 1998). The method can be considered to be a greedy algorithm for finding the parameters","that minimize the loss function","","","   ","","","","","","  ","","","","","","","where as before,","","","","",". The theo-","retical motivation for this algorithm goes back to the","PAC model of learning. Intuitively, it is useful to","note that this loss function is an upper bound on the","number of “ranking errors”, a ranking error being a","case where an incorrect candidate gets a higher value","for than a correct candidate. This follows because","for all",",","","",""," , where we define","","to be  for  , and  otherwise. Hence","     ","    ","where","","","","","","","","","","",". Note that","the number of ranking errors is","","","","","","",".","As an initial step,  is set to be","","     ","","","","","  ","","","","","","and all other parameters  for","","are set to be zero. The algorithm then proceeds for","iterations (","is usually chosen by cross validation on a development set). At each iteration, a single feature is chosen, and its weight is updated. Suppose the current parameter values are",", and a single feature","is chosen, its weight being updated through an in-","crement",", i.e.,","","","",". Then the new loss,","after this parameter update, will be","       ","","","","","","   ","","","","","","","where","","","","  ","","","","",". The boost-","ing algorithm chooses the feature/update pair   ","which is optimal in terms of minimizing the loss","function, i.e.,  ","  ","   ","  "," (1)","and then makes the update  ","",".","Figure 2 shows an algorithm which implements this greedy procedure. See (Collins 2000) for a full description of the method, including justifica-tion that the algorithm does in fact implement the update in Eq. 1 at each iteration.4","The algorithm relies on the following arrays:      ","","","","   ","","","","","","       ","","","  ","","",""," ","    ","  ","","","  ","","",""," ","    ","  ","","","  ","","","","  Thus  ","is an index from features to correct/incorrect candidate pairs where the  ’th feature takes value  on the correct candidate, and value  on the incorrect candidate. The array  ","is a similar index from features to examples. The arrays","     and","    ","are reverse indices from training examples","to features. 4.3 The voted perceptron Figure 3 shows the training phase of the perceptron algorithm, originally introduced in (Rosenblatt 1958). The algorithm maintains a parameter vector ",", which is initially set to be all zeros. The algorithm then makes a pass over the training set, at each training example storing a parameter vector  for  ",". The parameter vector is only modified when a mistake is made on an example. In this case the update is very simple, involving adding the difference of the offending examples’ representations (    ","","","","","in the figure). See","(Cristianini and Shawe-Taylor 2000) chapter 2 for","discussion of the perceptron algorithm, and theory","justifying this method for setting the parameters.","In the most basic form of the perceptron, the pa-","rameter values","are taken as the final parame-","ter settings, and the output on a new test exam-","ple with for"," ","is simply the highest","4","Strictly speaking, this is only the case if the smoothing pa-","rameter","is",". Input","","Examples  ","with initial scores","","","","","","Arrays"," ,   ,","    ","and","    ","as described in section 4.2."," Parameters are number of rounds of boosting ",", a smoothing parameter",". Initialize","","Set  ","","   ","","","","","  ","","","","",""," Set","       ","","For all",", set","","","  ","","","","","","","","",".","","Set","","","","","","",""," For ","  , calculate –  ","","","","    ","","","",""," –  ","","","","    ","","","",""," –"," ","                   ","Repeat for = 1 to"," Choose","  ","","   "," Set             ","","Update one parameter,","","","","","for","","","–  ","","","","  ","","","–  ","","  – for  ","     ,       – for  ","     ,      ","–","","for","","","","–  ","","","","  ","","","–  ","","  – for  ","     ,       – for  ","     ,      ","–"," For all features whose values of   and/or   have changed, recalculate"," ","  ","                  Output Final parameter setting Figure 2: The boosting algorithm.","Define:","","","","",".","Input: Examples","","","with feature vectors","","","","",".","Initialization: Set parameters    For ","  ","  "," ","If   Then    Else   ","  ",""," Output: Parameter vectors  for  Figure 3: The perceptron training algorithm for ranking problems.","Define:","","","","",".","Input: A set of candidates","for","  , A sequence of parameter vectors  for ","Initialization: Set   for"," ","( ","stores the number of votes for",")","For ","","","  ","","",""," "," "," ","Output:","where","","  Figure 4: Applying the voted perceptron to a test example. scoring candidate under these parameter values, i.e.,  ","where","","","","","",".","(Freund & Schapire 1999) describe a refinement","of the perceptron, the voted perceptron. The train-","ing phase is identical to that in figure 3. Note, how-","ever, that all parameter vectors  for  are stored. Thus the training phase can be thought of as a way of constructing  different parame-","ter settings. Each of these parameter settings will","have its own highest ranking candidate,","","where","   ","   ","",". The idea behind the voted","perceptron is to take each of the ","parameter settings to “vote” for a candidate, and the candidate which gets the most votes is returned as the most likely candidate. See figure 4 for the algorithm.5"]},{"title":"5 Experiments","paragraphs":["We applied the voted perceptron and boosting algorithms to the data described in section 2.3. Only features occurring on 5 or more distinct training sentences were included in the model. This resulted","5","Note that, for reasons of explication, the decoding algorithm we present is less efficient than necessary. For example, when    ","it is preferable to use some book-keeping to avoid recalculation of ","","and","  ","","  .","P R F Max-Ent 84.4 86.3 85.3 Boosting 87.3(18.6) 87.9(11.6) 87.6(15.6) Voted 87.3(18.6) 88.6(16.8) 87.9(17.7) Perceptron Figure 5: Results for the three tagging methods. "," precision,  ","recall,","","F-measure. Fig-","ures in parantheses are relative improvements in er-","ror rate over the maximum-entropy model. All fig-","ures are percentages. in 93,777 distinct features. The two methods were trained on the training portion (41,992 sentences) of the training set. We used the development set to pick the best values for tunable parameters in each algorithm. For boosting, the main parameter to pick is the number of rounds,",". We ran the algorithm for a total of 300,000 rounds, and found that the optimal value for F-measure on the development set occurred after 83,233 rounds. For the voted perceptron, the representation","","","was taken to be a vector  ","","","","","","","","  ","","where","is a parameter that influences the relative contribution of the log-likelihood term versus the other features. A value of    ","was found to give the best results on the development set. Figure 5 shows the results for the three methods on the test set. Both of the reranking algorithms show significant improvements over the baseline: a 15.6% relative reduction in error for boosting, and a 17.7% relative error reduction for the voted perceptron.","In our experiments we found the voted perceptron algorithm to be considerably more efficient in training, at some cost in computation on test examples. Another attractive property of the voted perceptron is that it can be used with kernels, for example the kernels over parse trees described in (Collins and Duffy 2001; Collins and Duffy 2002). (Collins and Duffy 2002) describe the voted perceptron applied to the named-entity data in this paper, but using kernel-based features rather than the explicit features described in this paper. See (Collins 2002) for additional work using perceptron algorithms to train tagging models, and a more thorough description of the theory underlying the perceptron algorithm applied to ranking problems."]},{"title":"6 Discussion","paragraphs":["A question regarding the approaches in this paper is whether the features we have described could be incorporated in a maximum-entropy tagger, giving similar improvements in accuracy. This section discusses why this is unlikely to be the case. The problem described here is closely related to the label bias problem described in (Lafferty et al. 2001).","One straightforward way to incorporate global features into the maximum-entropy model would be to introduce new features","","","which indicated whether the tagging decision","in the history","creates a particular global feature. For example, we could introduce a feature   ","","       if t = N and this decision creates an LWLC=1 feature  otherwise As an example, this would take the value  if its was tagged as N in the following context, She/N praised/N the/N University/S for/C its/? efforts to","because tagging its as N in this context would create","an entity whose last word was not capitalized, i.e.,","University for. Similar features could be created for","all of the global features introduced in this paper.","This example also illustrates why this approach","is unlikely to improve the performance of the","maximum-entropy tagger. The parameter","","","as-","sociated with this new feature can only affect the","score for a proposed sequence by modifying","","","at the point at which","","","","","",". In the example, this means that the LWLC=1 feature can only lower the score for the segmentation by lowering the probability of tagging its as N. But its has almost probably ","of not appearing as part of an entity, so",""," ","should be almost","whether   ","is  or  in this context! The decision which effectively created the entity University for was the decision to tag for as C, and this has already been made. The independence assumptions in maximum-entropy taggers of this form often lead points of local ambiguity (in this example the tag for the word for) to create globally implausible structures with unreasonably high scores. See (Collins 1999) section 8.4.2 for a discussion of this problem in the context of parsing. Acknowledgements Many thanks to Jack Minisi for annotating the named-entity data used in the experiments. Thanks also to Nigel Duffy, Rob Schapire and Yoram Singer for several useful discussions."]},{"title":"References","paragraphs":["Abney, S. 1997. Stochastic Attribute-Value Grammars. Computational Linguistics, 23(4):597-618.","Bikel, D., Schwartz, R., and Weischedel, R. (1999). An Algo-rithm that Learns What’s in a Name. In Machine Learning: Special Issue on Natural Language Learning, 34(1-3).","Borthwick, A., Sterling, J., Agichtein, E., and Grishman, R. (1998). Exploiting Diverse Knowledge Sources via Maximum Entropy in Named Entity Recognition. Proc. of the Sixth Workshop on Very Large Corpora.","Collins, M. (1999). Head-Driven Statistical Models for Natural Language Parsing. PhD Thesis, University of Pennsylvania.","Collins, M. (2000). Discriminative Reranking for Natural Language Parsing. Proceedings of the Seventeenth International Conference on Machine Learning (ICML 2000).","Collins, M., and Duffy, N. (2001). Convolution Kernels for Natural Language. In Proceedings of NIPS 14.","Collins, M., and Duffy, N. (2002). New Ranking Algorithms for Parsing and Tagging: Kernels over Discrete Structures, and the Voted Perceptron. In Proceedings of ACL 2002.","Collins, M. (2002). Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with the Perceptron Algorithm. In Proceedings of EMNLP 2002.","Cristianini, N., and Shawe-Tayor, J. (2000). An introduction to Support Vector Machines and other kernel-based learning methods. Cambridge University Press.","Della Pietra, S., Della Pietra, V., and Lafferty, J. (1997). Induc-ing Features of Random Fields. IEEE Transactions on Pattern Analysis and Machine Intelligence, 19(4), pp. 380-393.","Freund, Y. & Schapire, R. (1999). Large Margin Classifica-tion using the Perceptron Algorithm. In Machine Learning, 37(3):277–296.","Freund, Y., Iyer, R.,Schapire, R.E., & Singer, Y. (1998). An efficient boosting algorithm for combining preferences. In Machine Learning: Proceedings of the Fifteenth International Conference.","Johnson, M., Geman, S., Canon, S., Chi, Z. and Riezler, S. (1999). Estimators for Stochastic “Unification-based” Grammars. Proceedings of the ACL 1999.","Lafferty, J., McCallum, A., and Pereira, F. (2001). Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of ICML 2001.","McCallum, A., Freitag, D., and Pereira, F. (2000) Maximum entropy markov models for information extraction and segmentation. In Proceedings of ICML 2000.","Ratnaparkhi, A. (1996). A maximum entropy part-of-speech tagger. In Proceedings of the empirical methods in natural language processing conference.","Rosenblatt, F. (1958). The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain. Psychological Review, 65, 386–408. (Reprinted in Neurocomputing (MIT Press, 1998).)","Walker, M., Rambow, O., and Rogati, M. (2001). SPoT: a train-able sentence planner. In Proceedings of the 2nd Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL 2001)."]}],"references":[{"authors":[{"last":"Abney"},{"last":"S"}],"year":"1997","title":"Stochastic Attribute-Value Grammars","source":"Abney, S. 1997. Stochastic Attribute-Value Grammars. Computational Linguistics, 23(4):597-618."},{"authors":[{"first":"D.","last":"Bikel"},{"first":"R.","last":"Schwartz"},{"last":"Weischedel"},{"last":"R"}],"year":"1999","title":"An Algo-rithm that Learns What’s in a Name","source":"Bikel, D., Schwartz, R., and Weischedel, R. (1999). An Algo-rithm that Learns What’s in a Name. In Machine Learning: Special Issue on Natural Language Learning, 34(1-3)."},{"authors":[{"first":"A.","last":"Borthwick"},{"first":"J.","last":"Sterling"},{"first":"E.","last":"Agichtein"},{"last":"Grishman"},{"last":"R"}],"year":"1998","title":"Exploiting Diverse Knowledge Sources via Maximum Entropy in Named Entity Recognition","source":"Borthwick, A., Sterling, J., Agichtein, E., and Grishman, R. (1998). Exploiting Diverse Knowledge Sources via Maximum Entropy in Named Entity Recognition. Proc. of the Sixth Workshop on Very Large Corpora."},{"authors":[{"last":"Collins"},{"last":"M"}],"year":"1999","title":"Head-Driven Statistical Models for Natural Language Parsing","source":"Collins, M. (1999). Head-Driven Statistical Models for Natural Language Parsing. PhD Thesis, University of Pennsylvania."},{"authors":[{"last":"Collins"},{"last":"M"}],"year":"2000","title":"Discriminative Reranking for Natural Language Parsing","source":"Collins, M. (2000). Discriminative Reranking for Natural Language Parsing. Proceedings of the Seventeenth International Conference on Machine Learning (ICML 2000)."},{"authors":[{"first":"M.","last":"Collins"},{"last":"Duffy"},{"last":"N"}],"year":"2001","title":"Convolution Kernels for Natural Language","source":"Collins, M., and Duffy, N. (2001). Convolution Kernels for Natural Language. In Proceedings of NIPS 14."},{"authors":[{"first":"M.","last":"Collins"},{"last":"Duffy"},{"last":"N"}],"year":"2002","title":"New Ranking Algorithms for Parsing and Tagging: Kernels over Discrete Structures, and the Voted Perceptron","source":"Collins, M., and Duffy, N. (2002). New Ranking Algorithms for Parsing and Tagging: Kernels over Discrete Structures, and the Voted Perceptron. In Proceedings of ACL 2002."},{"authors":[{"last":"Collins"},{"last":"M"}],"year":"2002","title":"Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with the Perceptron Algorithm","source":"Collins, M. (2002). Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with the Perceptron Algorithm. In Proceedings of EMNLP 2002."},{"authors":[{"first":"N.","last":"Cristianini"},{"last":"Shawe-Tayor"},{"last":"J"}],"year":"2000","title":"An introduction to Support Vector Machines and other kernel-based learning methods","source":"Cristianini, N., and Shawe-Tayor, J. (2000). An introduction to Support Vector Machines and other kernel-based learning methods. Cambridge University Press."},{"authors":[{"first":"Della","last":"Pietra"},{"last":"S."},{"first":"Della","last":"Pietra"},{"last":"V."},{"last":"Lafferty"},{"last":"J"}],"year":"1997","title":"Induc-ing Features of Random Fields","source":"Della Pietra, S., Della Pietra, V., and Lafferty, J. (1997). Induc-ing Features of Random Fields. IEEE Transactions on Pattern Analysis and Machine Intelligence, 19(4), pp. 380-393."},{"authors":[{"first":"Y.","last":"Freund"},{"last":"Schapire"},{"last":"R"}],"year":"1999","title":"Large Margin Classifica-tion using the Perceptron Algorithm","source":"Freund, Y. & Schapire, R. (1999). Large Margin Classifica-tion using the Perceptron Algorithm. In Machine Learning, 37(3):277–296."},{"authors":[{"first":"Y.","last":"Freund"},{"first":"R.","last":"Iyer"},{"first":"R.","middle":"E.","last":"Schapire"},{"last":"Singer"},{"last":"Y"}],"year":"1998","title":"An efficient boosting algorithm for combining preferences","source":"Freund, Y., Iyer, R.,Schapire, R.E., & Singer, Y. (1998). An efficient boosting algorithm for combining preferences. In Machine Learning: Proceedings of the Fifteenth International Conference."},{"authors":[{"first":"M.","last":"Johnson"},{"first":"S.","last":"Geman"},{"first":"S.","last":"Canon"},{"first":"Z.","last":"Chi"},{"last":"Riezler"},{"last":"S"}],"year":"1999","title":"Estimators for Stochastic “Unification-based” Grammars","source":"Johnson, M., Geman, S., Canon, S., Chi, Z. and Riezler, S. (1999). Estimators for Stochastic “Unification-based” Grammars. Proceedings of the ACL 1999."},{"authors":[{"first":"J.","last":"Lafferty"},{"first":"A.","last":"McCallum"},{"last":"Pereira"},{"last":"F"}],"year":"2001","title":"Conditional random fields: Probabilistic models for segmenting and labeling sequence data","source":"Lafferty, J., McCallum, A., and Pereira, F. (2001). Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of ICML 2001."},{"authors":[],"source":"McCallum, A., Freitag, D., and Pereira, F. (2000) Maximum entropy markov models for information extraction and segmentation. In Proceedings of ICML 2000."},{"authors":[{"last":"Ratnaparkhi"},{"last":"A"}],"year":"1996","title":"A maximum entropy part-of-speech tagger","source":"Ratnaparkhi, A. (1996). A maximum entropy part-of-speech tagger. In Proceedings of the empirical methods in natural language processing conference."},{"authors":[{"last":"Rosenblatt"},{"last":"F"}],"year":"1958","title":"The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain","source":"Rosenblatt, F. (1958). The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain. Psychological Review, 65, 386–408. (Reprinted in Neurocomputing (MIT Press, 1998).)"},{"authors":[{"first":"M.","last":"Walker"},{"first":"O.","last":"Rambow"},{"last":"Rogati"},{"last":"M"}],"year":"2001","title":"SPoT: a train-able sentence planner","source":"Walker, M., Rambow, O., and Rogati, M. (2001). SPoT: a train-able sentence planner. In Proceedings of the 2nd Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL 2001)."}],"cites":[]}
