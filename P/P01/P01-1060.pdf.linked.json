{"sections":[{"title":"Parse Forest Computation of Expected Governors Helmut Schmid Institute for Computational Linguistics University of Stuttgart Azenbergstr. 12 70174 Stuttgart, Germany schmid@ims.uni-stuttgart.de Mats Rooth Department of Linguistics Cornell University Morrill Hall Ithaca, NY 14853, USA mats@cs.cornell.edu Abstract","paragraphs":["In a headed tree, each terminal word can be uniquely labeled with a governing word and grammatical relation. This labeling is a summary of a syntactic analysis which eliminates detail, reflects aspects of semantics, and for some grammatical relations (such as subject of finite verb) is nearly uncontroversial. We define a notion of expected governor markup, which sums vectors indexed by governors and scaled by probabilistic tree weights. The quantity is computed in a parse forest representation of the set of tree analyses for a given sentence, using vector sums and scaling by inside probability and flo w."]},{"title":"1 Introduction","paragraphs":["A labeled headed tree is one in which each non-terminal vertex has a distinguished head child, and in the usual way non-terminal nodes are labeled with non-terminal symbols (syntactic categories such as NP) and terminal vertices are labeled with terminal symbols (words such as","The governor algorithm was designed and implemented in the Reading Comprehension research group in the 2000 Workshop on Language Engineering at Johns Hopkins University. Thanks to Marc Light, Ellen Riloff, Pranav Anand, Brianne Brown, Eric Breck, Gideon Mann, and Mike Thelen for discussion and assistance. Oral presentations were made at that workshop in August 2000, and at the University of Sussex in January 2001. Thanks to Fred Jelinek, John Carroll, and other members of the audiences for their comments. S","NP  Peter VP V reads","NP   ","NP    D every","N    paper","PP:on "," P:on on","NP ","","N "," markup Figure 1: A tree with percolated lexical heads. reads).1","We work with syntactic trees in which terminals are in addition labeled with uninflected word forms (lemmas) derived from the lexicon. By percolating lemmas up the chains of heads, each node in a headed tree may be labeled with a lexical head. Figure 1 is an example, where lexical heads are written as subscripts. We use the notation  for the lexical head of a vertex  , and  for the ordinary category or word label of  . The governor label for a terminal vertex ","in such a labeled tree is a triple which represents the syntactic and lexical environment at the top of the chain of vertices headed by ",". Where is the maximal vertex of which ","is a head vertex,","and","is the parent of",", the governor label for  1 Headed trees may be constructed as tree domains, which","are sets of addresses of vertices. 0 is used as the relative ad-","dress of the head vertex, negative integers are used as relative","addresses of child vertices before the head, and positive in-","tegers are used as relative addresses of child vertices after","the head. A headed tree domain is a set of finitesequences","of integers","such that (i) if",", then","; (ii) if","and","","or  , then   .","position word governor label 1 Peter","NP,S,read 2 reads","S,STARTC,startw 3 every","D,NP,paper 4 paper","NP,VP,read 5 on","P:ON,PP:ON,markup 6 markup","NP,PP:ON,paper Figure 2: Governor labels for the terminals in the tree of Figure 1. For the head of the sentence, special symbols startc and startw are used as the parent category and parent lexical governor. is the tuple         ","",".2","Governor labels for the example tree are given in Figure 2.","As observed in Chomsky (1965), grammatical relations such as subject and object may be re-constructed as ordered pairs of category labels, such as","NP,S","for subject. So, a governor label encodes a grammatical relation and a governing lexical head.","Given a unique tree structure for a sentence, governor markup may be read off the tree. However, in view of the fact that robust broad coverage parsers frequently deliver thousands, millions, or thousands of millions of analyses for sentences of free text, basing annotation on a unique tree (such as the most probable tree analysis generated by a probabilistic grammar) appears arbitrary.","Note that different trees may produce the same governor labels for a given terminal position. Suppose for instance that the yield of the tree in Figure 1 has a different tree analysis in which the PP is a child of the VP, rather than NP. In this case, just as in the original tree, the label for the fourth terminal position (with word label paper) is","NP,VP,read",". Supposing that there are only two tree analyses, this label can be assigned to the fourth word with certainty, in the face of syntactic ambiguity. The algorithm we will definepools governor labels in this way."]},{"title":"2 Expected Governors","paragraphs":["Suppose that a probabilistic grammar licenses headed tree analyses   ","for a sentence",",","and assigns them probabilistic weights    .","2","In a headed tree domain,","is a head of","if","is of the","form ","for . that NP SC deprive .95 .99 would MD2 VFP deprive .98 1 deprive S STARTC startw 1 1 all DETPL2 NC student .83 .98 beginning NSG1 NPL1 student .75 .98 students NP VFP deprive .82 .98","NP VGP begin .16 of PP NP student .53","PP VFP deprive .38 .99 their DETPL2 NC lunch .92 .98 high ADJMOD NPL2 lunch .78 .23","ADJMOD NSG2 school .15 .76 school NCHAIN NPL1 lunch .16","NSG1 NPL1 lunch .76 .98 lunches NP PP of .91 .98 . PERC S deprive .88 .86","PERC X deprive .14 Figure 3: Expected governors in the sentence That would deprive all beginning students of their high school lunches. For a label","in column 2, column 3 gives","as computed with a PCFG weighting of trees, and column 4 gives ","as computed with a head-lexicalized weighting of trees. Values below 0.1 are omitted. According to the lexicalized model, the PP headed by of probably attaches to VFP (finiteverb phrase) rather than NP. Let  ","","be the governor labels for word","position determined by ","","","respectively.","We define a scheme which divides a count of 1","among the different governor labels.","For a given governor tuple , let","  ","","def","","","","  ","  (1) The definition sums the probabilistic weights of trees with markup",", and normalizes by the sum of the probabilities of all tree analyses of",".","The definition may be justified as follows. We work with a markup space  , where  is the set of category labels and ","is the","set of lemma labels. For a given markup triple",",","let","  ","","be the function which maps","to 1, and","to 0","for "," . We definea random variate","","","Trees","","","","which maps a tree to   , where","is the gov-","ernor markup for word position","which is de-","termined by tree",". The random variate","is de-","finedon labeled trees licensed by the probabilistic","grammar. Note that","","","","is a vector space","(with pointwise sums and scalar products), so that","expectations and conditional expectations may be","defined. In these terms, ","is the conditional ex-","pectation of",", conditioned on the yield being",".","This definition,instead of a single governor la-","bel for a given word position, gives us a set of","pairs of a markup","and a real number","   ","in [0,1], such that the real numbers in the pairs sum to 1. In our implementation (which is based on Schmid (2000a)), we use a cutoff of 0.1, and print only indices","where","    is above the cutoff. Figure 3 is an example.","A direct implementation of the above definition using an iteration over trees to compute ","would be unusable because in the robust grammar of English we work with, the number of tree analyses for a sentence is frequently large, greater than for about 1/10 of the sentences in the British Na-tional Corpus. We instead calculate ","in a parse forest representation of a set of tree analyses."]},{"title":"3 Parse Forests","paragraphs":["A parse forest (see also Billot and Lang (1989)) in labeled grammar notation is a tuple         "," where      ","","is a context free gram-","mar (consisting of non-terminals",", terminals","",", rules","",", and a start symbol   ) and  ","is a function which maps elements of","to non-terminals in an underlying grammar       and elements of "," to terminals in  . By using  ","on symbols on the left","hand and right hand sides of a parse forest rule, ","can be extended to map the set of parse forest","rules","to the set of underlying grammar rules  . ","","is also extended to map trees licensed by the parse forest grammar to trees licensed by the underlying grammar. An example is given in figure4. Where "," , let    be the set of trees licensed by     ","","which have","root symbol in the case of a symbol, and the set","of trees which have","as the rule expanding the","root in the case or a rule.    is defined to be","the multiset image of     under   .   ","is the multiset of inside trees represented by parse","S NP","VP","VP V","NP","VP VP","PP","NP"," NP","PP","NP"," D","N","PP P","NP","VP V","NP","NP","N","NP","Peter","V","reads","D ","every","N","paper","P","on","N","markup Figure 4: Rule set","of a labeled grammar representing two tree analyses of John reads every paper on markup. The labeling function drops subscripts, so that    VP   VP.","forest symbol or rule .3","Let     be the set of trees in   ","which contain as a symbol or","use as a rule.","    is definedto be the multiset image of      under   .","   ","is the multiset","of complete trees represented by the parse forest","symbol or rule",".","Where","is a probability function on trees li-","censed by the underlying grammar and","is a sym-","bol or rule in",",     def","      (2)  ","","def ","","   "," ","     (3)    ","is called the inside probability for and    is called the flo w for",".4","Parse forests are often constructed so that all inside trees represented by a parse forest nonterminal","have the same span, as well as the same parent category. To deal with headedness and lexicalization of a probabilistic grammar, we construct parse forests so that, in addition, all inside trees represented by a parse forest nonterminal have the same lexical head. We add to the labeled grammar a function","which labels parse forest symbols with lexical heads. In our implementation, an ordinary context free parse forest is","3","We use multisets rather than set images to achieve correctness of the inside algorithm in cases where","represents some tree more than once, something which is possible given the definition of labeled grammars. A correct parser produces a parse forest which represents every parse for the input sentence exactly once.","4","These quantities can be given probabilistic interpreta-tions and/or definitions, for instance with reference to conditionally expected rule frequencies for flo w. PF-INSIDE( ",") 1 Initial. float array ","","  ","","   ","0 2 for   3 do    ","4 for","in","in bottom-up order","5 do              6   lhs       lhs         7 return  Figure 5: Inside algorithm. firstconstructed by tabular parsing, and then in a second pass parse forest symbols are split according to headedness. Such an algorithm is shown in appendix B. This procedure gives worst case time and space complexity which is proportional to the fifth power of the length of the sentence. See Eisner and Satta (1999) for discussion and an algorithm with time and space requirements proportional to the fourth power of the length of the input sentence in the worst case. In practical experience with broad-coverage context free grammars of several languages, we have not observed super-cubic average time or space requirements for our implementation. We believe this is because, for our grammars and corpora, there is limited ambiguity in the position of the head within a given category-span combination.","The governor algorithm stated in the next section refers to headedness in parse forest rules. This can be represented by constructing parse forest rules (as well as ordinary grammar rules) with headed tree domains of depth one.5","Where","is a parse forest symbol on the right hand side of a parse forest rule",", we will simply state the condition “","is the head of","”.","The flo w and governor algorithms stated below call an algorithm PF-INSIDE    which","computes inside probabilities in , where ","is a function giving probability parameters for the underlying grammar. Any probability weighting of trees may be used which allows inside probabilities to be computed in parse forests. The inside","5","See footnote 1. Constructed in this way, the firstrule in parse forest in Figure 4 has domain  , and labeling","function S","NP   VP ",". When parse forest","rules are mapped to underlying grammar rules, the domain is","preserved, so that","","applied to the parse forest rule just de-","scribed is the tree with domain  and label function"," S","NP   VP ",". is the empty string. PF-FLOW(  ",") 1 Initial. float array ","","  ","","   ","0 2    ","3 for","in","in top-down order","4 do ","","","","      lhs     5 for  in rhs    6 do             7 return  Figure 6: Flow algorithm. algorithm for ordinary PCFGs is given in figure 5. The parameter ","maps the set of underlying","grammar rules which is the image of "," on ","to reals, with the interpretation of rule probabilities. In step 5, "," maps the parse forest rule  to a grammar rule     which is the argument of ",". The functions lhs and rhs map rules to their","left hand and right hand sides, respectively. Given an inside algorithm, the flo w ","may be computed by the flo w algorithm in Figure 6, or by the inside-outside algorithm."]},{"title":"4 Governors Algorithm","paragraphs":["The governor algorithm annotates parse forest symbols and rules with functions from governor labels to real numbers. Let","be a tree in the parse forest grammar, let  be a symbol in",", let","be the","maximal symbol in of which  is a head, or  itself if ","is a non-head child of its parent in",", and","let","be the parent of","in",". Recall that   "," ","(4) is a vector mapping the markup triple                ","","to 1 and other markups","to 0. We have constructed parse forests such","that         ","    ","","agrees with the","governor label for the lexical head of the node","corresponding to  in     ",".","A parse forest tree and symbol ","in","thus de-","termine the vector (4), where","and","","are defined","as above. Call the vector determined in this way","   . Where  is parse forest symbol in","and","","is a parse forest rule in",", let","","","def","","","         "," ","    "," (5) PF-GOVERNORS(  ) 1 "," PF-INSIDE    2 "," PF-FLOW     ","3 Initialize array  "," to empty maps from governor labels to float","4   ","  ","","","startc","startw","5 for","in in top-down order","6 do","","","","","","","","  lhs",""," ","7 for in rhs   ","8 do if","is the head of","9 then","","   ","","","","","10 else",""," ","","    ","","  ","",""," 11 return Figure 7: Parse forest computation of governor vector.","","","  def","","","          lhs   "," ","       (6) Assuming that ","    ","      ","is","a parse forest representing each tree analysis for","a sentence exactly once, the quantity ","for termi-","nal position (as defined in section 1) is found","by summing  for terminal symbols  in  ","which have string position",".6 The algorithm PF-GOVERNORS is stated in Fig-","ure 3. Working top down, if fills in an array","  ","which is supposed to agree with the quan-","tity","  ","defined above. Scaled governor vectors are created for non-head children in step 10, and summed down the chain of heads in step 9. In step 6, vectors are divided in proportion to inside probabilities (just as in the flo w algorithm), because the set of complete trees for the left hand side of","are partitioned among the parse forest rules which expand the left hand side of",".","Consider a parse forest rule",", and a parse forest symbol","on its right hand side which is not the head of",". In each tree in    ",",","is the top","of a chain of heads, because","is a non-head child","in rule",". In step 10, the governor tuple describing","the syntactic environment of","in trees in      (or rather, their images under   ) is constructed","6","This procedure requires that symbols in","","correspond to a unique string position, something which is not enforced by our definition of parse forests. Indeed, such cases may arise if parse forest symbols are constructed as pairs of grammar symbols and strings (Tendeau, 1998) rather than pairs of grammar symbols and spans. Our parser constructs parse forests organized according to span. as ","","","","","   ","","","","","",". The scalar multi-","plier    ","is"," ","   "," ","     the relative weight of trees in      . This is ap-","propriate because   ","as definedin equation (5) is to be scaled by the relative weight of trees in     ",".","In line 9 of the algorithm,","is summed into the","head child",". There is no scaling, because every","tree in      is a tree in     ",". A probability parameter vector ","is used in the inside algorithm. In our implementation, we can use either a probabilistic context free grammar, or a lexicalized context free grammar which conditions rules on parent category and parent lexical head, and conditions the heads of non-head children on child category, parent category, and parent head (Eisner, 1997; Charniak, 1995; Carroll and Rooth, 1998). The requisite information is directly represented in our parse forests by ","","and ",". Thus the call to PF-INSIDE in line 1 of PF-GOVERNORS may involve either a computation of PCFG inside probabilities, or head-lexicalized inside probabilities. However, in both cases the algorithm requires that the parse forest symbols be split according to heads, because of the reference to","","in line 10. Construction of head-marked parse forests is presented in the appendix.","The LoPar parser (Schmid, 2000a) on which our implementation of the governor algorithm is based represents the parse forest as a graph with at most binary branching structure. Nodes with more than two daughter nodes in a conventional parse forest are replaced with a right-branching tree structure and common sub-trees are shared between different analyses. The worst-case space complexity of this representation is cubic (cmp. Billot and Lang (1989)).","LoPar already provided functions for the computation of the head-marked parse forest, for the flo w computation and for traversing the parse forest in depth-first and topologically-sorted order (see Cormen et al. (1994)). So it was only necessary to add functions for data initialization, for the computation of the governor vector at each node and for printing the result."]},{"title":"5 Pooling of grammatical relations","paragraphs":["The governor labels defined above are derived from the specificsymbols of a context free grammar. In contrast, according to the general markup methodology of current computational linguistics, labels should not be tied to a specific grammar and formalism. The same markup labels should be produced by different systems, making it possible to substitute one system for another, and to compare systems using objective tests.","Carroll et al. (1998) and Carroll et al. (1999) propose a system of grammatical relation markup to which we would like to assimilate our proposal. As grammatical relation symbols, they use atomic labels such as dobj (direct object) an ncsubj (nonclausal subject). The labels are arranged in a hierarchy, with for instance subj having subtypes ncsubj, xsubj, and csubj.","There is another problem with the labels we have used so far. Our grammar codes a variety of features, such as the feature VFORM on verb projections. As a result, instead of a single object grammatical relation","NP,VP",", we have grammatical relations","NP,VP.N",",","NP,VP.FIN",",","NP,VP.TO",",  NP,VP.BASE",", and so forth. This may result in frequency mass being split among different but similar labels. For instance, a verb phrase will have read every paper might have some analyses in which read is the head of a base form VP and paper is the head of the object of read, and others where read is a head of a finite form VP, and paper is the head of the object of read. In this case, frequencies would be split between  NP,VP.BASE,read","and","NP,VP.FIN,read","as governor labels for paper. To address these problems, we employ a pooling function ","","which maps pairs of categories","to symbols such as ncsubj or obj. The gover-","nor tuple           is then replaced by                ","in the definition of the","governor label for a terminal vertex  . Line 10 of PF-GOVERNORS is changed to","","","","","","     ","","  ","",""," ","More fle xibility could be gained by using a rule and the address of a constituent on the right hand side as arguments of "," . This would allow the following assignments.   ","VP.FIN VC.FIN’ NP NP     dobj   ","VP.FIN VC.FIN’ NP NP","  obj2   ","VP.FIN VC.FIN’ VP.TO     xcomp   ","VP.FIN VP.FIN’ VP.TO     xmod The head of a rule is marked with a prime. In the firstpair, the objects in double object construction are distinguished using the address. In each case, the child-parent category pair is","NP,VP.FIN",", so that the original proposal could not distinguish the grammatical relations. In the second pair, a VP.TO argument is distinguished from a VP.TO modifier using the category of the head. In each case, the child-parent category pair is","VP.TO,VP.FIN",". Notice that in Line 10 of PF-GOVERNORS, the rule ","is available, so that the arguments of"," could be changed in this way."]},{"title":"6 Discussion","paragraphs":["The governor algorithm was designed as a component of Spot, a free-text question answering system. Current systems usually extract a set of candidate answers (e.g. sentences), score them and return the n highest-scoring candidates as possible answers. The system described in Harabagiu et al. (2000) scores possible answers based on the overlap in the semantic representations of the question and the answer candidates. Their semantic representation is basically identical to the head-head relations computed by the governor algorithm. However, Harabagiu et al. extract this information only from maximal probability parses whereas the governor algorithm considers all analyses of a sentence and returns all possible relations weighted with estimated frequencies. Our application in Spot works as follows: the question is parsed with a specialized question grammar, and features including the governor of the trace are extracted from the question. Governors are among the features used for ranking sentences, and answer terms within sentences. In collaboration with Pranav Anand and Eric Breck, we have incorporated governor markup in the question answering prototype, but not debugged or evaluated it.","Expected governor markup summarizes syntactic structure in a weighted parse forest which is the product of exhaustive parsing and inside-outside computation. This is a strategy of dumbing down the product of computation-ally intensive statistical parsing into unstructured markup. Estimated frequency computations in parse forests have previously been applied to tagging and chunking (Schulte im Walde and Schmid, 2000). Governor markup differs in that it is reflecti ve of higher-level syntax. The strategy has the advantage, in our view, that it allows one to base markup algorithms on relatively so-phisticated grammars, and to take advantage of the lexically sensitive probabilistic weighting of trees which is provided by a lexicalized probability model.","Localizing markup on the governed word in-creases pooling of frequencies, because the span of the phrase headed by the governed item is ignored. This idea could be exploited in other markup tasks. In a chunking task, categories and heads of chunks could be identified, rather than categories and boundaries."]},{"title":"References","paragraphs":["Sylvie Billot and Bernard Lang. 1989. The structure of shared forests in ambiguous parsing. In Proceedings of the 27th Annual Meeting of the ACL, University of British Columbia, Vancouver, B.C., Canada.","Glenn Carroll and Mats Rooth. 1998. Valence induction with a head-lexicalized PCFG. In Proceedings of Third Conference on Empirical Methods in Natural Language Processing, Granada, Spain.","John Carroll, Antonio Sanfilippo, and Ted Briscoe. 1998. Parser evaluation: a survey and a new proposal. In Proceedings of the International Conference of Language Resources and Evaluation, pages 447–454, Granada, Spain.","John Carroll, Guido Minnen, and Ted Briscoe. 1999. Corpus annotation for parser evaluation. In Proceedings of the EACL99 workshop on Linguistically Interpreted Corpora (LINC), Bergen, Norway, June.","Eugene Charniak. 1993. Statistical Language Learn-ing. The MIT Press, Cambridge, Massachusetts.","Eugene Charniak. 1995. Parsing with context-free grammars and word statistics. Technical Report CS-95-28, Department of Computer Science, Brown University.","Noam Chomsky. 1965. Aspects of the Theory of Syntax. M.I.T. Press, Cambridge, MA.","Thomas H. Cormen, Charles E. Leiserson, and Ronald L. Rivest. 1994. Introduction to Algorithms. The MIT Press, Cambridge, Massachusetts.","Jason Eisner and Giorgio Satta. 1999. Efficientpars-ing for bilexical context-free grammars and head automaton grammars. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics (ACL’99), College Park, MD.","Jason Eisner. 1997. Bilexical grammars and a cubic-time probabilistic parser. In Proceedings of the 4th international Workshop on Parsing Technologies, Cambridge, MA.","S. Harabagiu, D. Moldovan, M. Pasca, R. Mihalcea, M. Surdeanu, R. Bunescu, R. Gîrju, V. Rus, and P. Morarescu. 2000. Falcon: Boosting knowledge for answer engines. In Proceedings of the Ninth Text REtrieval Conference (TREC 9), Gaithersburg, MD, USA, November.","Helmut Schmid. 2000a. LoPar: Design and Implementation. Number 149 in Arbeitspapiere des Sonderforschungsbereiches 340. Institute for Computational Linguistics, University of Stuttgart.","Helmut Schmid. 2000b. Lopar man pages. Institute for Computational Linguistics, University of Stuttgart.","Sabine Schulte im Walde and Helmut Schmid. 2000. Robust german noun chunking with a probabilistic context-free grammar. In Proceedings of the 18th International Conference on Computational Linguistics, pages 726–732, Saarbrücken, Germany, August.","Frederic Tendeau. 1998. Computing abstract decora-tions of parse forests using dynamic programming and algebraic power series. Theoretical Computer Science, (199):145–166."]},{"title":"A Relation Between Flow and Inside-Outside Algorithm","paragraphs":["The inside-outside algorithm computes inside probabilities     and outside probabilities ","",".","We will show that these quantities are related","to the flo w  by the equation               "," .   ","","is the inside probability of the root symbol, which is also the sum of the probabilities of all parse trees. According to Charniak (1993), the outside probabilities in a parse forest are computed by:   ","    lhs            ","The outside probability of the start symbol is 1. We prove by induction over the depth of the parse forest that the following relationship holds:                ","It is easy to see that the assumption holds for the root symbol   :                    The flo w in a parse forest is computed by:    ","     lhs           lhs     Now, we insert the induction hypothesis:    ","    lhs       lhs               lhs     After a few transformations, we get the equation               "," lhs             which is equivalent to                 according to the definition of   . So, the induction hypothesis is generally true."]},{"title":"B Parse Forest Lexicalization","paragraphs":["The function LEXICALIZE below takes an unlexicalized parse forest as argument and returns a lexicalized parse forests, where each symbol is uniquely labeled with a lexical head. Symbols are split if they have more than one lexical head.","LEXICALIZE( )","1 initialize  as an empty parse forest","2 initialize array","   3 for  in   4 do "," NEWT   "," 5    ","6 for","in","in bottom-up order","7 do assume rhs   ","         8 assume   is the head of 9 for             "," ","     10 do if     11 then "," LEM      12 else ","      ","13         14 "," ","lhs   ","15 r’ ADD(","   ","  ) 16      ","lhs      17 return","LEXICALIZE creates new terminal symbols by calling the function NEWT. The new symbols are linked to the original ones by means of","",". For each rule in the old parse forest, the set of all possible combinations of the lexicalized daughter symbols is generated. The function LEM   ","returns the lemma associated with lexical rule . ADD(   "," ) 1 if   lhs     s.t.  ","  2 then     3 else "," NEWNT    4          lhs    5   ","6 ","NEWRULE       7            ","8 return ","For each combination of lexicalized daughter symbols, a new rule is inserted by calling ADD. ADD calls NEWNT to create new non-terminals and NEWRULE to generate new rules. A non-terminal is only created if no symbol with the same lexical head was linked to the original node."]}],"references":[{"authors":[{"first":"Sylvie","last":"Billot"},{"first":"Bernard","last":"Lang"}],"year":"1989","title":"The structure of shared forests in ambiguous parsing","source":"Sylvie Billot and Bernard Lang. 1989. The structure of shared forests in ambiguous parsing. In Proceedings of the 27th Annual Meeting of the ACL, University of British Columbia, Vancouver, B.C., Canada."},{"authors":[{"first":"Glenn","last":"Carroll"},{"first":"Mats","last":"Rooth"}],"year":"1998","title":"Valence induction with a head-lexicalized PCFG","source":"Glenn Carroll and Mats Rooth. 1998. Valence induction with a head-lexicalized PCFG. In Proceedings of Third Conference on Empirical Methods in Natural Language Processing, Granada, Spain."},{"authors":[{"first":"John","last":"Carroll"},{"first":"Antonio","last":"Sanfilippo"},{"first":"Ted","last":"Briscoe"}],"year":"1998","title":"Parser evaluation: a survey and a new proposal","source":"John Carroll, Antonio Sanfilippo, and Ted Briscoe. 1998. Parser evaluation: a survey and a new proposal. In Proceedings of the International Conference of Language Resources and Evaluation, pages 447–454, Granada, Spain."},{"authors":[{"first":"John","last":"Carroll"},{"first":"Guido","last":"Minnen"},{"first":"Ted","last":"Briscoe"}],"year":"1999","title":"Corpus annotation for parser evaluation","source":"John Carroll, Guido Minnen, and Ted Briscoe. 1999. Corpus annotation for parser evaluation. In Proceedings of the EACL99 workshop on Linguistically Interpreted Corpora (LINC), Bergen, Norway, June."},{"authors":[{"first":"Eugene","last":"Charniak"}],"year":"1993","title":"Statistical Language Learn-ing","source":"Eugene Charniak. 1993. Statistical Language Learn-ing. The MIT Press, Cambridge, Massachusetts."},{"authors":[{"first":"Eugene","last":"Charniak"}],"year":"1995","title":"Parsing with context-free grammars and word statistics","source":"Eugene Charniak. 1995. Parsing with context-free grammars and word statistics. Technical Report CS-95-28, Department of Computer Science, Brown University."},{"authors":[{"first":"Noam","last":"Chomsky"}],"year":"1965","title":"Aspects of the Theory of Syntax","source":"Noam Chomsky. 1965. Aspects of the Theory of Syntax. M.I.T. Press, Cambridge, MA."},{"authors":[{"first":"Thomas","middle":"H.","last":"Cormen"},{"first":"Charles","middle":"E.","last":"Leiserson"},{"first":"Ronald","middle":"L.","last":"Rivest"}],"year":"1994","title":"Introduction to Algorithms","source":"Thomas H. Cormen, Charles E. Leiserson, and Ronald L. Rivest. 1994. Introduction to Algorithms. The MIT Press, Cambridge, Massachusetts."},{"authors":[{"first":"Jason","last":"Eisner"},{"first":"Giorgio","last":"Satta"}],"year":"1999","title":"Efficientpars-ing for bilexical context-free grammars and head automaton grammars","source":"Jason Eisner and Giorgio Satta. 1999. Efficientpars-ing for bilexical context-free grammars and head automaton grammars. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics (ACL’99), College Park, MD."},{"authors":[{"first":"Jason","last":"Eisner"}],"year":"1997","title":"Bilexical grammars and a cubic-time probabilistic parser","source":"Jason Eisner. 1997. Bilexical grammars and a cubic-time probabilistic parser. In Proceedings of the 4th international Workshop on Parsing Technologies, Cambridge, MA."},{"authors":[{"first":"S.","last":"Harabagiu"},{"first":"D.","last":"Moldovan"},{"first":"M.","last":"Pasca"},{"first":"R.","last":"Mihalcea"},{"first":"M.","last":"Surdeanu"},{"first":"R.","last":"Bunescu"},{"first":"R.","last":"Gîrju"},{"first":"V.","last":"Rus"},{"first":"P.","last":"Morarescu"}],"year":"2000","title":"Falcon: Boosting knowledge for answer engines","source":"S. Harabagiu, D. Moldovan, M. Pasca, R. Mihalcea, M. Surdeanu, R. Bunescu, R. Gîrju, V. Rus, and P. Morarescu. 2000. Falcon: Boosting knowledge for answer engines. In Proceedings of the Ninth Text REtrieval Conference (TREC 9), Gaithersburg, MD, USA, November."},{"authors":[{"first":"Helmut","last":"Schmid"}],"year":"2000a","title":"LoPar: Design and Implementation","source":"Helmut Schmid. 2000a. LoPar: Design and Implementation. Number 149 in Arbeitspapiere des Sonderforschungsbereiches 340. Institute for Computational Linguistics, University of Stuttgart."},{"authors":[{"first":"Helmut","last":"Schmid"}],"year":"2000b","title":"Lopar man pages","source":"Helmut Schmid. 2000b. Lopar man pages. Institute for Computational Linguistics, University of Stuttgart."},{"authors":[{"first":"Sabine","middle":"Schulte im","last":"Walde"},{"first":"Helmut","last":"Schmid"}],"year":"2000","title":"Robust german noun chunking with a probabilistic context-free grammar","source":"Sabine Schulte im Walde and Helmut Schmid. 2000. Robust german noun chunking with a probabilistic context-free grammar. In Proceedings of the 18th International Conference on Computational Linguistics, pages 726–732, Saarbrücken, Germany, August."},{"authors":[{"first":"Frederic","last":"Tendeau"}],"year":"1998","title":"Computing abstract decora-tions of parse forests using dynamic programming and algebraic power series","source":"Frederic Tendeau. 1998. Computing abstract decora-tions of parse forests using dynamic programming and algebraic power series. Theoretical Computer Science, (199):145–166."}],"cites":[{"style":0,"text":"Chomsky (1965)","origin":{"pointer":"/sections/1/paragraphs/41","offset":15,"length":14},"authors":[{"last":"Chomsky"}],"year":"1965","references":["/references/6"]},{"style":0,"text":"Schmid (2000a)","origin":{"pointer":"/sections/2/paragraphs/81","offset":101,"length":14},"authors":[{"last":"Schmid"}],"year":"2000a","references":["/references/11"]},{"style":0,"text":"Billot and Lang (1989)","origin":{"pointer":"/sections/3/paragraphs/0","offset":25,"length":22},"authors":[{"last":"Billot"},{"last":"Lang"}],"year":"1989","references":["/references/0"]},{"style":0,"text":"Eisner and Satta (1999)","origin":{"pointer":"/sections/3/paragraphs/102","offset":377,"length":23},"authors":[{"last":"Eisner"},{"last":"Satta"}],"year":"1999","references":["/references/8"]},{"style":0,"text":"Tendeau, 1998","origin":{"pointer":"/sections/4/paragraphs/129","offset":218,"length":13},"authors":[{"last":"Tendeau"}],"year":"1998","references":["/references/14"]},{"style":0,"text":"Eisner, 1997","origin":{"pointer":"/sections/4/paragraphs/156","offset":316,"length":12},"authors":[{"last":"Eisner"}],"year":"1997","references":["/references/9"]},{"style":0,"text":"Charniak, 1995","origin":{"pointer":"/sections/4/paragraphs/156","offset":330,"length":14},"authors":[{"last":"Charniak"}],"year":"1995","references":["/references/5"]},{"style":0,"text":"Carroll and Rooth, 1998","origin":{"pointer":"/sections/4/paragraphs/156","offset":346,"length":23},"authors":[{"last":"Carroll"},{"last":"Rooth"}],"year":"1998","references":["/references/1"]},{"style":0,"text":"Schmid, 2000a","origin":{"pointer":"/sections/4/paragraphs/162","offset":18,"length":13},"authors":[{"last":"Schmid"}],"year":"2000a","references":["/references/11"]},{"style":0,"text":"Billot and Lang (1989)","origin":{"pointer":"/sections/4/paragraphs/162","offset":428,"length":22},"authors":[{"last":"Billot"},{"last":"Lang"}],"year":"1989","references":["/references/0"]},{"style":0,"text":"Cormen et al. (1994)","origin":{"pointer":"/sections/4/paragraphs/163","offset":199,"length":20},"authors":[{"last":"Cormen"},{"last":"al."}],"year":"1994","references":["/references/7"]},{"style":0,"text":"Carroll et al. (1998)","origin":{"pointer":"/sections/5/paragraphs/1","offset":0,"length":21},"authors":[{"last":"Carroll"},{"last":"al."}],"year":"1998","references":["/references/2"]},{"style":0,"text":"Carroll et al. (1999)","origin":{"pointer":"/sections/5/paragraphs/1","offset":26,"length":21},"authors":[{"last":"Carroll"},{"last":"al."}],"year":"1999","references":["/references/3"]},{"style":0,"text":"Harabagiu et al. (2000)","origin":{"pointer":"/sections/6/paragraphs/0","offset":276,"length":23},"authors":[{"last":"Harabagiu"},{"last":"al."}],"year":"2000","references":["/references/10"]},{"style":0,"text":"Walde and Schmid, 2000","origin":{"pointer":"/sections/6/paragraphs/1","offset":396,"length":22},"authors":[{"last":"Walde"},{"last":"Schmid"}],"year":"2000","references":["/references/13"]},{"style":0,"text":"Charniak (1993)","origin":{"pointer":"/sections/7/paragraphs/7","offset":122,"length":15},"authors":[{"last":"Charniak"}],"year":"1993","references":["/references/4"]}]}
