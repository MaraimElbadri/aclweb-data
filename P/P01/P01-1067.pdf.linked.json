{"sections":[{"title":"A Syntax-based Statistical Translation Model Kenji Yamada and Kevin Knight Information Sciences Institute University of Southern California 4676 Admiralty Way, Suite 1001 Marina del Rey, CA 90292 kyamada,knight @isi.edu Abstract","paragraphs":["We present a syntax-based statistical translation model. Our model transforms a source-language parse tree into a target-language string by apply-ing stochastic operations at each node. These operations capture linguistic differences such as word order and case marking. Model parameters are estimated in polynomial time using an EM algorithm. The model produces word alignments that are better than those produced by IBM Model 5."]},{"title":"1 Introduction","paragraphs":["A statistical translation model (TM) is a mathematical model in which the process of human-language translation is statistically modeled. Model parameters are automatically estimated using a corpus of translation pairs. TMs have been used for statistical machine translation (Berger et al., 1996), word alignment of a translation corpus (Melamed, 2000), multilingual document retrieval (Franz et al., 1999), automatic dictionary construction (Resnik and Melamed, 1997), and data preparation for word sense disambiguation programs (Brown et al., 1991). Developing a better TM is a fundamental issue for those applications.","Researchers at IBM first described such a statistical TM in (Brown et al., 1988). Their models are based on a string-to-string noisy channel model. The channel converts a sequence of words in one language (such as English) into another (such as French). The channel operations are movements, duplications, and translations, applied to each word independently. The movement is conditioned only on word classes and positions in the string, and the duplication and translation are conditioned only on the word identity. Mathematical details are fully described in (Brown et al., 1993).","One criticism of the IBM-style TM is that it does not model structural or syntactic aspects of the language. The TM was only demonstrated for a structurally similar language pair (English and French). It has been suspected that a language pair with very different word order such as English and Japanese would not be modeled well by these TMs.","To incorporate structural aspects of the language, our channel model accepts a parse tree as an input, i.e., the input sentence is preprocessed by a syntactic parser. The channel performs operations on each node of the parse tree. The operations are reordering child nodes, inserting extra words at each node, and translating leaf words. Figure 1 shows the overview of the operations of our model. Note that the output of our model is a string, not a parse tree. Therefore, parsing is only needed on the channel input side.","The reorder operation is intended to model translation between languages with different word orders, such as SVO-languages (English or Chinese) and SOV-languages (Japanese or Turkish). The word-insertion operation is intended to capture linguistic differences in specifying syntactic cases. E.g., English and French use structural position to specify case, while Japanese and Korean use case-marker particles.","Wang (1998) enhanced the IBM models by in-troducing phrases, and Och et al. (1999) used templates to capture phrasal sequences in a sentence. Both also tried to incorporate structural aspects of the language, however, neither handles 1. Channel Input 3. Inserted       2. Reordered","      ","","","kare ha ongaku wo kiku no ga daisuki desu 5. Channel Output","     ","","","","","","","     ","","","",""," 4. Translated","","                    ",""," VB PRP VB1 VB2 VB TO TO NN VB VB2 TO      VB1         VB  PRP     NN  TO VB     ","     VB2","TO VB               VB1  PRP     NN  TO VB     ","     VB2 TO VB PRP NN TO VB1                        Figure 1: Channel Operations: Reorder, Insert, and Translate nested structures.","Wu (1997) and Alshawi et al. (2000) showed statistical models based on syntactic structure. The way we handle syntactic parse trees is in-spired by their work, although their approach is not to model the translation process, but to formalize a model that generates two languages at the same time. Our channel operations are also similar to the mechanism in Twisted Pair Grammar (Jones and Havrilla, 1998) used in their knowledge-based system.","Following (Brown et al., 1993) and the other literature in TM, this paper only focuses the details of TM. Applications of our TM, such as machine translation or dictionary construction, will be described in a separate paper. Section 2 describes our model in detail. Section 3 shows experimental results. We conclude with Section 4, followed by an Appendix describing the training algorithm in more detail."]},{"title":"2 The Model 2.1 An Example","paragraphs":["We first introduce our translation model with an example. Section 2.2 will describe the model more formally. We assume that an English parse tree is fed into a noisy channel and that it is translated to a Japanese sentence.1","1","The parse tree is flattened to work well with the model. See Section 3.1 for details.","Figure 1 shows how the channel works. First, child nodes on each internal node are stochastically reordered. A node with","children has ","possible reorderings. The probability of tak-ing a specific reordering is given by the model’s r-table. Sample model parameters are shown in Table 1. We assume that only the sequence of child node labels influences the reordering. In Figure 1, the top VB node has a child sequence PRP-VB1-VB2. The probability of reordering it into PRP-VB2-VB1 is 0.723 (the second row in the r-table in Table 1). We also reorder VB-TO into TO-VB, and TO-NN into NN-TO, so therefore the probability of the second tree in Figure 1 is",".","Next, an extra word is stochastically inserted at each node. A word can be inserted either to the left of the node, to the right of the node, or nowhere. Brown et al. (1993) assumes that there is an invisible NULL word in the input sentence and it generates output words that are distributed into random positions. Here, we instead decide the position on the basis of the nodes of the input parse tree. The insertion probability is determined by the n-table. For simplicity, we split the n-table into two: a table for insert positions and a table for words to be inserted (Table 1). The node’s label and its parent’s label are used to in-dex the table for insert positions. For example, the PRP node in Figure 1 has parent VB, thus                        ","                                  ","                                                  ","","    ","      ","      ","","     ","      ","   ","","","                                                                      ","   ","","","","","","","","","","      ","","","","","","","","","    ","","","","","                                          ","    ","               ","              ","          ","","","","   ","","          ","    ","                                                   ","","","","","","","","","","","","","","","","","","","","","","     ","","","   ","","  ","","","","","","","","","","","",""," ","","","   ","","","","","","   ","","","","","","   ","","","","","","   ","","","","","","   ","","","","","","   ","","","        ","  ","","  ","              ","              ","              ","     ","","","   ","","","    ","  ","       ","","","","","","","","","","","","","","","","","","","","","","","","","","","","","   ","",""," r−table t−table n−table Table 1: Model Parameter Tables  parent=VB","node=PRP","is the conditioning in-dex. Using this label pair captures, for example, the regularity of inserting case-marker particles. When we decide which word to insert, no conditioning variable is used. That is, a function word like ga is just as likely to be inserted in one place as any other. In Figure 1, we inserted four words (ha, no, ga and desu) to create the third tree. The top VB node, two TO nodes, and the NN node inserted nothing. Therefore, the probability of obtaining the third tree given the second tree is        3.498e-9.","Finally, we apply the translate operation to each leaf. We assume that this operation is dependent only on the word itself and that no context is consulted.2","The model’s t-table specifies the probability for all cases. Suppose we obtained the translations shown in the fourth tree of Figure 1. The probability of the translate operation here is ",".","The total probability of the reorder, insert and translate operations in this example is 3.498e-9","1.828e-11. Note that there 2","When a TM is used in machine translation, the TM’s role is to provide a list of possible translations, and a language model addresses the context. See (Berger et al., 1996). are many other combinations of such operations that yield the same Japanese sentence. Therefore, the probability of the Japanese sentence given the English parse tree is the sum of all these probabilities.","We actually obtained the probability tables (Table 1) from a corpus of about two thousand pairs of English parse trees and Japanese sentences, completely automatically. Section 2.3 and Appendix 4 describe the training algorithm. 2.2 Formal Description","This section formally describes our translation","model. To make this paper comparable to (Brown","et al., 1993), we use English-French notation in","this section. We assume that an English parse","tree","is transformed into a French sentence",".","Let the English parse tree","consist of nodes","  ","","",", and let the output French sentence","consist of French words","","","","",". Three random variables,",",",", and","are chan-","nel operations applied to each node. Insertion","is an operation that inserts a French word just be-","fore or after the node. The insertion can be none,","left, or right. Also it decides what French word","to insert. Reorder","is an operation that changes","the order of the children of the node. If a node","has three children, e.g., there are","ways to reorder them. This operation applies only to non-terminal nodes in the tree. Translation","is an operation that translates a terminal English leaf word into a French word. This operation applies only to terminal nodes. Note that an English word can be translated into a French NULL word.","The notation "," stands for a set of values of "," . ","","is a","set of values of random variables associated with",""," . And","","","is the set of all ran-","dom variables associated with a parse tree","  ","","",". The probability of getting a French sentence","given an English parse tree","is","P","",""," Str   P    where Str    ","is the sequence of leaf words","of a tree transformed by","from",".","The probability of having a particular set of","values of random variables in a parse tree is P ","","","P","       P      ","This is an exact equation. Then, we assume that a transform operation is independent from other transform operations, and the random variables of each node are determined only by the node itself. So, we obtain P ","","","P","","","","   ","","","","","","       P      The random variables ","","are as-","sumed to be independent of each other. We also","assume that they are dependent on particular fea-","tures of the node  . Then, P    "," P         "," P      P      P     "," P   "," "," P     "," P      ","     ","                  ","where",",",", and","are the relevant features to","",", , and",", respectively. For example, we saw","that the parent node label and the node label were","used for",", and the syntactic category sequence of children was used for",". The last line in the above formula introduces a change in notation, meaning that those probabilities are the model parameters  ","  ,      , and      , where  ,  , and ","are the possible values for",",",", and",",","respectively.","In summary, the probability of getting a French","sentence given an English parse tree","is","P","",""," Str   P   ","   Str ","           ","                  ","where","","  ","","and ","","  ","","","","","","          ",".","The model parameters  ","  ,      , and       , that is, the probabilities P  ","  , P      and P    ","",", decide the behavior of the translation","model, and these are the probabilities we want to","estimate from a training corpus. 2.3 Automatic Parameter Estimation To estimate the model parameters, we use the EM algorithm (Dempster et al., 1977). The algorithm iteratively updates the model parameters to maximize the likelihood of the training corpus. First, the model parameters are initialized. We used a uniform distribution, but it can be a distribution taken from other models. For each iteration, the number of events are counted and weighted by the probabilities of the events. The probabilities of events are calculated from the current model parameters. The model parameters are re-estimated based on the counts, and used for the next iteration. In our case, an event is a pair of a value of a random variable (such as ",", , or",") and a feature value (such as  ,  , or ","). A separate counter is used for each event. Therefore, we need the same number of counters,     ,     , and   ","",",","as the number of entries in the probability tables,","  ","  ,      , and      .","The training procedure is the following:","1. Initialize all probability tables:    ,    , and     .","2. Reset all counters:    ,    , and","   . 3. For each pair ","  ","in the training corpus, For all ",", such that Str "," ,"," Let cnt = P   "," Str "," P   ","","For  ","","",",",""," ","  "," += cnt       "," += cnt     "," "," += cnt 4. For each     ,     , and     ,        ",""," ","","                      ",""," ","","   5. Repeat steps 2-4 for several iterations.","A straightforward implementation that tries all possible combinations of parameters "," , is very expensive, since there are    ","","     possible combinations, where    and    are the number of possible values for ","and , respectively ( is uniquely decided when ","and are given for a particular ","","). Appendix describes an efficient implementation that estimates the probability in polynomial time.3","With this efficient implementation, it took about 50 minutes per iteration on our corpus (about two thousand pairs of English parse trees and Japanese sentences. See the next section)."]},{"title":"3 Experiment","paragraphs":["To experiment, we trained our model on a small English-Japanese corpus. To evaluate performance, we examined alignments produced by the learned model. For comparison, we also trained IBM Model 5 on the same corpus. 3.1 Training We extracted 2121 translation sentence pairs from a Japanese-English dictionary. These sentences were mostly short ones. The average sentence length was 6.9 for English and 9.7 for Japanese. However, many rare words were used, which made the task difficult. The vocabulary size was 3463 tokens for English, and 3983 tokens for Japanese, with 2029 tokens for English and 2507 tokens for Japanese occurring only once in the corpus.","Brill’s part-of-speech (POS) tagger (Brill, 1995) and Collins’ parser (Collins, 1999) were used to obtain parse trees for the English side of the corpus. The output of Collins’ parser was","3","Note that the algorithm performs full EM counting, whereas the IBM models only permit counting over a sub-set of possible alignments. modified in the following way. First, to reduce the number of parameters in the model, each node was re-labelled with the POS of the node’s head word, and some POS labels were collapsed. For example, labels for different verb endings (such as VBD for -ed and VBG for -ing) were changed to the same label VB. There were then 30 different node labels, and 474 unique child label sequences.","Second, a subtree was flattened if the node’s head-word was the same as the parent’s head-word. For example, (NN1 (VB NN2)) was flattened to (NN1 VB NN2) if the VB was a head word for both NN1 and NN2. This flattening was motivated by various word orders in different languages. An English SVO structure is translated into SOV in Japanese, or into VSO in Arabic. These differences are easily modeled by the flattened subtree (NN1 VB NN2), rather than (NN1 (VB NN2)).","We ran 20 iterations of the EM algorithm as described in Section 2.2. IBM Model 5 was sequentially bootstrapped with Model 1, an HMM Model, and Model 3 (Och and Ney, 2000). Each preceding model and the final Model 5 were trained with five iterations (total 20 iterations). 3.2 Evaluation The training procedure resulted in the tables of estimated model parameters. Table 1 in Section 2.1 shows part of those parameters obtained by the training above.","To evaluate performance, we let the models generate the most probable alignment of the training corpus (called the Viterbi alignment). The alignment shows how the learned model induces the internal structure of the training data.","Figure 2 shows alignments produced by our model and IBM Model 5. Darker lines indicates that the particular alignment link was judged correct by humans. Three humans were asked to rate each alignment as okay (1.0 point), not sure (0.5 point), or wrong (0 point). The darkness of the lines in the figure reflects the human score. We obtained the average score of the first 50 sentence pairs in the corpus. We also counted the number of perfectly aligned sentence pairs in the 50 pairs. Perfect means that all alignments in a sentence pair were judged okay by all the human judges. he adores listening to music hypocrisy is abhorrent to them he has unusual ability in english he was ablaze with anger he adores listening to music hypocrisy is abhorrent to them he has unusual ability in english he was ablaze with anger Figure 2: Viterbi Alignments: our model (left) and IBM Model 5 (right). Darker lines are judged more correct by humans. The result was the following;","Alignment Perfect","ave. score sents","Our Model 0.582 10","IBM Model 5 0.431 0","Our model got a better result compared to IBM Model 5. Note that there were no perfect alignments from the IBM Model. Errors by the IBM Model were spread out over the whole set, while our errors were localized to some sentences. We expect that our model will therefore be easier to improve. Also, localized errors are good if the TM is used for corpus preparation or filtering.","We also measured training perplexity of the models. The perplexity of our model was 15.79, and that of IBM Model 5 was 9.84. For reference, the perplexity after 5 iterations of Model 1 was 24.01. Perplexity values roughly indicate the predictive power of the model. Generally, lower perplexity means a better model, but it might cause over-fitting to a training data. Since the IBM Model usually requires millions of training sentences, the lower perplexity value for the IBM Model is likely due to over-fitting."]},{"title":"4 Conclusion","paragraphs":["We have presented a syntax-based translation model that statistically models the translation process from an English parse tree into a foreign-language sentence. The model can make use of syntactic information and performs better for language pairs with different word orders and case marking schema. We conducted a small-scale experiment to compare the performance with IBM Model 5, and got better alignment results."]},{"title":"Appendix: An Efficient EM algorithm","paragraphs":["This appendix describes an efficient implementation of the EM algorithm for our translation model. This implementation uses a graph structure for a pair   ",". A graph node is either a","major-node or a subnode. A major-node shows a","pairing of a subtree of","and a substring of",". A","subnode shows a selection of a value "," for","the subtree-substring pair (Figure 3).","Let",""," ","","",""," ","","","be a substring of","from the word  with length",". Note this notation","is different from (Brown et al., 1993). A subtree","","","is a subtree of","below the node","",". We assume","that a subtree is",".","A major-node     ","","","is a pair of a subtree","","","and a substring   . The root of the graph is   ","","   , where","is the length of",". Each major-","node connects to several  -subnodes  ","","    , showing which value of  is selected. The arc between  ","   ","and  ","","   ","has weight P  ","  . A  -subnode  ","","   ","","connects to a final-node with weight P    "," if ","is a terminal node","in",". If","","is a non-terminal node, a -subnode","connects to several -subnodes        ","","",",","showing a selection of a value",". The weight of","the arc is P      .","A","-subnode is then connected to","-subnodes      ","","   ","",". The partition variable,",", shows","a particular way of partitioning","","",".","A","-subnode        ","","","","is then connected","to major-nodes which correspond to the children","of","","and the substring of   , decided by  ","",".","A major-node can be connected from different","-","subnodes. The arc weights between","-subnodes","and major-nodes are always 1.0. n P r P p","        ","                 (r|e) (n|e)        ","        "," Figure 3: Graph structure for efficient EM training.","This graph structure makes it easy to obtain P   ","  for a particular","and","  ","Str "," ","P   ","",". A trace starting from","the graph root, selecting one of the arcs from","major-nodes, ","-subnodes, and","-subnodes, and","all the arcs from","-subnodes, corresponds to a","particular , and the product of the weight on the","trace corresponds to P   ","",". Note that a trace forms a tree, making branches at the","-subnodes.","We define an alpha probability and a beta probability for each major-node, in analogy with the measures used in the inside-outside algorithm for probabilistic context free grammars (Baker, 1979).","The alpha probability (outside probability) is a path probability from the graph root to the node and the side branches of the node. The beta probability (inside probability) is a path probability below the node.","Figure 4 shows formulae for alpha-beta probabilities. From these definitions,","  ","Str "," ","P   ","     ","   ",". The counts     ,     , and     for each pair ","","","are also in the figure. Those formulae","replace the step 3 (in Section 2.3) for each training","pair, and these counts are used in the step 4. The graph structure is generated by expanding","the root node  ","","","","",". The beta probability for","each node is first calculated bottom-up, then the","alpha probability for each node is calculated top-","down. Once the alpha and beta probabilities for","each node are obtained, the counts are calculated","as above and used for updating the parameters. The complexity of this training algorithm is            . The cube comes from the number","of parse tree nodes (",") and the number of possible","French substrings (  )."]},{"title":"Acknowledgments","paragraphs":["This work was supported by DARPA-ITO grant N66001-00-1-9814."]},{"title":"References","paragraphs":["H. Alshawi, S. Bangalore, and S. Douglas. 2000. Learning dependency translation models as collections of finite state head transducers. Computational Linguistics, 26(1).","J. Baker. 1979. Trainable grammars for speech recognition. In Speech Communication Papers for the 97th Meeting of the Acoustical Sciety of America.","A. Berger, P. Brown, S. Della Pietra, V. Della Pietra, J. Gillett, J. Lafferty, R. Mercer, H. Printz, and L. Ures. 1996. Language Translation Apparatus and Method Using Context-Based Translation Models. U.S. Patent 5,510,981.","E. Brill. 1995. Transformation-based error-driven learning and natural language processing: A case study in part of specch tagging. Computational Linguistics, 21(4).","P. Brown, J. Cocke, S. Della Pietra, F. Jelinek, R. Mercer, and P. Roossin. 1988. A statistical approach to language translation. In COLING-88.","P. Brown, J. Cocke, S. Della Pietra, F. Jelinek, R. Mercer, and P. Roossin. 1991. Word-sense disambiguation using statistical methods. In ACL-91.","P. Brown, S. Della Pietra, V. Della Pietra, and R. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2).","","","",""," ","","","","","","  ",""," ","","","","   ","","","","","            ","","","","","","","","",""," ","","","","","","","","","","","","","","",""," ","","","","","","","","","","","   ","","","","","","   ","","","","","   ","",""," ","","","","","  ","","","","","","","","","","","","    ","","","","","",""," ","","","   ","","","","","","","","","",""," ","","  ","","","","","","","  ","","","","",""," ","  ","  ","","","","   ",""," ","  ","   ","    ",""," ","   ","","","  ","  ","  ","","","","   ","","",""," ","      ","    ",""," ","   ","","","","  ","  ","  ","","","","","","","   ","","","","","","","","","","","","","","","","","","",""," Figure 4: Formulae for alpha-beta probabilities, and the count derivation","M. Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania.","A. Dempster, N. Laird, and D. Rubin. 1977. Maximum likelihood from incomplete data via the em algorithm. Royal Statistical Society Series B, 39.","M. Franz, J. McCarley, and R. Ward. 1999. Ad hoc, cross-language and spoken document information retrieval at IBM. In TREC-8.","D. Jones and R. Havrilla. 1998. Twisted pair grammar: Support for rapid development of machine translation for low density languages. In AMTA98.","I. Melamed. 2000. Models of translational equivalence among words. Computational Linguistics, 26(2).","F. Och and H. Ney. 2000. Improved statistical alignment models. In ACL-2000.","F. Och, C. Tillmann, and H. Ney. 1999. Improved alignment models for statistical machine translation. In EMNLP-99.","P. Resnik and I. Melamed. 1997. Semi-automatic acquisition of domain-specific translation lexicons. In ANLP-97.","Y. Wang. 1998. Grammar Inference and Statistical Machine Translation. Ph.D. thesis, Carnegie Mellon University.","D. Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3)."]}],"references":[{"authors":[{"first":"H.","last":"Alshawi"},{"first":"S.","last":"Bangalore"},{"first":"S.","last":"Douglas"}],"year":"2000","title":"Learning dependency translation models as collections of finite state head transducers","source":"H. Alshawi, S. Bangalore, and S. Douglas. 2000. Learning dependency translation models as collections of finite state head transducers. Computational Linguistics, 26(1)."},{"authors":[{"first":"J.","last":"Baker"}],"year":"1979","title":"Trainable grammars for speech recognition","source":"J. Baker. 1979. Trainable grammars for speech recognition. In Speech Communication Papers for the 97th Meeting of the Acoustical Sciety of America."},{"authors":[{"first":"A.","last":"Berger"},{"first":"P.","last":"Brown"},{"first":"S.","middle":"Della","last":"Pietra"},{"first":"V.","middle":"Della","last":"Pietra"},{"first":"J.","last":"Gillett"},{"first":"J.","last":"Lafferty"},{"first":"R.","last":"Mercer"},{"first":"H.","last":"Printz"},{"first":"L.","last":"Ures"}],"year":"1996","title":"Language Translation Apparatus and Method Using Context-Based Translation Models","source":"A. Berger, P. Brown, S. Della Pietra, V. Della Pietra, J. Gillett, J. Lafferty, R. Mercer, H. Printz, and L. Ures. 1996. Language Translation Apparatus and Method Using Context-Based Translation Models. U.S. Patent 5,510,981."},{"authors":[{"first":"E.","last":"Brill"}],"year":"1995","title":"Transformation-based error-driven learning and natural language processing: A case study in part of specch tagging","source":"E. Brill. 1995. Transformation-based error-driven learning and natural language processing: A case study in part of specch tagging. Computational Linguistics, 21(4)."},{"authors":[{"first":"P.","last":"Brown"},{"first":"J.","last":"Cocke"},{"first":"S.","middle":"Della","last":"Pietra"},{"first":"F.","last":"Jelinek"},{"first":"R.","last":"Mercer"},{"first":"P.","last":"Roossin"}],"year":"1988","title":"A statistical approach to language translation","source":"P. Brown, J. Cocke, S. Della Pietra, F. Jelinek, R. Mercer, and P. Roossin. 1988. A statistical approach to language translation. In COLING-88."},{"authors":[{"first":"P.","last":"Brown"},{"first":"J.","last":"Cocke"},{"first":"S.","middle":"Della","last":"Pietra"},{"first":"F.","last":"Jelinek"},{"first":"R.","last":"Mercer"},{"first":"P.","last":"Roossin"}],"year":"1991","title":"Word-sense disambiguation using statistical methods","source":"P. Brown, J. Cocke, S. Della Pietra, F. Jelinek, R. Mercer, and P. Roossin. 1991. Word-sense disambiguation using statistical methods. In ACL-91."},{"authors":[{"first":"P.","last":"Brown"},{"first":"S.","middle":"Della","last":"Pietra"},{"first":"V.","middle":"Della","last":"Pietra"},{"first":"R.","last":"Mercer"}],"year":"1993","title":"The mathematics of statistical machine translation: Parameter estimation","source":"P. Brown, S. Della Pietra, V. Della Pietra, and R. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2)."},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":" "},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":"  "},{"authors":[],"source":""},{"authors":[],"source":" "},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":"   "},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":"            "},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":" "},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":" "},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":"   "},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":"   "},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":"   "},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":" "},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":"  "},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":"    "},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":" "},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":"   "},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":" "},{"authors":[],"source":""},{"authors":[],"source":"  "},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":"  "},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":" "},{"authors":[],"source":"  "},{"authors":[],"source":"  "},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":"   "},{"authors":[],"source":""},{"authors":[],"source":" "},{"authors":[],"source":"  "},{"authors":[],"source":"   "},{"authors":[],"source":"    "},{"authors":[],"source":""},{"authors":[],"source":" "},{"authors":[],"source":"   "},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":"  "},{"authors":[],"source":"  "},{"authors":[],"source":"  "},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":"   "},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":" "},{"authors":[],"source":"      "},{"authors":[],"source":"    "},{"authors":[],"source":""},{"authors":[],"source":" "},{"authors":[],"source":"   "},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":"  "},{"authors":[],"source":"  "},{"authors":[],"source":"  "},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":"   "},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":""},{"authors":[],"source":" Figure 4: Formulae for alpha-beta probabilities, and the count derivation"},{"authors":[{"first":"M.","last":"Collins"}],"year":"1999","title":"Head-Driven Statistical Models for Natural Language Parsing","source":"M. Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania."},{"authors":[{"first":"A.","last":"Dempster"},{"first":"N.","last":"Laird"},{"first":"D.","last":"Rubin"}],"year":"1977","title":"Maximum likelihood from incomplete data via the em algorithm","source":"A. Dempster, N. Laird, and D. Rubin. 1977. Maximum likelihood from incomplete data via the em algorithm. Royal Statistical Society Series B, 39."},{"authors":[{"first":"M.","last":"Franz"},{"first":"J.","last":"McCarley"},{"first":"R.","last":"Ward"}],"year":"1999","title":"Ad hoc, cross-language and spoken document information retrieval at IBM","source":"M. Franz, J. McCarley, and R. Ward. 1999. Ad hoc, cross-language and spoken document information retrieval at IBM. In TREC-8."},{"authors":[{"first":"D.","last":"Jones"},{"first":"R.","last":"Havrilla"}],"year":"1998","title":"Twisted pair grammar: Support for rapid development of machine translation for low density languages","source":"D. Jones and R. Havrilla. 1998. Twisted pair grammar: Support for rapid development of machine translation for low density languages. In AMTA98."},{"authors":[{"first":"I.","last":"Melamed"}],"year":"2000","title":"Models of translational equivalence among words","source":"I. Melamed. 2000. Models of translational equivalence among words. Computational Linguistics, 26(2)."},{"authors":[{"first":"F.","last":"Och"},{"first":"H.","last":"Ney"}],"year":"2000","title":"Improved statistical alignment models","source":"F. Och and H. Ney. 2000. Improved statistical alignment models. In ACL-2000."},{"authors":[{"first":"F.","last":"Och"},{"first":"C.","last":"Tillmann"},{"first":"H.","last":"Ney"}],"year":"1999","title":"Improved alignment models for statistical machine translation","source":"F. Och, C. Tillmann, and H. Ney. 1999. Improved alignment models for statistical machine translation. In EMNLP-99."},{"authors":[{"first":"P.","last":"Resnik"},{"first":"I.","last":"Melamed"}],"year":"1997","title":"Semi-automatic acquisition of domain-specific translation lexicons","source":"P. Resnik and I. Melamed. 1997. Semi-automatic acquisition of domain-specific translation lexicons. In ANLP-97."},{"authors":[{"first":"Y.","last":"Wang"}],"year":"1998","title":"Grammar Inference and Statistical Machine Translation","source":"Y. Wang. 1998. Grammar Inference and Statistical Machine Translation. Ph.D. thesis, Carnegie Mellon University."},{"authors":[{"first":"D.","last":"Wu"}],"year":"1997","title":"Stochastic inversion transduction grammars and bilingual parsing of parallel corpora","source":"D. Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3)."}],"cites":[{"style":0,"text":"Berger et al., 1996","origin":{"pointer":"/sections/1/paragraphs/0","offset":276,"length":19},"authors":[{"last":"Berger"},{"last":"al."}],"year":"1996","references":["/references/2"]},{"style":0,"text":"Melamed, 2000","origin":{"pointer":"/sections/1/paragraphs/0","offset":338,"length":13},"authors":[{"last":"Melamed"}],"year":"2000","references":["/references/202"]},{"style":0,"text":"Franz et al., 1999","origin":{"pointer":"/sections/1/paragraphs/0","offset":387,"length":18},"authors":[{"last":"Franz"},{"last":"al."}],"year":"1999","references":["/references/200"]},{"style":0,"text":"Resnik and Melamed, 1997","origin":{"pointer":"/sections/1/paragraphs/0","offset":443,"length":24},"authors":[{"last":"Resnik"},{"last":"Melamed"}],"year":"1997","references":["/references/205"]},{"style":0,"text":"Brown et al., 1991","origin":{"pointer":"/sections/1/paragraphs/0","offset":531,"length":18},"authors":[{"last":"Brown"},{"last":"al."}],"year":"1991","references":["/references/5"]},{"style":0,"text":"Brown et al., 1988","origin":{"pointer":"/sections/1/paragraphs/1","offset":61,"length":18},"authors":[{"last":"Brown"},{"last":"al."}],"year":"1988","references":["/references/4"]},{"style":0,"text":"Brown et al., 1993","origin":{"pointer":"/sections/1/paragraphs/1","offset":562,"length":18},"authors":[{"last":"Brown"},{"last":"al."}],"year":"1993","references":["/references/6"]},{"style":0,"text":"Wang (1998)","origin":{"pointer":"/sections/1/paragraphs/5","offset":0,"length":11},"authors":[{"last":"Wang"}],"year":"1998","references":["/references/206"]},{"style":0,"text":"Och et al. (1999)","origin":{"pointer":"/sections/1/paragraphs/5","offset":65,"length":17},"authors":[{"last":"Och"},{"last":"al."}],"year":"1999","references":["/references/204"]},{"style":0,"text":"Wu (1997)","origin":{"pointer":"/sections/1/paragraphs/30","offset":0,"length":9},"authors":[{"last":"Wu"}],"year":"1997","references":["/references/207"]},{"style":0,"text":"Alshawi et al. (2000)","origin":{"pointer":"/sections/1/paragraphs/30","offset":14,"length":21},"authors":[{"last":"Alshawi"},{"last":"al."}],"year":"2000","references":["/references/0"]},{"style":0,"text":"Jones and Havrilla, 1998","origin":{"pointer":"/sections/1/paragraphs/30","offset":379,"length":24},"authors":[{"last":"Jones"},{"last":"Havrilla"}],"year":"1998","references":["/references/201"]},{"style":0,"text":"Brown et al., 1993","origin":{"pointer":"/sections/1/paragraphs/31","offset":11,"length":18},"authors":[{"last":"Brown"},{"last":"al."}],"year":"1993","references":["/references/6"]},{"style":0,"text":"Brown et al. (1993)","origin":{"pointer":"/sections/2/paragraphs/7","offset":154,"length":19},"authors":[{"last":"Brown"},{"last":"al."}],"year":"1993","references":["/references/6"]},{"style":0,"text":"Berger et al., 1996","origin":{"pointer":"/sections/2/paragraphs/188","offset":152,"length":19},"authors":[{"last":"Berger"},{"last":"al."}],"year":"1996","references":["/references/2"]},{"style":0,"text":"Dempster et al., 1977","origin":{"pointer":"/sections/2/paragraphs/328","offset":127,"length":21},"authors":[{"last":"Dempster"},{"last":"al."}],"year":"1977","references":["/references/199"]},{"style":0,"text":"Brill, 1995","origin":{"pointer":"/sections/3/paragraphs/1","offset":37,"length":11},"authors":[{"last":"Brill"}],"year":"1995","references":["/references/3"]},{"style":0,"text":"Collins, 1999","origin":{"pointer":"/sections/3/paragraphs/1","offset":71,"length":13},"authors":[{"last":"Collins"}],"year":"1999","references":["/references/198"]},{"style":0,"text":"Och and Ney, 2000","origin":{"pointer":"/sections/3/paragraphs/5","offset":153,"length":17},"authors":[{"last":"Och"},{"last":"Ney"}],"year":"2000","references":["/references/203"]},{"style":0,"text":"Brown et al., 1993","origin":{"pointer":"/sections/5/paragraphs/21","offset":19,"length":18},"authors":[{"last":"Brown"},{"last":"al."}],"year":"1993","references":["/references/6"]},{"style":0,"text":"Baker, 1979","origin":{"pointer":"/sections/5/paragraphs/120","offset":182,"length":11},"authors":[{"last":"Baker"}],"year":"1979","references":["/references/1"]}]}
