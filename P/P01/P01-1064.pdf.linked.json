{"sections":[{"title":"A Statistical Model for Domain-Independent Text Segmentation Masao Utiyama and Hitoshi Isahara Communications Research Laboratory 2-2-2 Hikaridai Seika-cho, Soraku-gun, Kyoto, 619-0289 Japan mutiyama@crl.go.jp and isahara@crl.go.jp Abstract","paragraphs":["We propose a statistical method that finds the maximum-probability segmentation of a given text. This method does not require training data because it estimates probabilities from the given text. Therefore, it can be applied to any text in any domain. An experi-ment showed that the method is more accurate than or at least as accurate as a state-of-the-art text segmentation system."]},{"title":"1 Introduction","paragraphs":["Documents usually include various topics. Identifying and isolating topics by dividing documents, which is called text segmentation, is important for many natural language processing tasks, in-cluding information retrieval (Hearst and Plaunt, 1993; Salton et al., 1996) and summarization (Kan et al., 1998; Nakao, 2000). In information retrieval, users are often interested in particular topics (parts) of retrieved documents, in-stead of the documents themselves. To meet such needs, documents should be segmented into coherent topics. Summarization is often used for a long document that includes multiple topics. A summary of such a document can be composed of summaries of the component topics. Identification of topics is the task of text segmentation.","A lot of research has been done on text segmentation (Kozima, 1993; Hearst, 1994; Okumura and Honda, 1994; Salton et al., 1996; Yaari, 1997; Kan et al., 1998; Choi, 2000; Nakao, 2000). A major characteristic of the methods used in this research is that they do not require training data to segment given texts. Hearst (1994), for example, used only the similarity of word distributions in a given text to segment the text. Consequently, these methods can be applied to any text in any domain, even if training data do not exist. This property is important when text segmentation is applied to information retrieval or summarization, because both tasks deal with domain-independent documents.","Another application of text segmentation is the segmentation of a continuous broadcast news story into individual stories (Allan et al., 1998). In this application, systems relying on supervised learning (Yamron et al., 1998; Beeferman et al., 1999) achieve good performance because there are plenty of training data in the domain. These systems, however, can not be applied to domains for which no training data exist.","The text segmentation algorithm described in this paper is intended to be applied to the summarization of documents or speeches. Therefore, it should be able to handle domain-independent texts. The algorithm thus does not use any training data. It requires only the given documents for segmentation. It can, however, incorporate training data when they are available, as discussed in Section 5.","The algorithm selects the optimum segmentation in terms of the probability defined by a statistical model. This is a new approach for domain-independent text segmentation. Previous approaches usually used lexical cohesion to segment texts into topics. Kozima (1993), for example, used cohesion based on the spreading activa-tion on a semantic network. Hearst (1994) used the similarity of word distributions as measured by the cosine to gauge cohesion. Reynar (1994) used word repetition as a measure of cohesion. Choi (2000) used the rank of the cosine, rather than the cosine itself, to measure the similarity of sentences.","The statistical model for the algorithm is described in Section 2, and the algorithm for obtaining the maximum-probability segmentation is described in Section 3. Experimental results are presented in Section 4. Further discussion and our conclusions are given in Sections 5 and 6, respectively."]},{"title":"2 Statistical Model for Text Segmentation","paragraphs":["We first define the probability of a segmentation of a given text in this section. In the next section, we then describe the algorithm for selecting the most likely segmentation.","Let","  be a text consisting of"," words, and let    "," be a segmentation of ","consisting of","segments. Then the","probability of the segmentation","is defined by:            (1)","The most likely segmentation ","is given by:           (2) because  is a constant for a given text  . The definitions of     and    are given below, in that order. 2.1 Definition of     We define a topic by the distribution of words in that topic. We assume that different topics have different word distributions. We further assume that different topics are statistically independent of each other. We also assume that the words within the scope of a topic are statistically independent of each other given the topic.","Let","be the number of words in segment","",", and let","","","","be the","-th word in","",". If we define"," as","","","","    ","","",""," then ","","and","  ","",""," hold.","This means that  and"," correspond to each","other. Under our assumptions,     can be decomposed as follows:   ","","  ","","","     "," ","      "," ","       ","     ","    ",""," (3) Next, we define","    ",""," as:","    ","","","         (4)","where     is the number of words in","  that are the same as","   and is the number of different words in  . For example, if  , where  and  , then  ,   ,  ,  , and ",". Equation (4) is known as Laplace’s law (Manning and Schütze, 1999). ","     can be defined as: ","","","","","","","","   ","","","","",""," (5) for","","","","   "," ","","","","    ","   ","",""," (6) where","  ","    when"," "," and"," "," are the same word and","  ","   otherwise. For example,           ",". Equations (5) and (6) are used in Section 3 to describe the algorithm for finding the maximum-probability segmentation. 2.2 Definition of    The definition of   ","can vary depending on our prior information about the possibility of segmentation",". For example, we might know the average length of the segments and want to incorporate it into    . Our assumption, however, is that we do not have such prior information. Thus, we have to use some uninformative prior probability. We define    as  "," "," (7) Equation (7) is determined on the basis of its description length,1","   ; i.e.,  ","  (8)","where   ","    bits.2","This description","length is derived as follows: Suppose that there are two people, a sender and a receiver, both of whom know the text to be segmented. Only the sender knows the exact segmentation, and he/she should send a message so that the receiver can segment the text correctly. To this end, it is sufficient for the sender to send ","integers, i.e.,",", because these integers represent the lengths of segments and thus uniquely determine the segmentation once the text is known. A segment length  can be encoded using bits, because  is a number between 1 and",".","The total description length for all the segment","lengths is thus ","bits.3 Generally speaking,   ","takes a large value when the number of segments is small. On the other hand,    ","takes a large value when the number of segments is large. If only    ","is used to segment the text, then the resulting segmentation will have too many segments. By using both    and     , we can get a reasonable number of segments."]},{"title":"3 Algorithm for Finding the Maximum-Probability Segmentation","paragraphs":["To find the maximum-probability segmentation   , we first define the cost of segmentation","as","","          ","(9) 1 Stolcke and Omohundro uses description length priors","to induce the structure of hidden Markov models (Stolcke","and Omohundro, 1994). 2 ‘Log’ denotes the logarithm to the base 2. 3 We have used","","","as","before. But we use",""," in this paper, because it is easily interpreted as a","description length and the experimental results obtained by","using ","are slightly better than those obtained by us-","ing  ",". An anonymous reviewer suggests using a Pois-","son distribution whose parameter is","",", the average length","of a segment (in words), as prior probability. We leave it","for future work to compare the suitability of various prior","probabilities for text segmentation. and we then minimize","   ","to obtain  , because         ","  ","    (10)","    can be decomposed as follows:","  ","         ","  ","  ","   ","","    ","","     ","  ","  ","   ",""," ","       ","     "," ","","","    ","","    (11) where","","","    ","","     ","   ","","  "," ","           (12)","We further rewrite Equation (12) in the form","of Equation (13) below by using Equation (5)","and replacing","","with   ","","","","",""," , where  ","is the length of words, i.e.,the number of word tokens in words. Equation (13) is used to describe our algorithm in Section 3.1:","","","    ","","   ","             ","  ","",""," ","","","","   ","","","","   ","","","","   ","","","       (13) 3.1 Algorithm This section describes an algorithm for finding the minimum-cost segmentation. First, we define the terms and symbols used to describe the algorithm.","Given a text"," ","consisting of","","words, we define  as the position between","","and"," ","",", so that is just before","  and  is just after ",". Next, we define a graph  , where  is a set of nodes and  is a set of edges.  is defined as    "," (14) and  is defined as","     "," "," (15) where the edges are ordered; the initial vertex and the terminal vertex of   ","are","","and","",", respec-","tively. An example of","is shown in Figure 1.","We say that    covers","","  ",""," . This means that    represents a segment","","  ",""," . Thus, we define the cost    of edge    by using Equation (13):","","",""," ","","","","    (16) where  is the number of different words in ",". Given these definitions, we describe the algorithm to find the minimum-cost segmentation or maximum-probability segmentation as follows: Step 1. Calculate the cost    of edge    for   ","  by using Equation (16).","Step 2. Find the minimum-cost path from","","to","  . Algorithms for finding the minimum-cost path in a graph are well known. An algorithm that can provide a solution for Step 2 will be a simpler version of the algorithm used to find the maximum-probability solution in Japanese morphological analysis (Nagata, 1994). Therefore, a solution can be obtained by applying a dynamic programming (DP) algorithm.4","DP algorithms have also been used for text segmentation by other researchers (Ponte and Croft, 1997; Heinonen, 1998).","The path thus obtained represents the minimum-cost segmentation in","when edges correspond with segments. In Figure 1, for example, if    is the minimum-cost path, then      is the minimum-cost segmentation.","The algorithm automatically determines the number of segments. But the number of segments can also be specified explicitly by specifying the number of edges in the minimum-cost path.","The algorithm allows the text to be segmented anywhere between words; i.e., all the positions","4","A program that implements the algorithm described in this section is available at http: //www.crl.go.jp/jt/a132/members/mutiyama /softwares.html. between words are candidates for segment boundaries. It is easy, however, to modify the algorithm so that the text can only be segmented at particular positions, such as the ends of sentences or paragraphs. This is done by using a subset of  in Equation (15). We use only the edges whose initial and terminal vertices are candidate boundaries that meet particular conditions, such as being the ends of sentences or paragraphs. We then obtain the minimum-cost path by doing Steps 1 and 2. The minimum-cost segmentation thus obtained meets the boundary conditions. In this paper, we assume that the segment boundaries are at the ends of sentences. 3.2 Properties of the segmentation Generally speaking, the number of segments obtained by our algorithm is not sensitive to the length of a given text, which is counted in words. In other words, the number of segments is relatively stable with respect to variation in the text length. For example, the algorithm divides a newspaper editorial consisting of about 27 sentences into 4 to 6 segments, while on the other hand, it divides a long text consisting of over 1000 sentences into 10 to 20 segments. Thus, the number of segments is not proportional to text length. This is due to the term","  ","in Equation (11). The value of this term increases as the number of words increases. The term thus suppresses the division of a text when the length of the text is long.","This stability is desirable for summarization, because summarizing a given text requires selecting a relatively small number of topics from it. If a text segmentation system divides a given text into a relatively small number of segments, then a summary of the original text can be composed by combining summaries of the component segments (Kan et al., 1998; Nakao, 2000). A finer segmentation can be obtained by applying our algorithm recursively to each segment, if necessary.5","5","We segmented various texts without rigorous evaluation and found that our method is good at segmenting a text into a relatively small number of segments. On the other hand, the method is not good at segmenting a text into a large number of segments. For example, the method is good at segmenting a 1000-sentence text into 10 segments. In such a case, the segment boundaries seem to correspond well with topic boundaries. But, if the method is forced to segment the same text into 50 segments by specifying the number of g0 g2 g3 g4 g5g1w1 w2 w3 w4 w5 e01 e14 e35 e13 e45 Figure 1: Example of a graph."]},{"title":"4 Experiments 4.1 Material","paragraphs":["We used publicly available data to evaluate our system. This data was used by Choi (2000) to compare various domain-independent text segmentation systems.6","He evaluated","(Choi, 2000), TextTiling (Hearst, 1994), DotPlot (Reynar, 1998), and Segmenter (Kan et al., 1998) by using the data and reported that ","achieved the best performance among these systems.","The data description is as follows: “An artificial test corpus of 700 samples is used to assess the accuracy and speed performance of segmentation algorithms. A sample is a concatenation of ten text segments. A segment is the first","sentences of a randomly selected document from the Brown corpus. A sample is characterised by the range",".” (Choi, 2000) Table 1 gives the corpus statistics.","Range of","","","","","# samples 400 100 100 100 Table 1: Test corpus statistics. (Choi, 2000)","Segmentation accuracy was measured by the","probabilistic error metric","","proposed by Beefer-","man, et al. (1999).7","Low","","indicates high accu-edges in the minimum-cost path, then the resulting segmentation often contains very small segments consisting of only one or two sentences. We found empirically that segments obtained by recursive segmentation were better than those obtained by minimum-cost segmentation when the specified number of segments was somewhat larger than that of the minimum-cost path, whose number of segments was automatically determined by the algorithm.","6","The data is available from http://www.cs.man.ac.uk/c̃hoif/software/ C99-1.2-release.tgz. We used naacl00Exp/data/","1,2,3","/  3-11,3-5,6-8,9-11","/*, which is contained in the package, for our experiment.","7","Let","be a correct segmentation and let","be a segmentation proposed by a text segmentation system: Then the racy. 4.2 Experimental procedure and results The sample texts were preprocessed – i.e., punctuation and stop words were removed and the remaining words were stemmed – by a program using the libraries available in Choi’s package. The texts were then segmented by the systems listed in Tables 2 and 3. The segmentation boundaries were placed at the ends of sentences. The segmentations were evaluated by applying an evaluation program in Choi’s package.","The results are listed in Tables 2 and 3. ","is the result for our system when the numbers of segments were determined by the system.  ","","is","the result for our system when the numbers of seg-","ments were given beforehand.8 and    are the corresponding results for the systems described in Choi’s paper (Choi, 2000).9"," ","","","Total","","11% 13%","6%","6%","10%","","","13% 18% 10% 10% 13%","prob 7.9E-5 4.9E-3 2.5E-5 7.5E-8 9.7E-12","Table 2: Comparison of","",": the numbers of seg-","ments were determined by the systems.","In these tables, the symbol “","” indicates that","the difference in","","between the two systems is","statistically significant at the 1% level, based on “number","is the probability that a randomly chosen pair of words a distance of","words apart is inconsistently classified; that is, for one of the segmentations the pair lies in the same segment, while for the other the pair spans a segment boundary” (Beeferman et al., 1999), where","is chosen to be half the average reference segment length (in words).","8","If two segmentations have the same cost, then our systems arbitrarily select one of them; i.e., the systems select the segmentation processed previously.","9","The results for","","in Table 3 are slightly different from those listed in Table 6 of Choi’s paper (Choi, 2000). This is because the original results in that paper were based on 500 samples, while the results in our Table 3 were based on 700 samples (Choi, personal communication). ","","","","Total",""," 10%","9% 7%","5%","9%",""," 12% 11% 10% 9% 11%","prob 2.7E-4 0.080 2.3E-3 1.0E-4 6.8E-9","Table 3: Comparision of","",": the numbers of seg-","ments were given beforehand. a one-sided","-test of the null hypothesis of equal means. The probability of the null hypothesis being true is displayed in the row indicated by “prob”. The column labels, such as “ ","”, in-","dicate that the numbers in the column are the av-","erages of","","over the corresponding sample texts.","“Total” indicates the averages of","","over all the","text samples. These tables show statistically that our system","is more accurate than or at least as accurate as",". This means that our system is more accurate than or at least as accurate as previous domain-independent text segmentation systems, because","has been shown to be more accurate than previous domain-independent text segmentation systems.10"]},{"title":"5 Discussion 5.1 Evaluation","paragraphs":["Evaluation of the output of text segmentation systems is difficult because the required segmentations depend on the application. In this paper, we have used an artificial corpus to evaluate our system. We regard this as appropriate for comparing relative performance among systems.","It is important, however, to assess the performance of systems by using real texts. These texts should be domain independent. They should also be multi-lingual if we want to test the mul-","10","Speed performance is not our main concern in this paper. Our implementations of  and  are not optimum. However,  and  , which are implemented in C, run as fast as "," and   ",", which are implemented in Java (Choi, 2000), due to the difference in programming languages. The average run times for a sample text were"," "," sec.     ","sec. "," sec.     ","sec. on a Pentium III 750-MHz PC with 384-MB RAM running RedHat Linux 6.2. tilinguality of systems. For English, Klavans, et al. describe a segmentation corpus in which the texts were segmented by humans (Klavans et al., 1998). But, there are no such corpora for other languages. We are planning to build a segmentation corpus for Japanese, based on a corpus of speech transcriptions (Maekawa and Koiso, 2000). 5.2 Related work Our proposed algorithm finds the maximum-probability segmentation of a given text. This is a new approach for domain-independent text segmentation. A probabilistic approach, however, has already been proposed by Yamron, et al. for domain-dependent text segmentation (broadcast news story segmentation) (Yamron et al., 1998). They trained a hidden Markov model (HMM), whose states correspond to topics. Given a word sequence, their system assigns each word a topic so that the maximum-probability topic sequence is obtained. Their model is basically the same as that used for HMM part-of-speech (POS) taggers (Manning and Schütze, 1999), if we regard topics as POS tags.11","Finding topic boundaries is equivalent to finding topic transitions; i.e., a continuous topic or segment is a sequence of words with the same topic.","Their approach is indirect compared with our approach, which directly finds the maximum-probability segmentation. As a result, their model can not straightforwardly incorporate features pertaining to a segment itself, such as the average length of segments. Our model, on the other hand, can incorporate this information quite naturally. Suppose that the length of a segment","follows a normal distribution    , with a mean of"," and standard deviation of ","(Ponte and Croft, 1997). Then Equation (13) can be augmented to","","","    ","","     ","              ","  ","",""," ","","","","   ","","","","   ","","","","   ","","","             ","","","","   ","","","     (17) 11 The details are different, though. where ",". Equation (17) favors segments whose lengths are similar to the average length (in words).","Another major difference from their algorithm is that our algorithm does not require training data to estimate probabilities, while their algorithm does. Therefore, our algorithm can be applied to domain-independent texts, while their algorithm is restricted to domains for which training data are available. It would be interesting, however, to compare our algorithm with their algorithm for the case when training data are available. In such a case, our model should be extended to incorporate various features such as the average segment length, clue words, named entities, and so on (Reynar, 1999; Beeferman et al., 1999).","Our proposed algorithm naturally estimates the probabilities of words in segments. These probabilities, which are called word densities, have been used to detect important descriptions of words in texts (Kurohashi et al., 1997). This method is based on the assumption that the density of a word is high in a segment in which the word is discussed (defined and/or explained) in some depth. It would be interesting to apply our method to this application."]},{"title":"6 Conclusion","paragraphs":["We have proposed a statistical model for domain-independent text segmentation. This method finds the maximum-probability segmentation of a given text. The method has been shown to be more accurate than or at least as accurate as previous methods. We are planning to build a segmentation corpus for Japanese and evaluate our method against this corpus."]},{"title":"Acknowledgements","paragraphs":["We thank Freddy Y. Y. Choi for his text segmentation package."]},{"title":"References","paragraphs":["James Allan, Jaime Carbonell, George Doddington, Jonathan Yamron, and Yiming Yang. 1998. Topic detection and tracking pilot study final report. In Proc. of the DARPA Broadcast News Transcription and Understanding Workshop.","Doug Beeferman, Adam Berger, and John Lafferty. 1999. Statistical models for text segmentation. Machine Learning, 34(1-3):177–210.","Freddy Y. Y. Choi. 2000. Advances in domain independent linear text segmentation. In Proc. of NAACL-2000.","Marti A. Hearst and Christian Plaunt. 1993. Subtopic structuring for full-length document access. In Proc. of the Sixteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 59–68.","Marti A. Hearst. 1994. Multi-paragraph segmentation of expository text. In Proc. of ACL’94.","Oskari Heinonen. 1998. Optimal multi-paragraph text segmentation by dynamic programming. In Proc. of COLING-ACL’98.","Min-Yen Kan, Judith L. Klavans, and Kathleen R. McKeown. 1998. Linear segmentation and segment significance. In Proc. of WVLC-6, pages 197– 205.","Judith L. Klavans, Kathleen R. McKeown, Min-Yen Kan, and Susan Lee. 1998. Resources for the evaluation of summarization techniques. In Proceedings of the 1st International Conference on Language Resources and Evaluation (LREC), pages 899–902.","Hideki Kozima. 1993. Text segmentation based on similarity between words. In Proc. of ACL’93.","Sadao Kurohashi, Nobuyuki Shiraki, and Makoto Nagao. 1997. A method for detecting important descriptions of a word based on its density distribution in text (in Japanese). IPSJ (Information Processing Society of Japan) Journal, 38(4):845–854.","Kikuo Maekawa and Hanae Koiso. 2000. Design of spontaneous speech corpus for Japanese. In Proc of International Symposium: Toward the Realization of Spontaneous Speech Engineering, pages 70–77.","Christopher D. Manning and Hinrich Schütze. 1999. Foundations of Statistical Natural Language Processing. The MIT Press.","Masaaki Nagata. 1994. A stochastic Japanese morphological analyzer using a forward-DP backward-A","n-best search algorithm. In Proc. of COL-ING’94, pages 201–207.","Yoshio Nakao. 2000. An algorithm for one-page summarization of a long text based on thematic hierarchy detection. In Proc. of ACL’2000, pages 302– 309.","Manabu Okumura and Takeo Honda. 1994. Word sense disambiguation and text segmentation based on lexical cohesion. In Proc. of COLING-94.","Jay M. Ponte and W. Bruce Croft. 1997. Text segmentation by topic. In Proc. of the First European Conference on Research and Advanced Technology for Digital Libraries, pages 120–129.","Jeffrey C. Reynar. 1994. An automatic method of finding topic boundaries. In Proc. of ACL-94.","Jeffrey C. Reynar. 1998. Topic segmentation: Algorithms and applications. Ph.D. thesis, Computer and Information Science, University of Pennsylvania.","Jeffrey C. Reynar. 1999. Statistical models for topic segmentation. In Proc. of ACL-99, pages 357–364.","Gerard Salton, Amit Singhal, Chris Buckley, and Mandar Mitra. 1996. Automatic text decomposition using text segments and text themes. In Proc. of Hypertext’96.","Andreas Stolcke and Stephen M. Omohundro. 1994. Best-first model merging for hidden Markov model induction. Technical Report TR-94-003, ICSI, Berkeley, CA.","Yaakov Yaari. 1997. Segmentation of expository texts by hierarchical agglomerative clustering. In Proc. of the Recent Advances in Natural Language Processing.","J. P. Yamron, I. Carp, S. Lowe, and P. van Mulbregt. 1998. A hidden Markov model approach to text segmentation and event tracking. In Proc. of ICASSP-98."]}],"references":[{"authors":[{"first":"James","last":"Allan"},{"first":"Jaime","last":"Carbonell"},{"first":"George","last":"Doddington"},{"first":"Jonathan","last":"Yamron"},{"first":"Yiming","last":"Yang"}],"year":"1998","title":"Topic detection and tracking pilot study final report","source":"James Allan, Jaime Carbonell, George Doddington, Jonathan Yamron, and Yiming Yang. 1998. Topic detection and tracking pilot study final report. In Proc. of the DARPA Broadcast News Transcription and Understanding Workshop."},{"authors":[{"first":"Doug","last":"Beeferman"},{"first":"Adam","last":"Berger"},{"first":"John","last":"Lafferty"}],"year":"1999","title":"Statistical models for text segmentation","source":"Doug Beeferman, Adam Berger, and John Lafferty. 1999. Statistical models for text segmentation. Machine Learning, 34(1-3):177–210."},{"authors":[{"first":"Freddy","middle":"Y. Y.","last":"Choi"}],"year":"2000","title":"Advances in domain independent linear text segmentation","source":"Freddy Y. Y. Choi. 2000. Advances in domain independent linear text segmentation. In Proc. of NAACL-2000."},{"authors":[{"first":"Marti","middle":"A.","last":"Hearst"},{"first":"Christian","last":"Plaunt"}],"year":"1993","title":"Subtopic structuring for full-length document access","source":"Marti A. Hearst and Christian Plaunt. 1993. Subtopic structuring for full-length document access. In Proc. of the Sixteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 59–68."},{"authors":[{"first":"Marti","middle":"A.","last":"Hearst"}],"year":"1994","title":"Multi-paragraph segmentation of expository text","source":"Marti A. Hearst. 1994. Multi-paragraph segmentation of expository text. In Proc. of ACL’94."},{"authors":[{"first":"Oskari","last":"Heinonen"}],"year":"1998","title":"Optimal multi-paragraph text segmentation by dynamic programming","source":"Oskari Heinonen. 1998. Optimal multi-paragraph text segmentation by dynamic programming. In Proc. of COLING-ACL’98."},{"authors":[{"first":"Min-Yen","last":"Kan"},{"first":"Judith","middle":"L.","last":"Klavans"},{"first":"Kathleen","middle":"R.","last":"McKeown"}],"year":"1998","title":"Linear segmentation and segment significance","source":"Min-Yen Kan, Judith L. Klavans, and Kathleen R. McKeown. 1998. Linear segmentation and segment significance. In Proc. of WVLC-6, pages 197– 205."},{"authors":[{"first":"Judith","middle":"L.","last":"Klavans"},{"first":"Kathleen","middle":"R.","last":"McKeown"},{"first":"Min-Yen","last":"Kan"},{"first":"Susan","last":"Lee"}],"year":"1998","title":"Resources for the evaluation of summarization techniques","source":"Judith L. Klavans, Kathleen R. McKeown, Min-Yen Kan, and Susan Lee. 1998. Resources for the evaluation of summarization techniques. In Proceedings of the 1st International Conference on Language Resources and Evaluation (LREC), pages 899–902."},{"authors":[{"first":"Hideki","last":"Kozima"}],"year":"1993","title":"Text segmentation based on similarity between words","source":"Hideki Kozima. 1993. Text segmentation based on similarity between words. In Proc. of ACL’93."},{"authors":[{"first":"Sadao","last":"Kurohashi"},{"first":"Nobuyuki","last":"Shiraki"},{"first":"Makoto","last":"Nagao"}],"year":"1997","title":"A method for detecting important descriptions of a word based on its density distribution in text (in Japanese)","source":"Sadao Kurohashi, Nobuyuki Shiraki, and Makoto Nagao. 1997. A method for detecting important descriptions of a word based on its density distribution in text (in Japanese). IPSJ (Information Processing Society of Japan) Journal, 38(4):845–854."},{"authors":[{"first":"Kikuo","last":"Maekawa"},{"first":"Hanae","last":"Koiso"}],"year":"2000","title":"Design of spontaneous speech corpus for Japanese","source":"Kikuo Maekawa and Hanae Koiso. 2000. Design of spontaneous speech corpus for Japanese. In Proc of International Symposium: Toward the Realization of Spontaneous Speech Engineering, pages 70–77."},{"authors":[{"first":"Christopher","middle":"D.","last":"Manning"},{"first":"Hinrich","last":"Schütze"}],"year":"1999","title":"Foundations of Statistical Natural Language Processing","source":"Christopher D. Manning and Hinrich Schütze. 1999. Foundations of Statistical Natural Language Processing. The MIT Press."},{"authors":[],"source":"Masaaki Nagata. 1994. A stochastic Japanese morphological analyzer using a forward-DP backward-A"},{"authors":[],"source":"n-best search algorithm. In Proc. of COL-ING’94, pages 201–207."},{"authors":[{"first":"Yoshio","last":"Nakao"}],"year":"2000","title":"An algorithm for one-page summarization of a long text based on thematic hierarchy detection","source":"Yoshio Nakao. 2000. An algorithm for one-page summarization of a long text based on thematic hierarchy detection. In Proc. of ACL’2000, pages 302– 309."},{"authors":[{"first":"Manabu","last":"Okumura"},{"first":"Takeo","last":"Honda"}],"year":"1994","title":"Word sense disambiguation and text segmentation based on lexical cohesion","source":"Manabu Okumura and Takeo Honda. 1994. Word sense disambiguation and text segmentation based on lexical cohesion. In Proc. of COLING-94."},{"authors":[{"first":"Jay","middle":"M.","last":"Ponte"},{"first":"W.","middle":"Bruce","last":"Croft"}],"year":"1997","title":"Text segmentation by topic","source":"Jay M. Ponte and W. Bruce Croft. 1997. Text segmentation by topic. In Proc. of the First European Conference on Research and Advanced Technology for Digital Libraries, pages 120–129."},{"authors":[{"first":"Jeffrey","middle":"C.","last":"Reynar"}],"year":"1994","title":"An automatic method of finding topic boundaries","source":"Jeffrey C. Reynar. 1994. An automatic method of finding topic boundaries. In Proc. of ACL-94."},{"authors":[{"first":"Jeffrey","middle":"C.","last":"Reynar"}],"year":"1998","title":"Topic segmentation: Algorithms and applications","source":"Jeffrey C. Reynar. 1998. Topic segmentation: Algorithms and applications. Ph.D. thesis, Computer and Information Science, University of Pennsylvania."},{"authors":[{"first":"Jeffrey","middle":"C.","last":"Reynar"}],"year":"1999","title":"Statistical models for topic segmentation","source":"Jeffrey C. Reynar. 1999. Statistical models for topic segmentation. In Proc. of ACL-99, pages 357–364."},{"authors":[{"first":"Gerard","last":"Salton"},{"first":"Amit","last":"Singhal"},{"first":"Chris","last":"Buckley"},{"first":"Mandar","last":"Mitra"}],"year":"1996","title":"Automatic text decomposition using text segments and text themes","source":"Gerard Salton, Amit Singhal, Chris Buckley, and Mandar Mitra. 1996. Automatic text decomposition using text segments and text themes. In Proc. of Hypertext’96."},{"authors":[{"first":"Andreas","last":"Stolcke"},{"first":"Stephen","middle":"M.","last":"Omohundro"}],"year":"1994","title":"Best-first model merging for hidden Markov model induction","source":"Andreas Stolcke and Stephen M. Omohundro. 1994. Best-first model merging for hidden Markov model induction. Technical Report TR-94-003, ICSI, Berkeley, CA."},{"authors":[{"first":"Yaakov","last":"Yaari"}],"year":"1997","title":"Segmentation of expository texts by hierarchical agglomerative clustering","source":"Yaakov Yaari. 1997. Segmentation of expository texts by hierarchical agglomerative clustering. In Proc. of the Recent Advances in Natural Language Processing."},{"authors":[{"first":"J.","middle":"P.","last":"Yamron"},{"first":"I.","last":"Carp"},{"first":"S.","last":"Lowe"},{"first":"P.","last":"van Mulbregt"}],"year":"1998","title":"A hidden Markov model approach to text segmentation and event tracking","source":"J. P. Yamron, I. Carp, S. Lowe, and P. van Mulbregt. 1998. A hidden Markov model approach to text segmentation and event tracking. In Proc. of ICASSP-98."}],"cites":[{"style":0,"text":"Hearst and Plaunt, 1993","origin":{"pointer":"/sections/1/paragraphs/0","offset":224,"length":23},"authors":[{"last":"Hearst"},{"last":"Plaunt"}],"year":"1993","references":["/references/3"]},{"style":0,"text":"Salton et al., 1996","origin":{"pointer":"/sections/1/paragraphs/0","offset":249,"length":19},"authors":[{"last":"Salton"},{"last":"al."}],"year":"1996","references":["/references/20"]},{"style":0,"text":"Kan et al., 1998","origin":{"pointer":"/sections/1/paragraphs/0","offset":289,"length":16},"authors":[{"last":"Kan"},{"last":"al."}],"year":"1998","references":["/references/6"]},{"style":0,"text":"Nakao, 2000","origin":{"pointer":"/sections/1/paragraphs/0","offset":307,"length":11},"authors":[{"last":"Nakao"}],"year":"2000","references":["/references/14"]},{"style":0,"text":"Kozima, 1993","origin":{"pointer":"/sections/1/paragraphs/1","offset":54,"length":12},"authors":[{"last":"Kozima"}],"year":"1993","references":["/references/8"]},{"style":0,"text":"Hearst, 1994","origin":{"pointer":"/sections/1/paragraphs/1","offset":68,"length":12},"authors":[{"last":"Hearst"}],"year":"1994","references":["/references/4"]},{"style":0,"text":"Okumura and Honda, 1994","origin":{"pointer":"/sections/1/paragraphs/1","offset":82,"length":23},"authors":[{"last":"Okumura"},{"last":"Honda"}],"year":"1994","references":["/references/15"]},{"style":0,"text":"Salton et al., 1996","origin":{"pointer":"/sections/1/paragraphs/1","offset":107,"length":19},"authors":[{"last":"Salton"},{"last":"al."}],"year":"1996","references":["/references/20"]},{"style":0,"text":"Yaari, 1997","origin":{"pointer":"/sections/1/paragraphs/1","offset":128,"length":11},"authors":[{"last":"Yaari"}],"year":"1997","references":["/references/22"]},{"style":0,"text":"Kan et al., 1998","origin":{"pointer":"/sections/1/paragraphs/1","offset":141,"length":16},"authors":[{"last":"Kan"},{"last":"al."}],"year":"1998","references":["/references/6"]},{"style":0,"text":"Choi, 2000","origin":{"pointer":"/sections/1/paragraphs/1","offset":159,"length":10},"authors":[{"last":"Choi"}],"year":"2000","references":["/references/2"]},{"style":0,"text":"Nakao, 2000","origin":{"pointer":"/sections/1/paragraphs/1","offset":171,"length":11},"authors":[{"last":"Nakao"}],"year":"2000","references":["/references/14"]},{"style":0,"text":"Hearst (1994)","origin":{"pointer":"/sections/1/paragraphs/1","offset":311,"length":13},"authors":[{"last":"Hearst"}],"year":"1994","references":["/references/4"]},{"style":0,"text":"Allan et al., 1998","origin":{"pointer":"/sections/1/paragraphs/2","offset":123,"length":18},"authors":[{"last":"Allan"},{"last":"al."}],"year":"1998","references":["/references/0"]},{"style":0,"text":"Yamron et al., 1998","origin":{"pointer":"/sections/1/paragraphs/2","offset":205,"length":19},"authors":[{"last":"Yamron"},{"last":"al."}],"year":"1998","references":["/references/23"]},{"style":0,"text":"Beeferman et al., 1999","origin":{"pointer":"/sections/1/paragraphs/2","offset":226,"length":22},"authors":[{"last":"Beeferman"},{"last":"al."}],"year":"1999","references":["/references/1"]},{"style":0,"text":"Kozima (1993)","origin":{"pointer":"/sections/1/paragraphs/4","offset":252,"length":13},"authors":[{"last":"Kozima"}],"year":"1993","references":["/references/8"]},{"style":0,"text":"Hearst (1994)","origin":{"pointer":"/sections/1/paragraphs/4","offset":352,"length":13},"authors":[{"last":"Hearst"}],"year":"1994","references":["/references/4"]},{"style":0,"text":"Reynar (1994)","origin":{"pointer":"/sections/1/paragraphs/4","offset":453,"length":13},"authors":[{"last":"Reynar"}],"year":"1994","references":["/references/17"]},{"style":0,"text":"Choi (2000)","origin":{"pointer":"/sections/1/paragraphs/4","offset":514,"length":11},"authors":[{"last":"Choi"}],"year":"2000","references":["/references/2"]},{"style":0,"text":"Manning and Schütze, 1999","origin":{"pointer":"/sections/2/paragraphs/63","offset":42,"length":25},"authors":[{"last":"Manning"},{"last":"Schütze"}],"year":"1999","references":["/references/11"]},{"style":0,"text":"Omohundro, 1994","origin":{"pointer":"/sections/3/paragraphs/6","offset":4,"length":15},"authors":[{"last":"Omohundro"}],"year":"1994","references":[]},{"style":0,"text":"Nagata, 1994","origin":{"pointer":"/sections/3/paragraphs/143","offset":253,"length":12},"authors":[{"last":"Nagata"}],"year":"1994","references":[]},{"style":0,"text":"Ponte and Croft, 1997","origin":{"pointer":"/sections/3/paragraphs/144","offset":78,"length":21},"authors":[{"last":"Ponte"},{"last":"Croft"}],"year":"1997","references":["/references/16"]},{"style":0,"text":"Heinonen, 1998","origin":{"pointer":"/sections/3/paragraphs/144","offset":101,"length":14},"authors":[{"last":"Heinonen"}],"year":"1998","references":["/references/5"]},{"style":0,"text":"Kan et al., 1998","origin":{"pointer":"/sections/3/paragraphs/153","offset":341,"length":16},"authors":[{"last":"Kan"},{"last":"al."}],"year":"1998","references":["/references/6"]},{"style":0,"text":"Nakao, 2000","origin":{"pointer":"/sections/3/paragraphs/153","offset":359,"length":11},"authors":[{"last":"Nakao"}],"year":"2000","references":["/references/14"]},{"style":0,"text":"Choi (2000)","origin":{"pointer":"/sections/4/paragraphs/0","offset":78,"length":11},"authors":[{"last":"Choi"}],"year":"2000","references":["/references/2"]},{"style":0,"text":"Choi, 2000","origin":{"pointer":"/sections/4/paragraphs/2","offset":1,"length":10},"authors":[{"last":"Choi"}],"year":"2000","references":["/references/2"]},{"style":0,"text":"Hearst, 1994","origin":{"pointer":"/sections/4/paragraphs/2","offset":26,"length":12},"authors":[{"last":"Hearst"}],"year":"1994","references":["/references/4"]},{"style":0,"text":"Reynar, 1998","origin":{"pointer":"/sections/4/paragraphs/2","offset":50,"length":12},"authors":[{"last":"Reynar"}],"year":"1998","references":["/references/18"]},{"style":0,"text":"Kan et al., 1998","origin":{"pointer":"/sections/4/paragraphs/2","offset":80,"length":16},"authors":[{"last":"Kan"},{"last":"al."}],"year":"1998","references":["/references/6"]},{"style":0,"text":"Choi, 2000","origin":{"pointer":"/sections/4/paragraphs/6","offset":4,"length":10},"authors":[{"last":"Choi"}],"year":"2000","references":["/references/2"]},{"style":0,"text":"Choi, 2000","origin":{"pointer":"/sections/4/paragraphs/12","offset":60,"length":10},"authors":[{"last":"Choi"}],"year":"2000","references":["/references/2"]},{"style":0,"text":"Choi, 2000","origin":{"pointer":"/sections/4/paragraphs/35","offset":110,"length":10},"authors":[{"last":"Choi"}],"year":"2000","references":["/references/2"]},{"style":0,"text":"Beeferman et al., 1999","origin":{"pointer":"/sections/4/paragraphs/60","offset":171,"length":22},"authors":[{"last":"Beeferman"},{"last":"al."}],"year":"1999","references":["/references/1"]},{"style":0,"text":"Choi, 2000","origin":{"pointer":"/sections/4/paragraphs/67","offset":80,"length":10},"authors":[{"last":"Choi"}],"year":"2000","references":["/references/2"]},{"style":0,"text":"Choi, 2000","origin":{"pointer":"/sections/5/paragraphs/5","offset":33,"length":10},"authors":[{"last":"Choi"}],"year":"2000","references":["/references/2"]},{"style":0,"text":"Klavans et al., 1998","origin":{"pointer":"/sections/5/paragraphs/10","offset":205,"length":20},"authors":[{"last":"Klavans"},{"last":"al."}],"year":"1998","references":["/references/7"]},{"style":0,"text":"Maekawa and Koiso, 2000","origin":{"pointer":"/sections/5/paragraphs/10","offset":385,"length":23},"authors":[{"last":"Maekawa"},{"last":"Koiso"}],"year":"2000","references":["/references/10"]},{"style":0,"text":"Yamron et al., 1998","origin":{"pointer":"/sections/5/paragraphs/10","offset":731,"length":19},"authors":[{"last":"Yamron"},{"last":"al."}],"year":"1998","references":["/references/23"]},{"style":0,"text":"Manning and Schütze, 1999","origin":{"pointer":"/sections/5/paragraphs/10","offset":1037,"length":25},"authors":[{"last":"Manning"},{"last":"Schütze"}],"year":"1999","references":["/references/11"]},{"style":0,"text":"Ponte and Croft, 1997","origin":{"pointer":"/sections/5/paragraphs/15","offset":1,"length":21},"authors":[{"last":"Ponte"},{"last":"Croft"}],"year":"1997","references":["/references/16"]},{"style":0,"text":"Reynar, 1999","origin":{"pointer":"/sections/5/paragraphs/49","offset":588,"length":12},"authors":[{"last":"Reynar"}],"year":"1999","references":["/references/19"]},{"style":0,"text":"Beeferman et al., 1999","origin":{"pointer":"/sections/5/paragraphs/49","offset":602,"length":22},"authors":[{"last":"Beeferman"},{"last":"al."}],"year":"1999","references":["/references/1"]},{"style":0,"text":"Kurohashi et al., 1997","origin":{"pointer":"/sections/5/paragraphs/50","offset":204,"length":22},"authors":[{"last":"Kurohashi"},{"last":"al."}],"year":"1997","references":["/references/9"]}]}
